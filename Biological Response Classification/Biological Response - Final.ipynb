{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Load Data\n",
    "data = pd.read_csv('./Data/train.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (3751, 1777)\n"
     ]
    }
   ],
   "source": [
    "print('train: ' + str(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, counts = np.unique(data[:,0], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(['No Response', 'Positive Response'], counts/data.shape[0])\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "train_x, test_x, train_y, test_y = train_test_split(data[:,1:], data[:,0], test_size=0.2, stratify=data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "values, counts = np.unique(train_y, return_counts=True)\n",
    "\n",
    "plt.bar(['No Response', 'Positive Response'], counts/train_y.shape[0])\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to save results\n",
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename + '.json', 'w') as fp:\n",
    "        json.dump(results, fp)\n",
    "        \n",
    "def load_results(file_name):\n",
    "    with open(file_name) as f:\n",
    "        temp_results = json.load(f)\n",
    "    return temp_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a function to fit and evaluate DT at given alphas\n",
    "def evaluate_dtc(train_x, train_y, test_x, test_y, alphas, results):\n",
    "    \n",
    "    tested_alphas = [results[i][0] for i in list(results.keys())]\n",
    "    \n",
    "    i = len(list(results.keys()))\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        \n",
    "        # Skip if alpha has already been tested\n",
    "        if alpha in tested_alphas:\n",
    "            continue\n",
    "        \n",
    "        # Initialize model\n",
    "        model = DecisionTreeClassifier(ccp_alpha = alpha)\n",
    "        \n",
    "        # Fit model\n",
    "        model = model.fit(train_x, train_y)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        train_accuracy = model.score(train_x, train_y)\n",
    "        test_accuracy = model.score(test_x, test_y)\n",
    "        \n",
    "        # Save results\n",
    "        results[i] = (alpha, train_accuracy, test_accuracy)\n",
    "        \n",
    "        # Update i\n",
    "        i += 1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.7350199733688415\n"
     ]
    }
   ],
   "source": [
    "### Evaluate model without any pruning\n",
    "# Train model\n",
    "model = DecisionTreeClassifier()\n",
    "model = model.fit(train_x, train_y)\n",
    "\n",
    "# Evaluate performance\n",
    "train_accuracy = model.score(train_x, train_y)\n",
    "test_accuracy = model.score(test_x, test_y)\n",
    "\n",
    "print('Training accuracy: ' + str(train_accuracy))\n",
    "print('Test accuracy: ' + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Compute path for pruning\n",
    "path = model.cost_complexity_pruning_path(train_x, train_y)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max alpha: 0.11247197501181116\n",
      "Min alpha: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Max alpha: ' + str(np.max(ccp_alphas)))\n",
    "print('Min alpha: ' + str(np.min(ccp_alphas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-990f456d4956>:2: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha_list = [0+np.max(ccp_alphas)/i for i in range(6)]\n"
     ]
    }
   ],
   "source": [
    "### Run first search over alpha range\n",
    "alpha_list = [0+np.max(ccp_alphas)/i for i in range(6)]\n",
    "results_dtc = {}\n",
    "results_dtc = evaluate_dtc(train_x, train_y, test_x, test_y, alpha_list, results_dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x201defb0940>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_list = [results_dtc[i][0] for i in list(results_dtc.keys())]\n",
    "train_acc_list = [results_dtc[i][1] for i in list(results_dtc.keys())]\n",
    "test_acc_list = [results_dtc[i][2] for i in list(results_dtc.keys())]\n",
    "\n",
    "plt.plot(alpha_list, train_acc_list)\n",
    "plt.plot(alpha_list, test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized alpha: 0.05623598750590558\n",
      "Training accuracy: 73.87%\n",
      "Test accuracy: 72.97%\n"
     ]
    }
   ],
   "source": [
    "max_index = np.argmax(test_acc_list)\n",
    "\n",
    "print('Optimized alpha: ' + str(alpha_list[max_index]))\n",
    "print('Training accuracy: ' + str(round(train_acc_list[max_index]*100, 2)) + '%')\n",
    "print('Test accuracy: ' + str(round(test_acc_list[max_index]*100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save decision tree results\n",
    "save_results(results_dtc, 'results_dtc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dtc = load_results('results_dtc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_list = [results_dtc[i][0] for i in list(results_dtc.keys())]\n",
    "train_acc_list = [results_dtc[i][1]*100 for i in list(results_dtc.keys())]\n",
    "test_acc_list = [results_dtc[i][2]*100 for i in list(results_dtc.keys())]\n",
    "\n",
    "alpha_list, train_acc_list, test_acc_list = zip(*sorted(zip(alpha_list, train_acc_list, test_acc_list)))\n",
    "\n",
    "plt.scatter(alpha_list, train_acc_list, label='Training set')\n",
    "plt.scatter(alpha_list, test_acc_list, label='Test set')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Cost Complexity Pruning Threshold')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized alpha: 0.022494395002362232\n",
      "Training accuracy: 73.87%\n",
      "Test accuracy: 72.97%\n"
     ]
    }
   ],
   "source": [
    "max_index = np.argmax(test_acc_list)\n",
    "\n",
    "print('Optimized alpha: ' + str(alpha_list[max_index]))\n",
    "print('Training accuracy: ' + str(round(train_acc_list[max_index], 2)) + '%')\n",
    "print('Test accuracy: ' + str(round(test_acc_list[max_index], 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a function to fit and evaluate DT at given alphas\n",
    "def evaluate_nn(train_x, train_y, test_x, test_y, hyperparameter_dic, resutls_nn):\n",
    "    \n",
    "    layers = hyperparameter_dic['layers']\n",
    "    sizes = hyperparameter_dic['sizes']\n",
    "    activations = hyperparameter_dic['activations']\n",
    "    alphas = hyperparameter_dic['alphas']\n",
    "    \n",
    "    number_models = len(layers)*len(sizes)*len(activations)*len(alphas)\n",
    "    i = len(list(resutls_nn.keys()))\n",
    "    \n",
    "    for number_layers in layers:\n",
    "        for layer_size in sizes:\n",
    "            for activation in activations:\n",
    "                for alpha in alphas:\n",
    "                    \n",
    "                    for r_i in list(results_nn.keys()):\n",
    "                        if alpha == results_nn[r_i][0]:\n",
    "                            if layer_size == results_nn[r_i][4]:\n",
    "                                if number_layers == results_nn[r_i][3]:\n",
    "                                    if activation == results_nn[r_i][5]:\n",
    "                                        continue\n",
    "                        \n",
    "                    print('Training model {} of {}...'.format(i+1, number_models))\n",
    "                    \n",
    "                    # Initialize model\n",
    "                    model = MLPClassifier(\n",
    "                        hidden_layer_sizes = [layer_size]*number_layers,\n",
    "                        activation = activation,\n",
    "                        alpha = alpha,\n",
    "                        max_iter = 10000,\n",
    "                        learning_rate = 'invscaling',\n",
    "                        verbose = True\n",
    "                    )\n",
    "                                \n",
    "                    # Fit model\n",
    "                    model.fit(train_x, train_y)\n",
    "                                \n",
    "                    # Evaluate performance\n",
    "                    train_accuracy = model.score(train_x, train_y)\n",
    "                    test_accuracy = model.score(test_x, test_y)\n",
    "\n",
    "                    # Save resutls_nn\n",
    "                    resutls_nn[i] = (alpha, train_accuracy, test_accuracy, number_layers, layer_size, activation)\n",
    "\n",
    "                    # Update i\n",
    "                    i += 1\n",
    "\n",
    "    return resutls_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 48...\n",
      "Iteration 1, loss = 0.68049718\n",
      "Iteration 2, loss = 0.65500752\n",
      "Iteration 3, loss = 0.63657511\n",
      "Iteration 4, loss = 0.62164698\n",
      "Iteration 5, loss = 0.60770979\n",
      "Iteration 6, loss = 0.59518180\n",
      "Iteration 7, loss = 0.58358536\n",
      "Iteration 8, loss = 0.57264956\n",
      "Iteration 9, loss = 0.56264675\n",
      "Iteration 10, loss = 0.55270862\n",
      "Iteration 11, loss = 0.54332816\n",
      "Iteration 12, loss = 0.53422945\n",
      "Iteration 13, loss = 0.52542404\n",
      "Iteration 14, loss = 0.51818481\n",
      "Iteration 15, loss = 0.50970478\n",
      "Iteration 16, loss = 0.50283763\n",
      "Iteration 17, loss = 0.49520790\n",
      "Iteration 18, loss = 0.48943934\n",
      "Iteration 19, loss = 0.48310891\n",
      "Iteration 20, loss = 0.47617735\n",
      "Iteration 21, loss = 0.47072120\n",
      "Iteration 22, loss = 0.46489248\n",
      "Iteration 23, loss = 0.45990855\n",
      "Iteration 24, loss = 0.45527329\n",
      "Iteration 25, loss = 0.45051951\n",
      "Iteration 26, loss = 0.44697219\n",
      "Iteration 27, loss = 0.44083579\n",
      "Iteration 28, loss = 0.43632096\n",
      "Iteration 29, loss = 0.43278376\n",
      "Iteration 30, loss = 0.42925668\n",
      "Iteration 31, loss = 0.42527651\n",
      "Iteration 32, loss = 0.42183532\n",
      "Iteration 33, loss = 0.41827172\n",
      "Iteration 34, loss = 0.41512819\n",
      "Iteration 35, loss = 0.41210711\n",
      "Iteration 36, loss = 0.40929721\n",
      "Iteration 37, loss = 0.40573648\n",
      "Iteration 38, loss = 0.40312878\n",
      "Iteration 39, loss = 0.39984822\n",
      "Iteration 40, loss = 0.39772195\n",
      "Iteration 41, loss = 0.39522075\n",
      "Iteration 42, loss = 0.39531859\n",
      "Iteration 43, loss = 0.39009672\n",
      "Iteration 44, loss = 0.38834312\n",
      "Iteration 45, loss = 0.38559384\n",
      "Iteration 46, loss = 0.38480419\n",
      "Iteration 47, loss = 0.38230575\n",
      "Iteration 48, loss = 0.37972718\n",
      "Iteration 49, loss = 0.37847426\n",
      "Iteration 50, loss = 0.37564598\n",
      "Iteration 51, loss = 0.37436040\n",
      "Iteration 52, loss = 0.37319772\n",
      "Iteration 53, loss = 0.37025565\n",
      "Iteration 54, loss = 0.36934659\n",
      "Iteration 55, loss = 0.36782485\n",
      "Iteration 56, loss = 0.36587591\n",
      "Iteration 57, loss = 0.36423955\n",
      "Iteration 58, loss = 0.36234801\n",
      "Iteration 59, loss = 0.36359899\n",
      "Iteration 60, loss = 0.36060850\n",
      "Iteration 61, loss = 0.35876350\n",
      "Iteration 62, loss = 0.35726594\n",
      "Iteration 63, loss = 0.35581686\n",
      "Iteration 64, loss = 0.35387735\n",
      "Iteration 65, loss = 0.35257725\n",
      "Iteration 66, loss = 0.35105736\n",
      "Iteration 67, loss = 0.34995319\n",
      "Iteration 68, loss = 0.34782020\n",
      "Iteration 69, loss = 0.34771766\n",
      "Iteration 70, loss = 0.34642237\n",
      "Iteration 71, loss = 0.34446044\n",
      "Iteration 72, loss = 0.34291329\n",
      "Iteration 73, loss = 0.34200387\n",
      "Iteration 74, loss = 0.34082371\n",
      "Iteration 75, loss = 0.33958575\n",
      "Iteration 76, loss = 0.33902062\n",
      "Iteration 77, loss = 0.33754222\n",
      "Iteration 78, loss = 0.33708665\n",
      "Iteration 79, loss = 0.33492213\n",
      "Iteration 80, loss = 0.33458709\n",
      "Iteration 81, loss = 0.33466736\n",
      "Iteration 82, loss = 0.33221674\n",
      "Iteration 83, loss = 0.33177186\n",
      "Iteration 84, loss = 0.32957757\n",
      "Iteration 85, loss = 0.32883259\n",
      "Iteration 86, loss = 0.32830257\n",
      "Iteration 87, loss = 0.32851916\n",
      "Iteration 88, loss = 0.32688525\n",
      "Iteration 89, loss = 0.32480155\n",
      "Iteration 90, loss = 0.32439514\n",
      "Iteration 91, loss = 0.32346927\n",
      "Iteration 92, loss = 0.32252201\n",
      "Iteration 93, loss = 0.32146341\n",
      "Iteration 94, loss = 0.32231729\n",
      "Iteration 95, loss = 0.32135402\n",
      "Iteration 96, loss = 0.31911307\n",
      "Iteration 97, loss = 0.31942198\n",
      "Iteration 98, loss = 0.31882789\n",
      "Iteration 99, loss = 0.31637750\n",
      "Iteration 100, loss = 0.31495340\n",
      "Iteration 101, loss = 0.31442176\n",
      "Iteration 102, loss = 0.31399865\n",
      "Iteration 103, loss = 0.31459984\n",
      "Iteration 104, loss = 0.31211863\n",
      "Iteration 105, loss = 0.31068954\n",
      "Iteration 106, loss = 0.30980457\n",
      "Iteration 107, loss = 0.30962156\n",
      "Iteration 108, loss = 0.30967514\n",
      "Iteration 109, loss = 0.30811704\n",
      "Iteration 110, loss = 0.30762846\n",
      "Iteration 111, loss = 0.30707650\n",
      "Iteration 112, loss = 0.30500058\n",
      "Iteration 113, loss = 0.30509572\n",
      "Iteration 114, loss = 0.30496347\n",
      "Iteration 115, loss = 0.30373688\n",
      "Iteration 116, loss = 0.30331602\n",
      "Iteration 117, loss = 0.30099006\n",
      "Iteration 118, loss = 0.30077144\n",
      "Iteration 119, loss = 0.29951654\n",
      "Iteration 120, loss = 0.29932153\n",
      "Iteration 121, loss = 0.29916667\n",
      "Iteration 122, loss = 0.29850133\n",
      "Iteration 123, loss = 0.29841996\n",
      "Iteration 124, loss = 0.29608171\n",
      "Iteration 125, loss = 0.29778094\n",
      "Iteration 126, loss = 0.29560376\n",
      "Iteration 127, loss = 0.29465047\n",
      "Iteration 128, loss = 0.29546395\n",
      "Iteration 129, loss = 0.29404420\n",
      "Iteration 130, loss = 0.29276866\n",
      "Iteration 131, loss = 0.29304738\n",
      "Iteration 132, loss = 0.29147255\n",
      "Iteration 133, loss = 0.28968411\n",
      "Iteration 134, loss = 0.28896538\n",
      "Iteration 135, loss = 0.28909382\n",
      "Iteration 136, loss = 0.29031812\n",
      "Iteration 137, loss = 0.28881352\n",
      "Iteration 138, loss = 0.28684400\n",
      "Iteration 139, loss = 0.28637324\n",
      "Iteration 140, loss = 0.28603865\n",
      "Iteration 141, loss = 0.28513446\n",
      "Iteration 142, loss = 0.28430018\n",
      "Iteration 143, loss = 0.28386717\n",
      "Iteration 144, loss = 0.28248607\n",
      "Iteration 145, loss = 0.28188951\n",
      "Iteration 146, loss = 0.28144200\n",
      "Iteration 147, loss = 0.28117468\n",
      "Iteration 148, loss = 0.28146022\n",
      "Iteration 149, loss = 0.27902697\n",
      "Iteration 150, loss = 0.27895127\n",
      "Iteration 151, loss = 0.27766552\n",
      "Iteration 152, loss = 0.27745861\n",
      "Iteration 153, loss = 0.27766806\n",
      "Iteration 154, loss = 0.27795985\n",
      "Iteration 155, loss = 0.27633840\n",
      "Iteration 156, loss = 0.27613642\n",
      "Iteration 157, loss = 0.27526602\n",
      "Iteration 158, loss = 0.27421603\n",
      "Iteration 159, loss = 0.27443485\n",
      "Iteration 160, loss = 0.27364018\n",
      "Iteration 161, loss = 0.27297492\n",
      "Iteration 162, loss = 0.27223940\n",
      "Iteration 163, loss = 0.27118985\n",
      "Iteration 164, loss = 0.27211487\n",
      "Iteration 165, loss = 0.27312975\n",
      "Iteration 166, loss = 0.26954312\n",
      "Iteration 167, loss = 0.26910759\n",
      "Iteration 168, loss = 0.26835993\n",
      "Iteration 169, loss = 0.26849042\n",
      "Iteration 170, loss = 0.26675045\n",
      "Iteration 171, loss = 0.26806832\n",
      "Iteration 172, loss = 0.26773634\n",
      "Iteration 173, loss = 0.26513613\n",
      "Iteration 174, loss = 0.26678205\n",
      "Iteration 175, loss = 0.26469925\n",
      "Iteration 176, loss = 0.26404761\n",
      "Iteration 177, loss = 0.26316271\n",
      "Iteration 178, loss = 0.26460437\n",
      "Iteration 179, loss = 0.26503616\n",
      "Iteration 180, loss = 0.26164319\n",
      "Iteration 181, loss = 0.26133256\n",
      "Iteration 182, loss = 0.26328088\n",
      "Iteration 183, loss = 0.25975901\n",
      "Iteration 184, loss = 0.26004462\n",
      "Iteration 185, loss = 0.26005659\n",
      "Iteration 186, loss = 0.25905804\n",
      "Iteration 187, loss = 0.26063019\n",
      "Iteration 188, loss = 0.26045740\n",
      "Iteration 189, loss = 0.25841898\n",
      "Iteration 190, loss = 0.25587407\n",
      "Iteration 191, loss = 0.25590142\n",
      "Iteration 192, loss = 0.25489286\n",
      "Iteration 193, loss = 0.25495443\n",
      "Iteration 194, loss = 0.25435588\n",
      "Iteration 195, loss = 0.25513469\n",
      "Iteration 196, loss = 0.25463834\n",
      "Iteration 197, loss = 0.25243480\n",
      "Iteration 198, loss = 0.25222837\n",
      "Iteration 199, loss = 0.25158881\n",
      "Iteration 200, loss = 0.25122622\n",
      "Iteration 201, loss = 0.25150509\n",
      "Iteration 202, loss = 0.25213331\n",
      "Iteration 203, loss = 0.25052389\n",
      "Iteration 204, loss = 0.25159003\n",
      "Iteration 205, loss = 0.24890022\n",
      "Iteration 206, loss = 0.25001598\n",
      "Iteration 207, loss = 0.24855068\n",
      "Iteration 208, loss = 0.24696348\n",
      "Iteration 209, loss = 0.24650508\n",
      "Iteration 210, loss = 0.24709146\n",
      "Iteration 211, loss = 0.24583617\n",
      "Iteration 212, loss = 0.24560558\n",
      "Iteration 213, loss = 0.24565919\n",
      "Iteration 214, loss = 0.24454160\n",
      "Iteration 215, loss = 0.24848979\n",
      "Iteration 216, loss = 0.24468958\n",
      "Iteration 217, loss = 0.24362738\n",
      "Iteration 218, loss = 0.24220356\n",
      "Iteration 219, loss = 0.24192804\n",
      "Iteration 220, loss = 0.24256561\n",
      "Iteration 221, loss = 0.24146246\n",
      "Iteration 222, loss = 0.24123314\n",
      "Iteration 223, loss = 0.24092851\n",
      "Iteration 224, loss = 0.23944852\n",
      "Iteration 225, loss = 0.23871994\n",
      "Iteration 226, loss = 0.23854437\n",
      "Iteration 227, loss = 0.23800819\n",
      "Iteration 228, loss = 0.23706884\n",
      "Iteration 229, loss = 0.23734067\n",
      "Iteration 230, loss = 0.23706023\n",
      "Iteration 231, loss = 0.23569065\n",
      "Iteration 232, loss = 0.23652094\n",
      "Iteration 233, loss = 0.23608121\n",
      "Iteration 234, loss = 0.23455497\n",
      "Iteration 235, loss = 0.23419206\n",
      "Iteration 236, loss = 0.23388848\n",
      "Iteration 237, loss = 0.23298694\n",
      "Iteration 238, loss = 0.23328278\n",
      "Iteration 239, loss = 0.23277864\n",
      "Iteration 240, loss = 0.23153484\n",
      "Iteration 241, loss = 0.23185527\n",
      "Iteration 242, loss = 0.23183747\n",
      "Iteration 243, loss = 0.23095408\n",
      "Iteration 244, loss = 0.23045137\n",
      "Iteration 245, loss = 0.23243634\n",
      "Iteration 246, loss = 0.23076173\n",
      "Iteration 247, loss = 0.22799452\n",
      "Iteration 248, loss = 0.22916353\n",
      "Iteration 249, loss = 0.22758117\n",
      "Iteration 250, loss = 0.22739944\n",
      "Iteration 251, loss = 0.22738325\n",
      "Iteration 252, loss = 0.22634277\n",
      "Iteration 253, loss = 0.22611692\n",
      "Iteration 254, loss = 0.22537354\n",
      "Iteration 255, loss = 0.22459278\n",
      "Iteration 256, loss = 0.22462733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.22341731\n",
      "Iteration 258, loss = 0.22313110\n",
      "Iteration 259, loss = 0.22460381\n",
      "Iteration 260, loss = 0.22396018\n",
      "Iteration 261, loss = 0.22242069\n",
      "Iteration 262, loss = 0.22242855\n",
      "Iteration 263, loss = 0.22123697\n",
      "Iteration 264, loss = 0.22369808\n",
      "Iteration 265, loss = 0.22142937\n",
      "Iteration 266, loss = 0.21987193\n",
      "Iteration 267, loss = 0.21949137\n",
      "Iteration 268, loss = 0.21973023\n",
      "Iteration 269, loss = 0.21858411\n",
      "Iteration 270, loss = 0.21843909\n",
      "Iteration 271, loss = 0.21800752\n",
      "Iteration 272, loss = 0.21687848\n",
      "Iteration 273, loss = 0.21718603\n",
      "Iteration 274, loss = 0.21627684\n",
      "Iteration 275, loss = 0.21455326\n",
      "Iteration 276, loss = 0.21408440\n",
      "Iteration 277, loss = 0.21361328\n",
      "Iteration 278, loss = 0.21370143\n",
      "Iteration 279, loss = 0.21316376\n",
      "Iteration 280, loss = 0.21425731\n",
      "Iteration 281, loss = 0.21281208\n",
      "Iteration 282, loss = 0.21111649\n",
      "Iteration 283, loss = 0.21220416\n",
      "Iteration 284, loss = 0.21271665\n",
      "Iteration 285, loss = 0.21042153\n",
      "Iteration 286, loss = 0.21131716\n",
      "Iteration 287, loss = 0.20975210\n",
      "Iteration 288, loss = 0.21081411\n",
      "Iteration 289, loss = 0.20969596\n",
      "Iteration 290, loss = 0.20770145\n",
      "Iteration 291, loss = 0.20862258\n",
      "Iteration 292, loss = 0.20710724\n",
      "Iteration 293, loss = 0.20735171\n",
      "Iteration 294, loss = 0.20702244\n",
      "Iteration 295, loss = 0.20543474\n",
      "Iteration 296, loss = 0.20762804\n",
      "Iteration 297, loss = 0.20636784\n",
      "Iteration 298, loss = 0.20573064\n",
      "Iteration 299, loss = 0.20451408\n",
      "Iteration 300, loss = 0.20327737\n",
      "Iteration 301, loss = 0.20291508\n",
      "Iteration 302, loss = 0.20271128\n",
      "Iteration 303, loss = 0.20344871\n",
      "Iteration 304, loss = 0.20333884\n",
      "Iteration 305, loss = 0.20200428\n",
      "Iteration 306, loss = 0.20069794\n",
      "Iteration 307, loss = 0.20022016\n",
      "Iteration 308, loss = 0.19969657\n",
      "Iteration 309, loss = 0.19943118\n",
      "Iteration 310, loss = 0.19939816\n",
      "Iteration 311, loss = 0.19958077\n",
      "Iteration 312, loss = 0.19844244\n",
      "Iteration 313, loss = 0.19785863\n",
      "Iteration 314, loss = 0.19688703\n",
      "Iteration 315, loss = 0.19692908\n",
      "Iteration 316, loss = 0.19702631\n",
      "Iteration 317, loss = 0.19822704\n",
      "Iteration 318, loss = 0.19707663\n",
      "Iteration 319, loss = 0.19564448\n",
      "Iteration 320, loss = 0.19449115\n",
      "Iteration 321, loss = 0.19490639\n",
      "Iteration 322, loss = 0.19335753\n",
      "Iteration 323, loss = 0.19325799\n",
      "Iteration 324, loss = 0.19350158\n",
      "Iteration 325, loss = 0.19306288\n",
      "Iteration 326, loss = 0.19202269\n",
      "Iteration 327, loss = 0.19166068\n",
      "Iteration 328, loss = 0.19248194\n",
      "Iteration 329, loss = 0.19058960\n",
      "Iteration 330, loss = 0.19035969\n",
      "Iteration 331, loss = 0.18989168\n",
      "Iteration 332, loss = 0.19223928\n",
      "Iteration 333, loss = 0.19064608\n",
      "Iteration 334, loss = 0.18966225\n",
      "Iteration 335, loss = 0.18861854\n",
      "Iteration 336, loss = 0.18781035\n",
      "Iteration 337, loss = 0.18695825\n",
      "Iteration 338, loss = 0.18707066\n",
      "Iteration 339, loss = 0.18639298\n",
      "Iteration 340, loss = 0.18605570\n",
      "Iteration 341, loss = 0.18626775\n",
      "Iteration 342, loss = 0.18564103\n",
      "Iteration 343, loss = 0.18505616\n",
      "Iteration 344, loss = 0.18428045\n",
      "Iteration 345, loss = 0.18381702\n",
      "Iteration 346, loss = 0.18345117\n",
      "Iteration 347, loss = 0.18307809\n",
      "Iteration 348, loss = 0.18283645\n",
      "Iteration 349, loss = 0.18444824\n",
      "Iteration 350, loss = 0.18263547\n",
      "Iteration 351, loss = 0.18141449\n",
      "Iteration 352, loss = 0.18065205\n",
      "Iteration 353, loss = 0.18039675\n",
      "Iteration 354, loss = 0.18082550\n",
      "Iteration 355, loss = 0.18053445\n",
      "Iteration 356, loss = 0.17887572\n",
      "Iteration 357, loss = 0.17869562\n",
      "Iteration 358, loss = 0.17807347\n",
      "Iteration 359, loss = 0.17838119\n",
      "Iteration 360, loss = 0.17785628\n",
      "Iteration 361, loss = 0.17702217\n",
      "Iteration 362, loss = 0.17767098\n",
      "Iteration 363, loss = 0.17653251\n",
      "Iteration 364, loss = 0.17654059\n",
      "Iteration 365, loss = 0.17652691\n",
      "Iteration 366, loss = 0.17465951\n",
      "Iteration 367, loss = 0.17487769\n",
      "Iteration 368, loss = 0.17608459\n",
      "Iteration 369, loss = 0.17408289\n",
      "Iteration 370, loss = 0.17262589\n",
      "Iteration 371, loss = 0.17270738\n",
      "Iteration 372, loss = 0.17243503\n",
      "Iteration 373, loss = 0.17328721\n",
      "Iteration 374, loss = 0.17264491\n",
      "Iteration 375, loss = 0.17235861\n",
      "Iteration 376, loss = 0.17337947\n",
      "Iteration 377, loss = 0.17088927\n",
      "Iteration 378, loss = 0.17017782\n",
      "Iteration 379, loss = 0.16951728\n",
      "Iteration 380, loss = 0.16909669\n",
      "Iteration 381, loss = 0.16897140\n",
      "Iteration 382, loss = 0.16910173\n",
      "Iteration 383, loss = 0.16805203\n",
      "Iteration 384, loss = 0.16816552\n",
      "Iteration 385, loss = 0.16843901\n",
      "Iteration 386, loss = 0.16632175\n",
      "Iteration 387, loss = 0.16554907\n",
      "Iteration 388, loss = 0.16523492\n",
      "Iteration 389, loss = 0.16591015\n",
      "Iteration 390, loss = 0.16448802\n",
      "Iteration 391, loss = 0.16450780\n",
      "Iteration 392, loss = 0.16491859\n",
      "Iteration 393, loss = 0.16483300\n",
      "Iteration 394, loss = 0.16370240\n",
      "Iteration 395, loss = 0.16283974\n",
      "Iteration 396, loss = 0.16207893\n",
      "Iteration 397, loss = 0.16161748\n",
      "Iteration 398, loss = 0.16127042\n",
      "Iteration 399, loss = 0.16082718\n",
      "Iteration 400, loss = 0.16038677\n",
      "Iteration 401, loss = 0.16097930\n",
      "Iteration 402, loss = 0.15984383\n",
      "Iteration 403, loss = 0.15875257\n",
      "Iteration 404, loss = 0.15908179\n",
      "Iteration 405, loss = 0.15846627\n",
      "Iteration 406, loss = 0.15917492\n",
      "Iteration 407, loss = 0.16066379\n",
      "Iteration 408, loss = 0.15852769\n",
      "Iteration 409, loss = 0.15695579\n",
      "Iteration 410, loss = 0.15658303\n",
      "Iteration 411, loss = 0.15630477\n",
      "Iteration 412, loss = 0.15586460\n",
      "Iteration 413, loss = 0.15573287\n",
      "Iteration 414, loss = 0.15576435\n",
      "Iteration 415, loss = 0.15533427\n",
      "Iteration 416, loss = 0.15494512\n",
      "Iteration 417, loss = 0.15343712\n",
      "Iteration 418, loss = 0.15383178\n",
      "Iteration 419, loss = 0.15284530\n",
      "Iteration 420, loss = 0.15259672\n",
      "Iteration 421, loss = 0.15249200\n",
      "Iteration 422, loss = 0.15294940\n",
      "Iteration 423, loss = 0.15181874\n",
      "Iteration 424, loss = 0.15100085\n",
      "Iteration 425, loss = 0.15060214\n",
      "Iteration 426, loss = 0.14988369\n",
      "Iteration 427, loss = 0.14955899\n",
      "Iteration 428, loss = 0.15037900\n",
      "Iteration 429, loss = 0.14845301\n",
      "Iteration 430, loss = 0.15063350\n",
      "Iteration 431, loss = 0.14837492\n",
      "Iteration 432, loss = 0.15004513\n",
      "Iteration 433, loss = 0.14723019\n",
      "Iteration 434, loss = 0.14681517\n",
      "Iteration 435, loss = 0.14635479\n",
      "Iteration 436, loss = 0.14622309\n",
      "Iteration 437, loss = 0.14553088\n",
      "Iteration 438, loss = 0.14594948\n",
      "Iteration 439, loss = 0.14504319\n",
      "Iteration 440, loss = 0.14487684\n",
      "Iteration 441, loss = 0.14439400\n",
      "Iteration 442, loss = 0.14553039\n",
      "Iteration 443, loss = 0.14495566\n",
      "Iteration 444, loss = 0.14327209\n",
      "Iteration 445, loss = 0.14392904\n",
      "Iteration 446, loss = 0.14258432\n",
      "Iteration 447, loss = 0.14203834\n",
      "Iteration 448, loss = 0.14248900\n",
      "Iteration 449, loss = 0.14237489\n",
      "Iteration 450, loss = 0.14125871\n",
      "Iteration 451, loss = 0.14059590\n",
      "Iteration 452, loss = 0.14148462\n",
      "Iteration 453, loss = 0.13985510\n",
      "Iteration 454, loss = 0.14023049\n",
      "Iteration 455, loss = 0.13989626\n",
      "Iteration 456, loss = 0.13917585\n",
      "Iteration 457, loss = 0.13821324\n",
      "Iteration 458, loss = 0.13833583\n",
      "Iteration 459, loss = 0.13821225\n",
      "Iteration 460, loss = 0.13696812\n",
      "Iteration 461, loss = 0.13758775\n",
      "Iteration 462, loss = 0.13623933\n",
      "Iteration 463, loss = 0.13781971\n",
      "Iteration 464, loss = 0.13830527\n",
      "Iteration 465, loss = 0.13620555\n",
      "Iteration 466, loss = 0.13548138\n",
      "Iteration 467, loss = 0.13543059\n",
      "Iteration 468, loss = 0.13435858\n",
      "Iteration 469, loss = 0.13508775\n",
      "Iteration 470, loss = 0.13375280\n",
      "Iteration 471, loss = 0.13349129\n",
      "Iteration 472, loss = 0.13339420\n",
      "Iteration 473, loss = 0.13311778\n",
      "Iteration 474, loss = 0.13218715\n",
      "Iteration 475, loss = 0.13174471\n",
      "Iteration 476, loss = 0.13164969\n",
      "Iteration 477, loss = 0.13088823\n",
      "Iteration 478, loss = 0.13127451\n",
      "Iteration 479, loss = 0.13075871\n",
      "Iteration 480, loss = 0.13033164\n",
      "Iteration 481, loss = 0.12940855\n",
      "Iteration 482, loss = 0.12903305\n",
      "Iteration 483, loss = 0.12881223\n",
      "Iteration 484, loss = 0.12864600\n",
      "Iteration 485, loss = 0.12921551\n",
      "Iteration 486, loss = 0.12763705\n",
      "Iteration 487, loss = 0.12743058\n",
      "Iteration 488, loss = 0.12703234\n",
      "Iteration 489, loss = 0.12663825\n",
      "Iteration 490, loss = 0.12613468\n",
      "Iteration 491, loss = 0.12575272\n",
      "Iteration 492, loss = 0.12592047\n",
      "Iteration 493, loss = 0.12531734\n",
      "Iteration 494, loss = 0.12505703\n",
      "Iteration 495, loss = 0.12486354\n",
      "Iteration 496, loss = 0.12462173\n",
      "Iteration 497, loss = 0.12447694\n",
      "Iteration 498, loss = 0.12462000\n",
      "Iteration 499, loss = 0.12405627\n",
      "Iteration 500, loss = 0.12362658\n",
      "Iteration 501, loss = 0.12354612\n",
      "Iteration 502, loss = 0.12291850\n",
      "Iteration 503, loss = 0.12219947\n",
      "Iteration 504, loss = 0.12172801\n",
      "Iteration 505, loss = 0.12120098\n",
      "Iteration 506, loss = 0.12157663\n",
      "Iteration 507, loss = 0.12089200\n",
      "Iteration 508, loss = 0.12066868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 509, loss = 0.12031758\n",
      "Iteration 510, loss = 0.12062339\n",
      "Iteration 511, loss = 0.11936311\n",
      "Iteration 512, loss = 0.11974995\n",
      "Iteration 513, loss = 0.11979164\n",
      "Iteration 514, loss = 0.11853327\n",
      "Iteration 515, loss = 0.11805501\n",
      "Iteration 516, loss = 0.11799868\n",
      "Iteration 517, loss = 0.11765738\n",
      "Iteration 518, loss = 0.11808511\n",
      "Iteration 519, loss = 0.11986438\n",
      "Iteration 520, loss = 0.11743817\n",
      "Iteration 521, loss = 0.11618130\n",
      "Iteration 522, loss = 0.11560614\n",
      "Iteration 523, loss = 0.11540222\n",
      "Iteration 524, loss = 0.11532002\n",
      "Iteration 525, loss = 0.11549923\n",
      "Iteration 526, loss = 0.11522095\n",
      "Iteration 527, loss = 0.11442061\n",
      "Iteration 528, loss = 0.11424522\n",
      "Iteration 529, loss = 0.11404178\n",
      "Iteration 530, loss = 0.11342050\n",
      "Iteration 531, loss = 0.11309900\n",
      "Iteration 532, loss = 0.11254776\n",
      "Iteration 533, loss = 0.11257338\n",
      "Iteration 534, loss = 0.11291680\n",
      "Iteration 535, loss = 0.11188229\n",
      "Iteration 536, loss = 0.11161229\n",
      "Iteration 537, loss = 0.11105085\n",
      "Iteration 538, loss = 0.11126624\n",
      "Iteration 539, loss = 0.11096491\n",
      "Iteration 540, loss = 0.10980157\n",
      "Iteration 541, loss = 0.11031802\n",
      "Iteration 542, loss = 0.10979656\n",
      "Iteration 543, loss = 0.10958275\n",
      "Iteration 544, loss = 0.10900871\n",
      "Iteration 545, loss = 0.10938846\n",
      "Iteration 546, loss = 0.10968932\n",
      "Iteration 547, loss = 0.10787564\n",
      "Iteration 548, loss = 0.10778230\n",
      "Iteration 549, loss = 0.10707674\n",
      "Iteration 550, loss = 0.10700652\n",
      "Iteration 551, loss = 0.10856545\n",
      "Iteration 552, loss = 0.10695067\n",
      "Iteration 553, loss = 0.10612863\n",
      "Iteration 554, loss = 0.10664272\n",
      "Iteration 555, loss = 0.10549622\n",
      "Iteration 556, loss = 0.10500986\n",
      "Iteration 557, loss = 0.10538641\n",
      "Iteration 558, loss = 0.10457834\n",
      "Iteration 559, loss = 0.10488024\n",
      "Iteration 560, loss = 0.10505492\n",
      "Iteration 561, loss = 0.10373485\n",
      "Iteration 562, loss = 0.10403253\n",
      "Iteration 563, loss = 0.10326011\n",
      "Iteration 564, loss = 0.10374734\n",
      "Iteration 565, loss = 0.10309484\n",
      "Iteration 566, loss = 0.10292102\n",
      "Iteration 567, loss = 0.10209738\n",
      "Iteration 568, loss = 0.10164647\n",
      "Iteration 569, loss = 0.10198765\n",
      "Iteration 570, loss = 0.10125509\n",
      "Iteration 571, loss = 0.10077792\n",
      "Iteration 572, loss = 0.10154400\n",
      "Iteration 573, loss = 0.10164445\n",
      "Iteration 574, loss = 0.10131251\n",
      "Iteration 575, loss = 0.09964610\n",
      "Iteration 576, loss = 0.09906483\n",
      "Iteration 577, loss = 0.09990506\n",
      "Iteration 578, loss = 0.09841918\n",
      "Iteration 579, loss = 0.09898473\n",
      "Iteration 580, loss = 0.09847219\n",
      "Iteration 581, loss = 0.09777762\n",
      "Iteration 582, loss = 0.09714567\n",
      "Iteration 583, loss = 0.09735096\n",
      "Iteration 584, loss = 0.09768538\n",
      "Iteration 585, loss = 0.09716572\n",
      "Iteration 586, loss = 0.09700896\n",
      "Iteration 587, loss = 0.09658729\n",
      "Iteration 588, loss = 0.09539830\n",
      "Iteration 589, loss = 0.09532723\n",
      "Iteration 590, loss = 0.09499576\n",
      "Iteration 591, loss = 0.09470182\n",
      "Iteration 592, loss = 0.09433045\n",
      "Iteration 593, loss = 0.09397959\n",
      "Iteration 594, loss = 0.09442735\n",
      "Iteration 595, loss = 0.09393454\n",
      "Iteration 596, loss = 0.09306756\n",
      "Iteration 597, loss = 0.09442268\n",
      "Iteration 598, loss = 0.09358385\n",
      "Iteration 599, loss = 0.09238097\n",
      "Iteration 600, loss = 0.09234653\n",
      "Iteration 601, loss = 0.09168075\n",
      "Iteration 602, loss = 0.09158108\n",
      "Iteration 603, loss = 0.09163664\n",
      "Iteration 604, loss = 0.09111146\n",
      "Iteration 605, loss = 0.09126880\n",
      "Iteration 606, loss = 0.09084675\n",
      "Iteration 607, loss = 0.09060532\n",
      "Iteration 608, loss = 0.09021676\n",
      "Iteration 609, loss = 0.09057950\n",
      "Iteration 610, loss = 0.08946896\n",
      "Iteration 611, loss = 0.08906062\n",
      "Iteration 612, loss = 0.08916148\n",
      "Iteration 613, loss = 0.08899399\n",
      "Iteration 614, loss = 0.08913563\n",
      "Iteration 615, loss = 0.08830022\n",
      "Iteration 616, loss = 0.08869143\n",
      "Iteration 617, loss = 0.08819252\n",
      "Iteration 618, loss = 0.08741964\n",
      "Iteration 619, loss = 0.08756476\n",
      "Iteration 620, loss = 0.08726480\n",
      "Iteration 621, loss = 0.08748547\n",
      "Iteration 622, loss = 0.08677467\n",
      "Iteration 623, loss = 0.08653402\n",
      "Iteration 624, loss = 0.08707540\n",
      "Iteration 625, loss = 0.08578997\n",
      "Iteration 626, loss = 0.08561986\n",
      "Iteration 627, loss = 0.08576918\n",
      "Iteration 628, loss = 0.08612518\n",
      "Iteration 629, loss = 0.08531316\n",
      "Iteration 630, loss = 0.08497693\n",
      "Iteration 631, loss = 0.08406355\n",
      "Iteration 632, loss = 0.08421813\n",
      "Iteration 633, loss = 0.08399497\n",
      "Iteration 634, loss = 0.08341890\n",
      "Iteration 635, loss = 0.08387165\n",
      "Iteration 636, loss = 0.08374948\n",
      "Iteration 637, loss = 0.08369342\n",
      "Iteration 638, loss = 0.08283252\n",
      "Iteration 639, loss = 0.08235386\n",
      "Iteration 640, loss = 0.08258954\n",
      "Iteration 641, loss = 0.08212806\n",
      "Iteration 642, loss = 0.08186366\n",
      "Iteration 643, loss = 0.08155165\n",
      "Iteration 644, loss = 0.08130279\n",
      "Iteration 645, loss = 0.08131016\n",
      "Iteration 646, loss = 0.08108611\n",
      "Iteration 647, loss = 0.08058413\n",
      "Iteration 648, loss = 0.08016749\n",
      "Iteration 649, loss = 0.08057766\n",
      "Iteration 650, loss = 0.07987964\n",
      "Iteration 651, loss = 0.08010473\n",
      "Iteration 652, loss = 0.07980048\n",
      "Iteration 653, loss = 0.07919488\n",
      "Iteration 654, loss = 0.07909510\n",
      "Iteration 655, loss = 0.07910327\n",
      "Iteration 656, loss = 0.07915527\n",
      "Iteration 657, loss = 0.07848517\n",
      "Iteration 658, loss = 0.07822180\n",
      "Iteration 659, loss = 0.07791258\n",
      "Iteration 660, loss = 0.07749820\n",
      "Iteration 661, loss = 0.07738539\n",
      "Iteration 662, loss = 0.07748717\n",
      "Iteration 663, loss = 0.07784785\n",
      "Iteration 664, loss = 0.07665614\n",
      "Iteration 665, loss = 0.07625731\n",
      "Iteration 666, loss = 0.07691370\n",
      "Iteration 667, loss = 0.07704855\n",
      "Iteration 668, loss = 0.07641075\n",
      "Iteration 669, loss = 0.07632672\n",
      "Iteration 670, loss = 0.07645520\n",
      "Iteration 671, loss = 0.07596713\n",
      "Iteration 672, loss = 0.07516180\n",
      "Iteration 673, loss = 0.07498200\n",
      "Iteration 674, loss = 0.07450967\n",
      "Iteration 675, loss = 0.07441191\n",
      "Iteration 676, loss = 0.07401983\n",
      "Iteration 677, loss = 0.07423986\n",
      "Iteration 678, loss = 0.07423339\n",
      "Iteration 679, loss = 0.07361423\n",
      "Iteration 680, loss = 0.07500460\n",
      "Iteration 681, loss = 0.07425743\n",
      "Iteration 682, loss = 0.07278408\n",
      "Iteration 683, loss = 0.07345449\n",
      "Iteration 684, loss = 0.07261180\n",
      "Iteration 685, loss = 0.07208471\n",
      "Iteration 686, loss = 0.07196494\n",
      "Iteration 687, loss = 0.07169662\n",
      "Iteration 688, loss = 0.07154543\n",
      "Iteration 689, loss = 0.07165364\n",
      "Iteration 690, loss = 0.07126004\n",
      "Iteration 691, loss = 0.07090524\n",
      "Iteration 692, loss = 0.07051067\n",
      "Iteration 693, loss = 0.07055759\n",
      "Iteration 694, loss = 0.07001188\n",
      "Iteration 695, loss = 0.07009514\n",
      "Iteration 696, loss = 0.06958356\n",
      "Iteration 697, loss = 0.07077843\n",
      "Iteration 698, loss = 0.06983015\n",
      "Iteration 699, loss = 0.06960805\n",
      "Iteration 700, loss = 0.06881768\n",
      "Iteration 701, loss = 0.06867283\n",
      "Iteration 702, loss = 0.06929232\n",
      "Iteration 703, loss = 0.06906540\n",
      "Iteration 704, loss = 0.06853014\n",
      "Iteration 705, loss = 0.06814253\n",
      "Iteration 706, loss = 0.06801499\n",
      "Iteration 707, loss = 0.06823381\n",
      "Iteration 708, loss = 0.06722390\n",
      "Iteration 709, loss = 0.06724838\n",
      "Iteration 710, loss = 0.06678464\n",
      "Iteration 711, loss = 0.06657042\n",
      "Iteration 712, loss = 0.06696725\n",
      "Iteration 713, loss = 0.06929586\n",
      "Iteration 714, loss = 0.06616045\n",
      "Iteration 715, loss = 0.06658533\n",
      "Iteration 716, loss = 0.06594415\n",
      "Iteration 717, loss = 0.06629491\n",
      "Iteration 718, loss = 0.06561120\n",
      "Iteration 719, loss = 0.06530776\n",
      "Iteration 720, loss = 0.06490976\n",
      "Iteration 721, loss = 0.06492813\n",
      "Iteration 722, loss = 0.06448021\n",
      "Iteration 723, loss = 0.06424496\n",
      "Iteration 724, loss = 0.06413459\n",
      "Iteration 725, loss = 0.06371246\n",
      "Iteration 726, loss = 0.06376137\n",
      "Iteration 727, loss = 0.06351588\n",
      "Iteration 728, loss = 0.06350886\n",
      "Iteration 729, loss = 0.06389302\n",
      "Iteration 730, loss = 0.06334776\n",
      "Iteration 731, loss = 0.06314351\n",
      "Iteration 732, loss = 0.06290281\n",
      "Iteration 733, loss = 0.06301795\n",
      "Iteration 734, loss = 0.06226741\n",
      "Iteration 735, loss = 0.06238856\n",
      "Iteration 736, loss = 0.06181148\n",
      "Iteration 737, loss = 0.06169120\n",
      "Iteration 738, loss = 0.06136994\n",
      "Iteration 739, loss = 0.06177533\n",
      "Iteration 740, loss = 0.06131636\n",
      "Iteration 741, loss = 0.06086759\n",
      "Iteration 742, loss = 0.06103103\n",
      "Iteration 743, loss = 0.06152408\n",
      "Iteration 744, loss = 0.06069208\n",
      "Iteration 745, loss = 0.06097069\n",
      "Iteration 746, loss = 0.06034851\n",
      "Iteration 747, loss = 0.06037521\n",
      "Iteration 748, loss = 0.05971003\n",
      "Iteration 749, loss = 0.06029403\n",
      "Iteration 750, loss = 0.06157448\n",
      "Iteration 751, loss = 0.06023902\n",
      "Iteration 752, loss = 0.05962738\n",
      "Iteration 753, loss = 0.05995783\n",
      "Iteration 754, loss = 0.05919397\n",
      "Iteration 755, loss = 0.05857759\n",
      "Iteration 756, loss = 0.05838674\n",
      "Iteration 757, loss = 0.05840120\n",
      "Iteration 758, loss = 0.05834178\n",
      "Iteration 759, loss = 0.05818684\n",
      "Iteration 760, loss = 0.05785136\n",
      "Iteration 761, loss = 0.05792177\n",
      "Iteration 762, loss = 0.05757907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 763, loss = 0.05717932\n",
      "Iteration 764, loss = 0.05727832\n",
      "Iteration 765, loss = 0.05756918\n",
      "Iteration 766, loss = 0.05705099\n",
      "Iteration 767, loss = 0.05783279\n",
      "Iteration 768, loss = 0.05667361\n",
      "Iteration 769, loss = 0.05632890\n",
      "Iteration 770, loss = 0.05633713\n",
      "Iteration 771, loss = 0.05727628\n",
      "Iteration 772, loss = 0.05687697\n",
      "Iteration 773, loss = 0.05567720\n",
      "Iteration 774, loss = 0.05555527\n",
      "Iteration 775, loss = 0.05551533\n",
      "Iteration 776, loss = 0.05602899\n",
      "Iteration 777, loss = 0.05531755\n",
      "Iteration 778, loss = 0.05572728\n",
      "Iteration 779, loss = 0.05493170\n",
      "Iteration 780, loss = 0.05509396\n",
      "Iteration 781, loss = 0.05496098\n",
      "Iteration 782, loss = 0.05489239\n",
      "Iteration 783, loss = 0.05472665\n",
      "Iteration 784, loss = 0.05382879\n",
      "Iteration 785, loss = 0.05400366\n",
      "Iteration 786, loss = 0.05358698\n",
      "Iteration 787, loss = 0.05363580\n",
      "Iteration 788, loss = 0.05351321\n",
      "Iteration 789, loss = 0.05320649\n",
      "Iteration 790, loss = 0.05361299\n",
      "Iteration 791, loss = 0.05362173\n",
      "Iteration 792, loss = 0.05282355\n",
      "Iteration 793, loss = 0.05244933\n",
      "Iteration 794, loss = 0.05246601\n",
      "Iteration 795, loss = 0.05209594\n",
      "Iteration 796, loss = 0.05210609\n",
      "Iteration 797, loss = 0.05286230\n",
      "Iteration 798, loss = 0.05169350\n",
      "Iteration 799, loss = 0.05154722\n",
      "Iteration 800, loss = 0.05193985\n",
      "Iteration 801, loss = 0.05164228\n",
      "Iteration 802, loss = 0.05148180\n",
      "Iteration 803, loss = 0.05083808\n",
      "Iteration 804, loss = 0.05138308\n",
      "Iteration 805, loss = 0.05084724\n",
      "Iteration 806, loss = 0.05436669\n",
      "Iteration 807, loss = 0.05713553\n",
      "Iteration 808, loss = 0.05545435\n",
      "Iteration 809, loss = 0.05344311\n",
      "Iteration 810, loss = 0.05016246\n",
      "Iteration 811, loss = 0.05030223\n",
      "Iteration 812, loss = 0.04998162\n",
      "Iteration 813, loss = 0.05003841\n",
      "Iteration 814, loss = 0.04971012\n",
      "Iteration 815, loss = 0.04984921\n",
      "Iteration 816, loss = 0.04937187\n",
      "Iteration 817, loss = 0.04945228\n",
      "Iteration 818, loss = 0.04924760\n",
      "Iteration 819, loss = 0.04896141\n",
      "Iteration 820, loss = 0.04887451\n",
      "Iteration 821, loss = 0.04940072\n",
      "Iteration 822, loss = 0.04858070\n",
      "Iteration 823, loss = 0.04864342\n",
      "Iteration 824, loss = 0.04844726\n",
      "Iteration 825, loss = 0.04864103\n",
      "Iteration 826, loss = 0.04887229\n",
      "Iteration 827, loss = 0.04780685\n",
      "Iteration 828, loss = 0.04810048\n",
      "Iteration 829, loss = 0.04761548\n",
      "Iteration 830, loss = 0.04747588\n",
      "Iteration 831, loss = 0.04817460\n",
      "Iteration 832, loss = 0.04760399\n",
      "Iteration 833, loss = 0.04715560\n",
      "Iteration 834, loss = 0.04743187\n",
      "Iteration 835, loss = 0.04804158\n",
      "Iteration 836, loss = 0.04712563\n",
      "Iteration 837, loss = 0.04666735\n",
      "Iteration 838, loss = 0.04647280\n",
      "Iteration 839, loss = 0.04615406\n",
      "Iteration 840, loss = 0.04603427\n",
      "Iteration 841, loss = 0.04621874\n",
      "Iteration 842, loss = 0.04671932\n",
      "Iteration 843, loss = 0.04622024\n",
      "Iteration 844, loss = 0.04539680\n",
      "Iteration 845, loss = 0.04555843\n",
      "Iteration 846, loss = 0.04585212\n",
      "Iteration 847, loss = 0.04539875\n",
      "Iteration 848, loss = 0.04513315\n",
      "Iteration 849, loss = 0.04517906\n",
      "Iteration 850, loss = 0.04482336\n",
      "Iteration 851, loss = 0.04501978\n",
      "Iteration 852, loss = 0.04500253\n",
      "Iteration 853, loss = 0.04562602\n",
      "Iteration 854, loss = 0.04616822\n",
      "Iteration 855, loss = 0.04531788\n",
      "Iteration 856, loss = 0.04463312\n",
      "Iteration 857, loss = 0.04403384\n",
      "Iteration 858, loss = 0.04408532\n",
      "Iteration 859, loss = 0.04433416\n",
      "Iteration 860, loss = 0.04407846\n",
      "Iteration 861, loss = 0.04393582\n",
      "Iteration 862, loss = 0.04334469\n",
      "Iteration 863, loss = 0.04428947\n",
      "Iteration 864, loss = 0.04325252\n",
      "Iteration 865, loss = 0.04309768\n",
      "Iteration 866, loss = 0.04313407\n",
      "Iteration 867, loss = 0.04279192\n",
      "Iteration 868, loss = 0.04274495\n",
      "Iteration 869, loss = 0.04241748\n",
      "Iteration 870, loss = 0.04261447\n",
      "Iteration 871, loss = 0.04296851\n",
      "Iteration 872, loss = 0.04233666\n",
      "Iteration 873, loss = 0.04243328\n",
      "Iteration 874, loss = 0.04205190\n",
      "Iteration 875, loss = 0.04213821\n",
      "Iteration 876, loss = 0.04221652\n",
      "Iteration 877, loss = 0.04162508\n",
      "Iteration 878, loss = 0.04151317\n",
      "Iteration 879, loss = 0.04176016\n",
      "Iteration 880, loss = 0.04142028\n",
      "Iteration 881, loss = 0.04153711\n",
      "Iteration 882, loss = 0.04134780\n",
      "Iteration 883, loss = 0.04112142\n",
      "Iteration 884, loss = 0.04092146\n",
      "Iteration 885, loss = 0.04095717\n",
      "Iteration 886, loss = 0.04073540\n",
      "Iteration 887, loss = 0.04111329\n",
      "Iteration 888, loss = 0.04034519\n",
      "Iteration 889, loss = 0.04052632\n",
      "Iteration 890, loss = 0.04086815\n",
      "Iteration 891, loss = 0.04038819\n",
      "Iteration 892, loss = 0.04018790\n",
      "Iteration 893, loss = 0.04012065\n",
      "Iteration 894, loss = 0.04013488\n",
      "Iteration 895, loss = 0.04021127\n",
      "Iteration 896, loss = 0.03960239\n",
      "Iteration 897, loss = 0.04028248\n",
      "Iteration 898, loss = 0.03991732\n",
      "Iteration 899, loss = 0.03934706\n",
      "Iteration 900, loss = 0.03927795\n",
      "Iteration 901, loss = 0.03914942\n",
      "Iteration 902, loss = 0.03909512\n",
      "Iteration 903, loss = 0.03922584\n",
      "Iteration 904, loss = 0.03926065\n",
      "Iteration 905, loss = 0.03881408\n",
      "Iteration 906, loss = 0.03851141\n",
      "Iteration 907, loss = 0.03991016\n",
      "Iteration 908, loss = 0.03907520\n",
      "Iteration 909, loss = 0.03927006\n",
      "Iteration 910, loss = 0.03839261\n",
      "Iteration 911, loss = 0.03799404\n",
      "Iteration 912, loss = 0.03779245\n",
      "Iteration 913, loss = 0.03757260\n",
      "Iteration 914, loss = 0.03763105\n",
      "Iteration 915, loss = 0.03772071\n",
      "Iteration 916, loss = 0.03746157\n",
      "Iteration 917, loss = 0.03731901\n",
      "Iteration 918, loss = 0.03680528\n",
      "Iteration 919, loss = 0.03712415\n",
      "Iteration 920, loss = 0.03673584\n",
      "Iteration 921, loss = 0.03656325\n",
      "Iteration 922, loss = 0.03668696\n",
      "Iteration 923, loss = 0.03635313\n",
      "Iteration 924, loss = 0.03611899\n",
      "Iteration 925, loss = 0.03602876\n",
      "Iteration 926, loss = 0.03602157\n",
      "Iteration 927, loss = 0.03583894\n",
      "Iteration 928, loss = 0.03577391\n",
      "Iteration 929, loss = 0.03601729\n",
      "Iteration 930, loss = 0.03574383\n",
      "Iteration 931, loss = 0.03554340\n",
      "Iteration 932, loss = 0.03535004\n",
      "Iteration 933, loss = 0.03563859\n",
      "Iteration 934, loss = 0.03562298\n",
      "Iteration 935, loss = 0.03545594\n",
      "Iteration 936, loss = 0.03568966\n",
      "Iteration 937, loss = 0.03497214\n",
      "Iteration 938, loss = 0.03506350\n",
      "Iteration 939, loss = 0.03459917\n",
      "Iteration 940, loss = 0.03466292\n",
      "Iteration 941, loss = 0.03458064\n",
      "Iteration 942, loss = 0.03463657\n",
      "Iteration 943, loss = 0.03439579\n",
      "Iteration 944, loss = 0.03411126\n",
      "Iteration 945, loss = 0.03409974\n",
      "Iteration 946, loss = 0.03407172\n",
      "Iteration 947, loss = 0.03406047\n",
      "Iteration 948, loss = 0.03367461\n",
      "Iteration 949, loss = 0.03371158\n",
      "Iteration 950, loss = 0.03370587\n",
      "Iteration 951, loss = 0.03350142\n",
      "Iteration 952, loss = 0.03357724\n",
      "Iteration 953, loss = 0.03374855\n",
      "Iteration 954, loss = 0.03328070\n",
      "Iteration 955, loss = 0.03330454\n",
      "Iteration 956, loss = 0.03330546\n",
      "Iteration 957, loss = 0.03295304\n",
      "Iteration 958, loss = 0.03295134\n",
      "Iteration 959, loss = 0.03267881\n",
      "Iteration 960, loss = 0.03275230\n",
      "Iteration 961, loss = 0.03277815\n",
      "Iteration 962, loss = 0.03317047\n",
      "Iteration 963, loss = 0.03245956\n",
      "Iteration 964, loss = 0.03220299\n",
      "Iteration 965, loss = 0.03252396\n",
      "Iteration 966, loss = 0.03248884\n",
      "Iteration 967, loss = 0.03286046\n",
      "Iteration 968, loss = 0.03241348\n",
      "Iteration 969, loss = 0.03174059\n",
      "Iteration 970, loss = 0.03185797\n",
      "Iteration 971, loss = 0.03185085\n",
      "Iteration 972, loss = 0.03153675\n",
      "Iteration 973, loss = 0.03163685\n",
      "Iteration 974, loss = 0.03150926\n",
      "Iteration 975, loss = 0.03139224\n",
      "Iteration 976, loss = 0.03137604\n",
      "Iteration 977, loss = 0.03100908\n",
      "Iteration 978, loss = 0.03113923\n",
      "Iteration 979, loss = 0.03114004\n",
      "Iteration 980, loss = 0.03095279\n",
      "Iteration 981, loss = 0.03077721\n",
      "Iteration 982, loss = 0.03087956\n",
      "Iteration 983, loss = 0.03063483\n",
      "Iteration 984, loss = 0.03047434\n",
      "Iteration 985, loss = 0.03051073\n",
      "Iteration 986, loss = 0.03075368\n",
      "Iteration 987, loss = 0.03061768\n",
      "Iteration 988, loss = 0.03007673\n",
      "Iteration 989, loss = 0.03007172\n",
      "Iteration 990, loss = 0.03009691\n",
      "Iteration 991, loss = 0.02996574\n",
      "Iteration 992, loss = 0.02996374\n",
      "Iteration 993, loss = 0.03026468\n",
      "Iteration 994, loss = 0.03020176\n",
      "Iteration 995, loss = 0.02960954\n",
      "Iteration 996, loss = 0.03001828\n",
      "Iteration 997, loss = 0.02938498\n",
      "Iteration 998, loss = 0.02943198\n",
      "Iteration 999, loss = 0.02929644\n",
      "Iteration 1000, loss = 0.02955331\n",
      "Iteration 1001, loss = 0.02932339\n",
      "Iteration 1002, loss = 0.02929646\n",
      "Iteration 1003, loss = 0.02941280\n",
      "Iteration 1004, loss = 0.02901667\n",
      "Iteration 1005, loss = 0.02915113\n",
      "Iteration 1006, loss = 0.02872978\n",
      "Iteration 1007, loss = 0.02875917\n",
      "Iteration 1008, loss = 0.02910057\n",
      "Iteration 1009, loss = 0.02847258\n",
      "Iteration 1010, loss = 0.02993804\n",
      "Iteration 1011, loss = 0.02848547\n",
      "Iteration 1012, loss = 0.02822967\n",
      "Iteration 1013, loss = 0.02826747\n",
      "Iteration 1014, loss = 0.02829276\n",
      "Iteration 1015, loss = 0.02836782\n",
      "Iteration 1016, loss = 0.02860435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1017, loss = 0.02794366\n",
      "Iteration 1018, loss = 0.02797822\n",
      "Iteration 1019, loss = 0.02774512\n",
      "Iteration 1020, loss = 0.02762594\n",
      "Iteration 1021, loss = 0.02762840\n",
      "Iteration 1022, loss = 0.02740983\n",
      "Iteration 1023, loss = 0.02764161\n",
      "Iteration 1024, loss = 0.02777674\n",
      "Iteration 1025, loss = 0.02718797\n",
      "Iteration 1026, loss = 0.02755478\n",
      "Iteration 1027, loss = 0.02726230\n",
      "Iteration 1028, loss = 0.02713813\n",
      "Iteration 1029, loss = 0.02703453\n",
      "Iteration 1030, loss = 0.02685923\n",
      "Iteration 1031, loss = 0.02677380\n",
      "Iteration 1032, loss = 0.02682462\n",
      "Iteration 1033, loss = 0.02670580\n",
      "Iteration 1034, loss = 0.02650536\n",
      "Iteration 1035, loss = 0.02657207\n",
      "Iteration 1036, loss = 0.02657822\n",
      "Iteration 1037, loss = 0.02672623\n",
      "Iteration 1038, loss = 0.02675031\n",
      "Iteration 1039, loss = 0.02649805\n",
      "Iteration 1040, loss = 0.02637787\n",
      "Iteration 1041, loss = 0.02613251\n",
      "Iteration 1042, loss = 0.02652010\n",
      "Iteration 1043, loss = 0.02635122\n",
      "Iteration 1044, loss = 0.02645750\n",
      "Iteration 1045, loss = 0.02589374\n",
      "Iteration 1046, loss = 0.02578892\n",
      "Iteration 1047, loss = 0.02615658\n",
      "Iteration 1048, loss = 0.02557112\n",
      "Iteration 1049, loss = 0.02569947\n",
      "Iteration 1050, loss = 0.02546125\n",
      "Iteration 1051, loss = 0.02561846\n",
      "Iteration 1052, loss = 0.02528836\n",
      "Iteration 1053, loss = 0.02546114\n",
      "Iteration 1054, loss = 0.02512339\n",
      "Iteration 1055, loss = 0.02502515\n",
      "Iteration 1056, loss = 0.02503831\n",
      "Iteration 1057, loss = 0.02516844\n",
      "Iteration 1058, loss = 0.02522104\n",
      "Iteration 1059, loss = 0.02564802\n",
      "Iteration 1060, loss = 0.02483420\n",
      "Iteration 1061, loss = 0.02461167\n",
      "Iteration 1062, loss = 0.02468941\n",
      "Iteration 1063, loss = 0.02453189\n",
      "Iteration 1064, loss = 0.02468842\n",
      "Iteration 1065, loss = 0.02457502\n",
      "Iteration 1066, loss = 0.02467882\n",
      "Iteration 1067, loss = 0.02566873\n",
      "Iteration 1068, loss = 0.02447279\n",
      "Iteration 1069, loss = 0.02496860\n",
      "Iteration 1070, loss = 0.02448236\n",
      "Iteration 1071, loss = 0.02407919\n",
      "Iteration 1072, loss = 0.02409563\n",
      "Iteration 1073, loss = 0.02394169\n",
      "Iteration 1074, loss = 0.02379616\n",
      "Iteration 1075, loss = 0.02372826\n",
      "Iteration 1076, loss = 0.02396237\n",
      "Iteration 1077, loss = 0.02374387\n",
      "Iteration 1078, loss = 0.02361630\n",
      "Iteration 1079, loss = 0.02408411\n",
      "Iteration 1080, loss = 0.02393229\n",
      "Iteration 1081, loss = 0.02340068\n",
      "Iteration 1082, loss = 0.02341841\n",
      "Iteration 1083, loss = 0.02418662\n",
      "Iteration 1084, loss = 0.02349637\n",
      "Iteration 1085, loss = 0.02382651\n",
      "Iteration 1086, loss = 0.02324199\n",
      "Iteration 1087, loss = 0.02318615\n",
      "Iteration 1088, loss = 0.02288460\n",
      "Iteration 1089, loss = 0.02291580\n",
      "Iteration 1090, loss = 0.02281695\n",
      "Iteration 1091, loss = 0.02288883\n",
      "Iteration 1092, loss = 0.02267846\n",
      "Iteration 1093, loss = 0.02310326\n",
      "Iteration 1094, loss = 0.02315438\n",
      "Iteration 1095, loss = 0.02237932\n",
      "Iteration 1096, loss = 0.02267136\n",
      "Iteration 1097, loss = 0.02254373\n",
      "Iteration 1098, loss = 0.02244190\n",
      "Iteration 1099, loss = 0.02206382\n",
      "Iteration 1100, loss = 0.02220787\n",
      "Iteration 1101, loss = 0.02212477\n",
      "Iteration 1102, loss = 0.02212810\n",
      "Iteration 1103, loss = 0.02231359\n",
      "Iteration 1104, loss = 0.02208458\n",
      "Iteration 1105, loss = 0.02235571\n",
      "Iteration 1106, loss = 0.02177306\n",
      "Iteration 1107, loss = 0.02178389\n",
      "Iteration 1108, loss = 0.02182988\n",
      "Iteration 1109, loss = 0.02157632\n",
      "Iteration 1110, loss = 0.02171290\n",
      "Iteration 1111, loss = 0.02149919\n",
      "Iteration 1112, loss = 0.02165488\n",
      "Iteration 1113, loss = 0.02159829\n",
      "Iteration 1114, loss = 0.02173310\n",
      "Iteration 1115, loss = 0.02165206\n",
      "Iteration 1116, loss = 0.02119906\n",
      "Iteration 1117, loss = 0.02120737\n",
      "Iteration 1118, loss = 0.02128235\n",
      "Iteration 1119, loss = 0.02107260\n",
      "Iteration 1120, loss = 0.02107793\n",
      "Iteration 1121, loss = 0.02130084\n",
      "Iteration 1122, loss = 0.02105865\n",
      "Iteration 1123, loss = 0.02079423\n",
      "Iteration 1124, loss = 0.02081017\n",
      "Iteration 1125, loss = 0.02104235\n",
      "Iteration 1126, loss = 0.02154822\n",
      "Iteration 1127, loss = 0.02116979\n",
      "Iteration 1128, loss = 0.02082720\n",
      "Iteration 1129, loss = 0.02137680\n",
      "Iteration 1130, loss = 0.02079874\n",
      "Iteration 1131, loss = 0.02043569\n",
      "Iteration 1132, loss = 0.02064926\n",
      "Iteration 1133, loss = 0.02050077\n",
      "Iteration 1134, loss = 0.02031153\n",
      "Iteration 1135, loss = 0.02016032\n",
      "Iteration 1136, loss = 0.02019188\n",
      "Iteration 1137, loss = 0.02007578\n",
      "Iteration 1138, loss = 0.02018375\n",
      "Iteration 1139, loss = 0.02000625\n",
      "Iteration 1140, loss = 0.02030787\n",
      "Iteration 1141, loss = 0.01999391\n",
      "Iteration 1142, loss = 0.02040492\n",
      "Iteration 1143, loss = 0.01997418\n",
      "Iteration 1144, loss = 0.01974644\n",
      "Iteration 1145, loss = 0.01971868\n",
      "Iteration 1146, loss = 0.01984223\n",
      "Iteration 1147, loss = 0.01970193\n",
      "Iteration 1148, loss = 0.01946559\n",
      "Iteration 1149, loss = 0.01940626\n",
      "Iteration 1150, loss = 0.01941886\n",
      "Iteration 1151, loss = 0.01929930\n",
      "Iteration 1152, loss = 0.01916950\n",
      "Iteration 1153, loss = 0.01923768\n",
      "Iteration 1154, loss = 0.01912480\n",
      "Iteration 1155, loss = 0.01907690\n",
      "Iteration 1156, loss = 0.01910898\n",
      "Iteration 1157, loss = 0.01907394\n",
      "Iteration 1158, loss = 0.01893040\n",
      "Iteration 1159, loss = 0.01894087\n",
      "Iteration 1160, loss = 0.01886352\n",
      "Iteration 1161, loss = 0.01888950\n",
      "Iteration 1162, loss = 0.01897537\n",
      "Iteration 1163, loss = 0.01881238\n",
      "Iteration 1164, loss = 0.01883097\n",
      "Iteration 1165, loss = 0.01861929\n",
      "Iteration 1166, loss = 0.01868376\n",
      "Iteration 1167, loss = 0.01857788\n",
      "Iteration 1168, loss = 0.01845612\n",
      "Iteration 1169, loss = 0.01849887\n",
      "Iteration 1170, loss = 0.01836459\n",
      "Iteration 1171, loss = 0.01869085\n",
      "Iteration 1172, loss = 0.01839489\n",
      "Iteration 1173, loss = 0.01820751\n",
      "Iteration 1174, loss = 0.01825470\n",
      "Iteration 1175, loss = 0.01817150\n",
      "Iteration 1176, loss = 0.01811789\n",
      "Iteration 1177, loss = 0.01807984\n",
      "Iteration 1178, loss = 0.01804144\n",
      "Iteration 1179, loss = 0.01796737\n",
      "Iteration 1180, loss = 0.01782903\n",
      "Iteration 1181, loss = 0.01783302\n",
      "Iteration 1182, loss = 0.01813479\n",
      "Iteration 1183, loss = 0.01819603\n",
      "Iteration 1184, loss = 0.01803254\n",
      "Iteration 1185, loss = 0.01792830\n",
      "Iteration 1186, loss = 0.01792387\n",
      "Iteration 1187, loss = 0.01757471\n",
      "Iteration 1188, loss = 0.01739609\n",
      "Iteration 1189, loss = 0.01743885\n",
      "Iteration 1190, loss = 0.01760298\n",
      "Iteration 1191, loss = 0.01746766\n",
      "Iteration 1192, loss = 0.01742217\n",
      "Iteration 1193, loss = 0.01718670\n",
      "Iteration 1194, loss = 0.01744681\n",
      "Iteration 1195, loss = 0.01717970\n",
      "Iteration 1196, loss = 0.01704037\n",
      "Iteration 1197, loss = 0.01708064\n",
      "Iteration 1198, loss = 0.01719219\n",
      "Iteration 1199, loss = 0.01696355\n",
      "Iteration 1200, loss = 0.01692788\n",
      "Iteration 1201, loss = 0.01709635\n",
      "Iteration 1202, loss = 0.01717501\n",
      "Iteration 1203, loss = 0.01683396\n",
      "Iteration 1204, loss = 0.01674021\n",
      "Iteration 1205, loss = 0.01688179\n",
      "Iteration 1206, loss = 0.01699045\n",
      "Iteration 1207, loss = 0.01662434\n",
      "Iteration 1208, loss = 0.01646612\n",
      "Iteration 1209, loss = 0.01664424\n",
      "Iteration 1210, loss = 0.01654078\n",
      "Iteration 1211, loss = 0.01636716\n",
      "Iteration 1212, loss = 0.01646286\n",
      "Iteration 1213, loss = 0.01627209\n",
      "Iteration 1214, loss = 0.01635955\n",
      "Iteration 1215, loss = 0.01621217\n",
      "Iteration 1216, loss = 0.01624065\n",
      "Iteration 1217, loss = 0.01609353\n",
      "Iteration 1218, loss = 0.01607895\n",
      "Iteration 1219, loss = 0.01614252\n",
      "Iteration 1220, loss = 0.01609653\n",
      "Iteration 1221, loss = 0.01602415\n",
      "Iteration 1222, loss = 0.01604849\n",
      "Iteration 1223, loss = 0.01568491\n",
      "Iteration 1224, loss = 0.01664698\n",
      "Iteration 1225, loss = 0.01603184\n",
      "Iteration 1226, loss = 0.01582788\n",
      "Iteration 1227, loss = 0.01562983\n",
      "Iteration 1228, loss = 0.01568047\n",
      "Iteration 1229, loss = 0.01569769\n",
      "Iteration 1230, loss = 0.01554496\n",
      "Iteration 1231, loss = 0.01553161\n",
      "Iteration 1232, loss = 0.01540860\n",
      "Iteration 1233, loss = 0.01558792\n",
      "Iteration 1234, loss = 0.01554190\n",
      "Iteration 1235, loss = 0.01560033\n",
      "Iteration 1236, loss = 0.01563415\n",
      "Iteration 1237, loss = 0.01534829\n",
      "Iteration 1238, loss = 0.01534380\n",
      "Iteration 1239, loss = 0.01512527\n",
      "Iteration 1240, loss = 0.01515011\n",
      "Iteration 1241, loss = 0.01521437\n",
      "Iteration 1242, loss = 0.01518305\n",
      "Iteration 1243, loss = 0.01506998\n",
      "Iteration 1244, loss = 0.01515136\n",
      "Iteration 1245, loss = 0.01520815\n",
      "Iteration 1246, loss = 0.01541428\n",
      "Iteration 1247, loss = 0.01497656\n",
      "Iteration 1248, loss = 0.01472387\n",
      "Iteration 1249, loss = 0.01479894\n",
      "Iteration 1250, loss = 0.01460340\n",
      "Iteration 1251, loss = 0.01478471\n",
      "Iteration 1252, loss = 0.01479749\n",
      "Iteration 1253, loss = 0.01481083\n",
      "Iteration 1254, loss = 0.01468790\n",
      "Iteration 1255, loss = 0.01465232\n",
      "Iteration 1256, loss = 0.01452861\n",
      "Iteration 1257, loss = 0.01457100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1258, loss = 0.01482399\n",
      "Iteration 1259, loss = 0.01465208\n",
      "Iteration 1260, loss = 0.01448193\n",
      "Iteration 1261, loss = 0.01436669\n",
      "Iteration 1262, loss = 0.01424246\n",
      "Iteration 1263, loss = 0.01422108\n",
      "Iteration 1264, loss = 0.01430940\n",
      "Iteration 1265, loss = 0.01438812\n",
      "Iteration 1266, loss = 0.01428117\n",
      "Iteration 1267, loss = 0.01411968\n",
      "Iteration 1268, loss = 0.01413295\n",
      "Iteration 1269, loss = 0.01399135\n",
      "Iteration 1270, loss = 0.01399091\n",
      "Iteration 1271, loss = 0.01379933\n",
      "Iteration 1272, loss = 0.01390521\n",
      "Iteration 1273, loss = 0.01382875\n",
      "Iteration 1274, loss = 0.01385322\n",
      "Iteration 1275, loss = 0.01375535\n",
      "Iteration 1276, loss = 0.01369423\n",
      "Iteration 1277, loss = 0.01370187\n",
      "Iteration 1278, loss = 0.01367095\n",
      "Iteration 1279, loss = 0.01379500\n",
      "Iteration 1280, loss = 0.01396424\n",
      "Iteration 1281, loss = 0.01385888\n",
      "Iteration 1282, loss = 0.01359758\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 2 of 48...\n",
      "Iteration 1, loss = 0.68042846\n",
      "Iteration 2, loss = 0.66090221\n",
      "Iteration 3, loss = 0.64404592\n",
      "Iteration 4, loss = 0.62957658\n",
      "Iteration 5, loss = 0.61609208\n",
      "Iteration 6, loss = 0.60372195\n",
      "Iteration 7, loss = 0.59207571\n",
      "Iteration 8, loss = 0.58056472\n",
      "Iteration 9, loss = 0.56956950\n",
      "Iteration 10, loss = 0.55994702\n",
      "Iteration 11, loss = 0.54986864\n",
      "Iteration 12, loss = 0.54030434\n",
      "Iteration 13, loss = 0.53144013\n",
      "Iteration 14, loss = 0.52349808\n",
      "Iteration 15, loss = 0.51536682\n",
      "Iteration 16, loss = 0.50825468\n",
      "Iteration 17, loss = 0.50095604\n",
      "Iteration 18, loss = 0.49443159\n",
      "Iteration 19, loss = 0.48766070\n",
      "Iteration 20, loss = 0.48111226\n",
      "Iteration 21, loss = 0.47532707\n",
      "Iteration 22, loss = 0.46949681\n",
      "Iteration 23, loss = 0.46540471\n",
      "Iteration 24, loss = 0.45958892\n",
      "Iteration 25, loss = 0.45407665\n",
      "Iteration 26, loss = 0.44940619\n",
      "Iteration 27, loss = 0.44496968\n",
      "Iteration 28, loss = 0.44073333\n",
      "Iteration 29, loss = 0.43621544\n",
      "Iteration 30, loss = 0.43330055\n",
      "Iteration 31, loss = 0.42926003\n",
      "Iteration 32, loss = 0.42525842\n",
      "Iteration 33, loss = 0.42212368\n",
      "Iteration 34, loss = 0.41841821\n",
      "Iteration 35, loss = 0.41596492\n",
      "Iteration 36, loss = 0.41251623\n",
      "Iteration 37, loss = 0.40952506\n",
      "Iteration 38, loss = 0.40659892\n",
      "Iteration 39, loss = 0.40406027\n",
      "Iteration 40, loss = 0.40143291\n",
      "Iteration 41, loss = 0.39854860\n",
      "Iteration 42, loss = 0.39586819\n",
      "Iteration 43, loss = 0.39390384\n",
      "Iteration 44, loss = 0.39234992\n",
      "Iteration 45, loss = 0.38936486\n",
      "Iteration 46, loss = 0.38760616\n",
      "Iteration 47, loss = 0.38586045\n",
      "Iteration 48, loss = 0.38376824\n",
      "Iteration 49, loss = 0.38181723\n",
      "Iteration 50, loss = 0.37882113\n",
      "Iteration 51, loss = 0.37758841\n",
      "Iteration 52, loss = 0.37569771\n",
      "Iteration 53, loss = 0.37513006\n",
      "Iteration 54, loss = 0.37311201\n",
      "Iteration 55, loss = 0.37039430\n",
      "Iteration 56, loss = 0.36853280\n",
      "Iteration 57, loss = 0.36658563\n",
      "Iteration 58, loss = 0.36607199\n",
      "Iteration 59, loss = 0.36384354\n",
      "Iteration 60, loss = 0.36207312\n",
      "Iteration 61, loss = 0.36058985\n",
      "Iteration 62, loss = 0.35865974\n",
      "Iteration 63, loss = 0.35760221\n",
      "Iteration 64, loss = 0.35666420\n",
      "Iteration 65, loss = 0.35436738\n",
      "Iteration 66, loss = 0.35315532\n",
      "Iteration 67, loss = 0.35318918\n",
      "Iteration 68, loss = 0.35137391\n",
      "Iteration 69, loss = 0.35121014\n",
      "Iteration 70, loss = 0.34853382\n",
      "Iteration 71, loss = 0.34912439\n",
      "Iteration 72, loss = 0.34812575\n",
      "Iteration 73, loss = 0.34698506\n",
      "Iteration 74, loss = 0.34394922\n",
      "Iteration 75, loss = 0.34499343\n",
      "Iteration 76, loss = 0.34267167\n",
      "Iteration 77, loss = 0.34103851\n",
      "Iteration 78, loss = 0.33916601\n",
      "Iteration 79, loss = 0.33970433\n",
      "Iteration 80, loss = 0.33841202\n",
      "Iteration 81, loss = 0.33611664\n",
      "Iteration 82, loss = 0.33499860\n",
      "Iteration 83, loss = 0.33348390\n",
      "Iteration 84, loss = 0.33238762\n",
      "Iteration 85, loss = 0.33233407\n",
      "Iteration 86, loss = 0.33204878\n",
      "Iteration 87, loss = 0.33247641\n",
      "Iteration 88, loss = 0.32899498\n",
      "Iteration 89, loss = 0.32850122\n",
      "Iteration 90, loss = 0.32810091\n",
      "Iteration 91, loss = 0.32867610\n",
      "Iteration 92, loss = 0.32946591\n",
      "Iteration 93, loss = 0.32504986\n",
      "Iteration 94, loss = 0.32425736\n",
      "Iteration 95, loss = 0.32342593\n",
      "Iteration 96, loss = 0.32258320\n",
      "Iteration 97, loss = 0.32063863\n",
      "Iteration 98, loss = 0.31971943\n",
      "Iteration 99, loss = 0.31921309\n",
      "Iteration 100, loss = 0.31834624\n",
      "Iteration 101, loss = 0.31850060\n",
      "Iteration 102, loss = 0.31757430\n",
      "Iteration 103, loss = 0.31563549\n",
      "Iteration 104, loss = 0.31637489\n",
      "Iteration 105, loss = 0.31401149\n",
      "Iteration 106, loss = 0.31329246\n",
      "Iteration 107, loss = 0.31470773\n",
      "Iteration 108, loss = 0.31279756\n",
      "Iteration 109, loss = 0.31204809\n",
      "Iteration 110, loss = 0.31030422\n",
      "Iteration 111, loss = 0.30911620\n",
      "Iteration 112, loss = 0.30905791\n",
      "Iteration 113, loss = 0.30904149\n",
      "Iteration 114, loss = 0.30733660\n",
      "Iteration 115, loss = 0.30769535\n",
      "Iteration 116, loss = 0.30584323\n",
      "Iteration 117, loss = 0.30532601\n",
      "Iteration 118, loss = 0.30464693\n",
      "Iteration 119, loss = 0.30377964\n",
      "Iteration 120, loss = 0.30276156\n",
      "Iteration 121, loss = 0.30222312\n",
      "Iteration 122, loss = 0.30186469\n",
      "Iteration 123, loss = 0.30046082\n",
      "Iteration 124, loss = 0.30177875\n",
      "Iteration 125, loss = 0.29920320\n",
      "Iteration 126, loss = 0.29869283\n",
      "Iteration 127, loss = 0.29797163\n",
      "Iteration 128, loss = 0.29699868\n",
      "Iteration 129, loss = 0.29656294\n",
      "Iteration 130, loss = 0.29550797\n",
      "Iteration 131, loss = 0.29505413\n",
      "Iteration 132, loss = 0.29541275\n",
      "Iteration 133, loss = 0.29343303\n",
      "Iteration 134, loss = 0.29372310\n",
      "Iteration 135, loss = 0.29235489\n",
      "Iteration 136, loss = 0.29136652\n",
      "Iteration 137, loss = 0.29198553\n",
      "Iteration 138, loss = 0.29071950\n",
      "Iteration 139, loss = 0.29028363\n",
      "Iteration 140, loss = 0.28896073\n",
      "Iteration 141, loss = 0.28919028\n",
      "Iteration 142, loss = 0.28914485\n",
      "Iteration 143, loss = 0.28836392\n",
      "Iteration 144, loss = 0.28713565\n",
      "Iteration 145, loss = 0.28641960\n",
      "Iteration 146, loss = 0.28525961\n",
      "Iteration 147, loss = 0.28469193\n",
      "Iteration 148, loss = 0.28366907\n",
      "Iteration 149, loss = 0.28388352\n",
      "Iteration 150, loss = 0.28323037\n",
      "Iteration 151, loss = 0.28247844\n",
      "Iteration 152, loss = 0.28180724\n",
      "Iteration 153, loss = 0.28096539\n",
      "Iteration 154, loss = 0.28002416\n",
      "Iteration 155, loss = 0.27962227\n",
      "Iteration 156, loss = 0.27905480\n",
      "Iteration 157, loss = 0.28061414\n",
      "Iteration 158, loss = 0.27851891\n",
      "Iteration 159, loss = 0.27776262\n",
      "Iteration 160, loss = 0.27716145\n",
      "Iteration 161, loss = 0.27670344\n",
      "Iteration 162, loss = 0.27585801\n",
      "Iteration 163, loss = 0.27486708\n",
      "Iteration 164, loss = 0.27463055\n",
      "Iteration 165, loss = 0.27367659\n",
      "Iteration 166, loss = 0.27335303\n",
      "Iteration 167, loss = 0.27336752\n",
      "Iteration 168, loss = 0.27227615\n",
      "Iteration 169, loss = 0.27142312\n",
      "Iteration 170, loss = 0.27171179\n",
      "Iteration 171, loss = 0.27070522\n",
      "Iteration 172, loss = 0.26960880\n",
      "Iteration 173, loss = 0.26961745\n",
      "Iteration 174, loss = 0.26833786\n",
      "Iteration 175, loss = 0.26817475\n",
      "Iteration 176, loss = 0.26779609\n",
      "Iteration 177, loss = 0.26756610\n",
      "Iteration 178, loss = 0.26579401\n",
      "Iteration 179, loss = 0.26633435\n",
      "Iteration 180, loss = 0.26531951\n",
      "Iteration 181, loss = 0.26552672\n",
      "Iteration 182, loss = 0.26407475\n",
      "Iteration 183, loss = 0.26323452\n",
      "Iteration 184, loss = 0.26250862\n",
      "Iteration 185, loss = 0.26262896\n",
      "Iteration 186, loss = 0.26216009\n",
      "Iteration 187, loss = 0.26126134\n",
      "Iteration 188, loss = 0.26142366\n",
      "Iteration 189, loss = 0.25976529\n",
      "Iteration 190, loss = 0.25946711\n",
      "Iteration 191, loss = 0.25946385\n",
      "Iteration 192, loss = 0.25823325\n",
      "Iteration 193, loss = 0.25769113\n",
      "Iteration 194, loss = 0.25804321\n",
      "Iteration 195, loss = 0.25963809\n",
      "Iteration 196, loss = 0.25723909\n",
      "Iteration 197, loss = 0.25591462\n",
      "Iteration 198, loss = 0.25524212\n",
      "Iteration 199, loss = 0.25604543\n",
      "Iteration 200, loss = 0.25511360\n",
      "Iteration 201, loss = 0.25510706\n",
      "Iteration 202, loss = 0.25320111\n",
      "Iteration 203, loss = 0.25295127\n",
      "Iteration 204, loss = 0.25196444\n",
      "Iteration 205, loss = 0.25148213\n",
      "Iteration 206, loss = 0.25122627\n",
      "Iteration 207, loss = 0.25029771\n",
      "Iteration 208, loss = 0.25010644\n",
      "Iteration 209, loss = 0.24927792\n",
      "Iteration 210, loss = 0.24905626\n",
      "Iteration 211, loss = 0.24945596\n",
      "Iteration 212, loss = 0.24823074\n",
      "Iteration 213, loss = 0.24750287\n",
      "Iteration 214, loss = 0.24678533\n",
      "Iteration 215, loss = 0.24683680\n",
      "Iteration 216, loss = 0.24678196\n",
      "Iteration 217, loss = 0.24661048\n",
      "Iteration 218, loss = 0.24539496\n",
      "Iteration 219, loss = 0.24515588\n",
      "Iteration 220, loss = 0.24498673\n",
      "Iteration 221, loss = 0.24399999\n",
      "Iteration 222, loss = 0.24250005\n",
      "Iteration 223, loss = 0.24253044\n",
      "Iteration 224, loss = 0.24165563\n",
      "Iteration 225, loss = 0.24134931\n",
      "Iteration 226, loss = 0.24030125\n",
      "Iteration 227, loss = 0.24056413\n",
      "Iteration 228, loss = 0.23952630\n",
      "Iteration 229, loss = 0.23966224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 230, loss = 0.23933934\n",
      "Iteration 231, loss = 0.23812248\n",
      "Iteration 232, loss = 0.23802954\n",
      "Iteration 233, loss = 0.23690947\n",
      "Iteration 234, loss = 0.23656779\n",
      "Iteration 235, loss = 0.23758090\n",
      "Iteration 236, loss = 0.23646360\n",
      "Iteration 237, loss = 0.23638766\n",
      "Iteration 238, loss = 0.23526668\n",
      "Iteration 239, loss = 0.23433581\n",
      "Iteration 240, loss = 0.23400935\n",
      "Iteration 241, loss = 0.23343139\n",
      "Iteration 242, loss = 0.23280037\n",
      "Iteration 243, loss = 0.23335884\n",
      "Iteration 244, loss = 0.23267194\n",
      "Iteration 245, loss = 0.23181469\n",
      "Iteration 246, loss = 0.23008496\n",
      "Iteration 247, loss = 0.23036301\n",
      "Iteration 248, loss = 0.22934712\n",
      "Iteration 249, loss = 0.22870211\n",
      "Iteration 250, loss = 0.22841145\n",
      "Iteration 251, loss = 0.22907010\n",
      "Iteration 252, loss = 0.22982917\n",
      "Iteration 253, loss = 0.22793272\n",
      "Iteration 254, loss = 0.22724436\n",
      "Iteration 255, loss = 0.22653176\n",
      "Iteration 256, loss = 0.22549566\n",
      "Iteration 257, loss = 0.22625123\n",
      "Iteration 258, loss = 0.22513767\n",
      "Iteration 259, loss = 0.22416308\n",
      "Iteration 260, loss = 0.22424819\n",
      "Iteration 261, loss = 0.22301754\n",
      "Iteration 262, loss = 0.22250494\n",
      "Iteration 263, loss = 0.22266617\n",
      "Iteration 264, loss = 0.22175605\n",
      "Iteration 265, loss = 0.22155256\n",
      "Iteration 266, loss = 0.22046995\n",
      "Iteration 267, loss = 0.22004335\n",
      "Iteration 268, loss = 0.21954609\n",
      "Iteration 269, loss = 0.21898825\n",
      "Iteration 270, loss = 0.21958439\n",
      "Iteration 271, loss = 0.22049654\n",
      "Iteration 272, loss = 0.21898839\n",
      "Iteration 273, loss = 0.21845484\n",
      "Iteration 274, loss = 0.21821229\n",
      "Iteration 275, loss = 0.21664986\n",
      "Iteration 276, loss = 0.21785192\n",
      "Iteration 277, loss = 0.21693251\n",
      "Iteration 278, loss = 0.21497260\n",
      "Iteration 279, loss = 0.21519816\n",
      "Iteration 280, loss = 0.21415994\n",
      "Iteration 281, loss = 0.21279717\n",
      "Iteration 282, loss = 0.21472377\n",
      "Iteration 283, loss = 0.21256092\n",
      "Iteration 284, loss = 0.21251513\n",
      "Iteration 285, loss = 0.21137270\n",
      "Iteration 286, loss = 0.21293432\n",
      "Iteration 287, loss = 0.21099638\n",
      "Iteration 288, loss = 0.21040533\n",
      "Iteration 289, loss = 0.20958026\n",
      "Iteration 290, loss = 0.21000019\n",
      "Iteration 291, loss = 0.20896682\n",
      "Iteration 292, loss = 0.20826938\n",
      "Iteration 293, loss = 0.20787211\n",
      "Iteration 294, loss = 0.20720972\n",
      "Iteration 295, loss = 0.20698607\n",
      "Iteration 296, loss = 0.20632714\n",
      "Iteration 297, loss = 0.20634928\n",
      "Iteration 298, loss = 0.20668464\n",
      "Iteration 299, loss = 0.20515054\n",
      "Iteration 300, loss = 0.20499121\n",
      "Iteration 301, loss = 0.20463944\n",
      "Iteration 302, loss = 0.20367177\n",
      "Iteration 303, loss = 0.20374986\n",
      "Iteration 304, loss = 0.20273347\n",
      "Iteration 305, loss = 0.20306672\n",
      "Iteration 306, loss = 0.20244675\n",
      "Iteration 307, loss = 0.20124147\n",
      "Iteration 308, loss = 0.20058293\n",
      "Iteration 309, loss = 0.20082021\n",
      "Iteration 310, loss = 0.19962682\n",
      "Iteration 311, loss = 0.20052081\n",
      "Iteration 312, loss = 0.19996174\n",
      "Iteration 313, loss = 0.20096441\n",
      "Iteration 314, loss = 0.19886294\n",
      "Iteration 315, loss = 0.19798309\n",
      "Iteration 316, loss = 0.19734434\n",
      "Iteration 317, loss = 0.19672365\n",
      "Iteration 318, loss = 0.19573213\n",
      "Iteration 319, loss = 0.19523051\n",
      "Iteration 320, loss = 0.19498424\n",
      "Iteration 321, loss = 0.19438678\n",
      "Iteration 322, loss = 0.19361686\n",
      "Iteration 323, loss = 0.19326672\n",
      "Iteration 324, loss = 0.19327382\n",
      "Iteration 325, loss = 0.19234797\n",
      "Iteration 326, loss = 0.19293084\n",
      "Iteration 327, loss = 0.19172525\n",
      "Iteration 328, loss = 0.19084685\n",
      "Iteration 329, loss = 0.19052034\n",
      "Iteration 330, loss = 0.18969981\n",
      "Iteration 331, loss = 0.19011360\n",
      "Iteration 332, loss = 0.18996027\n",
      "Iteration 333, loss = 0.19058257\n",
      "Iteration 334, loss = 0.18784007\n",
      "Iteration 335, loss = 0.18833869\n",
      "Iteration 336, loss = 0.18776081\n",
      "Iteration 337, loss = 0.18739374\n",
      "Iteration 338, loss = 0.18708823\n",
      "Iteration 339, loss = 0.18617333\n",
      "Iteration 340, loss = 0.18556589\n",
      "Iteration 341, loss = 0.18691946\n",
      "Iteration 342, loss = 0.18514272\n",
      "Iteration 343, loss = 0.18434540\n",
      "Iteration 344, loss = 0.18411255\n",
      "Iteration 345, loss = 0.18346209\n",
      "Iteration 346, loss = 0.18395120\n",
      "Iteration 347, loss = 0.18310725\n",
      "Iteration 348, loss = 0.18274105\n",
      "Iteration 349, loss = 0.18280175\n",
      "Iteration 350, loss = 0.18165528\n",
      "Iteration 351, loss = 0.18148488\n",
      "Iteration 352, loss = 0.18073254\n",
      "Iteration 353, loss = 0.18117211\n",
      "Iteration 354, loss = 0.18253495\n",
      "Iteration 355, loss = 0.18054226\n",
      "Iteration 356, loss = 0.17899211\n",
      "Iteration 357, loss = 0.17854355\n",
      "Iteration 358, loss = 0.17814977\n",
      "Iteration 359, loss = 0.17754921\n",
      "Iteration 360, loss = 0.17717390\n",
      "Iteration 361, loss = 0.17660980\n",
      "Iteration 362, loss = 0.17701781\n",
      "Iteration 363, loss = 0.17627118\n",
      "Iteration 364, loss = 0.17675466\n",
      "Iteration 365, loss = 0.17628580\n",
      "Iteration 366, loss = 0.17488428\n",
      "Iteration 367, loss = 0.17409464\n",
      "Iteration 368, loss = 0.17548625\n",
      "Iteration 369, loss = 0.17442556\n",
      "Iteration 370, loss = 0.17416628\n",
      "Iteration 371, loss = 0.17556717\n",
      "Iteration 372, loss = 0.17471280\n",
      "Iteration 373, loss = 0.17311390\n",
      "Iteration 374, loss = 0.17304661\n",
      "Iteration 375, loss = 0.17162522\n",
      "Iteration 376, loss = 0.17040480\n",
      "Iteration 377, loss = 0.17012338\n",
      "Iteration 378, loss = 0.17041275\n",
      "Iteration 379, loss = 0.17154911\n",
      "Iteration 380, loss = 0.16935073\n",
      "Iteration 381, loss = 0.16891430\n",
      "Iteration 382, loss = 0.16855563\n",
      "Iteration 383, loss = 0.16842180\n",
      "Iteration 384, loss = 0.16831296\n",
      "Iteration 385, loss = 0.16751938\n",
      "Iteration 386, loss = 0.16663005\n",
      "Iteration 387, loss = 0.16642090\n",
      "Iteration 388, loss = 0.16568863\n",
      "Iteration 389, loss = 0.16560431\n",
      "Iteration 390, loss = 0.16507929\n",
      "Iteration 391, loss = 0.16542211\n",
      "Iteration 392, loss = 0.16471483\n",
      "Iteration 393, loss = 0.16388780\n",
      "Iteration 394, loss = 0.16398213\n",
      "Iteration 395, loss = 0.16327969\n",
      "Iteration 396, loss = 0.16335957\n",
      "Iteration 397, loss = 0.16602190\n",
      "Iteration 398, loss = 0.16289234\n",
      "Iteration 399, loss = 0.16318870\n",
      "Iteration 400, loss = 0.16307056\n",
      "Iteration 401, loss = 0.16138033\n",
      "Iteration 402, loss = 0.16196803\n",
      "Iteration 403, loss = 0.16211488\n",
      "Iteration 404, loss = 0.16053541\n",
      "Iteration 405, loss = 0.15965870\n",
      "Iteration 406, loss = 0.15893585\n",
      "Iteration 407, loss = 0.15866272\n",
      "Iteration 408, loss = 0.15842596\n",
      "Iteration 409, loss = 0.15854999\n",
      "Iteration 410, loss = 0.15874357\n",
      "Iteration 411, loss = 0.15664863\n",
      "Iteration 412, loss = 0.15713382\n",
      "Iteration 413, loss = 0.15667239\n",
      "Iteration 414, loss = 0.15549621\n",
      "Iteration 415, loss = 0.15519387\n",
      "Iteration 416, loss = 0.15532798\n",
      "Iteration 417, loss = 0.15489318\n",
      "Iteration 418, loss = 0.15531275\n",
      "Iteration 419, loss = 0.15495413\n",
      "Iteration 420, loss = 0.15470669\n",
      "Iteration 421, loss = 0.15355216\n",
      "Iteration 422, loss = 0.15271474\n",
      "Iteration 423, loss = 0.15266068\n",
      "Iteration 424, loss = 0.15232292\n",
      "Iteration 425, loss = 0.15254101\n",
      "Iteration 426, loss = 0.15229736\n",
      "Iteration 427, loss = 0.15217940\n",
      "Iteration 428, loss = 0.15139000\n",
      "Iteration 429, loss = 0.15118684\n",
      "Iteration 430, loss = 0.14966990\n",
      "Iteration 431, loss = 0.14925411\n",
      "Iteration 432, loss = 0.14957621\n",
      "Iteration 433, loss = 0.14876285\n",
      "Iteration 434, loss = 0.14898010\n",
      "Iteration 435, loss = 0.14999541\n",
      "Iteration 436, loss = 0.14921817\n",
      "Iteration 437, loss = 0.14778932\n",
      "Iteration 438, loss = 0.14742513\n",
      "Iteration 439, loss = 0.14743797\n",
      "Iteration 440, loss = 0.14610901\n",
      "Iteration 441, loss = 0.14632329\n",
      "Iteration 442, loss = 0.14673875\n",
      "Iteration 443, loss = 0.14566501\n",
      "Iteration 444, loss = 0.14496561\n",
      "Iteration 445, loss = 0.14467633\n",
      "Iteration 446, loss = 0.14417256\n",
      "Iteration 447, loss = 0.14363661\n",
      "Iteration 448, loss = 0.14391978\n",
      "Iteration 449, loss = 0.14357015\n",
      "Iteration 450, loss = 0.14328785\n",
      "Iteration 451, loss = 0.14348556\n",
      "Iteration 452, loss = 0.14262118\n",
      "Iteration 453, loss = 0.14400398\n",
      "Iteration 454, loss = 0.14390817\n",
      "Iteration 455, loss = 0.14412379\n",
      "Iteration 456, loss = 0.14383936\n",
      "Iteration 457, loss = 0.14178051\n",
      "Iteration 458, loss = 0.14034284\n",
      "Iteration 459, loss = 0.14021383\n",
      "Iteration 460, loss = 0.13977384\n",
      "Iteration 461, loss = 0.14030078\n",
      "Iteration 462, loss = 0.13921090\n",
      "Iteration 463, loss = 0.13814230\n",
      "Iteration 464, loss = 0.13909783\n",
      "Iteration 465, loss = 0.13806726\n",
      "Iteration 466, loss = 0.14003078\n",
      "Iteration 467, loss = 0.13853318\n",
      "Iteration 468, loss = 0.13827732\n",
      "Iteration 469, loss = 0.13685284\n",
      "Iteration 470, loss = 0.13710303\n",
      "Iteration 471, loss = 0.13629018\n",
      "Iteration 472, loss = 0.13650782\n",
      "Iteration 473, loss = 0.13545558\n",
      "Iteration 474, loss = 0.13561714\n",
      "Iteration 475, loss = 0.13511859\n",
      "Iteration 476, loss = 0.13465951\n",
      "Iteration 477, loss = 0.13427521\n",
      "Iteration 478, loss = 0.13433833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 479, loss = 0.13469042\n",
      "Iteration 480, loss = 0.13415564\n",
      "Iteration 481, loss = 0.13282801\n",
      "Iteration 482, loss = 0.13297264\n",
      "Iteration 483, loss = 0.13298366\n",
      "Iteration 484, loss = 0.13181339\n",
      "Iteration 485, loss = 0.13178160\n",
      "Iteration 486, loss = 0.13144967\n",
      "Iteration 487, loss = 0.13191024\n",
      "Iteration 488, loss = 0.13120045\n",
      "Iteration 489, loss = 0.13121894\n",
      "Iteration 490, loss = 0.13152672\n",
      "Iteration 491, loss = 0.13096821\n",
      "Iteration 492, loss = 0.12978837\n",
      "Iteration 493, loss = 0.12901977\n",
      "Iteration 494, loss = 0.12881920\n",
      "Iteration 495, loss = 0.12995327\n",
      "Iteration 496, loss = 0.12896271\n",
      "Iteration 497, loss = 0.12862924\n",
      "Iteration 498, loss = 0.12781186\n",
      "Iteration 499, loss = 0.12745449\n",
      "Iteration 500, loss = 0.12694759\n",
      "Iteration 501, loss = 0.12681203\n",
      "Iteration 502, loss = 0.12825625\n",
      "Iteration 503, loss = 0.12686447\n",
      "Iteration 504, loss = 0.12647075\n",
      "Iteration 505, loss = 0.12712816\n",
      "Iteration 506, loss = 0.12662351\n",
      "Iteration 507, loss = 0.12631709\n",
      "Iteration 508, loss = 0.12513403\n",
      "Iteration 509, loss = 0.12623489\n",
      "Iteration 510, loss = 0.12442730\n",
      "Iteration 511, loss = 0.12400335\n",
      "Iteration 512, loss = 0.12351230\n",
      "Iteration 513, loss = 0.12412159\n",
      "Iteration 514, loss = 0.12423846\n",
      "Iteration 515, loss = 0.12352232\n",
      "Iteration 516, loss = 0.12314595\n",
      "Iteration 517, loss = 0.12171342\n",
      "Iteration 518, loss = 0.12157707\n",
      "Iteration 519, loss = 0.12135140\n",
      "Iteration 520, loss = 0.12089901\n",
      "Iteration 521, loss = 0.12237448\n",
      "Iteration 522, loss = 0.12156855\n",
      "Iteration 523, loss = 0.12079674\n",
      "Iteration 524, loss = 0.12069245\n",
      "Iteration 525, loss = 0.12006422\n",
      "Iteration 526, loss = 0.11964849\n",
      "Iteration 527, loss = 0.11921286\n",
      "Iteration 528, loss = 0.11894590\n",
      "Iteration 529, loss = 0.11872143\n",
      "Iteration 530, loss = 0.11834388\n",
      "Iteration 531, loss = 0.11864942\n",
      "Iteration 532, loss = 0.11826873\n",
      "Iteration 533, loss = 0.11852675\n",
      "Iteration 534, loss = 0.11784596\n",
      "Iteration 535, loss = 0.11707738\n",
      "Iteration 536, loss = 0.11658575\n",
      "Iteration 537, loss = 0.11656266\n",
      "Iteration 538, loss = 0.11631222\n",
      "Iteration 539, loss = 0.11600090\n",
      "Iteration 540, loss = 0.11587338\n",
      "Iteration 541, loss = 0.11522344\n",
      "Iteration 542, loss = 0.11506143\n",
      "Iteration 543, loss = 0.11511565\n",
      "Iteration 544, loss = 0.11441208\n",
      "Iteration 545, loss = 0.11456346\n",
      "Iteration 546, loss = 0.11377267\n",
      "Iteration 547, loss = 0.11393193\n",
      "Iteration 548, loss = 0.11347591\n",
      "Iteration 549, loss = 0.11318743\n",
      "Iteration 550, loss = 0.11273928\n",
      "Iteration 551, loss = 0.11231122\n",
      "Iteration 552, loss = 0.11226618\n",
      "Iteration 553, loss = 0.11253736\n",
      "Iteration 554, loss = 0.11235828\n",
      "Iteration 555, loss = 0.11202835\n",
      "Iteration 556, loss = 0.11124939\n",
      "Iteration 557, loss = 0.11093153\n",
      "Iteration 558, loss = 0.11178582\n",
      "Iteration 559, loss = 0.11074455\n",
      "Iteration 560, loss = 0.11034945\n",
      "Iteration 561, loss = 0.11042994\n",
      "Iteration 562, loss = 0.11097323\n",
      "Iteration 563, loss = 0.10959892\n",
      "Iteration 564, loss = 0.10928133\n",
      "Iteration 565, loss = 0.10836631\n",
      "Iteration 566, loss = 0.10903239\n",
      "Iteration 567, loss = 0.10886114\n",
      "Iteration 568, loss = 0.10786153\n",
      "Iteration 569, loss = 0.10794311\n",
      "Iteration 570, loss = 0.10745439\n",
      "Iteration 571, loss = 0.10711409\n",
      "Iteration 572, loss = 0.10694204\n",
      "Iteration 573, loss = 0.10665171\n",
      "Iteration 574, loss = 0.10609524\n",
      "Iteration 575, loss = 0.10617698\n",
      "Iteration 576, loss = 0.10564416\n",
      "Iteration 577, loss = 0.10596007\n",
      "Iteration 578, loss = 0.10570172\n",
      "Iteration 579, loss = 0.10578899\n",
      "Iteration 580, loss = 0.10658018\n",
      "Iteration 581, loss = 0.10470943\n",
      "Iteration 582, loss = 0.10452989\n",
      "Iteration 583, loss = 0.10419973\n",
      "Iteration 584, loss = 0.10398626\n",
      "Iteration 585, loss = 0.10469860\n",
      "Iteration 586, loss = 0.10412344\n",
      "Iteration 587, loss = 0.10320950\n",
      "Iteration 588, loss = 0.10308059\n",
      "Iteration 589, loss = 0.10280732\n",
      "Iteration 590, loss = 0.10222384\n",
      "Iteration 591, loss = 0.10200944\n",
      "Iteration 592, loss = 0.10196962\n",
      "Iteration 593, loss = 0.10191483\n",
      "Iteration 594, loss = 0.10157332\n",
      "Iteration 595, loss = 0.10140157\n",
      "Iteration 596, loss = 0.10101858\n",
      "Iteration 597, loss = 0.10065332\n",
      "Iteration 598, loss = 0.10049909\n",
      "Iteration 599, loss = 0.09987334\n",
      "Iteration 600, loss = 0.10012626\n",
      "Iteration 601, loss = 0.09969298\n",
      "Iteration 602, loss = 0.09946525\n",
      "Iteration 603, loss = 0.09894213\n",
      "Iteration 604, loss = 0.09881125\n",
      "Iteration 605, loss = 0.09831274\n",
      "Iteration 606, loss = 0.09824073\n",
      "Iteration 607, loss = 0.09789695\n",
      "Iteration 608, loss = 0.09785187\n",
      "Iteration 609, loss = 0.09749296\n",
      "Iteration 610, loss = 0.09811337\n",
      "Iteration 611, loss = 0.09734731\n",
      "Iteration 612, loss = 0.09678652\n",
      "Iteration 613, loss = 0.09651985\n",
      "Iteration 614, loss = 0.09593863\n",
      "Iteration 615, loss = 0.09719238\n",
      "Iteration 616, loss = 0.09639823\n",
      "Iteration 617, loss = 0.09526451\n",
      "Iteration 618, loss = 0.09585900\n",
      "Iteration 619, loss = 0.09511232\n",
      "Iteration 620, loss = 0.09549525\n",
      "Iteration 621, loss = 0.09521000\n",
      "Iteration 622, loss = 0.09451752\n",
      "Iteration 623, loss = 0.09520763\n",
      "Iteration 624, loss = 0.09487356\n",
      "Iteration 625, loss = 0.09384564\n",
      "Iteration 626, loss = 0.09338235\n",
      "Iteration 627, loss = 0.09353457\n",
      "Iteration 628, loss = 0.09285058\n",
      "Iteration 629, loss = 0.09310623\n",
      "Iteration 630, loss = 0.09260999\n",
      "Iteration 631, loss = 0.09206553\n",
      "Iteration 632, loss = 0.09224290\n",
      "Iteration 633, loss = 0.09187144\n",
      "Iteration 634, loss = 0.09173757\n",
      "Iteration 635, loss = 0.09192288\n",
      "Iteration 636, loss = 0.09125684\n",
      "Iteration 637, loss = 0.09166888\n",
      "Iteration 638, loss = 0.09107714\n",
      "Iteration 639, loss = 0.09142769\n",
      "Iteration 640, loss = 0.09054494\n",
      "Iteration 641, loss = 0.09005470\n",
      "Iteration 642, loss = 0.09038995\n",
      "Iteration 643, loss = 0.09015306\n",
      "Iteration 644, loss = 0.09004532\n",
      "Iteration 645, loss = 0.08978134\n",
      "Iteration 646, loss = 0.08918509\n",
      "Iteration 647, loss = 0.08904105\n",
      "Iteration 648, loss = 0.08928896\n",
      "Iteration 649, loss = 0.08892708\n",
      "Iteration 650, loss = 0.08895961\n",
      "Iteration 651, loss = 0.08832698\n",
      "Iteration 652, loss = 0.08825774\n",
      "Iteration 653, loss = 0.08775683\n",
      "Iteration 654, loss = 0.09124539\n",
      "Iteration 655, loss = 0.09021866\n",
      "Iteration 656, loss = 0.08791521\n",
      "Iteration 657, loss = 0.08736441\n",
      "Iteration 658, loss = 0.08644816\n",
      "Iteration 659, loss = 0.08633316\n",
      "Iteration 660, loss = 0.08676453\n",
      "Iteration 661, loss = 0.08638644\n",
      "Iteration 662, loss = 0.08639957\n",
      "Iteration 663, loss = 0.08573092\n",
      "Iteration 664, loss = 0.08659840\n",
      "Iteration 665, loss = 0.08598207\n",
      "Iteration 666, loss = 0.08570374\n",
      "Iteration 667, loss = 0.08476880\n",
      "Iteration 668, loss = 0.08494679\n",
      "Iteration 669, loss = 0.08438009\n",
      "Iteration 670, loss = 0.08421482\n",
      "Iteration 671, loss = 0.08354130\n",
      "Iteration 672, loss = 0.08363206\n",
      "Iteration 673, loss = 0.08280312\n",
      "Iteration 674, loss = 0.08254566\n",
      "Iteration 675, loss = 0.08271467\n",
      "Iteration 676, loss = 0.08242926\n",
      "Iteration 677, loss = 0.08224405\n",
      "Iteration 678, loss = 0.08159175\n",
      "Iteration 679, loss = 0.08157165\n",
      "Iteration 680, loss = 0.08167163\n",
      "Iteration 681, loss = 0.08218939\n",
      "Iteration 682, loss = 0.08148686\n",
      "Iteration 683, loss = 0.08080188\n",
      "Iteration 684, loss = 0.08061338\n",
      "Iteration 685, loss = 0.08039032\n",
      "Iteration 686, loss = 0.08020524\n",
      "Iteration 687, loss = 0.08027707\n",
      "Iteration 688, loss = 0.08062868\n",
      "Iteration 689, loss = 0.08019659\n",
      "Iteration 690, loss = 0.07991134\n",
      "Iteration 691, loss = 0.08079217\n",
      "Iteration 692, loss = 0.08003070\n",
      "Iteration 693, loss = 0.07981180\n",
      "Iteration 694, loss = 0.07884541\n",
      "Iteration 695, loss = 0.07872052\n",
      "Iteration 696, loss = 0.07858065\n",
      "Iteration 697, loss = 0.07815134\n",
      "Iteration 698, loss = 0.07796949\n",
      "Iteration 699, loss = 0.07872115\n",
      "Iteration 700, loss = 0.07805127\n",
      "Iteration 701, loss = 0.07736820\n",
      "Iteration 702, loss = 0.07855691\n",
      "Iteration 703, loss = 0.07738945\n",
      "Iteration 704, loss = 0.07687921\n",
      "Iteration 705, loss = 0.07668605\n",
      "Iteration 706, loss = 0.07707174\n",
      "Iteration 707, loss = 0.07650135\n",
      "Iteration 708, loss = 0.07676941\n",
      "Iteration 709, loss = 0.07735033\n",
      "Iteration 710, loss = 0.07592781\n",
      "Iteration 711, loss = 0.07550633\n",
      "Iteration 712, loss = 0.07553248\n",
      "Iteration 713, loss = 0.07581024\n",
      "Iteration 714, loss = 0.07524253\n",
      "Iteration 715, loss = 0.07513188\n",
      "Iteration 716, loss = 0.07482608\n",
      "Iteration 717, loss = 0.07470119\n",
      "Iteration 718, loss = 0.07436421\n",
      "Iteration 719, loss = 0.07458222\n",
      "Iteration 720, loss = 0.07395479\n",
      "Iteration 721, loss = 0.07367042\n",
      "Iteration 722, loss = 0.07394066\n",
      "Iteration 723, loss = 0.07358252\n",
      "Iteration 724, loss = 0.07362405\n",
      "Iteration 725, loss = 0.07345769\n",
      "Iteration 726, loss = 0.07315773\n",
      "Iteration 727, loss = 0.07273589\n",
      "Iteration 728, loss = 0.07297216\n",
      "Iteration 729, loss = 0.07343741\n",
      "Iteration 730, loss = 0.07253123\n",
      "Iteration 731, loss = 0.07206646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 732, loss = 0.07198320\n",
      "Iteration 733, loss = 0.07259800\n",
      "Iteration 734, loss = 0.07146688\n",
      "Iteration 735, loss = 0.07200475\n",
      "Iteration 736, loss = 0.07151066\n",
      "Iteration 737, loss = 0.07118377\n",
      "Iteration 738, loss = 0.07149230\n",
      "Iteration 739, loss = 0.07103682\n",
      "Iteration 740, loss = 0.07085142\n",
      "Iteration 741, loss = 0.07070897\n",
      "Iteration 742, loss = 0.07080117\n",
      "Iteration 743, loss = 0.07068319\n",
      "Iteration 744, loss = 0.07012024\n",
      "Iteration 745, loss = 0.06960258\n",
      "Iteration 746, loss = 0.06998516\n",
      "Iteration 747, loss = 0.07117108\n",
      "Iteration 748, loss = 0.07007812\n",
      "Iteration 749, loss = 0.06938984\n",
      "Iteration 750, loss = 0.06998981\n",
      "Iteration 751, loss = 0.06925649\n",
      "Iteration 752, loss = 0.06866227\n",
      "Iteration 753, loss = 0.06880762\n",
      "Iteration 754, loss = 0.06849950\n",
      "Iteration 755, loss = 0.06821340\n",
      "Iteration 756, loss = 0.06815537\n",
      "Iteration 757, loss = 0.06823753\n",
      "Iteration 758, loss = 0.06814259\n",
      "Iteration 759, loss = 0.06769594\n",
      "Iteration 760, loss = 0.06770068\n",
      "Iteration 761, loss = 0.06826771\n",
      "Iteration 762, loss = 0.06832651\n",
      "Iteration 763, loss = 0.06742064\n",
      "Iteration 764, loss = 0.06813038\n",
      "Iteration 765, loss = 0.06649113\n",
      "Iteration 766, loss = 0.06683295\n",
      "Iteration 767, loss = 0.06606818\n",
      "Iteration 768, loss = 0.06689603\n",
      "Iteration 769, loss = 0.06702267\n",
      "Iteration 770, loss = 0.06693342\n",
      "Iteration 771, loss = 0.06595183\n",
      "Iteration 772, loss = 0.06571230\n",
      "Iteration 773, loss = 0.06559680\n",
      "Iteration 774, loss = 0.06600203\n",
      "Iteration 775, loss = 0.06505814\n",
      "Iteration 776, loss = 0.06497918\n",
      "Iteration 777, loss = 0.06500944\n",
      "Iteration 778, loss = 0.06551786\n",
      "Iteration 779, loss = 0.06494892\n",
      "Iteration 780, loss = 0.06507440\n",
      "Iteration 781, loss = 0.06505579\n",
      "Iteration 782, loss = 0.06451113\n",
      "Iteration 783, loss = 0.06464411\n",
      "Iteration 784, loss = 0.06465924\n",
      "Iteration 785, loss = 0.06441354\n",
      "Iteration 786, loss = 0.06364665\n",
      "Iteration 787, loss = 0.06359810\n",
      "Iteration 788, loss = 0.06335548\n",
      "Iteration 789, loss = 0.06318434\n",
      "Iteration 790, loss = 0.06319221\n",
      "Iteration 791, loss = 0.06325183\n",
      "Iteration 792, loss = 0.06284040\n",
      "Iteration 793, loss = 0.06263379\n",
      "Iteration 794, loss = 0.06263182\n",
      "Iteration 795, loss = 0.06274037\n",
      "Iteration 796, loss = 0.06235456\n",
      "Iteration 797, loss = 0.06207667\n",
      "Iteration 798, loss = 0.06218362\n",
      "Iteration 799, loss = 0.06195425\n",
      "Iteration 800, loss = 0.06180757\n",
      "Iteration 801, loss = 0.06191832\n",
      "Iteration 802, loss = 0.06145046\n",
      "Iteration 803, loss = 0.06146302\n",
      "Iteration 804, loss = 0.06141363\n",
      "Iteration 805, loss = 0.06162222\n",
      "Iteration 806, loss = 0.06247784\n",
      "Iteration 807, loss = 0.06297761\n",
      "Iteration 808, loss = 0.06607735\n",
      "Iteration 809, loss = 0.06277781\n",
      "Iteration 810, loss = 0.06096746\n",
      "Iteration 811, loss = 0.06075323\n",
      "Iteration 812, loss = 0.06071435\n",
      "Iteration 813, loss = 0.06012708\n",
      "Iteration 814, loss = 0.06013335\n",
      "Iteration 815, loss = 0.06083674\n",
      "Iteration 816, loss = 0.05990471\n",
      "Iteration 817, loss = 0.06034073\n",
      "Iteration 818, loss = 0.05965513\n",
      "Iteration 819, loss = 0.05962489\n",
      "Iteration 820, loss = 0.05945805\n",
      "Iteration 821, loss = 0.05980264\n",
      "Iteration 822, loss = 0.05891334\n",
      "Iteration 823, loss = 0.05854176\n",
      "Iteration 824, loss = 0.05840583\n",
      "Iteration 825, loss = 0.05834264\n",
      "Iteration 826, loss = 0.05922688\n",
      "Iteration 827, loss = 0.05878762\n",
      "Iteration 828, loss = 0.05863634\n",
      "Iteration 829, loss = 0.05822504\n",
      "Iteration 830, loss = 0.05780846\n",
      "Iteration 831, loss = 0.05789840\n",
      "Iteration 832, loss = 0.05849408\n",
      "Iteration 833, loss = 0.05894750\n",
      "Iteration 834, loss = 0.05788060\n",
      "Iteration 835, loss = 0.05755810\n",
      "Iteration 836, loss = 0.05808463\n",
      "Iteration 837, loss = 0.05718258\n",
      "Iteration 838, loss = 0.05698489\n",
      "Iteration 839, loss = 0.05692739\n",
      "Iteration 840, loss = 0.05740275\n",
      "Iteration 841, loss = 0.05656208\n",
      "Iteration 842, loss = 0.05657165\n",
      "Iteration 843, loss = 0.05680279\n",
      "Iteration 844, loss = 0.05616104\n",
      "Iteration 845, loss = 0.05604383\n",
      "Iteration 846, loss = 0.05589600\n",
      "Iteration 847, loss = 0.05573110\n",
      "Iteration 848, loss = 0.05576609\n",
      "Iteration 849, loss = 0.05541605\n",
      "Iteration 850, loss = 0.05614425\n",
      "Iteration 851, loss = 0.05664834\n",
      "Iteration 852, loss = 0.05554019\n",
      "Iteration 853, loss = 0.05499898\n",
      "Iteration 854, loss = 0.05536085\n",
      "Iteration 855, loss = 0.05497209\n",
      "Iteration 856, loss = 0.05460087\n",
      "Iteration 857, loss = 0.05447915\n",
      "Iteration 858, loss = 0.05443109\n",
      "Iteration 859, loss = 0.05458775\n",
      "Iteration 860, loss = 0.05436649\n",
      "Iteration 861, loss = 0.05410293\n",
      "Iteration 862, loss = 0.05420953\n",
      "Iteration 863, loss = 0.05387595\n",
      "Iteration 864, loss = 0.05400379\n",
      "Iteration 865, loss = 0.05375501\n",
      "Iteration 866, loss = 0.05369869\n",
      "Iteration 867, loss = 0.05361727\n",
      "Iteration 868, loss = 0.05378153\n",
      "Iteration 869, loss = 0.05387080\n",
      "Iteration 870, loss = 0.05323157\n",
      "Iteration 871, loss = 0.05321787\n",
      "Iteration 872, loss = 0.05293798\n",
      "Iteration 873, loss = 0.05405849\n",
      "Iteration 874, loss = 0.05359019\n",
      "Iteration 875, loss = 0.05292175\n",
      "Iteration 876, loss = 0.05316195\n",
      "Iteration 877, loss = 0.05247281\n",
      "Iteration 878, loss = 0.05324371\n",
      "Iteration 879, loss = 0.05343969\n",
      "Iteration 880, loss = 0.05241520\n",
      "Iteration 881, loss = 0.05256695\n",
      "Iteration 882, loss = 0.05229931\n",
      "Iteration 883, loss = 0.05237343\n",
      "Iteration 884, loss = 0.05192296\n",
      "Iteration 885, loss = 0.05215666\n",
      "Iteration 886, loss = 0.05196111\n",
      "Iteration 887, loss = 0.05174821\n",
      "Iteration 888, loss = 0.05171926\n",
      "Iteration 889, loss = 0.05182272\n",
      "Iteration 890, loss = 0.05119778\n",
      "Iteration 891, loss = 0.05127896\n",
      "Iteration 892, loss = 0.05133578\n",
      "Iteration 893, loss = 0.05066424\n",
      "Iteration 894, loss = 0.05092734\n",
      "Iteration 895, loss = 0.05120600\n",
      "Iteration 896, loss = 0.05049172\n",
      "Iteration 897, loss = 0.05034979\n",
      "Iteration 898, loss = 0.05048602\n",
      "Iteration 899, loss = 0.05004780\n",
      "Iteration 900, loss = 0.04998943\n",
      "Iteration 901, loss = 0.04995989\n",
      "Iteration 902, loss = 0.04970348\n",
      "Iteration 903, loss = 0.05015577\n",
      "Iteration 904, loss = 0.04992705\n",
      "Iteration 905, loss = 0.04964244\n",
      "Iteration 906, loss = 0.04959800\n",
      "Iteration 907, loss = 0.04959444\n",
      "Iteration 908, loss = 0.04945286\n",
      "Iteration 909, loss = 0.04922728\n",
      "Iteration 910, loss = 0.04988171\n",
      "Iteration 911, loss = 0.04932656\n",
      "Iteration 912, loss = 0.04926405\n",
      "Iteration 913, loss = 0.04869915\n",
      "Iteration 914, loss = 0.04880356\n",
      "Iteration 915, loss = 0.04871067\n",
      "Iteration 916, loss = 0.04864827\n",
      "Iteration 917, loss = 0.04902640\n",
      "Iteration 918, loss = 0.04857216\n",
      "Iteration 919, loss = 0.04826988\n",
      "Iteration 920, loss = 0.04813984\n",
      "Iteration 921, loss = 0.04791943\n",
      "Iteration 922, loss = 0.04809603\n",
      "Iteration 923, loss = 0.04798551\n",
      "Iteration 924, loss = 0.04798479\n",
      "Iteration 925, loss = 0.04770915\n",
      "Iteration 926, loss = 0.04776180\n",
      "Iteration 927, loss = 0.04757358\n",
      "Iteration 928, loss = 0.04728601\n",
      "Iteration 929, loss = 0.04726286\n",
      "Iteration 930, loss = 0.04729099\n",
      "Iteration 931, loss = 0.04710946\n",
      "Iteration 932, loss = 0.04692945\n",
      "Iteration 933, loss = 0.04715691\n",
      "Iteration 934, loss = 0.04723840\n",
      "Iteration 935, loss = 0.04675574\n",
      "Iteration 936, loss = 0.04713822\n",
      "Iteration 937, loss = 0.04697448\n",
      "Iteration 938, loss = 0.04676474\n",
      "Iteration 939, loss = 0.04656752\n",
      "Iteration 940, loss = 0.04665515\n",
      "Iteration 941, loss = 0.04638289\n",
      "Iteration 942, loss = 0.04642663\n",
      "Iteration 943, loss = 0.04595127\n",
      "Iteration 944, loss = 0.04648767\n",
      "Iteration 945, loss = 0.04608725\n",
      "Iteration 946, loss = 0.04599967\n",
      "Iteration 947, loss = 0.04602347\n",
      "Iteration 948, loss = 0.04566268\n",
      "Iteration 949, loss = 0.04557206\n",
      "Iteration 950, loss = 0.04539830\n",
      "Iteration 951, loss = 0.04560869\n",
      "Iteration 952, loss = 0.04502751\n",
      "Iteration 953, loss = 0.04509295\n",
      "Iteration 954, loss = 0.04506447\n",
      "Iteration 955, loss = 0.04516023\n",
      "Iteration 956, loss = 0.04574525\n",
      "Iteration 957, loss = 0.04481751\n",
      "Iteration 958, loss = 0.04473238\n",
      "Iteration 959, loss = 0.04470587\n",
      "Iteration 960, loss = 0.04523361\n",
      "Iteration 961, loss = 0.04431731\n",
      "Iteration 962, loss = 0.04444774\n",
      "Iteration 963, loss = 0.04448500\n",
      "Iteration 964, loss = 0.04436100\n",
      "Iteration 965, loss = 0.04455326\n",
      "Iteration 966, loss = 0.04395657\n",
      "Iteration 967, loss = 0.04383704\n",
      "Iteration 968, loss = 0.04373959\n",
      "Iteration 969, loss = 0.04374062\n",
      "Iteration 970, loss = 0.04361957\n",
      "Iteration 971, loss = 0.04339742\n",
      "Iteration 972, loss = 0.04366938\n",
      "Iteration 973, loss = 0.04351579\n",
      "Iteration 974, loss = 0.04335670\n",
      "Iteration 975, loss = 0.04326849\n",
      "Iteration 976, loss = 0.04304646\n",
      "Iteration 977, loss = 0.04318507\n",
      "Iteration 978, loss = 0.04300472\n",
      "Iteration 979, loss = 0.04290769\n",
      "Iteration 980, loss = 0.04286408\n",
      "Iteration 981, loss = 0.04286546\n",
      "Iteration 982, loss = 0.04266145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 983, loss = 0.04260928\n",
      "Iteration 984, loss = 0.04246155\n",
      "Iteration 985, loss = 0.04244508\n",
      "Iteration 986, loss = 0.04218731\n",
      "Iteration 987, loss = 0.04227324\n",
      "Iteration 988, loss = 0.04270553\n",
      "Iteration 989, loss = 0.04220841\n",
      "Iteration 990, loss = 0.04215729\n",
      "Iteration 991, loss = 0.04229377\n",
      "Iteration 992, loss = 0.04202815\n",
      "Iteration 993, loss = 0.04238437\n",
      "Iteration 994, loss = 0.04204930\n",
      "Iteration 995, loss = 0.04179886\n",
      "Iteration 996, loss = 0.04146297\n",
      "Iteration 997, loss = 0.04232471\n",
      "Iteration 998, loss = 0.04305717\n",
      "Iteration 999, loss = 0.04183700\n",
      "Iteration 1000, loss = 0.04109759\n",
      "Iteration 1001, loss = 0.04136971\n",
      "Iteration 1002, loss = 0.04103097\n",
      "Iteration 1003, loss = 0.04072771\n",
      "Iteration 1004, loss = 0.04070470\n",
      "Iteration 1005, loss = 0.04091230\n",
      "Iteration 1006, loss = 0.04057634\n",
      "Iteration 1007, loss = 0.04058645\n",
      "Iteration 1008, loss = 0.04079215\n",
      "Iteration 1009, loss = 0.04051672\n",
      "Iteration 1010, loss = 0.04064651\n",
      "Iteration 1011, loss = 0.04024824\n",
      "Iteration 1012, loss = 0.04027088\n",
      "Iteration 1013, loss = 0.04020572\n",
      "Iteration 1014, loss = 0.04038125\n",
      "Iteration 1015, loss = 0.04041005\n",
      "Iteration 1016, loss = 0.04012591\n",
      "Iteration 1017, loss = 0.04005713\n",
      "Iteration 1018, loss = 0.03967465\n",
      "Iteration 1019, loss = 0.03948036\n",
      "Iteration 1020, loss = 0.03950267\n",
      "Iteration 1021, loss = 0.03922080\n",
      "Iteration 1022, loss = 0.03951054\n",
      "Iteration 1023, loss = 0.03891771\n",
      "Iteration 1024, loss = 0.03868707\n",
      "Iteration 1025, loss = 0.03889942\n",
      "Iteration 1026, loss = 0.03939526\n",
      "Iteration 1027, loss = 0.03920543\n",
      "Iteration 1028, loss = 0.03971650\n",
      "Iteration 1029, loss = 0.03858274\n",
      "Iteration 1030, loss = 0.03798160\n",
      "Iteration 1031, loss = 0.03795608\n",
      "Iteration 1032, loss = 0.03825258\n",
      "Iteration 1033, loss = 0.03790510\n",
      "Iteration 1034, loss = 0.03779144\n",
      "Iteration 1035, loss = 0.03816432\n",
      "Iteration 1036, loss = 0.03768478\n",
      "Iteration 1037, loss = 0.03758496\n",
      "Iteration 1038, loss = 0.03752160\n",
      "Iteration 1039, loss = 0.03757746\n",
      "Iteration 1040, loss = 0.03738128\n",
      "Iteration 1041, loss = 0.03756805\n",
      "Iteration 1042, loss = 0.03720871\n",
      "Iteration 1043, loss = 0.03699019\n",
      "Iteration 1044, loss = 0.03702736\n",
      "Iteration 1045, loss = 0.03683923\n",
      "Iteration 1046, loss = 0.03663137\n",
      "Iteration 1047, loss = 0.03742534\n",
      "Iteration 1048, loss = 0.03684530\n",
      "Iteration 1049, loss = 0.03662830\n",
      "Iteration 1050, loss = 0.03697468\n",
      "Iteration 1051, loss = 0.03665507\n",
      "Iteration 1052, loss = 0.03662849\n",
      "Iteration 1053, loss = 0.03650279\n",
      "Iteration 1054, loss = 0.03665056\n",
      "Iteration 1055, loss = 0.03611485\n",
      "Iteration 1056, loss = 0.03611424\n",
      "Iteration 1057, loss = 0.03636850\n",
      "Iteration 1058, loss = 0.03606848\n",
      "Iteration 1059, loss = 0.03589864\n",
      "Iteration 1060, loss = 0.03575511\n",
      "Iteration 1061, loss = 0.03581599\n",
      "Iteration 1062, loss = 0.03573089\n",
      "Iteration 1063, loss = 0.03582978\n",
      "Iteration 1064, loss = 0.03557890\n",
      "Iteration 1065, loss = 0.03578949\n",
      "Iteration 1066, loss = 0.03539391\n",
      "Iteration 1067, loss = 0.03537269\n",
      "Iteration 1068, loss = 0.03570248\n",
      "Iteration 1069, loss = 0.03561454\n",
      "Iteration 1070, loss = 0.03527582\n",
      "Iteration 1071, loss = 0.03524013\n",
      "Iteration 1072, loss = 0.03527465\n",
      "Iteration 1073, loss = 0.03496430\n",
      "Iteration 1074, loss = 0.03475867\n",
      "Iteration 1075, loss = 0.03472252\n",
      "Iteration 1076, loss = 0.03464438\n",
      "Iteration 1077, loss = 0.03466592\n",
      "Iteration 1078, loss = 0.03476192\n",
      "Iteration 1079, loss = 0.03471963\n",
      "Iteration 1080, loss = 0.03441709\n",
      "Iteration 1081, loss = 0.03439776\n",
      "Iteration 1082, loss = 0.03427664\n",
      "Iteration 1083, loss = 0.03418507\n",
      "Iteration 1084, loss = 0.03421685\n",
      "Iteration 1085, loss = 0.03405754\n",
      "Iteration 1086, loss = 0.03415243\n",
      "Iteration 1087, loss = 0.03397838\n",
      "Iteration 1088, loss = 0.03383552\n",
      "Iteration 1089, loss = 0.03384853\n",
      "Iteration 1090, loss = 0.03402527\n",
      "Iteration 1091, loss = 0.03363251\n",
      "Iteration 1092, loss = 0.03383825\n",
      "Iteration 1093, loss = 0.03373094\n",
      "Iteration 1094, loss = 0.03353700\n",
      "Iteration 1095, loss = 0.03346860\n",
      "Iteration 1096, loss = 0.03370174\n",
      "Iteration 1097, loss = 0.03344567\n",
      "Iteration 1098, loss = 0.03324937\n",
      "Iteration 1099, loss = 0.03397065\n",
      "Iteration 1100, loss = 0.03355540\n",
      "Iteration 1101, loss = 0.03324686\n",
      "Iteration 1102, loss = 0.03325295\n",
      "Iteration 1103, loss = 0.03299836\n",
      "Iteration 1104, loss = 0.03367723\n",
      "Iteration 1105, loss = 0.03567171\n",
      "Iteration 1106, loss = 0.03356624\n",
      "Iteration 1107, loss = 0.03290656\n",
      "Iteration 1108, loss = 0.03277344\n",
      "Iteration 1109, loss = 0.03273504\n",
      "Iteration 1110, loss = 0.03274448\n",
      "Iteration 1111, loss = 0.03259605\n",
      "Iteration 1112, loss = 0.03258319\n",
      "Iteration 1113, loss = 0.03235721\n",
      "Iteration 1114, loss = 0.03238129\n",
      "Iteration 1115, loss = 0.03230889\n",
      "Iteration 1116, loss = 0.03212339\n",
      "Iteration 1117, loss = 0.03199343\n",
      "Iteration 1118, loss = 0.03190513\n",
      "Iteration 1119, loss = 0.03187396\n",
      "Iteration 1120, loss = 0.03188437\n",
      "Iteration 1121, loss = 0.03214599\n",
      "Iteration 1122, loss = 0.03249956\n",
      "Iteration 1123, loss = 0.03351130\n",
      "Iteration 1124, loss = 0.03245109\n",
      "Iteration 1125, loss = 0.03175266\n",
      "Iteration 1126, loss = 0.03167422\n",
      "Iteration 1127, loss = 0.03137882\n",
      "Iteration 1128, loss = 0.03133824\n",
      "Iteration 1129, loss = 0.03157529\n",
      "Iteration 1130, loss = 0.03142063\n",
      "Iteration 1131, loss = 0.03249000\n",
      "Iteration 1132, loss = 0.03177366\n",
      "Iteration 1133, loss = 0.03123042\n",
      "Iteration 1134, loss = 0.03152900\n",
      "Iteration 1135, loss = 0.03103959\n",
      "Iteration 1136, loss = 0.03084708\n",
      "Iteration 1137, loss = 0.03082201\n",
      "Iteration 1138, loss = 0.03087726\n",
      "Iteration 1139, loss = 0.03085123\n",
      "Iteration 1140, loss = 0.03072802\n",
      "Iteration 1141, loss = 0.03063800\n",
      "Iteration 1142, loss = 0.03047894\n",
      "Iteration 1143, loss = 0.03040101\n",
      "Iteration 1144, loss = 0.03042062\n",
      "Iteration 1145, loss = 0.03054522\n",
      "Iteration 1146, loss = 0.03051709\n",
      "Iteration 1147, loss = 0.03018366\n",
      "Iteration 1148, loss = 0.03053750\n",
      "Iteration 1149, loss = 0.03028881\n",
      "Iteration 1150, loss = 0.03030293\n",
      "Iteration 1151, loss = 0.03024167\n",
      "Iteration 1152, loss = 0.03045433\n",
      "Iteration 1153, loss = 0.03020553\n",
      "Iteration 1154, loss = 0.02989666\n",
      "Iteration 1155, loss = 0.03084211\n",
      "Iteration 1156, loss = 0.03047722\n",
      "Iteration 1157, loss = 0.02974358\n",
      "Iteration 1158, loss = 0.02984172\n",
      "Iteration 1159, loss = 0.02962739\n",
      "Iteration 1160, loss = 0.02969961\n",
      "Iteration 1161, loss = 0.02944498\n",
      "Iteration 1162, loss = 0.02949240\n",
      "Iteration 1163, loss = 0.02939144\n",
      "Iteration 1164, loss = 0.02929016\n",
      "Iteration 1165, loss = 0.02942117\n",
      "Iteration 1166, loss = 0.02938089\n",
      "Iteration 1167, loss = 0.02925673\n",
      "Iteration 1168, loss = 0.02934015\n",
      "Iteration 1169, loss = 0.02912888\n",
      "Iteration 1170, loss = 0.02905842\n",
      "Iteration 1171, loss = 0.02902308\n",
      "Iteration 1172, loss = 0.02881024\n",
      "Iteration 1173, loss = 0.02897255\n",
      "Iteration 1174, loss = 0.02900004\n",
      "Iteration 1175, loss = 0.02886648\n",
      "Iteration 1176, loss = 0.02852582\n",
      "Iteration 1177, loss = 0.02882170\n",
      "Iteration 1178, loss = 0.02852845\n",
      "Iteration 1179, loss = 0.02853976\n",
      "Iteration 1180, loss = 0.02872804\n",
      "Iteration 1181, loss = 0.02868461\n",
      "Iteration 1182, loss = 0.02861181\n",
      "Iteration 1183, loss = 0.02846888\n",
      "Iteration 1184, loss = 0.02836909\n",
      "Iteration 1185, loss = 0.02832803\n",
      "Iteration 1186, loss = 0.02825649\n",
      "Iteration 1187, loss = 0.02819335\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 3 of 48...\n",
      "Iteration 1, loss = 0.69316869\n",
      "Iteration 2, loss = 0.67903242\n",
      "Iteration 3, loss = 0.67339599\n",
      "Iteration 4, loss = 0.66349874\n",
      "Iteration 5, loss = 0.65316736\n",
      "Iteration 6, loss = 0.64112923\n",
      "Iteration 7, loss = 0.62919187\n",
      "Iteration 8, loss = 0.61706398\n",
      "Iteration 9, loss = 0.60541383\n",
      "Iteration 10, loss = 0.59419548\n",
      "Iteration 11, loss = 0.58330981\n",
      "Iteration 12, loss = 0.57221698\n",
      "Iteration 13, loss = 0.56162081\n",
      "Iteration 14, loss = 0.55204638\n",
      "Iteration 15, loss = 0.54214965\n",
      "Iteration 16, loss = 0.53328852\n",
      "Iteration 17, loss = 0.52497307\n",
      "Iteration 18, loss = 0.51760404\n",
      "Iteration 19, loss = 0.50989405\n",
      "Iteration 20, loss = 0.50288424\n",
      "Iteration 21, loss = 0.49613636\n",
      "Iteration 22, loss = 0.48998440\n",
      "Iteration 23, loss = 0.48496531\n",
      "Iteration 24, loss = 0.47793810\n",
      "Iteration 25, loss = 0.47318549\n",
      "Iteration 26, loss = 0.46770583\n",
      "Iteration 27, loss = 0.46384060\n",
      "Iteration 28, loss = 0.45906051\n",
      "Iteration 29, loss = 0.45407450\n",
      "Iteration 30, loss = 0.45020905\n",
      "Iteration 31, loss = 0.44627686\n",
      "Iteration 32, loss = 0.44305314\n",
      "Iteration 33, loss = 0.44021492\n",
      "Iteration 34, loss = 0.43623133\n",
      "Iteration 35, loss = 0.43253193\n",
      "Iteration 36, loss = 0.42964344\n",
      "Iteration 37, loss = 0.42645433\n",
      "Iteration 38, loss = 0.42355435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.42081976\n",
      "Iteration 40, loss = 0.41885654\n",
      "Iteration 41, loss = 0.41588072\n",
      "Iteration 42, loss = 0.41406549\n",
      "Iteration 43, loss = 0.41125664\n",
      "Iteration 44, loss = 0.40897622\n",
      "Iteration 45, loss = 0.40636788\n",
      "Iteration 46, loss = 0.40410824\n",
      "Iteration 47, loss = 0.40290324\n",
      "Iteration 48, loss = 0.39985665\n",
      "Iteration 49, loss = 0.39776561\n",
      "Iteration 50, loss = 0.39596073\n",
      "Iteration 51, loss = 0.39581388\n",
      "Iteration 52, loss = 0.39346973\n",
      "Iteration 53, loss = 0.39114492\n",
      "Iteration 54, loss = 0.38970439\n",
      "Iteration 55, loss = 0.38814476\n",
      "Iteration 56, loss = 0.38628911\n",
      "Iteration 57, loss = 0.38438237\n",
      "Iteration 58, loss = 0.38292561\n",
      "Iteration 59, loss = 0.38103912\n",
      "Iteration 60, loss = 0.38032516\n",
      "Iteration 61, loss = 0.37860964\n",
      "Iteration 62, loss = 0.37703306\n",
      "Iteration 63, loss = 0.37741884\n",
      "Iteration 64, loss = 0.37582602\n",
      "Iteration 65, loss = 0.37358037\n",
      "Iteration 66, loss = 0.37183183\n",
      "Iteration 67, loss = 0.37046005\n",
      "Iteration 68, loss = 0.36985166\n",
      "Iteration 69, loss = 0.36835205\n",
      "Iteration 70, loss = 0.36706219\n",
      "Iteration 71, loss = 0.36562232\n",
      "Iteration 72, loss = 0.36481784\n",
      "Iteration 73, loss = 0.36305093\n",
      "Iteration 74, loss = 0.36221980\n",
      "Iteration 75, loss = 0.36196273\n",
      "Iteration 76, loss = 0.35960447\n",
      "Iteration 77, loss = 0.35903527\n",
      "Iteration 78, loss = 0.35736807\n",
      "Iteration 79, loss = 0.35747325\n",
      "Iteration 80, loss = 0.35502306\n",
      "Iteration 81, loss = 0.35415129\n",
      "Iteration 82, loss = 0.35325556\n",
      "Iteration 83, loss = 0.35341786\n",
      "Iteration 84, loss = 0.35274885\n",
      "Iteration 85, loss = 0.35071208\n",
      "Iteration 86, loss = 0.34939647\n",
      "Iteration 87, loss = 0.34996734\n",
      "Iteration 88, loss = 0.34739267\n",
      "Iteration 89, loss = 0.34669269\n",
      "Iteration 90, loss = 0.34599780\n",
      "Iteration 91, loss = 0.34538279\n",
      "Iteration 92, loss = 0.34425983\n",
      "Iteration 93, loss = 0.34428452\n",
      "Iteration 94, loss = 0.34278591\n",
      "Iteration 95, loss = 0.34100034\n",
      "Iteration 96, loss = 0.34046836\n",
      "Iteration 97, loss = 0.33933373\n",
      "Iteration 98, loss = 0.33942894\n",
      "Iteration 99, loss = 0.33814478\n",
      "Iteration 100, loss = 0.33771967\n",
      "Iteration 101, loss = 0.33609407\n",
      "Iteration 102, loss = 0.33492397\n",
      "Iteration 103, loss = 0.33435187\n",
      "Iteration 104, loss = 0.33364547\n",
      "Iteration 105, loss = 0.33391298\n",
      "Iteration 106, loss = 0.33339179\n",
      "Iteration 107, loss = 0.33260514\n",
      "Iteration 108, loss = 0.33060104\n",
      "Iteration 109, loss = 0.33173782\n",
      "Iteration 110, loss = 0.33069715\n",
      "Iteration 111, loss = 0.33016601\n",
      "Iteration 112, loss = 0.32820808\n",
      "Iteration 113, loss = 0.32716664\n",
      "Iteration 114, loss = 0.32622958\n",
      "Iteration 115, loss = 0.32683302\n",
      "Iteration 116, loss = 0.32472585\n",
      "Iteration 117, loss = 0.32457133\n",
      "Iteration 118, loss = 0.32355040\n",
      "Iteration 119, loss = 0.32323116\n",
      "Iteration 120, loss = 0.32308522\n",
      "Iteration 121, loss = 0.32201172\n",
      "Iteration 122, loss = 0.32029086\n",
      "Iteration 123, loss = 0.31967530\n",
      "Iteration 124, loss = 0.31942719\n",
      "Iteration 125, loss = 0.31796216\n",
      "Iteration 126, loss = 0.31729819\n",
      "Iteration 127, loss = 0.31713077\n",
      "Iteration 128, loss = 0.31600063\n",
      "Iteration 129, loss = 0.31613635\n",
      "Iteration 130, loss = 0.31488408\n",
      "Iteration 131, loss = 0.31435555\n",
      "Iteration 132, loss = 0.31313183\n",
      "Iteration 133, loss = 0.31329833\n",
      "Iteration 134, loss = 0.31411091\n",
      "Iteration 135, loss = 0.31212605\n",
      "Iteration 136, loss = 0.31055602\n",
      "Iteration 137, loss = 0.31052920\n",
      "Iteration 138, loss = 0.30957365\n",
      "Iteration 139, loss = 0.30898123\n",
      "Iteration 140, loss = 0.30915811\n",
      "Iteration 141, loss = 0.30852512\n",
      "Iteration 142, loss = 0.30745973\n",
      "Iteration 143, loss = 0.30636116\n",
      "Iteration 144, loss = 0.30600154\n",
      "Iteration 145, loss = 0.30701163\n",
      "Iteration 146, loss = 0.30698580\n",
      "Iteration 147, loss = 0.30437748\n",
      "Iteration 148, loss = 0.30337159\n",
      "Iteration 149, loss = 0.30239598\n",
      "Iteration 150, loss = 0.30183359\n",
      "Iteration 151, loss = 0.30299875\n",
      "Iteration 152, loss = 0.30658027\n",
      "Iteration 153, loss = 0.30621253\n",
      "Iteration 154, loss = 0.29990365\n",
      "Iteration 155, loss = 0.30020541\n",
      "Iteration 156, loss = 0.30019259\n",
      "Iteration 157, loss = 0.29910812\n",
      "Iteration 158, loss = 0.29716655\n",
      "Iteration 159, loss = 0.29720743\n",
      "Iteration 160, loss = 0.29597924\n",
      "Iteration 161, loss = 0.29663642\n",
      "Iteration 162, loss = 0.29470366\n",
      "Iteration 163, loss = 0.29466353\n",
      "Iteration 164, loss = 0.29380845\n",
      "Iteration 165, loss = 0.29326597\n",
      "Iteration 166, loss = 0.29493986\n",
      "Iteration 167, loss = 0.29493701\n",
      "Iteration 168, loss = 0.29305977\n",
      "Iteration 169, loss = 0.29090897\n",
      "Iteration 170, loss = 0.29058593\n",
      "Iteration 171, loss = 0.28996464\n",
      "Iteration 172, loss = 0.28957699\n",
      "Iteration 173, loss = 0.28928125\n",
      "Iteration 174, loss = 0.28850463\n",
      "Iteration 175, loss = 0.28830267\n",
      "Iteration 176, loss = 0.28767622\n",
      "Iteration 177, loss = 0.28677465\n",
      "Iteration 178, loss = 0.28679059\n",
      "Iteration 179, loss = 0.28551042\n",
      "Iteration 180, loss = 0.28539665\n",
      "Iteration 181, loss = 0.28479753\n",
      "Iteration 182, loss = 0.28412513\n",
      "Iteration 183, loss = 0.28381274\n",
      "Iteration 184, loss = 0.28318756\n",
      "Iteration 185, loss = 0.28352493\n",
      "Iteration 186, loss = 0.28343296\n",
      "Iteration 187, loss = 0.28162876\n",
      "Iteration 188, loss = 0.28140874\n",
      "Iteration 189, loss = 0.28244194\n",
      "Iteration 190, loss = 0.28114437\n",
      "Iteration 191, loss = 0.28010226\n",
      "Iteration 192, loss = 0.27927471\n",
      "Iteration 193, loss = 0.27832103\n",
      "Iteration 194, loss = 0.27804769\n",
      "Iteration 195, loss = 0.27763804\n",
      "Iteration 196, loss = 0.27674082\n",
      "Iteration 197, loss = 0.27642162\n",
      "Iteration 198, loss = 0.27592947\n",
      "Iteration 199, loss = 0.27609419\n",
      "Iteration 200, loss = 0.27446225\n",
      "Iteration 201, loss = 0.27420539\n",
      "Iteration 202, loss = 0.27373941\n",
      "Iteration 203, loss = 0.27616261\n",
      "Iteration 204, loss = 0.27462872\n",
      "Iteration 205, loss = 0.27231062\n",
      "Iteration 206, loss = 0.27222060\n",
      "Iteration 207, loss = 0.27165917\n",
      "Iteration 208, loss = 0.27275282\n",
      "Iteration 209, loss = 0.27156809\n",
      "Iteration 210, loss = 0.27029131\n",
      "Iteration 211, loss = 0.26903712\n",
      "Iteration 212, loss = 0.26951473\n",
      "Iteration 213, loss = 0.26984859\n",
      "Iteration 214, loss = 0.26738277\n",
      "Iteration 215, loss = 0.26761205\n",
      "Iteration 216, loss = 0.26698236\n",
      "Iteration 217, loss = 0.26726364\n",
      "Iteration 218, loss = 0.26606413\n",
      "Iteration 219, loss = 0.26754637\n",
      "Iteration 220, loss = 0.26473639\n",
      "Iteration 221, loss = 0.26735227\n",
      "Iteration 222, loss = 0.26580633\n",
      "Iteration 223, loss = 0.26451936\n",
      "Iteration 224, loss = 0.26378905\n",
      "Iteration 225, loss = 0.26334531\n",
      "Iteration 226, loss = 0.26191682\n",
      "Iteration 227, loss = 0.26184537\n",
      "Iteration 228, loss = 0.26118980\n",
      "Iteration 229, loss = 0.26027955\n",
      "Iteration 230, loss = 0.26002876\n",
      "Iteration 231, loss = 0.25975686\n",
      "Iteration 232, loss = 0.25937913\n",
      "Iteration 233, loss = 0.25844511\n",
      "Iteration 234, loss = 0.25875581\n",
      "Iteration 235, loss = 0.25814081\n",
      "Iteration 236, loss = 0.25780994\n",
      "Iteration 237, loss = 0.25637810\n",
      "Iteration 238, loss = 0.25795065\n",
      "Iteration 239, loss = 0.25555827\n",
      "Iteration 240, loss = 0.25530680\n",
      "Iteration 241, loss = 0.25599020\n",
      "Iteration 242, loss = 0.25551648\n",
      "Iteration 243, loss = 0.25577384\n",
      "Iteration 244, loss = 0.25392894\n",
      "Iteration 245, loss = 0.25315683\n",
      "Iteration 246, loss = 0.25247067\n",
      "Iteration 247, loss = 0.25240747\n",
      "Iteration 248, loss = 0.25211582\n",
      "Iteration 249, loss = 0.25162810\n",
      "Iteration 250, loss = 0.25177641\n",
      "Iteration 251, loss = 0.25092728\n",
      "Iteration 252, loss = 0.24954265\n",
      "Iteration 253, loss = 0.24934329\n",
      "Iteration 254, loss = 0.24963557\n",
      "Iteration 255, loss = 0.25015364\n",
      "Iteration 256, loss = 0.24898519\n",
      "Iteration 257, loss = 0.24759769\n",
      "Iteration 258, loss = 0.24695815\n",
      "Iteration 259, loss = 0.24675029\n",
      "Iteration 260, loss = 0.24745787\n",
      "Iteration 261, loss = 0.24674067\n",
      "Iteration 262, loss = 0.24539973\n",
      "Iteration 263, loss = 0.24535765\n",
      "Iteration 264, loss = 0.24485710\n",
      "Iteration 265, loss = 0.24440139\n",
      "Iteration 266, loss = 0.24382839\n",
      "Iteration 267, loss = 0.24583415\n",
      "Iteration 268, loss = 0.24353330\n",
      "Iteration 269, loss = 0.24284704\n",
      "Iteration 270, loss = 0.24322128\n",
      "Iteration 271, loss = 0.24399040\n",
      "Iteration 272, loss = 0.24173630\n",
      "Iteration 273, loss = 0.24065107\n",
      "Iteration 274, loss = 0.23984994\n",
      "Iteration 275, loss = 0.24062689\n",
      "Iteration 276, loss = 0.23968996\n",
      "Iteration 277, loss = 0.24000778\n",
      "Iteration 278, loss = 0.23905137\n",
      "Iteration 279, loss = 0.23833200\n",
      "Iteration 280, loss = 0.23993090\n",
      "Iteration 281, loss = 0.23762729\n",
      "Iteration 282, loss = 0.23644928\n",
      "Iteration 283, loss = 0.23776492\n",
      "Iteration 284, loss = 0.23811698\n",
      "Iteration 285, loss = 0.23567088\n",
      "Iteration 286, loss = 0.23530427\n",
      "Iteration 287, loss = 0.23492132\n",
      "Iteration 288, loss = 0.23518072\n",
      "Iteration 289, loss = 0.23517881\n",
      "Iteration 290, loss = 0.23381013\n",
      "Iteration 291, loss = 0.23525201\n",
      "Iteration 292, loss = 0.23307981\n",
      "Iteration 293, loss = 0.23539673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 294, loss = 0.23269344\n",
      "Iteration 295, loss = 0.23142079\n",
      "Iteration 296, loss = 0.23141073\n",
      "Iteration 297, loss = 0.23115247\n",
      "Iteration 298, loss = 0.23250325\n",
      "Iteration 299, loss = 0.23069521\n",
      "Iteration 300, loss = 0.23025180\n",
      "Iteration 301, loss = 0.22970155\n",
      "Iteration 302, loss = 0.22902879\n",
      "Iteration 303, loss = 0.22822378\n",
      "Iteration 304, loss = 0.22753895\n",
      "Iteration 305, loss = 0.22762997\n",
      "Iteration 306, loss = 0.22688378\n",
      "Iteration 307, loss = 0.22698731\n",
      "Iteration 308, loss = 0.22696933\n",
      "Iteration 309, loss = 0.22590273\n",
      "Iteration 310, loss = 0.22540453\n",
      "Iteration 311, loss = 0.22545355\n",
      "Iteration 312, loss = 0.22585794\n",
      "Iteration 313, loss = 0.22395669\n",
      "Iteration 314, loss = 0.22341162\n",
      "Iteration 315, loss = 0.22294631\n",
      "Iteration 316, loss = 0.22234708\n",
      "Iteration 317, loss = 0.22305308\n",
      "Iteration 318, loss = 0.22229097\n",
      "Iteration 319, loss = 0.22184686\n",
      "Iteration 320, loss = 0.22071136\n",
      "Iteration 321, loss = 0.22115765\n",
      "Iteration 322, loss = 0.22024042\n",
      "Iteration 323, loss = 0.21981237\n",
      "Iteration 324, loss = 0.21908834\n",
      "Iteration 325, loss = 0.21826254\n",
      "Iteration 326, loss = 0.21873915\n",
      "Iteration 327, loss = 0.21784937\n",
      "Iteration 328, loss = 0.21759691\n",
      "Iteration 329, loss = 0.21699603\n",
      "Iteration 330, loss = 0.21784283\n",
      "Iteration 331, loss = 0.21723477\n",
      "Iteration 332, loss = 0.21599011\n",
      "Iteration 333, loss = 0.21636251\n",
      "Iteration 334, loss = 0.21549563\n",
      "Iteration 335, loss = 0.21513432\n",
      "Iteration 336, loss = 0.21435152\n",
      "Iteration 337, loss = 0.21368021\n",
      "Iteration 338, loss = 0.21489731\n",
      "Iteration 339, loss = 0.21515262\n",
      "Iteration 340, loss = 0.21394552\n",
      "Iteration 341, loss = 0.21329691\n",
      "Iteration 342, loss = 0.21237425\n",
      "Iteration 343, loss = 0.21242999\n",
      "Iteration 344, loss = 0.21127352\n",
      "Iteration 345, loss = 0.21201183\n",
      "Iteration 346, loss = 0.21053372\n",
      "Iteration 347, loss = 0.21005701\n",
      "Iteration 348, loss = 0.21011292\n",
      "Iteration 349, loss = 0.21022170\n",
      "Iteration 350, loss = 0.20956641\n",
      "Iteration 351, loss = 0.21041414\n",
      "Iteration 352, loss = 0.20899062\n",
      "Iteration 353, loss = 0.20873778\n",
      "Iteration 354, loss = 0.20701365\n",
      "Iteration 355, loss = 0.20751190\n",
      "Iteration 356, loss = 0.20647863\n",
      "Iteration 357, loss = 0.20628868\n",
      "Iteration 358, loss = 0.20599763\n",
      "Iteration 359, loss = 0.20657770\n",
      "Iteration 360, loss = 0.20637349\n",
      "Iteration 361, loss = 0.20524571\n",
      "Iteration 362, loss = 0.20448915\n",
      "Iteration 363, loss = 0.20419720\n",
      "Iteration 364, loss = 0.20394661\n",
      "Iteration 365, loss = 0.20289448\n",
      "Iteration 366, loss = 0.20370695\n",
      "Iteration 367, loss = 0.20373101\n",
      "Iteration 368, loss = 0.20238596\n",
      "Iteration 369, loss = 0.20275432\n",
      "Iteration 370, loss = 0.20204912\n",
      "Iteration 371, loss = 0.20137162\n",
      "Iteration 372, loss = 0.20090524\n",
      "Iteration 373, loss = 0.20054526\n",
      "Iteration 374, loss = 0.19962283\n",
      "Iteration 375, loss = 0.19952675\n",
      "Iteration 376, loss = 0.19965118\n",
      "Iteration 377, loss = 0.19878138\n",
      "Iteration 378, loss = 0.19968875\n",
      "Iteration 379, loss = 0.19857207\n",
      "Iteration 380, loss = 0.19830207\n",
      "Iteration 381, loss = 0.19946437\n",
      "Iteration 382, loss = 0.19739517\n",
      "Iteration 383, loss = 0.19692857\n",
      "Iteration 384, loss = 0.19782387\n",
      "Iteration 385, loss = 0.19647338\n",
      "Iteration 386, loss = 0.19691304\n",
      "Iteration 387, loss = 0.19509294\n",
      "Iteration 388, loss = 0.19470788\n",
      "Iteration 389, loss = 0.19465199\n",
      "Iteration 390, loss = 0.19482511\n",
      "Iteration 391, loss = 0.19491971\n",
      "Iteration 392, loss = 0.19421246\n",
      "Iteration 393, loss = 0.19364575\n",
      "Iteration 394, loss = 0.19236130\n",
      "Iteration 395, loss = 0.19325888\n",
      "Iteration 396, loss = 0.19273087\n",
      "Iteration 397, loss = 0.19203708\n",
      "Iteration 398, loss = 0.19271355\n",
      "Iteration 399, loss = 0.19135782\n",
      "Iteration 400, loss = 0.19121904\n",
      "Iteration 401, loss = 0.19046935\n",
      "Iteration 402, loss = 0.19118034\n",
      "Iteration 403, loss = 0.18970449\n",
      "Iteration 404, loss = 0.18917358\n",
      "Iteration 405, loss = 0.18910538\n",
      "Iteration 406, loss = 0.18867812\n",
      "Iteration 407, loss = 0.18820484\n",
      "Iteration 408, loss = 0.18806594\n",
      "Iteration 409, loss = 0.18755755\n",
      "Iteration 410, loss = 0.18757659\n",
      "Iteration 411, loss = 0.18710221\n",
      "Iteration 412, loss = 0.18683283\n",
      "Iteration 413, loss = 0.18710665\n",
      "Iteration 414, loss = 0.18998951\n",
      "Iteration 415, loss = 0.18831943\n",
      "Iteration 416, loss = 0.18667048\n",
      "Iteration 417, loss = 0.18589592\n",
      "Iteration 418, loss = 0.18617673\n",
      "Iteration 419, loss = 0.18528878\n",
      "Iteration 420, loss = 0.18432552\n",
      "Iteration 421, loss = 0.18433233\n",
      "Iteration 422, loss = 0.18360666\n",
      "Iteration 423, loss = 0.18316726\n",
      "Iteration 424, loss = 0.18292842\n",
      "Iteration 425, loss = 0.18303323\n",
      "Iteration 426, loss = 0.18220978\n",
      "Iteration 427, loss = 0.18219979\n",
      "Iteration 428, loss = 0.18162363\n",
      "Iteration 429, loss = 0.18138029\n",
      "Iteration 430, loss = 0.18111329\n",
      "Iteration 431, loss = 0.18122334\n",
      "Iteration 432, loss = 0.18090264\n",
      "Iteration 433, loss = 0.17987519\n",
      "Iteration 434, loss = 0.18006804\n",
      "Iteration 435, loss = 0.17931137\n",
      "Iteration 436, loss = 0.17929699\n",
      "Iteration 437, loss = 0.18033240\n",
      "Iteration 438, loss = 0.17899021\n",
      "Iteration 439, loss = 0.17894331\n",
      "Iteration 440, loss = 0.17806838\n",
      "Iteration 441, loss = 0.17745935\n",
      "Iteration 442, loss = 0.17703635\n",
      "Iteration 443, loss = 0.17699387\n",
      "Iteration 444, loss = 0.17670810\n",
      "Iteration 445, loss = 0.17667128\n",
      "Iteration 446, loss = 0.17601502\n",
      "Iteration 447, loss = 0.17593304\n",
      "Iteration 448, loss = 0.17604960\n",
      "Iteration 449, loss = 0.17450628\n",
      "Iteration 450, loss = 0.17619289\n",
      "Iteration 451, loss = 0.17806039\n",
      "Iteration 452, loss = 0.17588635\n",
      "Iteration 453, loss = 0.17365703\n",
      "Iteration 454, loss = 0.17324447\n",
      "Iteration 455, loss = 0.17336696\n",
      "Iteration 456, loss = 0.17347951\n",
      "Iteration 457, loss = 0.17248877\n",
      "Iteration 458, loss = 0.17249542\n",
      "Iteration 459, loss = 0.17260430\n",
      "Iteration 460, loss = 0.17160064\n",
      "Iteration 461, loss = 0.17106424\n",
      "Iteration 462, loss = 0.17101905\n",
      "Iteration 463, loss = 0.17101753\n",
      "Iteration 464, loss = 0.17045756\n",
      "Iteration 465, loss = 0.17104146\n",
      "Iteration 466, loss = 0.17141077\n",
      "Iteration 467, loss = 0.17144387\n",
      "Iteration 468, loss = 0.17163961\n",
      "Iteration 469, loss = 0.17052904\n",
      "Iteration 470, loss = 0.16973503\n",
      "Iteration 471, loss = 0.16846300\n",
      "Iteration 472, loss = 0.16800053\n",
      "Iteration 473, loss = 0.16761036\n",
      "Iteration 474, loss = 0.16784956\n",
      "Iteration 475, loss = 0.16803966\n",
      "Iteration 476, loss = 0.16744354\n",
      "Iteration 477, loss = 0.16764095\n",
      "Iteration 478, loss = 0.16651206\n",
      "Iteration 479, loss = 0.16644469\n",
      "Iteration 480, loss = 0.16649083\n",
      "Iteration 481, loss = 0.16568568\n",
      "Iteration 482, loss = 0.16555863\n",
      "Iteration 483, loss = 0.16496010\n",
      "Iteration 484, loss = 0.16447517\n",
      "Iteration 485, loss = 0.16466220\n",
      "Iteration 486, loss = 0.16411281\n",
      "Iteration 487, loss = 0.16360573\n",
      "Iteration 488, loss = 0.16338189\n",
      "Iteration 489, loss = 0.16310737\n",
      "Iteration 490, loss = 0.16272389\n",
      "Iteration 491, loss = 0.16268275\n",
      "Iteration 492, loss = 0.16255229\n",
      "Iteration 493, loss = 0.16198777\n",
      "Iteration 494, loss = 0.16188848\n",
      "Iteration 495, loss = 0.16178842\n",
      "Iteration 496, loss = 0.16147195\n",
      "Iteration 497, loss = 0.16125008\n",
      "Iteration 498, loss = 0.16071780\n",
      "Iteration 499, loss = 0.16039056\n",
      "Iteration 500, loss = 0.16118979\n",
      "Iteration 501, loss = 0.16160044\n",
      "Iteration 502, loss = 0.16166648\n",
      "Iteration 503, loss = 0.16091571\n",
      "Iteration 504, loss = 0.15935876\n",
      "Iteration 505, loss = 0.15890502\n",
      "Iteration 506, loss = 0.15868654\n",
      "Iteration 507, loss = 0.15797779\n",
      "Iteration 508, loss = 0.15852149\n",
      "Iteration 509, loss = 0.15845943\n",
      "Iteration 510, loss = 0.15723601\n",
      "Iteration 511, loss = 0.15880716\n",
      "Iteration 512, loss = 0.15905656\n",
      "Iteration 513, loss = 0.15830641\n",
      "Iteration 514, loss = 0.15746562\n",
      "Iteration 515, loss = 0.15671916\n",
      "Iteration 516, loss = 0.15984839\n",
      "Iteration 517, loss = 0.15776936\n",
      "Iteration 518, loss = 0.15570442\n",
      "Iteration 519, loss = 0.15561875\n",
      "Iteration 520, loss = 0.15538273\n",
      "Iteration 521, loss = 0.15451542\n",
      "Iteration 522, loss = 0.15416287\n",
      "Iteration 523, loss = 0.15395682\n",
      "Iteration 524, loss = 0.15382007\n",
      "Iteration 525, loss = 0.15384396\n",
      "Iteration 526, loss = 0.15511189\n",
      "Iteration 527, loss = 0.15318934\n",
      "Iteration 528, loss = 0.15386014\n",
      "Iteration 529, loss = 0.15328710\n",
      "Iteration 530, loss = 0.15201203\n",
      "Iteration 531, loss = 0.15228294\n",
      "Iteration 532, loss = 0.15236032\n",
      "Iteration 533, loss = 0.15198188\n",
      "Iteration 534, loss = 0.15240863\n",
      "Iteration 535, loss = 0.15146580\n",
      "Iteration 536, loss = 0.15052778\n",
      "Iteration 537, loss = 0.15003573\n",
      "Iteration 538, loss = 0.15010429\n",
      "Iteration 539, loss = 0.14993795\n",
      "Iteration 540, loss = 0.14962359\n",
      "Iteration 541, loss = 0.14917367\n",
      "Iteration 542, loss = 0.15039522\n",
      "Iteration 543, loss = 0.14924076\n",
      "Iteration 544, loss = 0.14837394\n",
      "Iteration 545, loss = 0.14832085\n",
      "Iteration 546, loss = 0.14799359\n",
      "Iteration 547, loss = 0.14793453\n",
      "Iteration 548, loss = 0.14729011\n",
      "Iteration 549, loss = 0.14709296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 550, loss = 0.14721911\n",
      "Iteration 551, loss = 0.14680018\n",
      "Iteration 552, loss = 0.14640852\n",
      "Iteration 553, loss = 0.14670158\n",
      "Iteration 554, loss = 0.14641310\n",
      "Iteration 555, loss = 0.14688416\n",
      "Iteration 556, loss = 0.14637439\n",
      "Iteration 557, loss = 0.14496916\n",
      "Iteration 558, loss = 0.14505300\n",
      "Iteration 559, loss = 0.14493022\n",
      "Iteration 560, loss = 0.14555041\n",
      "Iteration 561, loss = 0.14446373\n",
      "Iteration 562, loss = 0.14481077\n",
      "Iteration 563, loss = 0.14388478\n",
      "Iteration 564, loss = 0.14412619\n",
      "Iteration 565, loss = 0.14402632\n",
      "Iteration 566, loss = 0.14395430\n",
      "Iteration 567, loss = 0.14406448\n",
      "Iteration 568, loss = 0.14423044\n",
      "Iteration 569, loss = 0.14252506\n",
      "Iteration 570, loss = 0.14271302\n",
      "Iteration 571, loss = 0.14270095\n",
      "Iteration 572, loss = 0.14205379\n",
      "Iteration 573, loss = 0.14130356\n",
      "Iteration 574, loss = 0.14151489\n",
      "Iteration 575, loss = 0.14112688\n",
      "Iteration 576, loss = 0.14088078\n",
      "Iteration 577, loss = 0.14085723\n",
      "Iteration 578, loss = 0.14109714\n",
      "Iteration 579, loss = 0.14060675\n",
      "Iteration 580, loss = 0.14002696\n",
      "Iteration 581, loss = 0.13989027\n",
      "Iteration 582, loss = 0.14182519\n",
      "Iteration 583, loss = 0.14073211\n",
      "Iteration 584, loss = 0.14051519\n",
      "Iteration 585, loss = 0.13902603\n",
      "Iteration 586, loss = 0.13898542\n",
      "Iteration 587, loss = 0.13855170\n",
      "Iteration 588, loss = 0.13800193\n",
      "Iteration 589, loss = 0.13818726\n",
      "Iteration 590, loss = 0.13815306\n",
      "Iteration 591, loss = 0.13777076\n",
      "Iteration 592, loss = 0.13839286\n",
      "Iteration 593, loss = 0.13775363\n",
      "Iteration 594, loss = 0.13707357\n",
      "Iteration 595, loss = 0.13731048\n",
      "Iteration 596, loss = 0.13663597\n",
      "Iteration 597, loss = 0.13695920\n",
      "Iteration 598, loss = 0.13614294\n",
      "Iteration 599, loss = 0.13621573\n",
      "Iteration 600, loss = 0.13615401\n",
      "Iteration 601, loss = 0.13580324\n",
      "Iteration 602, loss = 0.13581356\n",
      "Iteration 603, loss = 0.13581211\n",
      "Iteration 604, loss = 0.13523149\n",
      "Iteration 605, loss = 0.13470119\n",
      "Iteration 606, loss = 0.13451006\n",
      "Iteration 607, loss = 0.13458205\n",
      "Iteration 608, loss = 0.13455758\n",
      "Iteration 609, loss = 0.13391527\n",
      "Iteration 610, loss = 0.13487545\n",
      "Iteration 611, loss = 0.13378658\n",
      "Iteration 612, loss = 0.13340282\n",
      "Iteration 613, loss = 0.13318966\n",
      "Iteration 614, loss = 0.13330748\n",
      "Iteration 615, loss = 0.13324003\n",
      "Iteration 616, loss = 0.13632426\n",
      "Iteration 617, loss = 0.13879280\n",
      "Iteration 618, loss = 0.13428195\n",
      "Iteration 619, loss = 0.13361535\n",
      "Iteration 620, loss = 0.13362287\n",
      "Iteration 621, loss = 0.13230601\n",
      "Iteration 622, loss = 0.13153572\n",
      "Iteration 623, loss = 0.13175178\n",
      "Iteration 624, loss = 0.13060717\n",
      "Iteration 625, loss = 0.13036516\n",
      "Iteration 626, loss = 0.13050135\n",
      "Iteration 627, loss = 0.13007073\n",
      "Iteration 628, loss = 0.13039819\n",
      "Iteration 629, loss = 0.13009811\n",
      "Iteration 630, loss = 0.12981375\n",
      "Iteration 631, loss = 0.12941091\n",
      "Iteration 632, loss = 0.12985025\n",
      "Iteration 633, loss = 0.12950880\n",
      "Iteration 634, loss = 0.12916295\n",
      "Iteration 635, loss = 0.12873357\n",
      "Iteration 636, loss = 0.12822449\n",
      "Iteration 637, loss = 0.12821305\n",
      "Iteration 638, loss = 0.12915928\n",
      "Iteration 639, loss = 0.12771451\n",
      "Iteration 640, loss = 0.12798868\n",
      "Iteration 641, loss = 0.12906645\n",
      "Iteration 642, loss = 0.12753162\n",
      "Iteration 643, loss = 0.12783719\n",
      "Iteration 644, loss = 0.12920295\n",
      "Iteration 645, loss = 0.13152641\n",
      "Iteration 646, loss = 0.12846034\n",
      "Iteration 647, loss = 0.12650384\n",
      "Iteration 648, loss = 0.12709900\n",
      "Iteration 649, loss = 0.12639708\n",
      "Iteration 650, loss = 0.12608006\n",
      "Iteration 651, loss = 0.12524479\n",
      "Iteration 652, loss = 0.12588701\n",
      "Iteration 653, loss = 0.12568269\n",
      "Iteration 654, loss = 0.12540833\n",
      "Iteration 655, loss = 0.12485730\n",
      "Iteration 656, loss = 0.12484857\n",
      "Iteration 657, loss = 0.12510065\n",
      "Iteration 658, loss = 0.12511157\n",
      "Iteration 659, loss = 0.12388942\n",
      "Iteration 660, loss = 0.12397570\n",
      "Iteration 661, loss = 0.12356623\n",
      "Iteration 662, loss = 0.12393353\n",
      "Iteration 663, loss = 0.12345322\n",
      "Iteration 664, loss = 0.12272009\n",
      "Iteration 665, loss = 0.12263659\n",
      "Iteration 666, loss = 0.12240067\n",
      "Iteration 667, loss = 0.12295811\n",
      "Iteration 668, loss = 0.12240509\n",
      "Iteration 669, loss = 0.12166845\n",
      "Iteration 670, loss = 0.12155611\n",
      "Iteration 671, loss = 0.12155621\n",
      "Iteration 672, loss = 0.12207124\n",
      "Iteration 673, loss = 0.12187136\n",
      "Iteration 674, loss = 0.12100612\n",
      "Iteration 675, loss = 0.12099060\n",
      "Iteration 676, loss = 0.12068257\n",
      "Iteration 677, loss = 0.12074579\n",
      "Iteration 678, loss = 0.12071681\n",
      "Iteration 679, loss = 0.12036185\n",
      "Iteration 680, loss = 0.12033762\n",
      "Iteration 681, loss = 0.12039409\n",
      "Iteration 682, loss = 0.11960288\n",
      "Iteration 683, loss = 0.11976858\n",
      "Iteration 684, loss = 0.11934007\n",
      "Iteration 685, loss = 0.11892025\n",
      "Iteration 686, loss = 0.11910637\n",
      "Iteration 687, loss = 0.11874718\n",
      "Iteration 688, loss = 0.11867930\n",
      "Iteration 689, loss = 0.11867227\n",
      "Iteration 690, loss = 0.11886632\n",
      "Iteration 691, loss = 0.11895051\n",
      "Iteration 692, loss = 0.11838098\n",
      "Iteration 693, loss = 0.11766530\n",
      "Iteration 694, loss = 0.11760806\n",
      "Iteration 695, loss = 0.11707799\n",
      "Iteration 696, loss = 0.11733374\n",
      "Iteration 697, loss = 0.11773239\n",
      "Iteration 698, loss = 0.11681354\n",
      "Iteration 699, loss = 0.11676329\n",
      "Iteration 700, loss = 0.11667817\n",
      "Iteration 701, loss = 0.11639266\n",
      "Iteration 702, loss = 0.11655855\n",
      "Iteration 703, loss = 0.11672074\n",
      "Iteration 704, loss = 0.11654117\n",
      "Iteration 705, loss = 0.11634768\n",
      "Iteration 706, loss = 0.11659040\n",
      "Iteration 707, loss = 0.11567868\n",
      "Iteration 708, loss = 0.11540876\n",
      "Iteration 709, loss = 0.11520464\n",
      "Iteration 710, loss = 0.11489643\n",
      "Iteration 711, loss = 0.11440221\n",
      "Iteration 712, loss = 0.11465438\n",
      "Iteration 713, loss = 0.11462370\n",
      "Iteration 714, loss = 0.11413589\n",
      "Iteration 715, loss = 0.11412708\n",
      "Iteration 716, loss = 0.11391701\n",
      "Iteration 717, loss = 0.11387228\n",
      "Iteration 718, loss = 0.11344816\n",
      "Iteration 719, loss = 0.11366658\n",
      "Iteration 720, loss = 0.11330229\n",
      "Iteration 721, loss = 0.11327472\n",
      "Iteration 722, loss = 0.11281589\n",
      "Iteration 723, loss = 0.11338178\n",
      "Iteration 724, loss = 0.11306827\n",
      "Iteration 725, loss = 0.11276389\n",
      "Iteration 726, loss = 0.11353031\n",
      "Iteration 727, loss = 0.11309375\n",
      "Iteration 728, loss = 0.11238396\n",
      "Iteration 729, loss = 0.11173813\n",
      "Iteration 730, loss = 0.11363666\n",
      "Iteration 731, loss = 0.11309068\n",
      "Iteration 732, loss = 0.11275713\n",
      "Iteration 733, loss = 0.11299530\n",
      "Iteration 734, loss = 0.11153698\n",
      "Iteration 735, loss = 0.11129277\n",
      "Iteration 736, loss = 0.11201899\n",
      "Iteration 737, loss = 0.11090499\n",
      "Iteration 738, loss = 0.11072253\n",
      "Iteration 739, loss = 0.11108350\n",
      "Iteration 740, loss = 0.11032124\n",
      "Iteration 741, loss = 0.11039784\n",
      "Iteration 742, loss = 0.10996756\n",
      "Iteration 743, loss = 0.10964294\n",
      "Iteration 744, loss = 0.10951678\n",
      "Iteration 745, loss = 0.10910941\n",
      "Iteration 746, loss = 0.10945435\n",
      "Iteration 747, loss = 0.10885102\n",
      "Iteration 748, loss = 0.10860864\n",
      "Iteration 749, loss = 0.10850222\n",
      "Iteration 750, loss = 0.10902766\n",
      "Iteration 751, loss = 0.10848173\n",
      "Iteration 752, loss = 0.10802644\n",
      "Iteration 753, loss = 0.10820867\n",
      "Iteration 754, loss = 0.10803286\n",
      "Iteration 755, loss = 0.10771934\n",
      "Iteration 756, loss = 0.10761548\n",
      "Iteration 757, loss = 0.10756568\n",
      "Iteration 758, loss = 0.10711535\n",
      "Iteration 759, loss = 0.10686306\n",
      "Iteration 760, loss = 0.10709802\n",
      "Iteration 761, loss = 0.10638332\n",
      "Iteration 762, loss = 0.10618395\n",
      "Iteration 763, loss = 0.10614284\n",
      "Iteration 764, loss = 0.10579018\n",
      "Iteration 765, loss = 0.10785619\n",
      "Iteration 766, loss = 0.10576918\n",
      "Iteration 767, loss = 0.10591784\n",
      "Iteration 768, loss = 0.10583345\n",
      "Iteration 769, loss = 0.10524508\n",
      "Iteration 770, loss = 0.10607749\n",
      "Iteration 771, loss = 0.10559470\n",
      "Iteration 772, loss = 0.10485788\n",
      "Iteration 773, loss = 0.10478177\n",
      "Iteration 774, loss = 0.10438105\n",
      "Iteration 775, loss = 0.10451907\n",
      "Iteration 776, loss = 0.10475447\n",
      "Iteration 777, loss = 0.10398655\n",
      "Iteration 778, loss = 0.10409402\n",
      "Iteration 779, loss = 0.10437477\n",
      "Iteration 780, loss = 0.10388033\n",
      "Iteration 781, loss = 0.10383052\n",
      "Iteration 782, loss = 0.10348868\n",
      "Iteration 783, loss = 0.10419705\n",
      "Iteration 784, loss = 0.10311919\n",
      "Iteration 785, loss = 0.10276428\n",
      "Iteration 786, loss = 0.10257600\n",
      "Iteration 787, loss = 0.10278990\n",
      "Iteration 788, loss = 0.10269516\n",
      "Iteration 789, loss = 0.10289379\n",
      "Iteration 790, loss = 0.10216822\n",
      "Iteration 791, loss = 0.10175045\n",
      "Iteration 792, loss = 0.10246667\n",
      "Iteration 793, loss = 0.10243979\n",
      "Iteration 794, loss = 0.10176378\n",
      "Iteration 795, loss = 0.10247305\n",
      "Iteration 796, loss = 0.10177421\n",
      "Iteration 797, loss = 0.10141995\n",
      "Iteration 798, loss = 0.10128289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 799, loss = 0.10155237\n",
      "Iteration 800, loss = 0.10128699\n",
      "Iteration 801, loss = 0.10275434\n",
      "Iteration 802, loss = 0.10175292\n",
      "Iteration 803, loss = 0.10064667\n",
      "Iteration 804, loss = 0.10020556\n",
      "Iteration 805, loss = 0.10024096\n",
      "Iteration 806, loss = 0.10024230\n",
      "Iteration 807, loss = 0.10038923\n",
      "Iteration 808, loss = 0.09985997\n",
      "Iteration 809, loss = 0.10014340\n",
      "Iteration 810, loss = 0.10024760\n",
      "Iteration 811, loss = 0.09945673\n",
      "Iteration 812, loss = 0.09914539\n",
      "Iteration 813, loss = 0.09882582\n",
      "Iteration 814, loss = 0.09892522\n",
      "Iteration 815, loss = 0.09924555\n",
      "Iteration 816, loss = 0.09877765\n",
      "Iteration 817, loss = 0.09891330\n",
      "Iteration 818, loss = 0.09960925\n",
      "Iteration 819, loss = 0.09906738\n",
      "Iteration 820, loss = 0.09919383\n",
      "Iteration 821, loss = 0.09825118\n",
      "Iteration 822, loss = 0.09782919\n",
      "Iteration 823, loss = 0.09764806\n",
      "Iteration 824, loss = 0.09716639\n",
      "Iteration 825, loss = 0.09834900\n",
      "Iteration 826, loss = 0.09736921\n",
      "Iteration 827, loss = 0.09821763\n",
      "Iteration 828, loss = 0.09699386\n",
      "Iteration 829, loss = 0.09657840\n",
      "Iteration 830, loss = 0.09669594\n",
      "Iteration 831, loss = 0.09665947\n",
      "Iteration 832, loss = 0.09665864\n",
      "Iteration 833, loss = 0.09719426\n",
      "Iteration 834, loss = 0.09635155\n",
      "Iteration 835, loss = 0.09612479\n",
      "Iteration 836, loss = 0.09646469\n",
      "Iteration 837, loss = 0.09696977\n",
      "Iteration 838, loss = 0.09743349\n",
      "Iteration 839, loss = 0.09842905\n",
      "Iteration 840, loss = 0.09944525\n",
      "Iteration 841, loss = 0.09645387\n",
      "Iteration 842, loss = 0.09656609\n",
      "Iteration 843, loss = 0.09541166\n",
      "Iteration 844, loss = 0.09546600\n",
      "Iteration 845, loss = 0.09520647\n",
      "Iteration 846, loss = 0.09534034\n",
      "Iteration 847, loss = 0.09500010\n",
      "Iteration 848, loss = 0.09492618\n",
      "Iteration 849, loss = 0.09517043\n",
      "Iteration 850, loss = 0.09510965\n",
      "Iteration 851, loss = 0.09429686\n",
      "Iteration 852, loss = 0.09508206\n",
      "Iteration 853, loss = 0.09528787\n",
      "Iteration 854, loss = 0.09533572\n",
      "Iteration 855, loss = 0.09423441\n",
      "Iteration 856, loss = 0.09356704\n",
      "Iteration 857, loss = 0.09357400\n",
      "Iteration 858, loss = 0.09361447\n",
      "Iteration 859, loss = 0.09365198\n",
      "Iteration 860, loss = 0.09369260\n",
      "Iteration 861, loss = 0.09400438\n",
      "Iteration 862, loss = 0.09304515\n",
      "Iteration 863, loss = 0.09309521\n",
      "Iteration 864, loss = 0.09292142\n",
      "Iteration 865, loss = 0.09362086\n",
      "Iteration 866, loss = 0.09265471\n",
      "Iteration 867, loss = 0.09219415\n",
      "Iteration 868, loss = 0.09236038\n",
      "Iteration 869, loss = 0.09230313\n",
      "Iteration 870, loss = 0.09218345\n",
      "Iteration 871, loss = 0.09250488\n",
      "Iteration 872, loss = 0.09240970\n",
      "Iteration 873, loss = 0.09214534\n",
      "Iteration 874, loss = 0.09205968\n",
      "Iteration 875, loss = 0.09198986\n",
      "Iteration 876, loss = 0.09143166\n",
      "Iteration 877, loss = 0.09145769\n",
      "Iteration 878, loss = 0.09126059\n",
      "Iteration 879, loss = 0.09120549\n",
      "Iteration 880, loss = 0.09127873\n",
      "Iteration 881, loss = 0.09090421\n",
      "Iteration 882, loss = 0.09065703\n",
      "Iteration 883, loss = 0.09037796\n",
      "Iteration 884, loss = 0.09068274\n",
      "Iteration 885, loss = 0.09084803\n",
      "Iteration 886, loss = 0.09036082\n",
      "Iteration 887, loss = 0.09051368\n",
      "Iteration 888, loss = 0.09059390\n",
      "Iteration 889, loss = 0.09121607\n",
      "Iteration 890, loss = 0.08963878\n",
      "Iteration 891, loss = 0.09065337\n",
      "Iteration 892, loss = 0.09023061\n",
      "Iteration 893, loss = 0.09007648\n",
      "Iteration 894, loss = 0.08999200\n",
      "Iteration 895, loss = 0.09040408\n",
      "Iteration 896, loss = 0.09012701\n",
      "Iteration 897, loss = 0.08981549\n",
      "Iteration 898, loss = 0.08898565\n",
      "Iteration 899, loss = 0.08907517\n",
      "Iteration 900, loss = 0.08887559\n",
      "Iteration 901, loss = 0.08886762\n",
      "Iteration 902, loss = 0.08889154\n",
      "Iteration 903, loss = 0.08845507\n",
      "Iteration 904, loss = 0.08891291\n",
      "Iteration 905, loss = 0.08821427\n",
      "Iteration 906, loss = 0.08847173\n",
      "Iteration 907, loss = 0.08851388\n",
      "Iteration 908, loss = 0.08871397\n",
      "Iteration 909, loss = 0.08854685\n",
      "Iteration 910, loss = 0.08826362\n",
      "Iteration 911, loss = 0.08839455\n",
      "Iteration 912, loss = 0.08787802\n",
      "Iteration 913, loss = 0.08769394\n",
      "Iteration 914, loss = 0.08729790\n",
      "Iteration 915, loss = 0.08816422\n",
      "Iteration 916, loss = 0.08744486\n",
      "Iteration 917, loss = 0.08722901\n",
      "Iteration 918, loss = 0.08750444\n",
      "Iteration 919, loss = 0.08721506\n",
      "Iteration 920, loss = 0.08705985\n",
      "Iteration 921, loss = 0.08691955\n",
      "Iteration 922, loss = 0.08706559\n",
      "Iteration 923, loss = 0.08681056\n",
      "Iteration 924, loss = 0.08669618\n",
      "Iteration 925, loss = 0.08708928\n",
      "Iteration 926, loss = 0.08638172\n",
      "Iteration 927, loss = 0.08695803\n",
      "Iteration 928, loss = 0.08603331\n",
      "Iteration 929, loss = 0.08649220\n",
      "Iteration 930, loss = 0.08596635\n",
      "Iteration 931, loss = 0.08645904\n",
      "Iteration 932, loss = 0.08594574\n",
      "Iteration 933, loss = 0.08580082\n",
      "Iteration 934, loss = 0.08604116\n",
      "Iteration 935, loss = 0.08621423\n",
      "Iteration 936, loss = 0.08601697\n",
      "Iteration 937, loss = 0.08548703\n",
      "Iteration 938, loss = 0.08513909\n",
      "Iteration 939, loss = 0.08560125\n",
      "Iteration 940, loss = 0.08552302\n",
      "Iteration 941, loss = 0.08486878\n",
      "Iteration 942, loss = 0.08487351\n",
      "Iteration 943, loss = 0.08504045\n",
      "Iteration 944, loss = 0.08472414\n",
      "Iteration 945, loss = 0.08456274\n",
      "Iteration 946, loss = 0.08453391\n",
      "Iteration 947, loss = 0.08473106\n",
      "Iteration 948, loss = 0.08477591\n",
      "Iteration 949, loss = 0.08462208\n",
      "Iteration 950, loss = 0.08485633\n",
      "Iteration 951, loss = 0.08418308\n",
      "Iteration 952, loss = 0.08558213\n",
      "Iteration 953, loss = 0.08461641\n",
      "Iteration 954, loss = 0.08425700\n",
      "Iteration 955, loss = 0.08384814\n",
      "Iteration 956, loss = 0.08448122\n",
      "Iteration 957, loss = 0.08433112\n",
      "Iteration 958, loss = 0.08404753\n",
      "Iteration 959, loss = 0.08635713\n",
      "Iteration 960, loss = 0.08507091\n",
      "Iteration 961, loss = 0.08471372\n",
      "Iteration 962, loss = 0.08458827\n",
      "Iteration 963, loss = 0.09022770\n",
      "Iteration 964, loss = 0.09077604\n",
      "Iteration 965, loss = 0.08579714\n",
      "Iteration 966, loss = 0.08357207\n",
      "Iteration 967, loss = 0.08333366\n",
      "Iteration 968, loss = 0.08291707\n",
      "Iteration 969, loss = 0.08326572\n",
      "Iteration 970, loss = 0.08282046\n",
      "Iteration 971, loss = 0.08236471\n",
      "Iteration 972, loss = 0.08244817\n",
      "Iteration 973, loss = 0.08234118\n",
      "Iteration 974, loss = 0.08234960\n",
      "Iteration 975, loss = 0.08213430\n",
      "Iteration 976, loss = 0.08194637\n",
      "Iteration 977, loss = 0.08229724\n",
      "Iteration 978, loss = 0.08202629\n",
      "Iteration 979, loss = 0.08183515\n",
      "Iteration 980, loss = 0.08171919\n",
      "Iteration 981, loss = 0.08155044\n",
      "Iteration 982, loss = 0.08144245\n",
      "Iteration 983, loss = 0.08178139\n",
      "Iteration 984, loss = 0.08169979\n",
      "Iteration 985, loss = 0.08171785\n",
      "Iteration 986, loss = 0.08126592\n",
      "Iteration 987, loss = 0.08121264\n",
      "Iteration 988, loss = 0.08136676\n",
      "Iteration 989, loss = 0.08127903\n",
      "Iteration 990, loss = 0.08078520\n",
      "Iteration 991, loss = 0.08140739\n",
      "Iteration 992, loss = 0.08085342\n",
      "Iteration 993, loss = 0.08120867\n",
      "Iteration 994, loss = 0.08072684\n",
      "Iteration 995, loss = 0.08081101\n",
      "Iteration 996, loss = 0.08143806\n",
      "Iteration 997, loss = 0.08205058\n",
      "Iteration 998, loss = 0.08102814\n",
      "Iteration 999, loss = 0.08141556\n",
      "Iteration 1000, loss = 0.08115703\n",
      "Iteration 1001, loss = 0.08002606\n",
      "Iteration 1002, loss = 0.08020798\n",
      "Iteration 1003, loss = 0.07990469\n",
      "Iteration 1004, loss = 0.08064924\n",
      "Iteration 1005, loss = 0.07993739\n",
      "Iteration 1006, loss = 0.08003970\n",
      "Iteration 1007, loss = 0.07966784\n",
      "Iteration 1008, loss = 0.08007279\n",
      "Iteration 1009, loss = 0.07949976\n",
      "Iteration 1010, loss = 0.07978010\n",
      "Iteration 1011, loss = 0.07956400\n",
      "Iteration 1012, loss = 0.07939980\n",
      "Iteration 1013, loss = 0.07920069\n",
      "Iteration 1014, loss = 0.08019332\n",
      "Iteration 1015, loss = 0.07881365\n",
      "Iteration 1016, loss = 0.07890872\n",
      "Iteration 1017, loss = 0.07935436\n",
      "Iteration 1018, loss = 0.07893123\n",
      "Iteration 1019, loss = 0.07854978\n",
      "Iteration 1020, loss = 0.07928122\n",
      "Iteration 1021, loss = 0.07892616\n",
      "Iteration 1022, loss = 0.07846859\n",
      "Iteration 1023, loss = 0.07891693\n",
      "Iteration 1024, loss = 0.07899228\n",
      "Iteration 1025, loss = 0.07812160\n",
      "Iteration 1026, loss = 0.07849829\n",
      "Iteration 1027, loss = 0.07815501\n",
      "Iteration 1028, loss = 0.07845415\n",
      "Iteration 1029, loss = 0.07795562\n",
      "Iteration 1030, loss = 0.07798215\n",
      "Iteration 1031, loss = 0.07791447\n",
      "Iteration 1032, loss = 0.07788335\n",
      "Iteration 1033, loss = 0.07808843\n",
      "Iteration 1034, loss = 0.07793112\n",
      "Iteration 1035, loss = 0.07786190\n",
      "Iteration 1036, loss = 0.07743856\n",
      "Iteration 1037, loss = 0.07791821\n",
      "Iteration 1038, loss = 0.07748355\n",
      "Iteration 1039, loss = 0.07743132\n",
      "Iteration 1040, loss = 0.07737921\n",
      "Iteration 1041, loss = 0.07741669\n",
      "Iteration 1042, loss = 0.07731627\n",
      "Iteration 1043, loss = 0.07713962\n",
      "Iteration 1044, loss = 0.07738401\n",
      "Iteration 1045, loss = 0.07722632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1046, loss = 0.07750860\n",
      "Iteration 1047, loss = 0.07704782\n",
      "Iteration 1048, loss = 0.07674260\n",
      "Iteration 1049, loss = 0.07637192\n",
      "Iteration 1050, loss = 0.07733645\n",
      "Iteration 1051, loss = 0.07623482\n",
      "Iteration 1052, loss = 0.07631055\n",
      "Iteration 1053, loss = 0.07632412\n",
      "Iteration 1054, loss = 0.07638437\n",
      "Iteration 1055, loss = 0.07656463\n",
      "Iteration 1056, loss = 0.07695463\n",
      "Iteration 1057, loss = 0.07619473\n",
      "Iteration 1058, loss = 0.07616970\n",
      "Iteration 1059, loss = 0.07642274\n",
      "Iteration 1060, loss = 0.07723612\n",
      "Iteration 1061, loss = 0.07672271\n",
      "Iteration 1062, loss = 0.07589762\n",
      "Iteration 1063, loss = 0.07654942\n",
      "Iteration 1064, loss = 0.07574909\n",
      "Iteration 1065, loss = 0.07581110\n",
      "Iteration 1066, loss = 0.07572329\n",
      "Iteration 1067, loss = 0.07599654\n",
      "Iteration 1068, loss = 0.07652524\n",
      "Iteration 1069, loss = 0.07570314\n",
      "Iteration 1070, loss = 0.07572615\n",
      "Iteration 1071, loss = 0.07612758\n",
      "Iteration 1072, loss = 0.07585507\n",
      "Iteration 1073, loss = 0.07566636\n",
      "Iteration 1074, loss = 0.07579351\n",
      "Iteration 1075, loss = 0.07574779\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 4 of 48...\n",
      "Iteration 1, loss = 0.67989180\n",
      "Iteration 2, loss = 0.60236588\n",
      "Iteration 3, loss = 0.56357203\n",
      "Iteration 4, loss = 0.53656710\n",
      "Iteration 5, loss = 0.51206634\n",
      "Iteration 6, loss = 0.49536968\n",
      "Iteration 7, loss = 0.47885819\n",
      "Iteration 8, loss = 0.46621854\n",
      "Iteration 9, loss = 0.45510817\n",
      "Iteration 10, loss = 0.44330252\n",
      "Iteration 11, loss = 0.43385964\n",
      "Iteration 12, loss = 0.42402266\n",
      "Iteration 13, loss = 0.41683543\n",
      "Iteration 14, loss = 0.41226234\n",
      "Iteration 15, loss = 0.40358152\n",
      "Iteration 16, loss = 0.39750191\n",
      "Iteration 17, loss = 0.39474522\n",
      "Iteration 18, loss = 0.38802145\n",
      "Iteration 19, loss = 0.38128726\n",
      "Iteration 20, loss = 0.37622305\n",
      "Iteration 21, loss = 0.37134334\n",
      "Iteration 22, loss = 0.36724341\n",
      "Iteration 23, loss = 0.36456418\n",
      "Iteration 24, loss = 0.36132194\n",
      "Iteration 25, loss = 0.35870772\n",
      "Iteration 26, loss = 0.35548185\n",
      "Iteration 27, loss = 0.35107433\n",
      "Iteration 28, loss = 0.34644714\n",
      "Iteration 29, loss = 0.34354337\n",
      "Iteration 30, loss = 0.33930076\n",
      "Iteration 31, loss = 0.33975091\n",
      "Iteration 32, loss = 0.34112995\n",
      "Iteration 33, loss = 0.33171801\n",
      "Iteration 34, loss = 0.33268264\n",
      "Iteration 35, loss = 0.33173252\n",
      "Iteration 36, loss = 0.32418850\n",
      "Iteration 37, loss = 0.32032093\n",
      "Iteration 38, loss = 0.31876117\n",
      "Iteration 39, loss = 0.31842340\n",
      "Iteration 40, loss = 0.31360455\n",
      "Iteration 41, loss = 0.31123098\n",
      "Iteration 42, loss = 0.31021977\n",
      "Iteration 43, loss = 0.30695634\n",
      "Iteration 44, loss = 0.30450638\n",
      "Iteration 45, loss = 0.30456197\n",
      "Iteration 46, loss = 0.30124353\n",
      "Iteration 47, loss = 0.30034035\n",
      "Iteration 48, loss = 0.29888597\n",
      "Iteration 49, loss = 0.29486980\n",
      "Iteration 50, loss = 0.29269805\n",
      "Iteration 51, loss = 0.29240592\n",
      "Iteration 52, loss = 0.28936164\n",
      "Iteration 53, loss = 0.28747357\n",
      "Iteration 54, loss = 0.28477845\n",
      "Iteration 55, loss = 0.28504492\n",
      "Iteration 56, loss = 0.28147664\n",
      "Iteration 57, loss = 0.27911038\n",
      "Iteration 58, loss = 0.27753082\n",
      "Iteration 59, loss = 0.27659142\n",
      "Iteration 60, loss = 0.27237020\n",
      "Iteration 61, loss = 0.27193937\n",
      "Iteration 62, loss = 0.27067074\n",
      "Iteration 63, loss = 0.26850596\n",
      "Iteration 64, loss = 0.26788026\n",
      "Iteration 65, loss = 0.26744357\n",
      "Iteration 66, loss = 0.26548801\n",
      "Iteration 67, loss = 0.26270366\n",
      "Iteration 68, loss = 0.26429415\n",
      "Iteration 69, loss = 0.26179542\n",
      "Iteration 70, loss = 0.25987280\n",
      "Iteration 71, loss = 0.25869555\n",
      "Iteration 72, loss = 0.25951269\n",
      "Iteration 73, loss = 0.25215615\n",
      "Iteration 74, loss = 0.24995108\n",
      "Iteration 75, loss = 0.24986913\n",
      "Iteration 76, loss = 0.24760307\n",
      "Iteration 77, loss = 0.24529363\n",
      "Iteration 78, loss = 0.24613026\n",
      "Iteration 79, loss = 0.24503199\n",
      "Iteration 80, loss = 0.24312895\n",
      "Iteration 81, loss = 0.24184929\n",
      "Iteration 82, loss = 0.23975008\n",
      "Iteration 83, loss = 0.23917985\n",
      "Iteration 84, loss = 0.23791646\n",
      "Iteration 85, loss = 0.23957054\n",
      "Iteration 86, loss = 0.23978101\n",
      "Iteration 87, loss = 0.23760012\n",
      "Iteration 88, loss = 0.23474623\n",
      "Iteration 89, loss = 0.22859025\n",
      "Iteration 90, loss = 0.22780969\n",
      "Iteration 91, loss = 0.22757854\n",
      "Iteration 92, loss = 0.22563087\n",
      "Iteration 93, loss = 0.22296291\n",
      "Iteration 94, loss = 0.22246809\n",
      "Iteration 95, loss = 0.22329527\n",
      "Iteration 96, loss = 0.21813101\n",
      "Iteration 97, loss = 0.21811187\n",
      "Iteration 98, loss = 0.21667710\n",
      "Iteration 99, loss = 0.21511054\n",
      "Iteration 100, loss = 0.21486564\n",
      "Iteration 101, loss = 0.21276620\n",
      "Iteration 102, loss = 0.21405305\n",
      "Iteration 103, loss = 0.21275757\n",
      "Iteration 104, loss = 0.21265832\n",
      "Iteration 105, loss = 0.20847679\n",
      "Iteration 106, loss = 0.20710672\n",
      "Iteration 107, loss = 0.20672077\n",
      "Iteration 108, loss = 0.20528861\n",
      "Iteration 109, loss = 0.20521616\n",
      "Iteration 110, loss = 0.20291245\n",
      "Iteration 111, loss = 0.20072824\n",
      "Iteration 112, loss = 0.19902550\n",
      "Iteration 113, loss = 0.19828657\n",
      "Iteration 114, loss = 0.19798761\n",
      "Iteration 115, loss = 0.19628523\n",
      "Iteration 116, loss = 0.19917576\n",
      "Iteration 117, loss = 0.19507312\n",
      "Iteration 118, loss = 0.19394162\n",
      "Iteration 119, loss = 0.19109319\n",
      "Iteration 120, loss = 0.18948064\n",
      "Iteration 121, loss = 0.18944327\n",
      "Iteration 122, loss = 0.18961043\n",
      "Iteration 123, loss = 0.18990813\n",
      "Iteration 124, loss = 0.19163396\n",
      "Iteration 125, loss = 0.18350695\n",
      "Iteration 126, loss = 0.18489858\n",
      "Iteration 127, loss = 0.18179706\n",
      "Iteration 128, loss = 0.18179617\n",
      "Iteration 129, loss = 0.18139865\n",
      "Iteration 130, loss = 0.18015189\n",
      "Iteration 131, loss = 0.17841406\n",
      "Iteration 132, loss = 0.17694072\n",
      "Iteration 133, loss = 0.17640686\n",
      "Iteration 134, loss = 0.17384252\n",
      "Iteration 135, loss = 0.17287533\n",
      "Iteration 136, loss = 0.17532552\n",
      "Iteration 137, loss = 0.17406343\n",
      "Iteration 138, loss = 0.17125287\n",
      "Iteration 139, loss = 0.16911513\n",
      "Iteration 140, loss = 0.16822028\n",
      "Iteration 141, loss = 0.16713351\n",
      "Iteration 142, loss = 0.16550491\n",
      "Iteration 143, loss = 0.16469703\n",
      "Iteration 144, loss = 0.16451899\n",
      "Iteration 145, loss = 0.16280461\n",
      "Iteration 146, loss = 0.16255275\n",
      "Iteration 147, loss = 0.16157170\n",
      "Iteration 148, loss = 0.15939170\n",
      "Iteration 149, loss = 0.15953532\n",
      "Iteration 150, loss = 0.15851068\n",
      "Iteration 151, loss = 0.15591543\n",
      "Iteration 152, loss = 0.15486294\n",
      "Iteration 153, loss = 0.15419966\n",
      "Iteration 154, loss = 0.15493320\n",
      "Iteration 155, loss = 0.15195400\n",
      "Iteration 156, loss = 0.15177322\n",
      "Iteration 157, loss = 0.15048006\n",
      "Iteration 158, loss = 0.15004581\n",
      "Iteration 159, loss = 0.14970066\n",
      "Iteration 160, loss = 0.14814791\n",
      "Iteration 161, loss = 0.14683847\n",
      "Iteration 162, loss = 0.14606480\n",
      "Iteration 163, loss = 0.14498158\n",
      "Iteration 164, loss = 0.14647747\n",
      "Iteration 165, loss = 0.14304252\n",
      "Iteration 166, loss = 0.14166708\n",
      "Iteration 167, loss = 0.14374632\n",
      "Iteration 168, loss = 0.13990099\n",
      "Iteration 169, loss = 0.13850263\n",
      "Iteration 170, loss = 0.13792495\n",
      "Iteration 171, loss = 0.13703322\n",
      "Iteration 172, loss = 0.13747229\n",
      "Iteration 173, loss = 0.13725044\n",
      "Iteration 174, loss = 0.13623065\n",
      "Iteration 175, loss = 0.13756857\n",
      "Iteration 176, loss = 0.13413326\n",
      "Iteration 177, loss = 0.13164098\n",
      "Iteration 178, loss = 0.13089471\n",
      "Iteration 179, loss = 0.12939223\n",
      "Iteration 180, loss = 0.12885949\n",
      "Iteration 181, loss = 0.12792713\n",
      "Iteration 182, loss = 0.12709566\n",
      "Iteration 183, loss = 0.12719116\n",
      "Iteration 184, loss = 0.12754305\n",
      "Iteration 185, loss = 0.12549836\n",
      "Iteration 186, loss = 0.12456878\n",
      "Iteration 187, loss = 0.12679863\n",
      "Iteration 188, loss = 0.12585524\n",
      "Iteration 189, loss = 0.12432158\n",
      "Iteration 190, loss = 0.12132181\n",
      "Iteration 191, loss = 0.12246429\n",
      "Iteration 192, loss = 0.12083000\n",
      "Iteration 193, loss = 0.11750261\n",
      "Iteration 194, loss = 0.11765649\n",
      "Iteration 195, loss = 0.11576875\n",
      "Iteration 196, loss = 0.11555443\n",
      "Iteration 197, loss = 0.11515597\n",
      "Iteration 198, loss = 0.11686007\n",
      "Iteration 199, loss = 0.11647647\n",
      "Iteration 200, loss = 0.11573690\n",
      "Iteration 201, loss = 0.11309462\n",
      "Iteration 202, loss = 0.11123482\n",
      "Iteration 203, loss = 0.11117613\n",
      "Iteration 204, loss = 0.11266029\n",
      "Iteration 205, loss = 0.11551921\n",
      "Iteration 206, loss = 0.10888455\n",
      "Iteration 207, loss = 0.10792161\n",
      "Iteration 208, loss = 0.10775675\n",
      "Iteration 209, loss = 0.10755824\n",
      "Iteration 210, loss = 0.10596667\n",
      "Iteration 211, loss = 0.10477796\n",
      "Iteration 212, loss = 0.10533552\n",
      "Iteration 213, loss = 0.10626123\n",
      "Iteration 214, loss = 0.10428539\n",
      "Iteration 215, loss = 0.10218028\n",
      "Iteration 216, loss = 0.10263908\n",
      "Iteration 217, loss = 0.10277422\n",
      "Iteration 218, loss = 0.10007003\n",
      "Iteration 219, loss = 0.10046701\n",
      "Iteration 220, loss = 0.09926060\n",
      "Iteration 221, loss = 0.09791598\n",
      "Iteration 222, loss = 0.09757536\n",
      "Iteration 223, loss = 0.09855141\n",
      "Iteration 224, loss = 0.09583016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 225, loss = 0.09618702\n",
      "Iteration 226, loss = 0.09756251\n",
      "Iteration 227, loss = 0.09426880\n",
      "Iteration 228, loss = 0.09355886\n",
      "Iteration 229, loss = 0.09341351\n",
      "Iteration 230, loss = 0.09269540\n",
      "Iteration 231, loss = 0.09158300\n",
      "Iteration 232, loss = 0.09357027\n",
      "Iteration 233, loss = 0.09091847\n",
      "Iteration 234, loss = 0.08991753\n",
      "Iteration 235, loss = 0.09215166\n",
      "Iteration 236, loss = 0.09466190\n",
      "Iteration 237, loss = 0.09048819\n",
      "Iteration 238, loss = 0.09161548\n",
      "Iteration 239, loss = 0.09196590\n",
      "Iteration 240, loss = 0.08796441\n",
      "Iteration 241, loss = 0.08765419\n",
      "Iteration 242, loss = 0.08680190\n",
      "Iteration 243, loss = 0.08808147\n",
      "Iteration 244, loss = 0.08645101\n",
      "Iteration 245, loss = 0.08806575\n",
      "Iteration 246, loss = 0.08560821\n",
      "Iteration 247, loss = 0.08377981\n",
      "Iteration 248, loss = 0.08307187\n",
      "Iteration 249, loss = 0.08255857\n",
      "Iteration 250, loss = 0.08155581\n",
      "Iteration 251, loss = 0.08096648\n",
      "Iteration 252, loss = 0.08112956\n",
      "Iteration 253, loss = 0.07975065\n",
      "Iteration 254, loss = 0.07919294\n",
      "Iteration 255, loss = 0.08055284\n",
      "Iteration 256, loss = 0.07861903\n",
      "Iteration 257, loss = 0.07894911\n",
      "Iteration 258, loss = 0.07910542\n",
      "Iteration 259, loss = 0.07783132\n",
      "Iteration 260, loss = 0.07659178\n",
      "Iteration 261, loss = 0.07538377\n",
      "Iteration 262, loss = 0.07785866\n",
      "Iteration 263, loss = 0.07683195\n",
      "Iteration 264, loss = 0.07519168\n",
      "Iteration 265, loss = 0.07449018\n",
      "Iteration 266, loss = 0.07585577\n",
      "Iteration 267, loss = 0.07379953\n",
      "Iteration 268, loss = 0.07253024\n",
      "Iteration 269, loss = 0.07223708\n",
      "Iteration 270, loss = 0.07183026\n",
      "Iteration 271, loss = 0.07098564\n",
      "Iteration 272, loss = 0.07083808\n",
      "Iteration 273, loss = 0.07113995\n",
      "Iteration 274, loss = 0.07305141\n",
      "Iteration 275, loss = 0.06936540\n",
      "Iteration 276, loss = 0.06982186\n",
      "Iteration 277, loss = 0.06935456\n",
      "Iteration 278, loss = 0.06863328\n",
      "Iteration 279, loss = 0.06702941\n",
      "Iteration 280, loss = 0.06716187\n",
      "Iteration 281, loss = 0.06831928\n",
      "Iteration 282, loss = 0.06763177\n",
      "Iteration 283, loss = 0.06772844\n",
      "Iteration 284, loss = 0.06716112\n",
      "Iteration 285, loss = 0.06557087\n",
      "Iteration 286, loss = 0.06557245\n",
      "Iteration 287, loss = 0.06488999\n",
      "Iteration 288, loss = 0.06421992\n",
      "Iteration 289, loss = 0.06470674\n",
      "Iteration 290, loss = 0.06495585\n",
      "Iteration 291, loss = 0.06317598\n",
      "Iteration 292, loss = 0.06281903\n",
      "Iteration 293, loss = 0.06192085\n",
      "Iteration 294, loss = 0.06273291\n",
      "Iteration 295, loss = 0.06197044\n",
      "Iteration 296, loss = 0.06101181\n",
      "Iteration 297, loss = 0.06118175\n",
      "Iteration 298, loss = 0.06098387\n",
      "Iteration 299, loss = 0.06027451\n",
      "Iteration 300, loss = 0.05949344\n",
      "Iteration 301, loss = 0.05854427\n",
      "Iteration 302, loss = 0.05906299\n",
      "Iteration 303, loss = 0.05847486\n",
      "Iteration 304, loss = 0.05799795\n",
      "Iteration 305, loss = 0.05873304\n",
      "Iteration 306, loss = 0.05897419\n",
      "Iteration 307, loss = 0.05719077\n",
      "Iteration 308, loss = 0.05662025\n",
      "Iteration 309, loss = 0.05586615\n",
      "Iteration 310, loss = 0.05844594\n",
      "Iteration 311, loss = 0.05732627\n",
      "Iteration 312, loss = 0.05718950\n",
      "Iteration 313, loss = 0.05652807\n",
      "Iteration 314, loss = 0.05446087\n",
      "Iteration 315, loss = 0.05473381\n",
      "Iteration 316, loss = 0.05502431\n",
      "Iteration 317, loss = 0.05364094\n",
      "Iteration 318, loss = 0.05525992\n",
      "Iteration 319, loss = 0.05438171\n",
      "Iteration 320, loss = 0.05392068\n",
      "Iteration 321, loss = 0.05307820\n",
      "Iteration 322, loss = 0.05233694\n",
      "Iteration 323, loss = 0.05281806\n",
      "Iteration 324, loss = 0.05240530\n",
      "Iteration 325, loss = 0.05136918\n",
      "Iteration 326, loss = 0.05079171\n",
      "Iteration 327, loss = 0.05099090\n",
      "Iteration 328, loss = 0.05120981\n",
      "Iteration 329, loss = 0.05019433\n",
      "Iteration 330, loss = 0.04935069\n",
      "Iteration 331, loss = 0.04914048\n",
      "Iteration 332, loss = 0.04897806\n",
      "Iteration 333, loss = 0.04881938\n",
      "Iteration 334, loss = 0.04873004\n",
      "Iteration 335, loss = 0.04860933\n",
      "Iteration 336, loss = 0.04975028\n",
      "Iteration 337, loss = 0.04876643\n",
      "Iteration 338, loss = 0.04715710\n",
      "Iteration 339, loss = 0.04665009\n",
      "Iteration 340, loss = 0.04732419\n",
      "Iteration 341, loss = 0.04624089\n",
      "Iteration 342, loss = 0.04652481\n",
      "Iteration 343, loss = 0.04573919\n",
      "Iteration 344, loss = 0.04606360\n",
      "Iteration 345, loss = 0.04760029\n",
      "Iteration 346, loss = 0.04522779\n",
      "Iteration 347, loss = 0.04575604\n",
      "Iteration 348, loss = 0.04471674\n",
      "Iteration 349, loss = 0.04585896\n",
      "Iteration 350, loss = 0.04448678\n",
      "Iteration 351, loss = 0.04357034\n",
      "Iteration 352, loss = 0.04301623\n",
      "Iteration 353, loss = 0.04345885\n",
      "Iteration 354, loss = 0.04595022\n",
      "Iteration 355, loss = 0.04449254\n",
      "Iteration 356, loss = 0.04290117\n",
      "Iteration 357, loss = 0.04203070\n",
      "Iteration 358, loss = 0.04137226\n",
      "Iteration 359, loss = 0.04099209\n",
      "Iteration 360, loss = 0.04146081\n",
      "Iteration 361, loss = 0.04070946\n",
      "Iteration 362, loss = 0.04079952\n",
      "Iteration 363, loss = 0.04103284\n",
      "Iteration 364, loss = 0.03991849\n",
      "Iteration 365, loss = 0.03993075\n",
      "Iteration 366, loss = 0.03987581\n",
      "Iteration 367, loss = 0.03941712\n",
      "Iteration 368, loss = 0.03885999\n",
      "Iteration 369, loss = 0.03890681\n",
      "Iteration 370, loss = 0.03890382\n",
      "Iteration 371, loss = 0.03893329\n",
      "Iteration 372, loss = 0.03876910\n",
      "Iteration 373, loss = 0.04108272\n",
      "Iteration 374, loss = 0.03825325\n",
      "Iteration 375, loss = 0.03728940\n",
      "Iteration 376, loss = 0.03790823\n",
      "Iteration 377, loss = 0.03714622\n",
      "Iteration 378, loss = 0.03836662\n",
      "Iteration 379, loss = 0.03677585\n",
      "Iteration 380, loss = 0.03617877\n",
      "Iteration 381, loss = 0.03630361\n",
      "Iteration 382, loss = 0.03632239\n",
      "Iteration 383, loss = 0.03635836\n",
      "Iteration 384, loss = 0.03671209\n",
      "Iteration 385, loss = 0.03825212\n",
      "Iteration 386, loss = 0.03549283\n",
      "Iteration 387, loss = 0.03489194\n",
      "Iteration 388, loss = 0.03496817\n",
      "Iteration 389, loss = 0.03428948\n",
      "Iteration 390, loss = 0.03449307\n",
      "Iteration 391, loss = 0.03425711\n",
      "Iteration 392, loss = 0.03430417\n",
      "Iteration 393, loss = 0.03356506\n",
      "Iteration 394, loss = 0.03408038\n",
      "Iteration 395, loss = 0.03431585\n",
      "Iteration 396, loss = 0.03377361\n",
      "Iteration 397, loss = 0.03356549\n",
      "Iteration 398, loss = 0.03342296\n",
      "Iteration 399, loss = 0.03353395\n",
      "Iteration 400, loss = 0.03375711\n",
      "Iteration 401, loss = 0.03299357\n",
      "Iteration 402, loss = 0.03283740\n",
      "Iteration 403, loss = 0.03282013\n",
      "Iteration 404, loss = 0.03309093\n",
      "Iteration 405, loss = 0.03195368\n",
      "Iteration 406, loss = 0.03203592\n",
      "Iteration 407, loss = 0.03108104\n",
      "Iteration 408, loss = 0.03100352\n",
      "Iteration 409, loss = 0.03117834\n",
      "Iteration 410, loss = 0.03062320\n",
      "Iteration 411, loss = 0.03034015\n",
      "Iteration 412, loss = 0.03096835\n",
      "Iteration 413, loss = 0.03053180\n",
      "Iteration 414, loss = 0.03010592\n",
      "Iteration 415, loss = 0.03055835\n",
      "Iteration 416, loss = 0.03035728\n",
      "Iteration 417, loss = 0.02986798\n",
      "Iteration 418, loss = 0.02997307\n",
      "Iteration 419, loss = 0.03019680\n",
      "Iteration 420, loss = 0.02925707\n",
      "Iteration 421, loss = 0.02857906\n",
      "Iteration 422, loss = 0.02827226\n",
      "Iteration 423, loss = 0.02823559\n",
      "Iteration 424, loss = 0.02880406\n",
      "Iteration 425, loss = 0.02840070\n",
      "Iteration 426, loss = 0.02764408\n",
      "Iteration 427, loss = 0.02787955\n",
      "Iteration 428, loss = 0.02778669\n",
      "Iteration 429, loss = 0.02671750\n",
      "Iteration 430, loss = 0.02734268\n",
      "Iteration 431, loss = 0.02714814\n",
      "Iteration 432, loss = 0.02803229\n",
      "Iteration 433, loss = 0.02827606\n",
      "Iteration 434, loss = 0.02865463\n",
      "Iteration 435, loss = 0.02775212\n",
      "Iteration 436, loss = 0.02697776\n",
      "Iteration 437, loss = 0.02645299\n",
      "Iteration 438, loss = 0.02573878\n",
      "Iteration 439, loss = 0.02595684\n",
      "Iteration 440, loss = 0.02546271\n",
      "Iteration 441, loss = 0.02542943\n",
      "Iteration 442, loss = 0.02515536\n",
      "Iteration 443, loss = 0.02514432\n",
      "Iteration 444, loss = 0.02503534\n",
      "Iteration 445, loss = 0.02564320\n",
      "Iteration 446, loss = 0.02473366\n",
      "Iteration 447, loss = 0.02413074\n",
      "Iteration 448, loss = 0.02417142\n",
      "Iteration 449, loss = 0.02440804\n",
      "Iteration 450, loss = 0.02425718\n",
      "Iteration 451, loss = 0.02453632\n",
      "Iteration 452, loss = 0.02445590\n",
      "Iteration 453, loss = 0.02334995\n",
      "Iteration 454, loss = 0.02393805\n",
      "Iteration 455, loss = 0.02356537\n",
      "Iteration 456, loss = 0.02409069\n",
      "Iteration 457, loss = 0.02379876\n",
      "Iteration 458, loss = 0.02346847\n",
      "Iteration 459, loss = 0.02339887\n",
      "Iteration 460, loss = 0.02263329\n",
      "Iteration 461, loss = 0.02281921\n",
      "Iteration 462, loss = 0.02240907\n",
      "Iteration 463, loss = 0.02328887\n",
      "Iteration 464, loss = 0.02274674\n",
      "Iteration 465, loss = 0.02280767\n",
      "Iteration 466, loss = 0.02230157\n",
      "Iteration 467, loss = 0.02182286\n",
      "Iteration 468, loss = 0.02150768\n",
      "Iteration 469, loss = 0.02137423\n",
      "Iteration 470, loss = 0.02118664\n",
      "Iteration 471, loss = 0.02160451\n",
      "Iteration 472, loss = 0.02129392\n",
      "Iteration 473, loss = 0.02132541\n",
      "Iteration 474, loss = 0.02099554\n",
      "Iteration 475, loss = 0.02205107\n",
      "Iteration 476, loss = 0.02112840\n",
      "Iteration 477, loss = 0.02100849\n",
      "Iteration 478, loss = 0.02127035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 479, loss = 0.02071741\n",
      "Iteration 480, loss = 0.02023357\n",
      "Iteration 481, loss = 0.02017504\n",
      "Iteration 482, loss = 0.02127079\n",
      "Iteration 483, loss = 0.02042924\n",
      "Iteration 484, loss = 0.02034999\n",
      "Iteration 485, loss = 0.01971937\n",
      "Iteration 486, loss = 0.02025569\n",
      "Iteration 487, loss = 0.01928462\n",
      "Iteration 488, loss = 0.01921898\n",
      "Iteration 489, loss = 0.01927812\n",
      "Iteration 490, loss = 0.01917373\n",
      "Iteration 491, loss = 0.01941655\n",
      "Iteration 492, loss = 0.01922579\n",
      "Iteration 493, loss = 0.01886069\n",
      "Iteration 494, loss = 0.01845424\n",
      "Iteration 495, loss = 0.01912749\n",
      "Iteration 496, loss = 0.01926898\n",
      "Iteration 497, loss = 0.01947596\n",
      "Iteration 498, loss = 0.01866217\n",
      "Iteration 499, loss = 0.01815563\n",
      "Iteration 500, loss = 0.01833060\n",
      "Iteration 501, loss = 0.01805332\n",
      "Iteration 502, loss = 0.01787838\n",
      "Iteration 503, loss = 0.01809314\n",
      "Iteration 504, loss = 0.01772873\n",
      "Iteration 505, loss = 0.01819607\n",
      "Iteration 506, loss = 0.01811076\n",
      "Iteration 507, loss = 0.01757209\n",
      "Iteration 508, loss = 0.01710768\n",
      "Iteration 509, loss = 0.01768098\n",
      "Iteration 510, loss = 0.01781169\n",
      "Iteration 511, loss = 0.01716701\n",
      "Iteration 512, loss = 0.01769793\n",
      "Iteration 513, loss = 0.01768906\n",
      "Iteration 514, loss = 0.01691932\n",
      "Iteration 515, loss = 0.01728400\n",
      "Iteration 516, loss = 0.02106186\n",
      "Iteration 517, loss = 0.03481143\n",
      "Iteration 518, loss = 0.02411861\n",
      "Iteration 519, loss = 0.02471101\n",
      "Iteration 520, loss = 0.02025625\n",
      "Iteration 521, loss = 0.01787161\n",
      "Iteration 522, loss = 0.01768746\n",
      "Iteration 523, loss = 0.01675027\n",
      "Iteration 524, loss = 0.01700977\n",
      "Iteration 525, loss = 0.01729888\n",
      "Iteration 526, loss = 0.01633313\n",
      "Iteration 527, loss = 0.01664681\n",
      "Iteration 528, loss = 0.01620164\n",
      "Iteration 529, loss = 0.01644781\n",
      "Iteration 530, loss = 0.01657964\n",
      "Iteration 531, loss = 0.01579302\n",
      "Iteration 532, loss = 0.01583168\n",
      "Iteration 533, loss = 0.01564714\n",
      "Iteration 534, loss = 0.01538873\n",
      "Iteration 535, loss = 0.01533906\n",
      "Iteration 536, loss = 0.01533655\n",
      "Iteration 537, loss = 0.01640921\n",
      "Iteration 538, loss = 0.01542505\n",
      "Iteration 539, loss = 0.01586458\n",
      "Iteration 540, loss = 0.01577066\n",
      "Iteration 541, loss = 0.01580141\n",
      "Iteration 542, loss = 0.01516409\n",
      "Iteration 543, loss = 0.01524557\n",
      "Iteration 544, loss = 0.01524941\n",
      "Iteration 545, loss = 0.01541479\n",
      "Iteration 546, loss = 0.01490484\n",
      "Iteration 547, loss = 0.01469286\n",
      "Iteration 548, loss = 0.01491824\n",
      "Iteration 549, loss = 0.01538225\n",
      "Iteration 550, loss = 0.01425046\n",
      "Iteration 551, loss = 0.01464118\n",
      "Iteration 552, loss = 0.01490830\n",
      "Iteration 553, loss = 0.01527418\n",
      "Iteration 554, loss = 0.01470214\n",
      "Iteration 555, loss = 0.01513005\n",
      "Iteration 556, loss = 0.01515972\n",
      "Iteration 557, loss = 0.01458612\n",
      "Iteration 558, loss = 0.01362899\n",
      "Iteration 559, loss = 0.01430656\n",
      "Iteration 560, loss = 0.01460570\n",
      "Iteration 561, loss = 0.01436764\n",
      "Iteration 562, loss = 0.01362646\n",
      "Iteration 563, loss = 0.01377659\n",
      "Iteration 564, loss = 0.01358380\n",
      "Iteration 565, loss = 0.01367774\n",
      "Iteration 566, loss = 0.01367383\n",
      "Iteration 567, loss = 0.01339845\n",
      "Iteration 568, loss = 0.01310487\n",
      "Iteration 569, loss = 0.01322009\n",
      "Iteration 570, loss = 0.01357295\n",
      "Iteration 571, loss = 0.01341061\n",
      "Iteration 572, loss = 0.01325005\n",
      "Iteration 573, loss = 0.01283774\n",
      "Iteration 574, loss = 0.01367184\n",
      "Iteration 575, loss = 0.01431020\n",
      "Iteration 576, loss = 0.01420399\n",
      "Iteration 577, loss = 0.01283031\n",
      "Iteration 578, loss = 0.01269754\n",
      "Iteration 579, loss = 0.01236113\n",
      "Iteration 580, loss = 0.01239050\n",
      "Iteration 581, loss = 0.01222400\n",
      "Iteration 582, loss = 0.01262099\n",
      "Iteration 583, loss = 0.01376024\n",
      "Iteration 584, loss = 0.01302041\n",
      "Iteration 585, loss = 0.01236894\n",
      "Iteration 586, loss = 0.01228120\n",
      "Iteration 587, loss = 0.01192964\n",
      "Iteration 588, loss = 0.01176110\n",
      "Iteration 589, loss = 0.01167914\n",
      "Iteration 590, loss = 0.01153820\n",
      "Iteration 591, loss = 0.01228752\n",
      "Iteration 592, loss = 0.01205970\n",
      "Iteration 593, loss = 0.01172635\n",
      "Iteration 594, loss = 0.01150776\n",
      "Iteration 595, loss = 0.01172852\n",
      "Iteration 596, loss = 0.01134991\n",
      "Iteration 597, loss = 0.01122130\n",
      "Iteration 598, loss = 0.01257318\n",
      "Iteration 599, loss = 0.01231665\n",
      "Iteration 600, loss = 0.01250146\n",
      "Iteration 601, loss = 0.01115185\n",
      "Iteration 602, loss = 0.01130941\n",
      "Iteration 603, loss = 0.01111718\n",
      "Iteration 604, loss = 0.01210476\n",
      "Iteration 605, loss = 0.01303128\n",
      "Iteration 606, loss = 0.01135240\n",
      "Iteration 607, loss = 0.01079688\n",
      "Iteration 608, loss = 0.01165703\n",
      "Iteration 609, loss = 0.01162138\n",
      "Iteration 610, loss = 0.01128837\n",
      "Iteration 611, loss = 0.01094251\n",
      "Iteration 612, loss = 0.01133858\n",
      "Iteration 613, loss = 0.01081386\n",
      "Iteration 614, loss = 0.01034552\n",
      "Iteration 615, loss = 0.01037789\n",
      "Iteration 616, loss = 0.01016643\n",
      "Iteration 617, loss = 0.01021736\n",
      "Iteration 618, loss = 0.00999144\n",
      "Iteration 619, loss = 0.01002156\n",
      "Iteration 620, loss = 0.00973600\n",
      "Iteration 621, loss = 0.00981573\n",
      "Iteration 622, loss = 0.00958091\n",
      "Iteration 623, loss = 0.00992333\n",
      "Iteration 624, loss = 0.00995822\n",
      "Iteration 625, loss = 0.00987530\n",
      "Iteration 626, loss = 0.00955685\n",
      "Iteration 627, loss = 0.00952378\n",
      "Iteration 628, loss = 0.01050926\n",
      "Iteration 629, loss = 0.01011335\n",
      "Iteration 630, loss = 0.00985945\n",
      "Iteration 631, loss = 0.00958586\n",
      "Iteration 632, loss = 0.00955692\n",
      "Iteration 633, loss = 0.00955864\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 5 of 48...\n",
      "Iteration 1, loss = 0.67740458\n",
      "Iteration 2, loss = 0.59967471\n",
      "Iteration 3, loss = 0.55710473\n",
      "Iteration 4, loss = 0.52957452\n",
      "Iteration 5, loss = 0.50717441\n",
      "Iteration 6, loss = 0.48733036\n",
      "Iteration 7, loss = 0.47219921\n",
      "Iteration 8, loss = 0.46089572\n",
      "Iteration 9, loss = 0.44849487\n",
      "Iteration 10, loss = 0.43752161\n",
      "Iteration 11, loss = 0.43141155\n",
      "Iteration 12, loss = 0.42017945\n",
      "Iteration 13, loss = 0.41115839\n",
      "Iteration 14, loss = 0.41163137\n",
      "Iteration 15, loss = 0.40039073\n",
      "Iteration 16, loss = 0.39404298\n",
      "Iteration 17, loss = 0.38766788\n",
      "Iteration 18, loss = 0.38362917\n",
      "Iteration 19, loss = 0.37934977\n",
      "Iteration 20, loss = 0.37429271\n",
      "Iteration 21, loss = 0.36975496\n",
      "Iteration 22, loss = 0.36921111\n",
      "Iteration 23, loss = 0.36708424\n",
      "Iteration 24, loss = 0.35983594\n",
      "Iteration 25, loss = 0.35536921\n",
      "Iteration 26, loss = 0.35555667\n",
      "Iteration 27, loss = 0.34900622\n",
      "Iteration 28, loss = 0.34833513\n",
      "Iteration 29, loss = 0.34349263\n",
      "Iteration 30, loss = 0.34079639\n",
      "Iteration 31, loss = 0.33829652\n",
      "Iteration 32, loss = 0.33593966\n",
      "Iteration 33, loss = 0.33102385\n",
      "Iteration 34, loss = 0.32759888\n",
      "Iteration 35, loss = 0.32582046\n",
      "Iteration 36, loss = 0.32411328\n",
      "Iteration 37, loss = 0.32173083\n",
      "Iteration 38, loss = 0.31917540\n",
      "Iteration 39, loss = 0.31556652\n",
      "Iteration 40, loss = 0.31399238\n",
      "Iteration 41, loss = 0.31251090\n",
      "Iteration 42, loss = 0.30978524\n",
      "Iteration 43, loss = 0.30882198\n",
      "Iteration 44, loss = 0.30489418\n",
      "Iteration 45, loss = 0.30240264\n",
      "Iteration 46, loss = 0.30184495\n",
      "Iteration 47, loss = 0.29982931\n",
      "Iteration 48, loss = 0.29790317\n",
      "Iteration 49, loss = 0.29445327\n",
      "Iteration 50, loss = 0.29732278\n",
      "Iteration 51, loss = 0.29101869\n",
      "Iteration 52, loss = 0.28982247\n",
      "Iteration 53, loss = 0.28564140\n",
      "Iteration 54, loss = 0.28383768\n",
      "Iteration 55, loss = 0.28301565\n",
      "Iteration 56, loss = 0.28094788\n",
      "Iteration 57, loss = 0.28068206\n",
      "Iteration 58, loss = 0.27949609\n",
      "Iteration 59, loss = 0.27511790\n",
      "Iteration 60, loss = 0.27362710\n",
      "Iteration 61, loss = 0.27371399\n",
      "Iteration 62, loss = 0.26915769\n",
      "Iteration 63, loss = 0.26680378\n",
      "Iteration 64, loss = 0.26646936\n",
      "Iteration 65, loss = 0.26493562\n",
      "Iteration 66, loss = 0.26386709\n",
      "Iteration 67, loss = 0.26055483\n",
      "Iteration 68, loss = 0.26009316\n",
      "Iteration 69, loss = 0.25842831\n",
      "Iteration 70, loss = 0.25958298\n",
      "Iteration 71, loss = 0.25522095\n",
      "Iteration 72, loss = 0.25258329\n",
      "Iteration 73, loss = 0.25173020\n",
      "Iteration 74, loss = 0.25173886\n",
      "Iteration 75, loss = 0.24680821\n",
      "Iteration 76, loss = 0.24610759\n",
      "Iteration 77, loss = 0.24576255\n",
      "Iteration 78, loss = 0.24480772\n",
      "Iteration 79, loss = 0.24199885\n",
      "Iteration 80, loss = 0.24148295\n",
      "Iteration 81, loss = 0.23786562\n",
      "Iteration 82, loss = 0.23642189\n",
      "Iteration 83, loss = 0.23635231\n",
      "Iteration 84, loss = 0.23412813\n",
      "Iteration 85, loss = 0.23414970\n",
      "Iteration 86, loss = 0.23486169\n",
      "Iteration 87, loss = 0.23317768\n",
      "Iteration 88, loss = 0.22820554\n",
      "Iteration 89, loss = 0.23000131\n",
      "Iteration 90, loss = 0.22839165\n",
      "Iteration 91, loss = 0.22820783\n",
      "Iteration 92, loss = 0.22907457\n",
      "Iteration 93, loss = 0.22426488\n",
      "Iteration 94, loss = 0.22225734\n",
      "Iteration 95, loss = 0.21923223\n",
      "Iteration 96, loss = 0.21771058\n",
      "Iteration 97, loss = 0.21969365\n",
      "Iteration 98, loss = 0.21520493\n",
      "Iteration 99, loss = 0.21562329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, loss = 0.21428033\n",
      "Iteration 101, loss = 0.21001534\n",
      "Iteration 102, loss = 0.20949705\n",
      "Iteration 103, loss = 0.21060295\n",
      "Iteration 104, loss = 0.20970159\n",
      "Iteration 105, loss = 0.20638204\n",
      "Iteration 106, loss = 0.20682437\n",
      "Iteration 107, loss = 0.20438868\n",
      "Iteration 108, loss = 0.20504105\n",
      "Iteration 109, loss = 0.19917841\n",
      "Iteration 110, loss = 0.20144517\n",
      "Iteration 111, loss = 0.19872261\n",
      "Iteration 112, loss = 0.19672971\n",
      "Iteration 113, loss = 0.19424494\n",
      "Iteration 114, loss = 0.19781321\n",
      "Iteration 115, loss = 0.19566918\n",
      "Iteration 116, loss = 0.19147236\n",
      "Iteration 117, loss = 0.19010359\n",
      "Iteration 118, loss = 0.18949756\n",
      "Iteration 119, loss = 0.19239537\n",
      "Iteration 120, loss = 0.18829773\n",
      "Iteration 121, loss = 0.18594637\n",
      "Iteration 122, loss = 0.19149178\n",
      "Iteration 123, loss = 0.18616304\n",
      "Iteration 124, loss = 0.18527258\n",
      "Iteration 125, loss = 0.18153751\n",
      "Iteration 126, loss = 0.18103769\n",
      "Iteration 127, loss = 0.17980317\n",
      "Iteration 128, loss = 0.18056925\n",
      "Iteration 129, loss = 0.18006625\n",
      "Iteration 130, loss = 0.17553048\n",
      "Iteration 131, loss = 0.17388484\n",
      "Iteration 132, loss = 0.17943730\n",
      "Iteration 133, loss = 0.17561322\n",
      "Iteration 134, loss = 0.17233435\n",
      "Iteration 135, loss = 0.17219517\n",
      "Iteration 136, loss = 0.16821904\n",
      "Iteration 137, loss = 0.17008177\n",
      "Iteration 138, loss = 0.16766263\n",
      "Iteration 139, loss = 0.16616335\n",
      "Iteration 140, loss = 0.16704798\n",
      "Iteration 141, loss = 0.16410037\n",
      "Iteration 142, loss = 0.16156306\n",
      "Iteration 143, loss = 0.16164608\n",
      "Iteration 144, loss = 0.16100791\n",
      "Iteration 145, loss = 0.15893738\n",
      "Iteration 146, loss = 0.15763869\n",
      "Iteration 147, loss = 0.15803773\n",
      "Iteration 148, loss = 0.15661088\n",
      "Iteration 149, loss = 0.15664446\n",
      "Iteration 150, loss = 0.16064020\n",
      "Iteration 151, loss = 0.15262277\n",
      "Iteration 152, loss = 0.15239052\n",
      "Iteration 153, loss = 0.15268399\n",
      "Iteration 154, loss = 0.15145050\n",
      "Iteration 155, loss = 0.15008499\n",
      "Iteration 156, loss = 0.14945654\n",
      "Iteration 157, loss = 0.14915015\n",
      "Iteration 158, loss = 0.14628084\n",
      "Iteration 159, loss = 0.14575657\n",
      "Iteration 160, loss = 0.14440276\n",
      "Iteration 161, loss = 0.14607025\n",
      "Iteration 162, loss = 0.14350872\n",
      "Iteration 163, loss = 0.14435981\n",
      "Iteration 164, loss = 0.14116019\n",
      "Iteration 165, loss = 0.14100604\n",
      "Iteration 166, loss = 0.13825743\n",
      "Iteration 167, loss = 0.14057027\n",
      "Iteration 168, loss = 0.14117667\n",
      "Iteration 169, loss = 0.13738197\n",
      "Iteration 170, loss = 0.13715593\n",
      "Iteration 171, loss = 0.13536729\n",
      "Iteration 172, loss = 0.13381650\n",
      "Iteration 173, loss = 0.13224013\n",
      "Iteration 174, loss = 0.13268081\n",
      "Iteration 175, loss = 0.13296754\n",
      "Iteration 176, loss = 0.12978650\n",
      "Iteration 177, loss = 0.12983905\n",
      "Iteration 178, loss = 0.12845282\n",
      "Iteration 179, loss = 0.12802241\n",
      "Iteration 180, loss = 0.12714172\n",
      "Iteration 181, loss = 0.12706937\n",
      "Iteration 182, loss = 0.12496874\n",
      "Iteration 183, loss = 0.12741250\n",
      "Iteration 184, loss = 0.12365147\n",
      "Iteration 185, loss = 0.12335211\n",
      "Iteration 186, loss = 0.12354618\n",
      "Iteration 187, loss = 0.12703969\n",
      "Iteration 188, loss = 0.12235323\n",
      "Iteration 189, loss = 0.11909050\n",
      "Iteration 190, loss = 0.11907356\n",
      "Iteration 191, loss = 0.11827304\n",
      "Iteration 192, loss = 0.11886465\n",
      "Iteration 193, loss = 0.11701007\n",
      "Iteration 194, loss = 0.11668270\n",
      "Iteration 195, loss = 0.11641478\n",
      "Iteration 196, loss = 0.11464465\n",
      "Iteration 197, loss = 0.11370211\n",
      "Iteration 198, loss = 0.11324566\n",
      "Iteration 199, loss = 0.11153720\n",
      "Iteration 200, loss = 0.11494694\n",
      "Iteration 201, loss = 0.11158527\n",
      "Iteration 202, loss = 0.11057772\n",
      "Iteration 203, loss = 0.11032638\n",
      "Iteration 204, loss = 0.10855634\n",
      "Iteration 205, loss = 0.10867628\n",
      "Iteration 206, loss = 0.10880418\n",
      "Iteration 207, loss = 0.10832153\n",
      "Iteration 208, loss = 0.10995057\n",
      "Iteration 209, loss = 0.10787847\n",
      "Iteration 210, loss = 0.10760237\n",
      "Iteration 211, loss = 0.10631156\n",
      "Iteration 212, loss = 0.10435162\n",
      "Iteration 213, loss = 0.10478102\n",
      "Iteration 214, loss = 0.10701220\n",
      "Iteration 215, loss = 0.10375725\n",
      "Iteration 216, loss = 0.10285345\n",
      "Iteration 217, loss = 0.10190394\n",
      "Iteration 218, loss = 0.09999268\n",
      "Iteration 219, loss = 0.09849765\n",
      "Iteration 220, loss = 0.09800853\n",
      "Iteration 221, loss = 0.09868336\n",
      "Iteration 222, loss = 0.09838554\n",
      "Iteration 223, loss = 0.09758935\n",
      "Iteration 224, loss = 0.09785741\n",
      "Iteration 225, loss = 0.09536393\n",
      "Iteration 226, loss = 0.09461636\n",
      "Iteration 227, loss = 0.09407394\n",
      "Iteration 228, loss = 0.09404569\n",
      "Iteration 229, loss = 0.09415395\n",
      "Iteration 230, loss = 0.09493461\n",
      "Iteration 231, loss = 0.09422001\n",
      "Iteration 232, loss = 0.09509008\n",
      "Iteration 233, loss = 0.09142797\n",
      "Iteration 234, loss = 0.08999193\n",
      "Iteration 235, loss = 0.08997794\n",
      "Iteration 236, loss = 0.08979560\n",
      "Iteration 237, loss = 0.09022849\n",
      "Iteration 238, loss = 0.08849450\n",
      "Iteration 239, loss = 0.08846795\n",
      "Iteration 240, loss = 0.08764250\n",
      "Iteration 241, loss = 0.08655771\n",
      "Iteration 242, loss = 0.08628354\n",
      "Iteration 243, loss = 0.08589511\n",
      "Iteration 244, loss = 0.08538757\n",
      "Iteration 245, loss = 0.08602740\n",
      "Iteration 246, loss = 0.08390647\n",
      "Iteration 247, loss = 0.08325422\n",
      "Iteration 248, loss = 0.08483977\n",
      "Iteration 249, loss = 0.08336872\n",
      "Iteration 250, loss = 0.08443028\n",
      "Iteration 251, loss = 0.08699755\n",
      "Iteration 252, loss = 0.08241235\n",
      "Iteration 253, loss = 0.08017830\n",
      "Iteration 254, loss = 0.08101097\n",
      "Iteration 255, loss = 0.08009065\n",
      "Iteration 256, loss = 0.08002134\n",
      "Iteration 257, loss = 0.08310544\n",
      "Iteration 258, loss = 0.08599902\n",
      "Iteration 259, loss = 0.08148212\n",
      "Iteration 260, loss = 0.07951018\n",
      "Iteration 261, loss = 0.07804468\n",
      "Iteration 262, loss = 0.07592282\n",
      "Iteration 263, loss = 0.07631153\n",
      "Iteration 264, loss = 0.07569074\n",
      "Iteration 265, loss = 0.07483293\n",
      "Iteration 266, loss = 0.07545630\n",
      "Iteration 267, loss = 0.07507022\n",
      "Iteration 268, loss = 0.07500493\n",
      "Iteration 269, loss = 0.07375216\n",
      "Iteration 270, loss = 0.07386470\n",
      "Iteration 271, loss = 0.07248166\n",
      "Iteration 272, loss = 0.07182282\n",
      "Iteration 273, loss = 0.07210891\n",
      "Iteration 274, loss = 0.07185234\n",
      "Iteration 275, loss = 0.07118425\n",
      "Iteration 276, loss = 0.07133743\n",
      "Iteration 277, loss = 0.07086888\n",
      "Iteration 278, loss = 0.07022205\n",
      "Iteration 279, loss = 0.06914956\n",
      "Iteration 280, loss = 0.06925216\n",
      "Iteration 281, loss = 0.06881914\n",
      "Iteration 282, loss = 0.07049865\n",
      "Iteration 283, loss = 0.06806325\n",
      "Iteration 284, loss = 0.06783364\n",
      "Iteration 285, loss = 0.06848258\n",
      "Iteration 286, loss = 0.06680557\n",
      "Iteration 287, loss = 0.06862740\n",
      "Iteration 288, loss = 0.06677619\n",
      "Iteration 289, loss = 0.06918492\n",
      "Iteration 290, loss = 0.06555338\n",
      "Iteration 291, loss = 0.06473833\n",
      "Iteration 292, loss = 0.06414865\n",
      "Iteration 293, loss = 0.06379455\n",
      "Iteration 294, loss = 0.06426272\n",
      "Iteration 295, loss = 0.06316730\n",
      "Iteration 296, loss = 0.06273988\n",
      "Iteration 297, loss = 0.06265503\n",
      "Iteration 298, loss = 0.06242035\n",
      "Iteration 299, loss = 0.06153176\n",
      "Iteration 300, loss = 0.06139367\n",
      "Iteration 301, loss = 0.06135337\n",
      "Iteration 302, loss = 0.06168273\n",
      "Iteration 303, loss = 0.06127371\n",
      "Iteration 304, loss = 0.06000860\n",
      "Iteration 305, loss = 0.05986113\n",
      "Iteration 306, loss = 0.06063211\n",
      "Iteration 307, loss = 0.06038902\n",
      "Iteration 308, loss = 0.05947469\n",
      "Iteration 309, loss = 0.05871906\n",
      "Iteration 310, loss = 0.06004901\n",
      "Iteration 311, loss = 0.06005805\n",
      "Iteration 312, loss = 0.06031932\n",
      "Iteration 313, loss = 0.05707506\n",
      "Iteration 314, loss = 0.05758140\n",
      "Iteration 315, loss = 0.05717569\n",
      "Iteration 316, loss = 0.05606280\n",
      "Iteration 317, loss = 0.05616539\n",
      "Iteration 318, loss = 0.05583229\n",
      "Iteration 319, loss = 0.05515187\n",
      "Iteration 320, loss = 0.05503819\n",
      "Iteration 321, loss = 0.05427916\n",
      "Iteration 322, loss = 0.05415186\n",
      "Iteration 323, loss = 0.05596542\n",
      "Iteration 324, loss = 0.05398752\n",
      "Iteration 325, loss = 0.05378184\n",
      "Iteration 326, loss = 0.05367823\n",
      "Iteration 327, loss = 0.05194250\n",
      "Iteration 328, loss = 0.05296277\n",
      "Iteration 329, loss = 0.05201881\n",
      "Iteration 330, loss = 0.05419107\n",
      "Iteration 331, loss = 0.05171668\n",
      "Iteration 332, loss = 0.05102441\n",
      "Iteration 333, loss = 0.05107408\n",
      "Iteration 334, loss = 0.05030188\n",
      "Iteration 335, loss = 0.05027316\n",
      "Iteration 336, loss = 0.05001665\n",
      "Iteration 337, loss = 0.04955131\n",
      "Iteration 338, loss = 0.04948772\n",
      "Iteration 339, loss = 0.05074612\n",
      "Iteration 340, loss = 0.04914180\n",
      "Iteration 341, loss = 0.04810785\n",
      "Iteration 342, loss = 0.04985703\n",
      "Iteration 343, loss = 0.04833079\n",
      "Iteration 344, loss = 0.04744552\n",
      "Iteration 345, loss = 0.04821051\n",
      "Iteration 346, loss = 0.04838662\n",
      "Iteration 347, loss = 0.04722061\n",
      "Iteration 348, loss = 0.04666446\n",
      "Iteration 349, loss = 0.04752805\n",
      "Iteration 350, loss = 0.04710896\n",
      "Iteration 351, loss = 0.04643691\n",
      "Iteration 352, loss = 0.04554928\n",
      "Iteration 353, loss = 0.04596670\n",
      "Iteration 354, loss = 0.04856546\n",
      "Iteration 355, loss = 0.04619587\n",
      "Iteration 356, loss = 0.04505026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 357, loss = 0.04447453\n",
      "Iteration 358, loss = 0.04488639\n",
      "Iteration 359, loss = 0.04420417\n",
      "Iteration 360, loss = 0.04358537\n",
      "Iteration 361, loss = 0.04476196\n",
      "Iteration 362, loss = 0.04426503\n",
      "Iteration 363, loss = 0.04348323\n",
      "Iteration 364, loss = 0.04310980\n",
      "Iteration 365, loss = 0.04277337\n",
      "Iteration 366, loss = 0.04295515\n",
      "Iteration 367, loss = 0.04249112\n",
      "Iteration 368, loss = 0.04222390\n",
      "Iteration 369, loss = 0.04171158\n",
      "Iteration 370, loss = 0.04232643\n",
      "Iteration 371, loss = 0.04247226\n",
      "Iteration 372, loss = 0.04091691\n",
      "Iteration 373, loss = 0.04060078\n",
      "Iteration 374, loss = 0.04074347\n",
      "Iteration 375, loss = 0.04018179\n",
      "Iteration 376, loss = 0.04061921\n",
      "Iteration 377, loss = 0.04104257\n",
      "Iteration 378, loss = 0.04533170\n",
      "Iteration 379, loss = 0.04170051\n",
      "Iteration 380, loss = 0.03969719\n",
      "Iteration 381, loss = 0.03925200\n",
      "Iteration 382, loss = 0.03965559\n",
      "Iteration 383, loss = 0.04195017\n",
      "Iteration 384, loss = 0.05386562\n",
      "Iteration 385, loss = 0.05548495\n",
      "Iteration 386, loss = 0.04821672\n",
      "Iteration 387, loss = 0.04614142\n",
      "Iteration 388, loss = 0.04474327\n",
      "Iteration 389, loss = 0.04317466\n",
      "Iteration 390, loss = 0.04172757\n",
      "Iteration 391, loss = 0.04140862\n",
      "Iteration 392, loss = 0.04109553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 6 of 48...\n",
      "Iteration 1, loss = 0.65888615\n",
      "Iteration 2, loss = 0.57781966\n",
      "Iteration 3, loss = 0.54032305\n",
      "Iteration 4, loss = 0.50846177\n",
      "Iteration 5, loss = 0.48788323\n",
      "Iteration 6, loss = 0.47134850\n",
      "Iteration 7, loss = 0.45697393\n",
      "Iteration 8, loss = 0.44338130\n",
      "Iteration 9, loss = 0.43241579\n",
      "Iteration 10, loss = 0.42398184\n",
      "Iteration 11, loss = 0.42079132\n",
      "Iteration 12, loss = 0.40867617\n",
      "Iteration 13, loss = 0.40034219\n",
      "Iteration 14, loss = 0.39563138\n",
      "Iteration 15, loss = 0.38956126\n",
      "Iteration 16, loss = 0.38650556\n",
      "Iteration 17, loss = 0.37965329\n",
      "Iteration 18, loss = 0.37525926\n",
      "Iteration 19, loss = 0.37079865\n",
      "Iteration 20, loss = 0.36672747\n",
      "Iteration 21, loss = 0.36388239\n",
      "Iteration 22, loss = 0.35996625\n",
      "Iteration 23, loss = 0.36280400\n",
      "Iteration 24, loss = 0.36123574\n",
      "Iteration 25, loss = 0.35077049\n",
      "Iteration 26, loss = 0.34835092\n",
      "Iteration 27, loss = 0.34119984\n",
      "Iteration 28, loss = 0.34033208\n",
      "Iteration 29, loss = 0.33742878\n",
      "Iteration 30, loss = 0.33469538\n",
      "Iteration 31, loss = 0.33181069\n",
      "Iteration 32, loss = 0.32948418\n",
      "Iteration 33, loss = 0.32726248\n",
      "Iteration 34, loss = 0.32725470\n",
      "Iteration 35, loss = 0.32277653\n",
      "Iteration 36, loss = 0.31958497\n",
      "Iteration 37, loss = 0.31820041\n",
      "Iteration 38, loss = 0.31460132\n",
      "Iteration 39, loss = 0.31224345\n",
      "Iteration 40, loss = 0.30977093\n",
      "Iteration 41, loss = 0.30886083\n",
      "Iteration 42, loss = 0.30653356\n",
      "Iteration 43, loss = 0.30463541\n",
      "Iteration 44, loss = 0.30208048\n",
      "Iteration 45, loss = 0.29973257\n",
      "Iteration 46, loss = 0.29749591\n",
      "Iteration 47, loss = 0.29727289\n",
      "Iteration 48, loss = 0.29927619\n",
      "Iteration 49, loss = 0.29323217\n",
      "Iteration 50, loss = 0.29175972\n",
      "Iteration 51, loss = 0.28897771\n",
      "Iteration 52, loss = 0.28987631\n",
      "Iteration 53, loss = 0.28687893\n",
      "Iteration 54, loss = 0.28308778\n",
      "Iteration 55, loss = 0.28424348\n",
      "Iteration 56, loss = 0.28050683\n",
      "Iteration 57, loss = 0.27941830\n",
      "Iteration 58, loss = 0.27473918\n",
      "Iteration 59, loss = 0.27530510\n",
      "Iteration 60, loss = 0.27440281\n",
      "Iteration 61, loss = 0.27803395\n",
      "Iteration 62, loss = 0.27379215\n",
      "Iteration 63, loss = 0.27318775\n",
      "Iteration 64, loss = 0.26881403\n",
      "Iteration 65, loss = 0.26591555\n",
      "Iteration 66, loss = 0.26359205\n",
      "Iteration 67, loss = 0.26731295\n",
      "Iteration 68, loss = 0.27320615\n",
      "Iteration 69, loss = 0.26390810\n",
      "Iteration 70, loss = 0.26008769\n",
      "Iteration 71, loss = 0.25644744\n",
      "Iteration 72, loss = 0.25837990\n",
      "Iteration 73, loss = 0.25810676\n",
      "Iteration 74, loss = 0.25738118\n",
      "Iteration 75, loss = 0.26234596\n",
      "Iteration 76, loss = 0.25706269\n",
      "Iteration 77, loss = 0.24898065\n",
      "Iteration 78, loss = 0.24820520\n",
      "Iteration 79, loss = 0.24599791\n",
      "Iteration 80, loss = 0.24378869\n",
      "Iteration 81, loss = 0.24508246\n",
      "Iteration 82, loss = 0.24017687\n",
      "Iteration 83, loss = 0.24079812\n",
      "Iteration 84, loss = 0.24032213\n",
      "Iteration 85, loss = 0.23679713\n",
      "Iteration 86, loss = 0.23665727\n",
      "Iteration 87, loss = 0.23591863\n",
      "Iteration 88, loss = 0.23662939\n",
      "Iteration 89, loss = 0.23204851\n",
      "Iteration 90, loss = 0.23123482\n",
      "Iteration 91, loss = 0.23168106\n",
      "Iteration 92, loss = 0.23174009\n",
      "Iteration 93, loss = 0.23287124\n",
      "Iteration 94, loss = 0.23002653\n",
      "Iteration 95, loss = 0.22613903\n",
      "Iteration 96, loss = 0.22491142\n",
      "Iteration 97, loss = 0.22506578\n",
      "Iteration 98, loss = 0.22288201\n",
      "Iteration 99, loss = 0.22492496\n",
      "Iteration 100, loss = 0.22056525\n",
      "Iteration 101, loss = 0.22045999\n",
      "Iteration 102, loss = 0.22006539\n",
      "Iteration 103, loss = 0.21824607\n",
      "Iteration 104, loss = 0.21459427\n",
      "Iteration 105, loss = 0.21446428\n",
      "Iteration 106, loss = 0.21300049\n",
      "Iteration 107, loss = 0.21342162\n",
      "Iteration 108, loss = 0.21441535\n",
      "Iteration 109, loss = 0.21193189\n",
      "Iteration 110, loss = 0.20876791\n",
      "Iteration 111, loss = 0.20887022\n",
      "Iteration 112, loss = 0.20603375\n",
      "Iteration 113, loss = 0.20529837\n",
      "Iteration 114, loss = 0.20623101\n",
      "Iteration 115, loss = 0.20679754\n",
      "Iteration 116, loss = 0.20131803\n",
      "Iteration 117, loss = 0.20026218\n",
      "Iteration 118, loss = 0.20262732\n",
      "Iteration 119, loss = 0.20005877\n",
      "Iteration 120, loss = 0.19866814\n",
      "Iteration 121, loss = 0.19836936\n",
      "Iteration 122, loss = 0.19609238\n",
      "Iteration 123, loss = 0.19669245\n",
      "Iteration 124, loss = 0.19374144\n",
      "Iteration 125, loss = 0.19306900\n",
      "Iteration 126, loss = 0.19155781\n",
      "Iteration 127, loss = 0.19145023\n",
      "Iteration 128, loss = 0.18838849\n",
      "Iteration 129, loss = 0.18816188\n",
      "Iteration 130, loss = 0.18737801\n",
      "Iteration 131, loss = 0.18630182\n",
      "Iteration 132, loss = 0.18464826\n",
      "Iteration 133, loss = 0.18403857\n",
      "Iteration 134, loss = 0.18398040\n",
      "Iteration 135, loss = 0.18316775\n",
      "Iteration 136, loss = 0.18236349\n",
      "Iteration 137, loss = 0.18029462\n",
      "Iteration 138, loss = 0.18462238\n",
      "Iteration 139, loss = 0.18138211\n",
      "Iteration 140, loss = 0.17695281\n",
      "Iteration 141, loss = 0.17639157\n",
      "Iteration 142, loss = 0.17575123\n",
      "Iteration 143, loss = 0.17335534\n",
      "Iteration 144, loss = 0.17699754\n",
      "Iteration 145, loss = 0.17597367\n",
      "Iteration 146, loss = 0.17487603\n",
      "Iteration 147, loss = 0.17298047\n",
      "Iteration 148, loss = 0.17023369\n",
      "Iteration 149, loss = 0.16923666\n",
      "Iteration 150, loss = 0.16858764\n",
      "Iteration 151, loss = 0.16741952\n",
      "Iteration 152, loss = 0.16572962\n",
      "Iteration 153, loss = 0.16461043\n",
      "Iteration 154, loss = 0.16579644\n",
      "Iteration 155, loss = 0.16712975\n",
      "Iteration 156, loss = 0.16693094\n",
      "Iteration 157, loss = 0.16341651\n",
      "Iteration 158, loss = 0.16510336\n",
      "Iteration 159, loss = 0.17024048\n",
      "Iteration 160, loss = 0.16266036\n",
      "Iteration 161, loss = 0.15873933\n",
      "Iteration 162, loss = 0.15762221\n",
      "Iteration 163, loss = 0.15957096\n",
      "Iteration 164, loss = 0.15577635\n",
      "Iteration 165, loss = 0.15497184\n",
      "Iteration 166, loss = 0.15398861\n",
      "Iteration 167, loss = 0.15341645\n",
      "Iteration 168, loss = 0.15279883\n",
      "Iteration 169, loss = 0.15109434\n",
      "Iteration 170, loss = 0.15044501\n",
      "Iteration 171, loss = 0.14987111\n",
      "Iteration 172, loss = 0.14925736\n",
      "Iteration 173, loss = 0.15174724\n",
      "Iteration 174, loss = 0.14969698\n",
      "Iteration 175, loss = 0.14783995\n",
      "Iteration 176, loss = 0.14671838\n",
      "Iteration 177, loss = 0.14712147\n",
      "Iteration 178, loss = 0.14749072\n",
      "Iteration 179, loss = 0.14315965\n",
      "Iteration 180, loss = 0.14484649\n",
      "Iteration 181, loss = 0.14151453\n",
      "Iteration 182, loss = 0.14134190\n",
      "Iteration 183, loss = 0.14184037\n",
      "Iteration 184, loss = 0.14162022\n",
      "Iteration 185, loss = 0.13898934\n",
      "Iteration 186, loss = 0.13897410\n",
      "Iteration 187, loss = 0.14101943\n",
      "Iteration 188, loss = 0.14203112\n",
      "Iteration 189, loss = 0.14352855\n",
      "Iteration 190, loss = 0.14226748\n",
      "Iteration 191, loss = 0.13596584\n",
      "Iteration 192, loss = 0.13586542\n",
      "Iteration 193, loss = 0.13756867\n",
      "Iteration 194, loss = 0.13230725\n",
      "Iteration 195, loss = 0.13087617\n",
      "Iteration 196, loss = 0.13132300\n",
      "Iteration 197, loss = 0.13197869\n",
      "Iteration 198, loss = 0.13258730\n",
      "Iteration 199, loss = 0.12920783\n",
      "Iteration 200, loss = 0.12861103\n",
      "Iteration 201, loss = 0.12735239\n",
      "Iteration 202, loss = 0.12963244\n",
      "Iteration 203, loss = 0.12941804\n",
      "Iteration 204, loss = 0.12935744\n",
      "Iteration 205, loss = 0.12937711\n",
      "Iteration 206, loss = 0.12595225\n",
      "Iteration 207, loss = 0.12512988\n",
      "Iteration 208, loss = 0.12405531\n",
      "Iteration 209, loss = 0.12200365\n",
      "Iteration 210, loss = 0.12218674\n",
      "Iteration 211, loss = 0.12359171\n",
      "Iteration 212, loss = 0.12046751\n",
      "Iteration 213, loss = 0.12155780\n",
      "Iteration 214, loss = 0.12068679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 215, loss = 0.11904842\n",
      "Iteration 216, loss = 0.11992517\n",
      "Iteration 217, loss = 0.11921752\n",
      "Iteration 218, loss = 0.11982031\n",
      "Iteration 219, loss = 0.11770193\n",
      "Iteration 220, loss = 0.11650469\n",
      "Iteration 221, loss = 0.11499541\n",
      "Iteration 222, loss = 0.11395021\n",
      "Iteration 223, loss = 0.11358722\n",
      "Iteration 224, loss = 0.11452170\n",
      "Iteration 225, loss = 0.11532825\n",
      "Iteration 226, loss = 0.11198176\n",
      "Iteration 227, loss = 0.11221492\n",
      "Iteration 228, loss = 0.11284334\n",
      "Iteration 229, loss = 0.10989339\n",
      "Iteration 230, loss = 0.11069930\n",
      "Iteration 231, loss = 0.10992618\n",
      "Iteration 232, loss = 0.11062413\n",
      "Iteration 233, loss = 0.10868282\n",
      "Iteration 234, loss = 0.10783558\n",
      "Iteration 235, loss = 0.10713835\n",
      "Iteration 236, loss = 0.10957649\n",
      "Iteration 237, loss = 0.10786263\n",
      "Iteration 238, loss = 0.10550102\n",
      "Iteration 239, loss = 0.10631477\n",
      "Iteration 240, loss = 0.10643085\n",
      "Iteration 241, loss = 0.10894229\n",
      "Iteration 242, loss = 0.10568221\n",
      "Iteration 243, loss = 0.10382982\n",
      "Iteration 244, loss = 0.10247443\n",
      "Iteration 245, loss = 0.10187127\n",
      "Iteration 246, loss = 0.10202376\n",
      "Iteration 247, loss = 0.10121759\n",
      "Iteration 248, loss = 0.10132155\n",
      "Iteration 249, loss = 0.10016717\n",
      "Iteration 250, loss = 0.09849767\n",
      "Iteration 251, loss = 0.10005213\n",
      "Iteration 252, loss = 0.10270918\n",
      "Iteration 253, loss = 0.09954459\n",
      "Iteration 254, loss = 0.09766870\n",
      "Iteration 255, loss = 0.09729374\n",
      "Iteration 256, loss = 0.09768291\n",
      "Iteration 257, loss = 0.09612871\n",
      "Iteration 258, loss = 0.09667215\n",
      "Iteration 259, loss = 0.09810640\n",
      "Iteration 260, loss = 0.09717640\n",
      "Iteration 261, loss = 0.09477634\n",
      "Iteration 262, loss = 0.09517672\n",
      "Iteration 263, loss = 0.09370381\n",
      "Iteration 264, loss = 0.09301283\n",
      "Iteration 265, loss = 0.09356747\n",
      "Iteration 266, loss = 0.09326677\n",
      "Iteration 267, loss = 0.09186907\n",
      "Iteration 268, loss = 0.09141752\n",
      "Iteration 269, loss = 0.09102531\n",
      "Iteration 270, loss = 0.08974942\n",
      "Iteration 271, loss = 0.08996788\n",
      "Iteration 272, loss = 0.09067824\n",
      "Iteration 273, loss = 0.09003983\n",
      "Iteration 274, loss = 0.09529987\n",
      "Iteration 275, loss = 0.09196168\n",
      "Iteration 276, loss = 0.09527011\n",
      "Iteration 277, loss = 0.09225568\n",
      "Iteration 278, loss = 0.08973764\n",
      "Iteration 279, loss = 0.09042312\n",
      "Iteration 280, loss = 0.08871705\n",
      "Iteration 281, loss = 0.08875332\n",
      "Iteration 282, loss = 0.08750821\n",
      "Iteration 283, loss = 0.08588782\n",
      "Iteration 284, loss = 0.08482827\n",
      "Iteration 285, loss = 0.08639380\n",
      "Iteration 286, loss = 0.08526770\n",
      "Iteration 287, loss = 0.08427200\n",
      "Iteration 288, loss = 0.08394670\n",
      "Iteration 289, loss = 0.08378838\n",
      "Iteration 290, loss = 0.08382571\n",
      "Iteration 291, loss = 0.08506000\n",
      "Iteration 292, loss = 0.08225252\n",
      "Iteration 293, loss = 0.08306780\n",
      "Iteration 294, loss = 0.08182998\n",
      "Iteration 295, loss = 0.08279118\n",
      "Iteration 296, loss = 0.08132234\n",
      "Iteration 297, loss = 0.08180979\n",
      "Iteration 298, loss = 0.08013825\n",
      "Iteration 299, loss = 0.08171250\n",
      "Iteration 300, loss = 0.08190787\n",
      "Iteration 301, loss = 0.07867972\n",
      "Iteration 302, loss = 0.07760419\n",
      "Iteration 303, loss = 0.07784607\n",
      "Iteration 304, loss = 0.07727195\n",
      "Iteration 305, loss = 0.07774997\n",
      "Iteration 306, loss = 0.07723649\n",
      "Iteration 307, loss = 0.07660583\n",
      "Iteration 308, loss = 0.07752729\n",
      "Iteration 309, loss = 0.07568745\n",
      "Iteration 310, loss = 0.07458871\n",
      "Iteration 311, loss = 0.07487225\n",
      "Iteration 312, loss = 0.07602978\n",
      "Iteration 313, loss = 0.07533138\n",
      "Iteration 314, loss = 0.07573570\n",
      "Iteration 315, loss = 0.07317762\n",
      "Iteration 316, loss = 0.07274862\n",
      "Iteration 317, loss = 0.07296395\n",
      "Iteration 318, loss = 0.07301915\n",
      "Iteration 319, loss = 0.07251584\n",
      "Iteration 320, loss = 0.07208597\n",
      "Iteration 321, loss = 0.07390068\n",
      "Iteration 322, loss = 0.07062384\n",
      "Iteration 323, loss = 0.07125491\n",
      "Iteration 324, loss = 0.07055958\n",
      "Iteration 325, loss = 0.06987172\n",
      "Iteration 326, loss = 0.07209449\n",
      "Iteration 327, loss = 0.06960481\n",
      "Iteration 328, loss = 0.06908492\n",
      "Iteration 329, loss = 0.06864910\n",
      "Iteration 330, loss = 0.06858388\n",
      "Iteration 331, loss = 0.06952974\n",
      "Iteration 332, loss = 0.06771973\n",
      "Iteration 333, loss = 0.06822094\n",
      "Iteration 334, loss = 0.06781392\n",
      "Iteration 335, loss = 0.06714796\n",
      "Iteration 336, loss = 0.06588645\n",
      "Iteration 337, loss = 0.06748768\n",
      "Iteration 338, loss = 0.06643071\n",
      "Iteration 339, loss = 0.06694259\n",
      "Iteration 340, loss = 0.06585715\n",
      "Iteration 341, loss = 0.06590859\n",
      "Iteration 342, loss = 0.06487074\n",
      "Iteration 343, loss = 0.06446447\n",
      "Iteration 344, loss = 0.06501565\n",
      "Iteration 345, loss = 0.06535424\n",
      "Iteration 346, loss = 0.06399711\n",
      "Iteration 347, loss = 0.06537342\n",
      "Iteration 348, loss = 0.06322766\n",
      "Iteration 349, loss = 0.06362192\n",
      "Iteration 350, loss = 0.06439426\n",
      "Iteration 351, loss = 0.06490581\n",
      "Iteration 352, loss = 0.06367164\n",
      "Iteration 353, loss = 0.06458543\n",
      "Iteration 354, loss = 0.06162736\n",
      "Iteration 355, loss = 0.06277946\n",
      "Iteration 356, loss = 0.06160764\n",
      "Iteration 357, loss = 0.06072119\n",
      "Iteration 358, loss = 0.06238152\n",
      "Iteration 359, loss = 0.06101863\n",
      "Iteration 360, loss = 0.06135755\n",
      "Iteration 361, loss = 0.06158500\n",
      "Iteration 362, loss = 0.06184099\n",
      "Iteration 363, loss = 0.06073942\n",
      "Iteration 364, loss = 0.06027363\n",
      "Iteration 365, loss = 0.05980339\n",
      "Iteration 366, loss = 0.06077403\n",
      "Iteration 367, loss = 0.05987993\n",
      "Iteration 368, loss = 0.05938637\n",
      "Iteration 369, loss = 0.06010817\n",
      "Iteration 370, loss = 0.05884400\n",
      "Iteration 371, loss = 0.05828543\n",
      "Iteration 372, loss = 0.05856600\n",
      "Iteration 373, loss = 0.05801844\n",
      "Iteration 374, loss = 0.05768524\n",
      "Iteration 375, loss = 0.05736928\n",
      "Iteration 376, loss = 0.05625447\n",
      "Iteration 377, loss = 0.05624292\n",
      "Iteration 378, loss = 0.05649956\n",
      "Iteration 379, loss = 0.05582050\n",
      "Iteration 380, loss = 0.05685367\n",
      "Iteration 381, loss = 0.05550133\n",
      "Iteration 382, loss = 0.05557109\n",
      "Iteration 383, loss = 0.05574830\n",
      "Iteration 384, loss = 0.05592791\n",
      "Iteration 385, loss = 0.05567848\n",
      "Iteration 386, loss = 0.05445846\n",
      "Iteration 387, loss = 0.05423534\n",
      "Iteration 388, loss = 0.05403809\n",
      "Iteration 389, loss = 0.05384031\n",
      "Iteration 390, loss = 0.05414868\n",
      "Iteration 391, loss = 0.05561193\n",
      "Iteration 392, loss = 0.05537630\n",
      "Iteration 393, loss = 0.05369850\n",
      "Iteration 394, loss = 0.05285053\n",
      "Iteration 395, loss = 0.05334874\n",
      "Iteration 396, loss = 0.05300707\n",
      "Iteration 397, loss = 0.05287773\n",
      "Iteration 398, loss = 0.05209265\n",
      "Iteration 399, loss = 0.05378149\n",
      "Iteration 400, loss = 0.05406049\n",
      "Iteration 401, loss = 0.05305709\n",
      "Iteration 402, loss = 0.05199025\n",
      "Iteration 403, loss = 0.05143456\n",
      "Iteration 404, loss = 0.05212051\n",
      "Iteration 405, loss = 0.05175518\n",
      "Iteration 406, loss = 0.05057241\n",
      "Iteration 407, loss = 0.05141184\n",
      "Iteration 408, loss = 0.05107066\n",
      "Iteration 409, loss = 0.05191880\n",
      "Iteration 410, loss = 0.05050119\n",
      "Iteration 411, loss = 0.05073182\n",
      "Iteration 412, loss = 0.05014675\n",
      "Iteration 413, loss = 0.05145730\n",
      "Iteration 414, loss = 0.05170435\n",
      "Iteration 415, loss = 0.05073312\n",
      "Iteration 416, loss = 0.04898324\n",
      "Iteration 417, loss = 0.04969307\n",
      "Iteration 418, loss = 0.04890314\n",
      "Iteration 419, loss = 0.04811310\n",
      "Iteration 420, loss = 0.04789292\n",
      "Iteration 421, loss = 0.04818930\n",
      "Iteration 422, loss = 0.04792885\n",
      "Iteration 423, loss = 0.04815175\n",
      "Iteration 424, loss = 0.04917859\n",
      "Iteration 425, loss = 0.04874577\n",
      "Iteration 426, loss = 0.05159269\n",
      "Iteration 427, loss = 0.07225584\n",
      "Iteration 428, loss = 0.06430282\n",
      "Iteration 429, loss = 0.06269033\n",
      "Iteration 430, loss = 0.05603547\n",
      "Iteration 431, loss = 0.05239131\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 7 of 48...\n",
      "Iteration 1, loss = 0.67830619\n",
      "Iteration 2, loss = 0.64670151\n",
      "Iteration 3, loss = 0.62522131\n",
      "Iteration 4, loss = 0.60615743\n",
      "Iteration 5, loss = 0.58939886\n",
      "Iteration 6, loss = 0.57437365\n",
      "Iteration 7, loss = 0.56060487\n",
      "Iteration 8, loss = 0.54654896\n",
      "Iteration 9, loss = 0.53400850\n",
      "Iteration 10, loss = 0.52267457\n",
      "Iteration 11, loss = 0.51155065\n",
      "Iteration 12, loss = 0.50081702\n",
      "Iteration 13, loss = 0.49172926\n",
      "Iteration 14, loss = 0.48257205\n",
      "Iteration 15, loss = 0.47411805\n",
      "Iteration 16, loss = 0.46745294\n",
      "Iteration 17, loss = 0.45924258\n",
      "Iteration 18, loss = 0.45389875\n",
      "Iteration 19, loss = 0.44582324\n",
      "Iteration 20, loss = 0.44073686\n",
      "Iteration 21, loss = 0.43432465\n",
      "Iteration 22, loss = 0.42907153\n",
      "Iteration 23, loss = 0.42624909\n",
      "Iteration 24, loss = 0.41954445\n",
      "Iteration 25, loss = 0.41530025\n",
      "Iteration 26, loss = 0.41117725\n",
      "Iteration 27, loss = 0.40708103\n",
      "Iteration 28, loss = 0.40459916\n",
      "Iteration 29, loss = 0.40062853\n",
      "Iteration 30, loss = 0.39771527\n",
      "Iteration 31, loss = 0.39544200\n",
      "Iteration 32, loss = 0.38985952\n",
      "Iteration 33, loss = 0.38671667\n",
      "Iteration 34, loss = 0.38424922\n",
      "Iteration 35, loss = 0.38545414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.38179430\n",
      "Iteration 37, loss = 0.37822566\n",
      "Iteration 38, loss = 0.37353983\n",
      "Iteration 39, loss = 0.37314874\n",
      "Iteration 40, loss = 0.37054927\n",
      "Iteration 41, loss = 0.36859200\n",
      "Iteration 42, loss = 0.36544069\n",
      "Iteration 43, loss = 0.36512387\n",
      "Iteration 44, loss = 0.36220883\n",
      "Iteration 45, loss = 0.35960999\n",
      "Iteration 46, loss = 0.35816700\n",
      "Iteration 47, loss = 0.35649640\n",
      "Iteration 48, loss = 0.35427334\n",
      "Iteration 49, loss = 0.35248885\n",
      "Iteration 50, loss = 0.35060687\n",
      "Iteration 51, loss = 0.34882246\n",
      "Iteration 52, loss = 0.34740602\n",
      "Iteration 53, loss = 0.34576619\n",
      "Iteration 54, loss = 0.34511019\n",
      "Iteration 55, loss = 0.34269824\n",
      "Iteration 56, loss = 0.34189628\n",
      "Iteration 57, loss = 0.33977834\n",
      "Iteration 58, loss = 0.34182035\n",
      "Iteration 59, loss = 0.34158192\n",
      "Iteration 60, loss = 0.33509941\n",
      "Iteration 61, loss = 0.33493742\n",
      "Iteration 62, loss = 0.33248170\n",
      "Iteration 63, loss = 0.33229217\n",
      "Iteration 64, loss = 0.33079706\n",
      "Iteration 65, loss = 0.32917002\n",
      "Iteration 66, loss = 0.32800470\n",
      "Iteration 67, loss = 0.32809060\n",
      "Iteration 68, loss = 0.32552412\n",
      "Iteration 69, loss = 0.32336563\n",
      "Iteration 70, loss = 0.32217878\n",
      "Iteration 71, loss = 0.32134957\n",
      "Iteration 72, loss = 0.32107908\n",
      "Iteration 73, loss = 0.31991369\n",
      "Iteration 74, loss = 0.31834969\n",
      "Iteration 75, loss = 0.31822139\n",
      "Iteration 76, loss = 0.31607115\n",
      "Iteration 77, loss = 0.31549788\n",
      "Iteration 78, loss = 0.31365809\n",
      "Iteration 79, loss = 0.31236229\n",
      "Iteration 80, loss = 0.31193547\n",
      "Iteration 81, loss = 0.31118001\n",
      "Iteration 82, loss = 0.30953114\n",
      "Iteration 83, loss = 0.30797090\n",
      "Iteration 84, loss = 0.30703627\n",
      "Iteration 85, loss = 0.30596359\n",
      "Iteration 86, loss = 0.30546154\n",
      "Iteration 87, loss = 0.30405017\n",
      "Iteration 88, loss = 0.30539459\n",
      "Iteration 89, loss = 0.30282998\n",
      "Iteration 90, loss = 0.30252210\n",
      "Iteration 91, loss = 0.30299845\n",
      "Iteration 92, loss = 0.30359326\n",
      "Iteration 93, loss = 0.29923286\n",
      "Iteration 94, loss = 0.29775224\n",
      "Iteration 95, loss = 0.29637756\n",
      "Iteration 96, loss = 0.29564573\n",
      "Iteration 97, loss = 0.29468622\n",
      "Iteration 98, loss = 0.29470457\n",
      "Iteration 99, loss = 0.29462410\n",
      "Iteration 100, loss = 0.29288560\n",
      "Iteration 101, loss = 0.29116196\n",
      "Iteration 102, loss = 0.29003746\n",
      "Iteration 103, loss = 0.29009529\n",
      "Iteration 104, loss = 0.28882863\n",
      "Iteration 105, loss = 0.28727820\n",
      "Iteration 106, loss = 0.28604045\n",
      "Iteration 107, loss = 0.28681106\n",
      "Iteration 108, loss = 0.28598544\n",
      "Iteration 109, loss = 0.28494654\n",
      "Iteration 110, loss = 0.28397941\n",
      "Iteration 111, loss = 0.28440418\n",
      "Iteration 112, loss = 0.28819973\n",
      "Iteration 113, loss = 0.28909454\n",
      "Iteration 114, loss = 0.27983451\n",
      "Iteration 115, loss = 0.27871912\n",
      "Iteration 116, loss = 0.28238228\n",
      "Iteration 117, loss = 0.27829394\n",
      "Iteration 118, loss = 0.27737168\n",
      "Iteration 119, loss = 0.27732888\n",
      "Iteration 120, loss = 0.27736006\n",
      "Iteration 121, loss = 0.27814273\n",
      "Iteration 122, loss = 0.27501799\n",
      "Iteration 123, loss = 0.27310925\n",
      "Iteration 124, loss = 0.27256602\n",
      "Iteration 125, loss = 0.27490498\n",
      "Iteration 126, loss = 0.27470300\n",
      "Iteration 127, loss = 0.27094104\n",
      "Iteration 128, loss = 0.27081932\n",
      "Iteration 129, loss = 0.27140813\n",
      "Iteration 130, loss = 0.26886935\n",
      "Iteration 131, loss = 0.26730885\n",
      "Iteration 132, loss = 0.26609946\n",
      "Iteration 133, loss = 0.26590985\n",
      "Iteration 134, loss = 0.26563638\n",
      "Iteration 135, loss = 0.26624644\n",
      "Iteration 136, loss = 0.26428912\n",
      "Iteration 137, loss = 0.26570439\n",
      "Iteration 138, loss = 0.26464952\n",
      "Iteration 139, loss = 0.26412130\n",
      "Iteration 140, loss = 0.26137435\n",
      "Iteration 141, loss = 0.26098722\n",
      "Iteration 142, loss = 0.25938117\n",
      "Iteration 143, loss = 0.26108228\n",
      "Iteration 144, loss = 0.25870684\n",
      "Iteration 145, loss = 0.25749812\n",
      "Iteration 146, loss = 0.25645486\n",
      "Iteration 147, loss = 0.25661307\n",
      "Iteration 148, loss = 0.25661852\n",
      "Iteration 149, loss = 0.25503998\n",
      "Iteration 150, loss = 0.25429783\n",
      "Iteration 151, loss = 0.25362415\n",
      "Iteration 152, loss = 0.25429900\n",
      "Iteration 153, loss = 0.25348985\n",
      "Iteration 154, loss = 0.25279059\n",
      "Iteration 155, loss = 0.25403086\n",
      "Iteration 156, loss = 0.25221745\n",
      "Iteration 157, loss = 0.25069272\n",
      "Iteration 158, loss = 0.25113181\n",
      "Iteration 159, loss = 0.24916395\n",
      "Iteration 160, loss = 0.24819310\n",
      "Iteration 161, loss = 0.24891418\n",
      "Iteration 162, loss = 0.24816840\n",
      "Iteration 163, loss = 0.24804275\n",
      "Iteration 164, loss = 0.24672000\n",
      "Iteration 165, loss = 0.24694764\n",
      "Iteration 166, loss = 0.24557829\n",
      "Iteration 167, loss = 0.24339550\n",
      "Iteration 168, loss = 0.24629355\n",
      "Iteration 169, loss = 0.24629610\n",
      "Iteration 170, loss = 0.24444774\n",
      "Iteration 171, loss = 0.24388557\n",
      "Iteration 172, loss = 0.24098337\n",
      "Iteration 173, loss = 0.24080540\n",
      "Iteration 174, loss = 0.24062160\n",
      "Iteration 175, loss = 0.23985914\n",
      "Iteration 176, loss = 0.23911778\n",
      "Iteration 177, loss = 0.23923222\n",
      "Iteration 178, loss = 0.23830518\n",
      "Iteration 179, loss = 0.23776736\n",
      "Iteration 180, loss = 0.23759364\n",
      "Iteration 181, loss = 0.23583598\n",
      "Iteration 182, loss = 0.23641458\n",
      "Iteration 183, loss = 0.23581268\n",
      "Iteration 184, loss = 0.23492154\n",
      "Iteration 185, loss = 0.23384451\n",
      "Iteration 186, loss = 0.23384107\n",
      "Iteration 187, loss = 0.23328010\n",
      "Iteration 188, loss = 0.23194326\n",
      "Iteration 189, loss = 0.23195924\n",
      "Iteration 190, loss = 0.23169973\n",
      "Iteration 191, loss = 0.23007291\n",
      "Iteration 192, loss = 0.22951919\n",
      "Iteration 193, loss = 0.23017830\n",
      "Iteration 194, loss = 0.22909064\n",
      "Iteration 195, loss = 0.23032189\n",
      "Iteration 196, loss = 0.22836061\n",
      "Iteration 197, loss = 0.22677058\n",
      "Iteration 198, loss = 0.22918278\n",
      "Iteration 199, loss = 0.22652430\n",
      "Iteration 200, loss = 0.22621601\n",
      "Iteration 201, loss = 0.22564625\n",
      "Iteration 202, loss = 0.22379690\n",
      "Iteration 203, loss = 0.22371947\n",
      "Iteration 204, loss = 0.22319693\n",
      "Iteration 205, loss = 0.22729163\n",
      "Iteration 206, loss = 0.22298604\n",
      "Iteration 207, loss = 0.22094000\n",
      "Iteration 208, loss = 0.22160887\n",
      "Iteration 209, loss = 0.22001657\n",
      "Iteration 210, loss = 0.22081443\n",
      "Iteration 211, loss = 0.22300130\n",
      "Iteration 212, loss = 0.22066242\n",
      "Iteration 213, loss = 0.21897533\n",
      "Iteration 214, loss = 0.21787339\n",
      "Iteration 215, loss = 0.21765879\n",
      "Iteration 216, loss = 0.21849383\n",
      "Iteration 217, loss = 0.21701845\n",
      "Iteration 218, loss = 0.21538192\n",
      "Iteration 219, loss = 0.21423888\n",
      "Iteration 220, loss = 0.21363924\n",
      "Iteration 221, loss = 0.21445133\n",
      "Iteration 222, loss = 0.21354270\n",
      "Iteration 223, loss = 0.21374599\n",
      "Iteration 224, loss = 0.21264271\n",
      "Iteration 225, loss = 0.21230058\n",
      "Iteration 226, loss = 0.21120687\n",
      "Iteration 227, loss = 0.21083895\n",
      "Iteration 228, loss = 0.21151056\n",
      "Iteration 229, loss = 0.20926970\n",
      "Iteration 230, loss = 0.21016005\n",
      "Iteration 231, loss = 0.20905232\n",
      "Iteration 232, loss = 0.20737305\n",
      "Iteration 233, loss = 0.20961057\n",
      "Iteration 234, loss = 0.20891369\n",
      "Iteration 235, loss = 0.20701108\n",
      "Iteration 236, loss = 0.20661159\n",
      "Iteration 237, loss = 0.20692834\n",
      "Iteration 238, loss = 0.20569518\n",
      "Iteration 239, loss = 0.20375587\n",
      "Iteration 240, loss = 0.20362541\n",
      "Iteration 241, loss = 0.20270190\n",
      "Iteration 242, loss = 0.20264278\n",
      "Iteration 243, loss = 0.20344668\n",
      "Iteration 244, loss = 0.20278012\n",
      "Iteration 245, loss = 0.20080793\n",
      "Iteration 246, loss = 0.20230533\n",
      "Iteration 247, loss = 0.20013756\n",
      "Iteration 248, loss = 0.19968788\n",
      "Iteration 249, loss = 0.19917486\n",
      "Iteration 250, loss = 0.19912634\n",
      "Iteration 251, loss = 0.20237477\n",
      "Iteration 252, loss = 0.20132968\n",
      "Iteration 253, loss = 0.19878860\n",
      "Iteration 254, loss = 0.19541375\n",
      "Iteration 255, loss = 0.19527574\n",
      "Iteration 256, loss = 0.19556700\n",
      "Iteration 257, loss = 0.19471383\n",
      "Iteration 258, loss = 0.19452095\n",
      "Iteration 259, loss = 0.19485969\n",
      "Iteration 260, loss = 0.19492001\n",
      "Iteration 261, loss = 0.19375847\n",
      "Iteration 262, loss = 0.19115563\n",
      "Iteration 263, loss = 0.19149982\n",
      "Iteration 264, loss = 0.19059398\n",
      "Iteration 265, loss = 0.18982245\n",
      "Iteration 266, loss = 0.19013943\n",
      "Iteration 267, loss = 0.18997274\n",
      "Iteration 268, loss = 0.18882101\n",
      "Iteration 269, loss = 0.18988996\n",
      "Iteration 270, loss = 0.18719278\n",
      "Iteration 271, loss = 0.18701444\n",
      "Iteration 272, loss = 0.18645818\n",
      "Iteration 273, loss = 0.18651428\n",
      "Iteration 274, loss = 0.18634155\n",
      "Iteration 275, loss = 0.18634239\n",
      "Iteration 276, loss = 0.18664613\n",
      "Iteration 277, loss = 0.18439160\n",
      "Iteration 278, loss = 0.18414021\n",
      "Iteration 279, loss = 0.18265377\n",
      "Iteration 280, loss = 0.18262855\n",
      "Iteration 281, loss = 0.18191883\n",
      "Iteration 282, loss = 0.18303615\n",
      "Iteration 283, loss = 0.18164737\n",
      "Iteration 284, loss = 0.18098121\n",
      "Iteration 285, loss = 0.17911062\n",
      "Iteration 286, loss = 0.17867605\n",
      "Iteration 287, loss = 0.17836028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 288, loss = 0.17931674\n",
      "Iteration 289, loss = 0.17778168\n",
      "Iteration 290, loss = 0.17722533\n",
      "Iteration 291, loss = 0.17666313\n",
      "Iteration 292, loss = 0.17605034\n",
      "Iteration 293, loss = 0.17648923\n",
      "Iteration 294, loss = 0.17467693\n",
      "Iteration 295, loss = 0.17511528\n",
      "Iteration 296, loss = 0.17376782\n",
      "Iteration 297, loss = 0.17266455\n",
      "Iteration 298, loss = 0.17323896\n",
      "Iteration 299, loss = 0.17175963\n",
      "Iteration 300, loss = 0.17134428\n",
      "Iteration 301, loss = 0.17134670\n",
      "Iteration 302, loss = 0.17088237\n",
      "Iteration 303, loss = 0.17061060\n",
      "Iteration 304, loss = 0.16965258\n",
      "Iteration 305, loss = 0.16956386\n",
      "Iteration 306, loss = 0.16875535\n",
      "Iteration 307, loss = 0.16927342\n",
      "Iteration 308, loss = 0.16695176\n",
      "Iteration 309, loss = 0.16708391\n",
      "Iteration 310, loss = 0.16638175\n",
      "Iteration 311, loss = 0.16529037\n",
      "Iteration 312, loss = 0.16521912\n",
      "Iteration 313, loss = 0.16497630\n",
      "Iteration 314, loss = 0.16376168\n",
      "Iteration 315, loss = 0.16538105\n",
      "Iteration 316, loss = 0.16353359\n",
      "Iteration 317, loss = 0.16264586\n",
      "Iteration 318, loss = 0.16358122\n",
      "Iteration 319, loss = 0.16099492\n",
      "Iteration 320, loss = 0.16223484\n",
      "Iteration 321, loss = 0.16316506\n",
      "Iteration 322, loss = 0.16131736\n",
      "Iteration 323, loss = 0.15981635\n",
      "Iteration 324, loss = 0.15955492\n",
      "Iteration 325, loss = 0.15887853\n",
      "Iteration 326, loss = 0.15893830\n",
      "Iteration 327, loss = 0.15845341\n",
      "Iteration 328, loss = 0.15697000\n",
      "Iteration 329, loss = 0.15715637\n",
      "Iteration 330, loss = 0.15669862\n",
      "Iteration 331, loss = 0.15701597\n",
      "Iteration 332, loss = 0.15548402\n",
      "Iteration 333, loss = 0.15562519\n",
      "Iteration 334, loss = 0.15515337\n",
      "Iteration 335, loss = 0.15388173\n",
      "Iteration 336, loss = 0.15306529\n",
      "Iteration 337, loss = 0.15226257\n",
      "Iteration 338, loss = 0.15256021\n",
      "Iteration 339, loss = 0.15230580\n",
      "Iteration 340, loss = 0.15164632\n",
      "Iteration 341, loss = 0.15106395\n",
      "Iteration 342, loss = 0.15050679\n",
      "Iteration 343, loss = 0.14947720\n",
      "Iteration 344, loss = 0.15160824\n",
      "Iteration 345, loss = 0.15349336\n",
      "Iteration 346, loss = 0.15065467\n",
      "Iteration 347, loss = 0.14786692\n",
      "Iteration 348, loss = 0.14708489\n",
      "Iteration 349, loss = 0.14780631\n",
      "Iteration 350, loss = 0.14593229\n",
      "Iteration 351, loss = 0.14633994\n",
      "Iteration 352, loss = 0.14542193\n",
      "Iteration 353, loss = 0.14425620\n",
      "Iteration 354, loss = 0.14356640\n",
      "Iteration 355, loss = 0.14426617\n",
      "Iteration 356, loss = 0.14303617\n",
      "Iteration 357, loss = 0.14291506\n",
      "Iteration 358, loss = 0.14312609\n",
      "Iteration 359, loss = 0.14236111\n",
      "Iteration 360, loss = 0.14074219\n",
      "Iteration 361, loss = 0.14090177\n",
      "Iteration 362, loss = 0.14042133\n",
      "Iteration 363, loss = 0.14138109\n",
      "Iteration 364, loss = 0.14032617\n",
      "Iteration 365, loss = 0.13986109\n",
      "Iteration 366, loss = 0.13887626\n",
      "Iteration 367, loss = 0.13924775\n",
      "Iteration 368, loss = 0.13904400\n",
      "Iteration 369, loss = 0.13781040\n",
      "Iteration 370, loss = 0.13601428\n",
      "Iteration 371, loss = 0.13581888\n",
      "Iteration 372, loss = 0.13876840\n",
      "Iteration 373, loss = 0.13923804\n",
      "Iteration 374, loss = 0.13708547\n",
      "Iteration 375, loss = 0.13780528\n",
      "Iteration 376, loss = 0.13783174\n",
      "Iteration 377, loss = 0.13621287\n",
      "Iteration 378, loss = 0.13412895\n",
      "Iteration 379, loss = 0.13261251\n",
      "Iteration 380, loss = 0.13253968\n",
      "Iteration 381, loss = 0.13357598\n",
      "Iteration 382, loss = 0.13158653\n",
      "Iteration 383, loss = 0.13062292\n",
      "Iteration 384, loss = 0.13025533\n",
      "Iteration 385, loss = 0.13151571\n",
      "Iteration 386, loss = 0.12961570\n",
      "Iteration 387, loss = 0.12930176\n",
      "Iteration 388, loss = 0.12852728\n",
      "Iteration 389, loss = 0.12839848\n",
      "Iteration 390, loss = 0.12724729\n",
      "Iteration 391, loss = 0.12693920\n",
      "Iteration 392, loss = 0.12612134\n",
      "Iteration 393, loss = 0.12607471\n",
      "Iteration 394, loss = 0.12546018\n",
      "Iteration 395, loss = 0.12526432\n",
      "Iteration 396, loss = 0.12507938\n",
      "Iteration 397, loss = 0.12512605\n",
      "Iteration 398, loss = 0.12481571\n",
      "Iteration 399, loss = 0.12333495\n",
      "Iteration 400, loss = 0.12356642\n",
      "Iteration 401, loss = 0.12387201\n",
      "Iteration 402, loss = 0.12230975\n",
      "Iteration 403, loss = 0.12207275\n",
      "Iteration 404, loss = 0.12072663\n",
      "Iteration 405, loss = 0.12167548\n",
      "Iteration 406, loss = 0.12007437\n",
      "Iteration 407, loss = 0.12099231\n",
      "Iteration 408, loss = 0.12022950\n",
      "Iteration 409, loss = 0.12279112\n",
      "Iteration 410, loss = 0.12146209\n",
      "Iteration 411, loss = 0.11807898\n",
      "Iteration 412, loss = 0.11773031\n",
      "Iteration 413, loss = 0.11812046\n",
      "Iteration 414, loss = 0.11799917\n",
      "Iteration 415, loss = 0.11729554\n",
      "Iteration 416, loss = 0.11799809\n",
      "Iteration 417, loss = 0.11652960\n",
      "Iteration 418, loss = 0.11520715\n",
      "Iteration 419, loss = 0.11478848\n",
      "Iteration 420, loss = 0.11458565\n",
      "Iteration 421, loss = 0.11448418\n",
      "Iteration 422, loss = 0.11321361\n",
      "Iteration 423, loss = 0.11349125\n",
      "Iteration 424, loss = 0.11264854\n",
      "Iteration 425, loss = 0.11284562\n",
      "Iteration 426, loss = 0.11214177\n",
      "Iteration 427, loss = 0.11247883\n",
      "Iteration 428, loss = 0.11128775\n",
      "Iteration 429, loss = 0.11051044\n",
      "Iteration 430, loss = 0.11101378\n",
      "Iteration 431, loss = 0.11171996\n",
      "Iteration 432, loss = 0.11055708\n",
      "Iteration 433, loss = 0.10921904\n",
      "Iteration 434, loss = 0.10938091\n",
      "Iteration 435, loss = 0.10971484\n",
      "Iteration 436, loss = 0.10853721\n",
      "Iteration 437, loss = 0.10753504\n",
      "Iteration 438, loss = 0.10809658\n",
      "Iteration 439, loss = 0.10694887\n",
      "Iteration 440, loss = 0.10597967\n",
      "Iteration 441, loss = 0.10601454\n",
      "Iteration 442, loss = 0.10597709\n",
      "Iteration 443, loss = 0.10561188\n",
      "Iteration 444, loss = 0.10720571\n",
      "Iteration 445, loss = 0.10437779\n",
      "Iteration 446, loss = 0.10519202\n",
      "Iteration 447, loss = 0.10550863\n",
      "Iteration 448, loss = 0.10414026\n",
      "Iteration 449, loss = 0.10234999\n",
      "Iteration 450, loss = 0.10266244\n",
      "Iteration 451, loss = 0.10312383\n",
      "Iteration 452, loss = 0.10207635\n",
      "Iteration 453, loss = 0.10111508\n",
      "Iteration 454, loss = 0.10170653\n",
      "Iteration 455, loss = 0.10593834\n",
      "Iteration 456, loss = 0.10398301\n",
      "Iteration 457, loss = 0.10077325\n",
      "Iteration 458, loss = 0.09972542\n",
      "Iteration 459, loss = 0.10006464\n",
      "Iteration 460, loss = 0.09876999\n",
      "Iteration 461, loss = 0.09852256\n",
      "Iteration 462, loss = 0.09737754\n",
      "Iteration 463, loss = 0.10049882\n",
      "Iteration 464, loss = 0.10018316\n",
      "Iteration 465, loss = 0.09748220\n",
      "Iteration 466, loss = 0.09701176\n",
      "Iteration 467, loss = 0.09658427\n",
      "Iteration 468, loss = 0.09703245\n",
      "Iteration 469, loss = 0.09568459\n",
      "Iteration 470, loss = 0.09525489\n",
      "Iteration 471, loss = 0.09522490\n",
      "Iteration 472, loss = 0.09391363\n",
      "Iteration 473, loss = 0.09446515\n",
      "Iteration 474, loss = 0.09380918\n",
      "Iteration 475, loss = 0.09424494\n",
      "Iteration 476, loss = 0.09443431\n",
      "Iteration 477, loss = 0.09320588\n",
      "Iteration 478, loss = 0.09378312\n",
      "Iteration 479, loss = 0.09219495\n",
      "Iteration 480, loss = 0.09195200\n",
      "Iteration 481, loss = 0.09165139\n",
      "Iteration 482, loss = 0.09056877\n",
      "Iteration 483, loss = 0.09039210\n",
      "Iteration 484, loss = 0.09060542\n",
      "Iteration 485, loss = 0.09039049\n",
      "Iteration 486, loss = 0.09124864\n",
      "Iteration 487, loss = 0.08901788\n",
      "Iteration 488, loss = 0.08935642\n",
      "Iteration 489, loss = 0.08966086\n",
      "Iteration 490, loss = 0.08888138\n",
      "Iteration 491, loss = 0.08891865\n",
      "Iteration 492, loss = 0.08714741\n",
      "Iteration 493, loss = 0.08718344\n",
      "Iteration 494, loss = 0.08695145\n",
      "Iteration 495, loss = 0.08671259\n",
      "Iteration 496, loss = 0.08579383\n",
      "Iteration 497, loss = 0.08644525\n",
      "Iteration 498, loss = 0.08596922\n",
      "Iteration 499, loss = 0.08593823\n",
      "Iteration 500, loss = 0.08618117\n",
      "Iteration 501, loss = 0.08635728\n",
      "Iteration 502, loss = 0.08536476\n",
      "Iteration 503, loss = 0.08516875\n",
      "Iteration 504, loss = 0.08447865\n",
      "Iteration 505, loss = 0.08403925\n",
      "Iteration 506, loss = 0.08362536\n",
      "Iteration 507, loss = 0.08346539\n",
      "Iteration 508, loss = 0.08213861\n",
      "Iteration 509, loss = 0.08220907\n",
      "Iteration 510, loss = 0.08225157\n",
      "Iteration 511, loss = 0.08107688\n",
      "Iteration 512, loss = 0.08277438\n",
      "Iteration 513, loss = 0.08036743\n",
      "Iteration 514, loss = 0.08062756\n",
      "Iteration 515, loss = 0.08026112\n",
      "Iteration 516, loss = 0.07993978\n",
      "Iteration 517, loss = 0.07968178\n",
      "Iteration 518, loss = 0.07960051\n",
      "Iteration 519, loss = 0.07979090\n",
      "Iteration 520, loss = 0.08043538\n",
      "Iteration 521, loss = 0.07859466\n",
      "Iteration 522, loss = 0.07838979\n",
      "Iteration 523, loss = 0.07741335\n",
      "Iteration 524, loss = 0.07760251\n",
      "Iteration 525, loss = 0.07765356\n",
      "Iteration 526, loss = 0.07679828\n",
      "Iteration 527, loss = 0.07623574\n",
      "Iteration 528, loss = 0.07631623\n",
      "Iteration 529, loss = 0.07620029\n",
      "Iteration 530, loss = 0.07590211\n",
      "Iteration 531, loss = 0.07628550\n",
      "Iteration 532, loss = 0.07559266\n",
      "Iteration 533, loss = 0.07503297\n",
      "Iteration 534, loss = 0.07503545\n",
      "Iteration 535, loss = 0.07488217\n",
      "Iteration 536, loss = 0.07536285\n",
      "Iteration 537, loss = 0.07557471\n",
      "Iteration 538, loss = 0.07365366\n",
      "Iteration 539, loss = 0.07345119\n",
      "Iteration 540, loss = 0.07287476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 541, loss = 0.07260908\n",
      "Iteration 542, loss = 0.07243254\n",
      "Iteration 543, loss = 0.07218681\n",
      "Iteration 544, loss = 0.07183180\n",
      "Iteration 545, loss = 0.07203724\n",
      "Iteration 546, loss = 0.07147771\n",
      "Iteration 547, loss = 0.07147798\n",
      "Iteration 548, loss = 0.07085746\n",
      "Iteration 549, loss = 0.07026132\n",
      "Iteration 550, loss = 0.07033215\n",
      "Iteration 551, loss = 0.06967451\n",
      "Iteration 552, loss = 0.06962745\n",
      "Iteration 553, loss = 0.07000348\n",
      "Iteration 554, loss = 0.06883830\n",
      "Iteration 555, loss = 0.06835321\n",
      "Iteration 556, loss = 0.06858983\n",
      "Iteration 557, loss = 0.06821814\n",
      "Iteration 558, loss = 0.06850909\n",
      "Iteration 559, loss = 0.06787064\n",
      "Iteration 560, loss = 0.06890600\n",
      "Iteration 561, loss = 0.06859279\n",
      "Iteration 562, loss = 0.06778623\n",
      "Iteration 563, loss = 0.06707012\n",
      "Iteration 564, loss = 0.06610837\n",
      "Iteration 565, loss = 0.06624493\n",
      "Iteration 566, loss = 0.06631162\n",
      "Iteration 567, loss = 0.06531634\n",
      "Iteration 568, loss = 0.06547415\n",
      "Iteration 569, loss = 0.06525754\n",
      "Iteration 570, loss = 0.06475164\n",
      "Iteration 571, loss = 0.06447078\n",
      "Iteration 572, loss = 0.06605141\n",
      "Iteration 573, loss = 0.06468292\n",
      "Iteration 574, loss = 0.06436235\n",
      "Iteration 575, loss = 0.06350315\n",
      "Iteration 576, loss = 0.06354944\n",
      "Iteration 577, loss = 0.06292540\n",
      "Iteration 578, loss = 0.06270191\n",
      "Iteration 579, loss = 0.06306659\n",
      "Iteration 580, loss = 0.06316715\n",
      "Iteration 581, loss = 0.06370005\n",
      "Iteration 582, loss = 0.06282330\n",
      "Iteration 583, loss = 0.06161284\n",
      "Iteration 584, loss = 0.06146733\n",
      "Iteration 585, loss = 0.06182150\n",
      "Iteration 586, loss = 0.06145459\n",
      "Iteration 587, loss = 0.06065564\n",
      "Iteration 588, loss = 0.06068213\n",
      "Iteration 589, loss = 0.05997107\n",
      "Iteration 590, loss = 0.06015214\n",
      "Iteration 591, loss = 0.05955575\n",
      "Iteration 592, loss = 0.05931509\n",
      "Iteration 593, loss = 0.05967805\n",
      "Iteration 594, loss = 0.05933870\n",
      "Iteration 595, loss = 0.05958962\n",
      "Iteration 596, loss = 0.05876151\n",
      "Iteration 597, loss = 0.05845253\n",
      "Iteration 598, loss = 0.05945002\n",
      "Iteration 599, loss = 0.05777983\n",
      "Iteration 600, loss = 0.05825010\n",
      "Iteration 601, loss = 0.05848252\n",
      "Iteration 602, loss = 0.05847326\n",
      "Iteration 603, loss = 0.05709027\n",
      "Iteration 604, loss = 0.05728570\n",
      "Iteration 605, loss = 0.05641986\n",
      "Iteration 606, loss = 0.05678179\n",
      "Iteration 607, loss = 0.05699282\n",
      "Iteration 608, loss = 0.05633895\n",
      "Iteration 609, loss = 0.05605529\n",
      "Iteration 610, loss = 0.05541101\n",
      "Iteration 611, loss = 0.05508410\n",
      "Iteration 612, loss = 0.05461086\n",
      "Iteration 613, loss = 0.05438085\n",
      "Iteration 614, loss = 0.05456588\n",
      "Iteration 615, loss = 0.05419957\n",
      "Iteration 616, loss = 0.05424747\n",
      "Iteration 617, loss = 0.05393952\n",
      "Iteration 618, loss = 0.05369269\n",
      "Iteration 619, loss = 0.05357734\n",
      "Iteration 620, loss = 0.05323036\n",
      "Iteration 621, loss = 0.05400253\n",
      "Iteration 622, loss = 0.05302578\n",
      "Iteration 623, loss = 0.05335042\n",
      "Iteration 624, loss = 0.05602754\n",
      "Iteration 625, loss = 0.05283859\n",
      "Iteration 626, loss = 0.05469799\n",
      "Iteration 627, loss = 0.05332840\n",
      "Iteration 628, loss = 0.05940711\n",
      "Iteration 629, loss = 0.06048709\n",
      "Iteration 630, loss = 0.05467408\n",
      "Iteration 631, loss = 0.05229096\n",
      "Iteration 632, loss = 0.05109355\n",
      "Iteration 633, loss = 0.05069558\n",
      "Iteration 634, loss = 0.05082219\n",
      "Iteration 635, loss = 0.05115734\n",
      "Iteration 636, loss = 0.05080953\n",
      "Iteration 637, loss = 0.05041766\n",
      "Iteration 638, loss = 0.05098248\n",
      "Iteration 639, loss = 0.05042912\n",
      "Iteration 640, loss = 0.05004508\n",
      "Iteration 641, loss = 0.04957594\n",
      "Iteration 642, loss = 0.04898297\n",
      "Iteration 643, loss = 0.04883040\n",
      "Iteration 644, loss = 0.04898568\n",
      "Iteration 645, loss = 0.04867905\n",
      "Iteration 646, loss = 0.04795776\n",
      "Iteration 647, loss = 0.04832400\n",
      "Iteration 648, loss = 0.04811725\n",
      "Iteration 649, loss = 0.04757236\n",
      "Iteration 650, loss = 0.04796759\n",
      "Iteration 651, loss = 0.04741340\n",
      "Iteration 652, loss = 0.04784987\n",
      "Iteration 653, loss = 0.04779475\n",
      "Iteration 654, loss = 0.04711475\n",
      "Iteration 655, loss = 0.04683208\n",
      "Iteration 656, loss = 0.04640517\n",
      "Iteration 657, loss = 0.04630232\n",
      "Iteration 658, loss = 0.04627284\n",
      "Iteration 659, loss = 0.04611265\n",
      "Iteration 660, loss = 0.04602235\n",
      "Iteration 661, loss = 0.04604184\n",
      "Iteration 662, loss = 0.04631468\n",
      "Iteration 663, loss = 0.04574614\n",
      "Iteration 664, loss = 0.04662105\n",
      "Iteration 665, loss = 0.04541971\n",
      "Iteration 666, loss = 0.04473211\n",
      "Iteration 667, loss = 0.04528719\n",
      "Iteration 668, loss = 0.04427508\n",
      "Iteration 669, loss = 0.04440379\n",
      "Iteration 670, loss = 0.04412425\n",
      "Iteration 671, loss = 0.04411605\n",
      "Iteration 672, loss = 0.04452138\n",
      "Iteration 673, loss = 0.04415703\n",
      "Iteration 674, loss = 0.04386931\n",
      "Iteration 675, loss = 0.04350272\n",
      "Iteration 676, loss = 0.04377817\n",
      "Iteration 677, loss = 0.04272771\n",
      "Iteration 678, loss = 0.04276940\n",
      "Iteration 679, loss = 0.04284231\n",
      "Iteration 680, loss = 0.04261270\n",
      "Iteration 681, loss = 0.04242232\n",
      "Iteration 682, loss = 0.04283437\n",
      "Iteration 683, loss = 0.04271222\n",
      "Iteration 684, loss = 0.04362580\n",
      "Iteration 685, loss = 0.04187036\n",
      "Iteration 686, loss = 0.04217251\n",
      "Iteration 687, loss = 0.04123229\n",
      "Iteration 688, loss = 0.04145475\n",
      "Iteration 689, loss = 0.04163164\n",
      "Iteration 690, loss = 0.04080562\n",
      "Iteration 691, loss = 0.04062458\n",
      "Iteration 692, loss = 0.04082411\n",
      "Iteration 693, loss = 0.04066969\n",
      "Iteration 694, loss = 0.04036093\n",
      "Iteration 695, loss = 0.04055889\n",
      "Iteration 696, loss = 0.03984750\n",
      "Iteration 697, loss = 0.03949945\n",
      "Iteration 698, loss = 0.03950039\n",
      "Iteration 699, loss = 0.03954916\n",
      "Iteration 700, loss = 0.03918966\n",
      "Iteration 701, loss = 0.03938784\n",
      "Iteration 702, loss = 0.03889035\n",
      "Iteration 703, loss = 0.03929631\n",
      "Iteration 704, loss = 0.03856199\n",
      "Iteration 705, loss = 0.03905455\n",
      "Iteration 706, loss = 0.03849998\n",
      "Iteration 707, loss = 0.03850156\n",
      "Iteration 708, loss = 0.03826872\n",
      "Iteration 709, loss = 0.03818906\n",
      "Iteration 710, loss = 0.03794407\n",
      "Iteration 711, loss = 0.03867641\n",
      "Iteration 712, loss = 0.03803526\n",
      "Iteration 713, loss = 0.03753700\n",
      "Iteration 714, loss = 0.03765550\n",
      "Iteration 715, loss = 0.03707413\n",
      "Iteration 716, loss = 0.03702860\n",
      "Iteration 717, loss = 0.03724829\n",
      "Iteration 718, loss = 0.03766189\n",
      "Iteration 719, loss = 0.03686793\n",
      "Iteration 720, loss = 0.03668350\n",
      "Iteration 721, loss = 0.03619419\n",
      "Iteration 722, loss = 0.03612097\n",
      "Iteration 723, loss = 0.03605074\n",
      "Iteration 724, loss = 0.03615091\n",
      "Iteration 725, loss = 0.03587376\n",
      "Iteration 726, loss = 0.03572701\n",
      "Iteration 727, loss = 0.03593924\n",
      "Iteration 728, loss = 0.03614927\n",
      "Iteration 729, loss = 0.03591334\n",
      "Iteration 730, loss = 0.03516093\n",
      "Iteration 731, loss = 0.03524178\n",
      "Iteration 732, loss = 0.03486730\n",
      "Iteration 733, loss = 0.03483035\n",
      "Iteration 734, loss = 0.03515831\n",
      "Iteration 735, loss = 0.03482645\n",
      "Iteration 736, loss = 0.03437472\n",
      "Iteration 737, loss = 0.03418056\n",
      "Iteration 738, loss = 0.03422706\n",
      "Iteration 739, loss = 0.03388791\n",
      "Iteration 740, loss = 0.03412943\n",
      "Iteration 741, loss = 0.03413424\n",
      "Iteration 742, loss = 0.03389699\n",
      "Iteration 743, loss = 0.03335454\n",
      "Iteration 744, loss = 0.03330110\n",
      "Iteration 745, loss = 0.03314464\n",
      "Iteration 746, loss = 0.03334568\n",
      "Iteration 747, loss = 0.03312964\n",
      "Iteration 748, loss = 0.03422746\n",
      "Iteration 749, loss = 0.03603724\n",
      "Iteration 750, loss = 0.03402477\n",
      "Iteration 751, loss = 0.03269235\n",
      "Iteration 752, loss = 0.03252757\n",
      "Iteration 753, loss = 0.03228111\n",
      "Iteration 754, loss = 0.03193379\n",
      "Iteration 755, loss = 0.03194874\n",
      "Iteration 756, loss = 0.03181689\n",
      "Iteration 757, loss = 0.03271425\n",
      "Iteration 758, loss = 0.03229350\n",
      "Iteration 759, loss = 0.03197846\n",
      "Iteration 760, loss = 0.03149270\n",
      "Iteration 761, loss = 0.03138002\n",
      "Iteration 762, loss = 0.03128103\n",
      "Iteration 763, loss = 0.03136646\n",
      "Iteration 764, loss = 0.03122381\n",
      "Iteration 765, loss = 0.03087433\n",
      "Iteration 766, loss = 0.03096163\n",
      "Iteration 767, loss = 0.03093094\n",
      "Iteration 768, loss = 0.03060084\n",
      "Iteration 769, loss = 0.03035389\n",
      "Iteration 770, loss = 0.03023910\n",
      "Iteration 771, loss = 0.03024196\n",
      "Iteration 772, loss = 0.03003454\n",
      "Iteration 773, loss = 0.03029802\n",
      "Iteration 774, loss = 0.02980495\n",
      "Iteration 775, loss = 0.03028866\n",
      "Iteration 776, loss = 0.03024424\n",
      "Iteration 777, loss = 0.02954263\n",
      "Iteration 778, loss = 0.02946920\n",
      "Iteration 779, loss = 0.02921956\n",
      "Iteration 780, loss = 0.02918373\n",
      "Iteration 781, loss = 0.02911581\n",
      "Iteration 782, loss = 0.02901868\n",
      "Iteration 783, loss = 0.02983325\n",
      "Iteration 784, loss = 0.02963701\n",
      "Iteration 785, loss = 0.02868512\n",
      "Iteration 786, loss = 0.02878450\n",
      "Iteration 787, loss = 0.02843868\n",
      "Iteration 788, loss = 0.02832555\n",
      "Iteration 789, loss = 0.02847602\n",
      "Iteration 790, loss = 0.02823999\n",
      "Iteration 791, loss = 0.02807719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 792, loss = 0.02796125\n",
      "Iteration 793, loss = 0.02763686\n",
      "Iteration 794, loss = 0.02768593\n",
      "Iteration 795, loss = 0.02788477\n",
      "Iteration 796, loss = 0.02806801\n",
      "Iteration 797, loss = 0.02747777\n",
      "Iteration 798, loss = 0.02773540\n",
      "Iteration 799, loss = 0.02721007\n",
      "Iteration 800, loss = 0.02752495\n",
      "Iteration 801, loss = 0.02729863\n",
      "Iteration 802, loss = 0.02717111\n",
      "Iteration 803, loss = 0.02703775\n",
      "Iteration 804, loss = 0.02690374\n",
      "Iteration 805, loss = 0.02753603\n",
      "Iteration 806, loss = 0.02704550\n",
      "Iteration 807, loss = 0.02677477\n",
      "Iteration 808, loss = 0.02624084\n",
      "Iteration 809, loss = 0.02617408\n",
      "Iteration 810, loss = 0.02626365\n",
      "Iteration 811, loss = 0.02599722\n",
      "Iteration 812, loss = 0.02614517\n",
      "Iteration 813, loss = 0.02669977\n",
      "Iteration 814, loss = 0.02602314\n",
      "Iteration 815, loss = 0.02590157\n",
      "Iteration 816, loss = 0.02575019\n",
      "Iteration 817, loss = 0.02576064\n",
      "Iteration 818, loss = 0.02529188\n",
      "Iteration 819, loss = 0.02542603\n",
      "Iteration 820, loss = 0.02535770\n",
      "Iteration 821, loss = 0.02511723\n",
      "Iteration 822, loss = 0.02489709\n",
      "Iteration 823, loss = 0.02517157\n",
      "Iteration 824, loss = 0.02470194\n",
      "Iteration 825, loss = 0.02471613\n",
      "Iteration 826, loss = 0.02467024\n",
      "Iteration 827, loss = 0.02460410\n",
      "Iteration 828, loss = 0.02487513\n",
      "Iteration 829, loss = 0.02437557\n",
      "Iteration 830, loss = 0.02425660\n",
      "Iteration 831, loss = 0.02435844\n",
      "Iteration 832, loss = 0.02422238\n",
      "Iteration 833, loss = 0.02399154\n",
      "Iteration 834, loss = 0.02404880\n",
      "Iteration 835, loss = 0.02453537\n",
      "Iteration 836, loss = 0.02389041\n",
      "Iteration 837, loss = 0.02356306\n",
      "Iteration 838, loss = 0.02345078\n",
      "Iteration 839, loss = 0.02338394\n",
      "Iteration 840, loss = 0.02380612\n",
      "Iteration 841, loss = 0.02346244\n",
      "Iteration 842, loss = 0.02359328\n",
      "Iteration 843, loss = 0.02312014\n",
      "Iteration 844, loss = 0.02321494\n",
      "Iteration 845, loss = 0.02282362\n",
      "Iteration 846, loss = 0.02283569\n",
      "Iteration 847, loss = 0.02288889\n",
      "Iteration 848, loss = 0.02287644\n",
      "Iteration 849, loss = 0.02261065\n",
      "Iteration 850, loss = 0.02289038\n",
      "Iteration 851, loss = 0.02255804\n",
      "Iteration 852, loss = 0.02227176\n",
      "Iteration 853, loss = 0.02272700\n",
      "Iteration 854, loss = 0.02238060\n",
      "Iteration 855, loss = 0.02254997\n",
      "Iteration 856, loss = 0.02221980\n",
      "Iteration 857, loss = 0.02190511\n",
      "Iteration 858, loss = 0.02207188\n",
      "Iteration 859, loss = 0.02194251\n",
      "Iteration 860, loss = 0.02157007\n",
      "Iteration 861, loss = 0.02146413\n",
      "Iteration 862, loss = 0.02135903\n",
      "Iteration 863, loss = 0.02178251\n",
      "Iteration 864, loss = 0.02111544\n",
      "Iteration 865, loss = 0.02134164\n",
      "Iteration 866, loss = 0.02136161\n",
      "Iteration 867, loss = 0.02123468\n",
      "Iteration 868, loss = 0.02109025\n",
      "Iteration 869, loss = 0.02098109\n",
      "Iteration 870, loss = 0.02094710\n",
      "Iteration 871, loss = 0.02105352\n",
      "Iteration 872, loss = 0.02101077\n",
      "Iteration 873, loss = 0.02143195\n",
      "Iteration 874, loss = 0.02124477\n",
      "Iteration 875, loss = 0.02093263\n",
      "Iteration 876, loss = 0.02076027\n",
      "Iteration 877, loss = 0.02064200\n",
      "Iteration 878, loss = 0.02028267\n",
      "Iteration 879, loss = 0.02023747\n",
      "Iteration 880, loss = 0.02040290\n",
      "Iteration 881, loss = 0.02016654\n",
      "Iteration 882, loss = 0.02011208\n",
      "Iteration 883, loss = 0.01977350\n",
      "Iteration 884, loss = 0.01990195\n",
      "Iteration 885, loss = 0.01997109\n",
      "Iteration 886, loss = 0.01989689\n",
      "Iteration 887, loss = 0.01982911\n",
      "Iteration 888, loss = 0.01984028\n",
      "Iteration 889, loss = 0.01956233\n",
      "Iteration 890, loss = 0.01933536\n",
      "Iteration 891, loss = 0.01928654\n",
      "Iteration 892, loss = 0.01966592\n",
      "Iteration 893, loss = 0.01949414\n",
      "Iteration 894, loss = 0.01981748\n",
      "Iteration 895, loss = 0.01947573\n",
      "Iteration 896, loss = 0.01915593\n",
      "Iteration 897, loss = 0.01892417\n",
      "Iteration 898, loss = 0.01882002\n",
      "Iteration 899, loss = 0.01885490\n",
      "Iteration 900, loss = 0.01881120\n",
      "Iteration 901, loss = 0.01924990\n",
      "Iteration 902, loss = 0.01892286\n",
      "Iteration 903, loss = 0.01872851\n",
      "Iteration 904, loss = 0.01841733\n",
      "Iteration 905, loss = 0.01837104\n",
      "Iteration 906, loss = 0.01855440\n",
      "Iteration 907, loss = 0.01851851\n",
      "Iteration 908, loss = 0.01880148\n",
      "Iteration 909, loss = 0.01840051\n",
      "Iteration 910, loss = 0.01796295\n",
      "Iteration 911, loss = 0.01788273\n",
      "Iteration 912, loss = 0.01828587\n",
      "Iteration 913, loss = 0.01811067\n",
      "Iteration 914, loss = 0.01826471\n",
      "Iteration 915, loss = 0.01771965\n",
      "Iteration 916, loss = 0.01803828\n",
      "Iteration 917, loss = 0.01866730\n",
      "Iteration 918, loss = 0.01841277\n",
      "Iteration 919, loss = 0.01770280\n",
      "Iteration 920, loss = 0.01751755\n",
      "Iteration 921, loss = 0.01758798\n",
      "Iteration 922, loss = 0.01756481\n",
      "Iteration 923, loss = 0.01801415\n",
      "Iteration 924, loss = 0.01764777\n",
      "Iteration 925, loss = 0.01714492\n",
      "Iteration 926, loss = 0.01715043\n",
      "Iteration 927, loss = 0.01710679\n",
      "Iteration 928, loss = 0.01666802\n",
      "Iteration 929, loss = 0.01667155\n",
      "Iteration 930, loss = 0.01679447\n",
      "Iteration 931, loss = 0.01679386\n",
      "Iteration 932, loss = 0.01691522\n",
      "Iteration 933, loss = 0.01675687\n",
      "Iteration 934, loss = 0.01664491\n",
      "Iteration 935, loss = 0.01669835\n",
      "Iteration 936, loss = 0.01660897\n",
      "Iteration 937, loss = 0.01650809\n",
      "Iteration 938, loss = 0.01635540\n",
      "Iteration 939, loss = 0.01646799\n",
      "Iteration 940, loss = 0.01634894\n",
      "Iteration 941, loss = 0.01621306\n",
      "Iteration 942, loss = 0.01591023\n",
      "Iteration 943, loss = 0.01590456\n",
      "Iteration 944, loss = 0.01588099\n",
      "Iteration 945, loss = 0.01588381\n",
      "Iteration 946, loss = 0.01577843\n",
      "Iteration 947, loss = 0.01558371\n",
      "Iteration 948, loss = 0.01572991\n",
      "Iteration 949, loss = 0.01576859\n",
      "Iteration 950, loss = 0.01575805\n",
      "Iteration 951, loss = 0.01621039\n",
      "Iteration 952, loss = 0.01600754\n",
      "Iteration 953, loss = 0.01559362\n",
      "Iteration 954, loss = 0.01522807\n",
      "Iteration 955, loss = 0.01522644\n",
      "Iteration 956, loss = 0.01544016\n",
      "Iteration 957, loss = 0.01532005\n",
      "Iteration 958, loss = 0.01507346\n",
      "Iteration 959, loss = 0.01520667\n",
      "Iteration 960, loss = 0.01542110\n",
      "Iteration 961, loss = 0.01521117\n",
      "Iteration 962, loss = 0.01508447\n",
      "Iteration 963, loss = 0.01483765\n",
      "Iteration 964, loss = 0.01472381\n",
      "Iteration 965, loss = 0.01457161\n",
      "Iteration 966, loss = 0.01454943\n",
      "Iteration 967, loss = 0.01480542\n",
      "Iteration 968, loss = 0.01467048\n",
      "Iteration 969, loss = 0.01465904\n",
      "Iteration 970, loss = 0.01467550\n",
      "Iteration 971, loss = 0.01477684\n",
      "Iteration 972, loss = 0.01445377\n",
      "Iteration 973, loss = 0.01478817\n",
      "Iteration 974, loss = 0.01442389\n",
      "Iteration 975, loss = 0.01439277\n",
      "Iteration 976, loss = 0.01406870\n",
      "Iteration 977, loss = 0.01441809\n",
      "Iteration 978, loss = 0.01430847\n",
      "Iteration 979, loss = 0.01414619\n",
      "Iteration 980, loss = 0.01399162\n",
      "Iteration 981, loss = 0.01378689\n",
      "Iteration 982, loss = 0.01371341\n",
      "Iteration 983, loss = 0.01380618\n",
      "Iteration 984, loss = 0.01398951\n",
      "Iteration 985, loss = 0.01388933\n",
      "Iteration 986, loss = 0.01366348\n",
      "Iteration 987, loss = 0.01353418\n",
      "Iteration 988, loss = 0.01409854\n",
      "Iteration 989, loss = 0.01405229\n",
      "Iteration 990, loss = 0.01378966\n",
      "Iteration 991, loss = 0.01395726\n",
      "Iteration 992, loss = 0.01440189\n",
      "Iteration 993, loss = 0.01440717\n",
      "Iteration 994, loss = 0.01344585\n",
      "Iteration 995, loss = 0.01340018\n",
      "Iteration 996, loss = 0.01344136\n",
      "Iteration 997, loss = 0.01339009\n",
      "Iteration 998, loss = 0.01340432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 8 of 48...\n",
      "Iteration 1, loss = 0.68604829\n",
      "Iteration 2, loss = 0.66592132\n",
      "Iteration 3, loss = 0.65119958\n",
      "Iteration 4, loss = 0.63409539\n",
      "Iteration 5, loss = 0.61863364\n",
      "Iteration 6, loss = 0.60274391\n",
      "Iteration 7, loss = 0.58778616\n",
      "Iteration 8, loss = 0.57291282\n",
      "Iteration 9, loss = 0.55888649\n",
      "Iteration 10, loss = 0.54598769\n",
      "Iteration 11, loss = 0.53282842\n",
      "Iteration 12, loss = 0.52184582\n",
      "Iteration 13, loss = 0.51063381\n",
      "Iteration 14, loss = 0.49993580\n",
      "Iteration 15, loss = 0.49007022\n",
      "Iteration 16, loss = 0.48251621\n",
      "Iteration 17, loss = 0.47493974\n",
      "Iteration 18, loss = 0.46802834\n",
      "Iteration 19, loss = 0.45928805\n",
      "Iteration 20, loss = 0.45274631\n",
      "Iteration 21, loss = 0.44668907\n",
      "Iteration 22, loss = 0.44052064\n",
      "Iteration 23, loss = 0.43565570\n",
      "Iteration 24, loss = 0.43103616\n",
      "Iteration 25, loss = 0.42564532\n",
      "Iteration 26, loss = 0.42138523\n",
      "Iteration 27, loss = 0.41751762\n",
      "Iteration 28, loss = 0.41301895\n",
      "Iteration 29, loss = 0.40938784\n",
      "Iteration 30, loss = 0.40866086\n",
      "Iteration 31, loss = 0.40246294\n",
      "Iteration 32, loss = 0.40008261\n",
      "Iteration 33, loss = 0.40017420\n",
      "Iteration 34, loss = 0.39442477\n",
      "Iteration 35, loss = 0.39087661\n",
      "Iteration 36, loss = 0.38862628\n",
      "Iteration 37, loss = 0.38469022\n",
      "Iteration 38, loss = 0.38341249\n",
      "Iteration 39, loss = 0.37977319\n",
      "Iteration 40, loss = 0.37837214\n",
      "Iteration 41, loss = 0.37624395\n",
      "Iteration 42, loss = 0.37348064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.37132675\n",
      "Iteration 44, loss = 0.36906733\n",
      "Iteration 45, loss = 0.36717698\n",
      "Iteration 46, loss = 0.36582983\n",
      "Iteration 47, loss = 0.36275071\n",
      "Iteration 48, loss = 0.36225917\n",
      "Iteration 49, loss = 0.35950542\n",
      "Iteration 50, loss = 0.35839614\n",
      "Iteration 51, loss = 0.35869189\n",
      "Iteration 52, loss = 0.35450524\n",
      "Iteration 53, loss = 0.35227177\n",
      "Iteration 54, loss = 0.35156254\n",
      "Iteration 55, loss = 0.35076224\n",
      "Iteration 56, loss = 0.34895975\n",
      "Iteration 57, loss = 0.34690424\n",
      "Iteration 58, loss = 0.34563056\n",
      "Iteration 59, loss = 0.34498446\n",
      "Iteration 60, loss = 0.34286337\n",
      "Iteration 61, loss = 0.34071637\n",
      "Iteration 62, loss = 0.34176261\n",
      "Iteration 63, loss = 0.33861299\n",
      "Iteration 64, loss = 0.33602771\n",
      "Iteration 65, loss = 0.33686742\n",
      "Iteration 66, loss = 0.33703654\n",
      "Iteration 67, loss = 0.33317460\n",
      "Iteration 68, loss = 0.33208637\n",
      "Iteration 69, loss = 0.33035358\n",
      "Iteration 70, loss = 0.32878461\n",
      "Iteration 71, loss = 0.32734039\n",
      "Iteration 72, loss = 0.32642756\n",
      "Iteration 73, loss = 0.32488738\n",
      "Iteration 74, loss = 0.32459177\n",
      "Iteration 75, loss = 0.32256456\n",
      "Iteration 76, loss = 0.32195282\n",
      "Iteration 77, loss = 0.32028285\n",
      "Iteration 78, loss = 0.31974198\n",
      "Iteration 79, loss = 0.31892725\n",
      "Iteration 80, loss = 0.31814881\n",
      "Iteration 81, loss = 0.31569315\n",
      "Iteration 82, loss = 0.31456587\n",
      "Iteration 83, loss = 0.31504101\n",
      "Iteration 84, loss = 0.31327935\n",
      "Iteration 85, loss = 0.31164642\n",
      "Iteration 86, loss = 0.31035173\n",
      "Iteration 87, loss = 0.31021832\n",
      "Iteration 88, loss = 0.30846979\n",
      "Iteration 89, loss = 0.30715907\n",
      "Iteration 90, loss = 0.30836088\n",
      "Iteration 91, loss = 0.30489145\n",
      "Iteration 92, loss = 0.30661859\n",
      "Iteration 93, loss = 0.30362294\n",
      "Iteration 94, loss = 0.30314520\n",
      "Iteration 95, loss = 0.30182281\n",
      "Iteration 96, loss = 0.30123058\n",
      "Iteration 97, loss = 0.30329455\n",
      "Iteration 98, loss = 0.30318072\n",
      "Iteration 99, loss = 0.29853301\n",
      "Iteration 100, loss = 0.29879424\n",
      "Iteration 101, loss = 0.29689555\n",
      "Iteration 102, loss = 0.29582219\n",
      "Iteration 103, loss = 0.29578487\n",
      "Iteration 104, loss = 0.29353257\n",
      "Iteration 105, loss = 0.29244942\n",
      "Iteration 106, loss = 0.29217711\n",
      "Iteration 107, loss = 0.29039208\n",
      "Iteration 108, loss = 0.28964803\n",
      "Iteration 109, loss = 0.29018352\n",
      "Iteration 110, loss = 0.28774835\n",
      "Iteration 111, loss = 0.28805603\n",
      "Iteration 112, loss = 0.28750880\n",
      "Iteration 113, loss = 0.28729068\n",
      "Iteration 114, loss = 0.28471417\n",
      "Iteration 115, loss = 0.28441362\n",
      "Iteration 116, loss = 0.28565321\n",
      "Iteration 117, loss = 0.28299547\n",
      "Iteration 118, loss = 0.28143048\n",
      "Iteration 119, loss = 0.28133017\n",
      "Iteration 120, loss = 0.28125723\n",
      "Iteration 121, loss = 0.27984582\n",
      "Iteration 122, loss = 0.28065079\n",
      "Iteration 123, loss = 0.28016947\n",
      "Iteration 124, loss = 0.28075307\n",
      "Iteration 125, loss = 0.27596962\n",
      "Iteration 126, loss = 0.27543872\n",
      "Iteration 127, loss = 0.27590826\n",
      "Iteration 128, loss = 0.27373652\n",
      "Iteration 129, loss = 0.27381841\n",
      "Iteration 130, loss = 0.27305637\n",
      "Iteration 131, loss = 0.27176455\n",
      "Iteration 132, loss = 0.27096169\n",
      "Iteration 133, loss = 0.27105739\n",
      "Iteration 134, loss = 0.27032870\n",
      "Iteration 135, loss = 0.27029695\n",
      "Iteration 136, loss = 0.26841738\n",
      "Iteration 137, loss = 0.26823460\n",
      "Iteration 138, loss = 0.26670640\n",
      "Iteration 139, loss = 0.26605539\n",
      "Iteration 140, loss = 0.26723504\n",
      "Iteration 141, loss = 0.26579774\n",
      "Iteration 142, loss = 0.26575007\n",
      "Iteration 143, loss = 0.26323869\n",
      "Iteration 144, loss = 0.26412459\n",
      "Iteration 145, loss = 0.26672232\n",
      "Iteration 146, loss = 0.26156401\n",
      "Iteration 147, loss = 0.26003910\n",
      "Iteration 148, loss = 0.25977314\n",
      "Iteration 149, loss = 0.25997480\n",
      "Iteration 150, loss = 0.26042387\n",
      "Iteration 151, loss = 0.25819195\n",
      "Iteration 152, loss = 0.26078686\n",
      "Iteration 153, loss = 0.25775319\n",
      "Iteration 154, loss = 0.25563826\n",
      "Iteration 155, loss = 0.25488420\n",
      "Iteration 156, loss = 0.25461963\n",
      "Iteration 157, loss = 0.25502771\n",
      "Iteration 158, loss = 0.25442066\n",
      "Iteration 159, loss = 0.25585155\n",
      "Iteration 160, loss = 0.25568841\n",
      "Iteration 161, loss = 0.25185787\n",
      "Iteration 162, loss = 0.25010886\n",
      "Iteration 163, loss = 0.25192868\n",
      "Iteration 164, loss = 0.25031936\n",
      "Iteration 165, loss = 0.25156588\n",
      "Iteration 166, loss = 0.25261640\n",
      "Iteration 167, loss = 0.24930432\n",
      "Iteration 168, loss = 0.24789214\n",
      "Iteration 169, loss = 0.24626537\n",
      "Iteration 170, loss = 0.24816479\n",
      "Iteration 171, loss = 0.24463819\n",
      "Iteration 172, loss = 0.24466374\n",
      "Iteration 173, loss = 0.24386910\n",
      "Iteration 174, loss = 0.24326898\n",
      "Iteration 175, loss = 0.24393207\n",
      "Iteration 176, loss = 0.24516100\n",
      "Iteration 177, loss = 0.24331510\n",
      "Iteration 178, loss = 0.24104723\n",
      "Iteration 179, loss = 0.23947445\n",
      "Iteration 180, loss = 0.23915640\n",
      "Iteration 181, loss = 0.23845902\n",
      "Iteration 182, loss = 0.23910123\n",
      "Iteration 183, loss = 0.23733842\n",
      "Iteration 184, loss = 0.23655630\n",
      "Iteration 185, loss = 0.23682109\n",
      "Iteration 186, loss = 0.23641243\n",
      "Iteration 187, loss = 0.23561579\n",
      "Iteration 188, loss = 0.23405535\n",
      "Iteration 189, loss = 0.23358958\n",
      "Iteration 190, loss = 0.23251396\n",
      "Iteration 191, loss = 0.23215246\n",
      "Iteration 192, loss = 0.23212301\n",
      "Iteration 193, loss = 0.23315178\n",
      "Iteration 194, loss = 0.23465794\n",
      "Iteration 195, loss = 0.23348419\n",
      "Iteration 196, loss = 0.23247551\n",
      "Iteration 197, loss = 0.22856888\n",
      "Iteration 198, loss = 0.22998275\n",
      "Iteration 199, loss = 0.22807769\n",
      "Iteration 200, loss = 0.22703713\n",
      "Iteration 201, loss = 0.22636694\n",
      "Iteration 202, loss = 0.22661704\n",
      "Iteration 203, loss = 0.22651968\n",
      "Iteration 204, loss = 0.22556001\n",
      "Iteration 205, loss = 0.22551733\n",
      "Iteration 206, loss = 0.22318979\n",
      "Iteration 207, loss = 0.22321259\n",
      "Iteration 208, loss = 0.22178428\n",
      "Iteration 209, loss = 0.22307850\n",
      "Iteration 210, loss = 0.22249003\n",
      "Iteration 211, loss = 0.22061117\n",
      "Iteration 212, loss = 0.21918436\n",
      "Iteration 213, loss = 0.21952833\n",
      "Iteration 214, loss = 0.21854237\n",
      "Iteration 215, loss = 0.21993257\n",
      "Iteration 216, loss = 0.21935523\n",
      "Iteration 217, loss = 0.21697317\n",
      "Iteration 218, loss = 0.21593665\n",
      "Iteration 219, loss = 0.21609498\n",
      "Iteration 220, loss = 0.21618758\n",
      "Iteration 221, loss = 0.21390885\n",
      "Iteration 222, loss = 0.21457670\n",
      "Iteration 223, loss = 0.21447900\n",
      "Iteration 224, loss = 0.21440552\n",
      "Iteration 225, loss = 0.21317253\n",
      "Iteration 226, loss = 0.21261380\n",
      "Iteration 227, loss = 0.21114876\n",
      "Iteration 228, loss = 0.21104995\n",
      "Iteration 229, loss = 0.21105932\n",
      "Iteration 230, loss = 0.20919633\n",
      "Iteration 231, loss = 0.20903531\n",
      "Iteration 232, loss = 0.21002105\n",
      "Iteration 233, loss = 0.20916458\n",
      "Iteration 234, loss = 0.20753519\n",
      "Iteration 235, loss = 0.20801710\n",
      "Iteration 236, loss = 0.20553958\n",
      "Iteration 237, loss = 0.20505683\n",
      "Iteration 238, loss = 0.20493272\n",
      "Iteration 239, loss = 0.20518637\n",
      "Iteration 240, loss = 0.20397826\n",
      "Iteration 241, loss = 0.20332006\n",
      "Iteration 242, loss = 0.20256546\n",
      "Iteration 243, loss = 0.20217008\n",
      "Iteration 244, loss = 0.20091981\n",
      "Iteration 245, loss = 0.20172908\n",
      "Iteration 246, loss = 0.19948129\n",
      "Iteration 247, loss = 0.19962291\n",
      "Iteration 248, loss = 0.19915646\n",
      "Iteration 249, loss = 0.19804836\n",
      "Iteration 250, loss = 0.19749668\n",
      "Iteration 251, loss = 0.19758845\n",
      "Iteration 252, loss = 0.19630253\n",
      "Iteration 253, loss = 0.19554781\n",
      "Iteration 254, loss = 0.19489578\n",
      "Iteration 255, loss = 0.19519632\n",
      "Iteration 256, loss = 0.19452177\n",
      "Iteration 257, loss = 0.19415818\n",
      "Iteration 258, loss = 0.19302275\n",
      "Iteration 259, loss = 0.19322003\n",
      "Iteration 260, loss = 0.19374337\n",
      "Iteration 261, loss = 0.19327041\n",
      "Iteration 262, loss = 0.19449008\n",
      "Iteration 263, loss = 0.19069891\n",
      "Iteration 264, loss = 0.19006301\n",
      "Iteration 265, loss = 0.18959627\n",
      "Iteration 266, loss = 0.18930761\n",
      "Iteration 267, loss = 0.18859247\n",
      "Iteration 268, loss = 0.18766756\n",
      "Iteration 269, loss = 0.18815342\n",
      "Iteration 270, loss = 0.18650922\n",
      "Iteration 271, loss = 0.18592651\n",
      "Iteration 272, loss = 0.18611923\n",
      "Iteration 273, loss = 0.18524383\n",
      "Iteration 274, loss = 0.18414696\n",
      "Iteration 275, loss = 0.18328380\n",
      "Iteration 276, loss = 0.18257724\n",
      "Iteration 277, loss = 0.18232288\n",
      "Iteration 278, loss = 0.18241477\n",
      "Iteration 279, loss = 0.18249409\n",
      "Iteration 280, loss = 0.18039704\n",
      "Iteration 281, loss = 0.18093309\n",
      "Iteration 282, loss = 0.18174675\n",
      "Iteration 283, loss = 0.18029872\n",
      "Iteration 284, loss = 0.17904214\n",
      "Iteration 285, loss = 0.17743196\n",
      "Iteration 286, loss = 0.17745664\n",
      "Iteration 287, loss = 0.17674346\n",
      "Iteration 288, loss = 0.17624329\n",
      "Iteration 289, loss = 0.17524116\n",
      "Iteration 290, loss = 0.17551446\n",
      "Iteration 291, loss = 0.17502296\n",
      "Iteration 292, loss = 0.17594554\n",
      "Iteration 293, loss = 0.17492990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 294, loss = 0.17357680\n",
      "Iteration 295, loss = 0.17169076\n",
      "Iteration 296, loss = 0.17182619\n",
      "Iteration 297, loss = 0.17203773\n",
      "Iteration 298, loss = 0.17286977\n",
      "Iteration 299, loss = 0.17219045\n",
      "Iteration 300, loss = 0.17024943\n",
      "Iteration 301, loss = 0.17001874\n",
      "Iteration 302, loss = 0.16922570\n",
      "Iteration 303, loss = 0.16849296\n",
      "Iteration 304, loss = 0.16851565\n",
      "Iteration 305, loss = 0.16712677\n",
      "Iteration 306, loss = 0.16620081\n",
      "Iteration 307, loss = 0.16578464\n",
      "Iteration 308, loss = 0.16546916\n",
      "Iteration 309, loss = 0.16493714\n",
      "Iteration 310, loss = 0.16447643\n",
      "Iteration 311, loss = 0.16471484\n",
      "Iteration 312, loss = 0.16444835\n",
      "Iteration 313, loss = 0.16251985\n",
      "Iteration 314, loss = 0.16211250\n",
      "Iteration 315, loss = 0.16265412\n",
      "Iteration 316, loss = 0.16149196\n",
      "Iteration 317, loss = 0.16046130\n",
      "Iteration 318, loss = 0.16139675\n",
      "Iteration 319, loss = 0.15939585\n",
      "Iteration 320, loss = 0.15963344\n",
      "Iteration 321, loss = 0.15888962\n",
      "Iteration 322, loss = 0.15833365\n",
      "Iteration 323, loss = 0.15898960\n",
      "Iteration 324, loss = 0.15762467\n",
      "Iteration 325, loss = 0.15781498\n",
      "Iteration 326, loss = 0.15810649\n",
      "Iteration 327, loss = 0.15561229\n",
      "Iteration 328, loss = 0.15548392\n",
      "Iteration 329, loss = 0.15467100\n",
      "Iteration 330, loss = 0.15668545\n",
      "Iteration 331, loss = 0.15432075\n",
      "Iteration 332, loss = 0.15403260\n",
      "Iteration 333, loss = 0.15261037\n",
      "Iteration 334, loss = 0.15212704\n",
      "Iteration 335, loss = 0.15291537\n",
      "Iteration 336, loss = 0.15216280\n",
      "Iteration 337, loss = 0.15273254\n",
      "Iteration 338, loss = 0.15221507\n",
      "Iteration 339, loss = 0.15128766\n",
      "Iteration 340, loss = 0.14877426\n",
      "Iteration 341, loss = 0.14926474\n",
      "Iteration 342, loss = 0.14829576\n",
      "Iteration 343, loss = 0.14773217\n",
      "Iteration 344, loss = 0.14710958\n",
      "Iteration 345, loss = 0.14625639\n",
      "Iteration 346, loss = 0.14633430\n",
      "Iteration 347, loss = 0.14551101\n",
      "Iteration 348, loss = 0.14508400\n",
      "Iteration 349, loss = 0.14480609\n",
      "Iteration 350, loss = 0.14451618\n",
      "Iteration 351, loss = 0.14421968\n",
      "Iteration 352, loss = 0.14433257\n",
      "Iteration 353, loss = 0.14361923\n",
      "Iteration 354, loss = 0.14183951\n",
      "Iteration 355, loss = 0.14296870\n",
      "Iteration 356, loss = 0.14190795\n",
      "Iteration 357, loss = 0.14122102\n",
      "Iteration 358, loss = 0.13992024\n",
      "Iteration 359, loss = 0.14109956\n",
      "Iteration 360, loss = 0.14048947\n",
      "Iteration 361, loss = 0.13948556\n",
      "Iteration 362, loss = 0.13802100\n",
      "Iteration 363, loss = 0.13821331\n",
      "Iteration 364, loss = 0.13723442\n",
      "Iteration 365, loss = 0.13687419\n",
      "Iteration 366, loss = 0.13715188\n",
      "Iteration 367, loss = 0.13641839\n",
      "Iteration 368, loss = 0.13630968\n",
      "Iteration 369, loss = 0.13507738\n",
      "Iteration 370, loss = 0.13511355\n",
      "Iteration 371, loss = 0.13436165\n",
      "Iteration 372, loss = 0.13363983\n",
      "Iteration 373, loss = 0.13342840\n",
      "Iteration 374, loss = 0.13286977\n",
      "Iteration 375, loss = 0.13280910\n",
      "Iteration 376, loss = 0.13269047\n",
      "Iteration 377, loss = 0.13324312\n",
      "Iteration 378, loss = 0.13174946\n",
      "Iteration 379, loss = 0.13059241\n",
      "Iteration 380, loss = 0.13161348\n",
      "Iteration 381, loss = 0.13028597\n",
      "Iteration 382, loss = 0.12934562\n",
      "Iteration 383, loss = 0.12870974\n",
      "Iteration 384, loss = 0.12828795\n",
      "Iteration 385, loss = 0.12807861\n",
      "Iteration 386, loss = 0.12832938\n",
      "Iteration 387, loss = 0.12759214\n",
      "Iteration 388, loss = 0.12771201\n",
      "Iteration 389, loss = 0.12724272\n",
      "Iteration 390, loss = 0.12644382\n",
      "Iteration 391, loss = 0.12640902\n",
      "Iteration 392, loss = 0.12543364\n",
      "Iteration 393, loss = 0.12413715\n",
      "Iteration 394, loss = 0.12451000\n",
      "Iteration 395, loss = 0.12456064\n",
      "Iteration 396, loss = 0.12326041\n",
      "Iteration 397, loss = 0.12397402\n",
      "Iteration 398, loss = 0.12434097\n",
      "Iteration 399, loss = 0.12229186\n",
      "Iteration 400, loss = 0.12161393\n",
      "Iteration 401, loss = 0.12152047\n",
      "Iteration 402, loss = 0.12377324\n",
      "Iteration 403, loss = 0.12241177\n",
      "Iteration 404, loss = 0.12127276\n",
      "Iteration 405, loss = 0.11938549\n",
      "Iteration 406, loss = 0.12013478\n",
      "Iteration 407, loss = 0.11875978\n",
      "Iteration 408, loss = 0.11867223\n",
      "Iteration 409, loss = 0.12016639\n",
      "Iteration 410, loss = 0.11975884\n",
      "Iteration 411, loss = 0.11716452\n",
      "Iteration 412, loss = 0.11746593\n",
      "Iteration 413, loss = 0.11696585\n",
      "Iteration 414, loss = 0.11553123\n",
      "Iteration 415, loss = 0.11560771\n",
      "Iteration 416, loss = 0.11462098\n",
      "Iteration 417, loss = 0.11552667\n",
      "Iteration 418, loss = 0.11461322\n",
      "Iteration 419, loss = 0.11475178\n",
      "Iteration 420, loss = 0.11433761\n",
      "Iteration 421, loss = 0.11335443\n",
      "Iteration 422, loss = 0.11257305\n",
      "Iteration 423, loss = 0.11218201\n",
      "Iteration 424, loss = 0.11147768\n",
      "Iteration 425, loss = 0.11206537\n",
      "Iteration 426, loss = 0.11112020\n",
      "Iteration 427, loss = 0.11166367\n",
      "Iteration 428, loss = 0.11048827\n",
      "Iteration 429, loss = 0.11157923\n",
      "Iteration 430, loss = 0.11028929\n",
      "Iteration 431, loss = 0.10916391\n",
      "Iteration 432, loss = 0.10935613\n",
      "Iteration 433, loss = 0.10855386\n",
      "Iteration 434, loss = 0.10920375\n",
      "Iteration 435, loss = 0.10782810\n",
      "Iteration 436, loss = 0.10727710\n",
      "Iteration 437, loss = 0.10716454\n",
      "Iteration 438, loss = 0.10703075\n",
      "Iteration 439, loss = 0.10605336\n",
      "Iteration 440, loss = 0.10739590\n",
      "Iteration 441, loss = 0.10766462\n",
      "Iteration 442, loss = 0.10784817\n",
      "Iteration 443, loss = 0.10657600\n",
      "Iteration 444, loss = 0.10558946\n",
      "Iteration 445, loss = 0.10402568\n",
      "Iteration 446, loss = 0.10359543\n",
      "Iteration 447, loss = 0.10407923\n",
      "Iteration 448, loss = 0.10324484\n",
      "Iteration 449, loss = 0.10283809\n",
      "Iteration 450, loss = 0.10247560\n",
      "Iteration 451, loss = 0.10255018\n",
      "Iteration 452, loss = 0.10167613\n",
      "Iteration 453, loss = 0.10108631\n",
      "Iteration 454, loss = 0.10299627\n",
      "Iteration 455, loss = 0.10137964\n",
      "Iteration 456, loss = 0.10044548\n",
      "Iteration 457, loss = 0.09962799\n",
      "Iteration 458, loss = 0.10055567\n",
      "Iteration 459, loss = 0.10549998\n",
      "Iteration 460, loss = 0.10782683\n",
      "Iteration 461, loss = 0.10280754\n",
      "Iteration 462, loss = 0.10036025\n",
      "Iteration 463, loss = 0.09897407\n",
      "Iteration 464, loss = 0.09923045\n",
      "Iteration 465, loss = 0.09761582\n",
      "Iteration 466, loss = 0.09663381\n",
      "Iteration 467, loss = 0.09648557\n",
      "Iteration 468, loss = 0.09621300\n",
      "Iteration 469, loss = 0.09636246\n",
      "Iteration 470, loss = 0.09587410\n",
      "Iteration 471, loss = 0.09528364\n",
      "Iteration 472, loss = 0.09467840\n",
      "Iteration 473, loss = 0.09457754\n",
      "Iteration 474, loss = 0.09446500\n",
      "Iteration 475, loss = 0.09440868\n",
      "Iteration 476, loss = 0.09368476\n",
      "Iteration 477, loss = 0.09438360\n",
      "Iteration 478, loss = 0.09332352\n",
      "Iteration 479, loss = 0.09279826\n",
      "Iteration 480, loss = 0.09289544\n",
      "Iteration 481, loss = 0.09194359\n",
      "Iteration 482, loss = 0.09199259\n",
      "Iteration 483, loss = 0.09144639\n",
      "Iteration 484, loss = 0.09072371\n",
      "Iteration 485, loss = 0.09089239\n",
      "Iteration 486, loss = 0.09107114\n",
      "Iteration 487, loss = 0.09116425\n",
      "Iteration 488, loss = 0.09034143\n",
      "Iteration 489, loss = 0.08978779\n",
      "Iteration 490, loss = 0.08920306\n",
      "Iteration 491, loss = 0.08931451\n",
      "Iteration 492, loss = 0.08894381\n",
      "Iteration 493, loss = 0.08880784\n",
      "Iteration 494, loss = 0.09066526\n",
      "Iteration 495, loss = 0.08858244\n",
      "Iteration 496, loss = 0.08755348\n",
      "Iteration 497, loss = 0.08708983\n",
      "Iteration 498, loss = 0.08715792\n",
      "Iteration 499, loss = 0.08639566\n",
      "Iteration 500, loss = 0.08601154\n",
      "Iteration 501, loss = 0.08600590\n",
      "Iteration 502, loss = 0.08615678\n",
      "Iteration 503, loss = 0.08573297\n",
      "Iteration 504, loss = 0.08598843\n",
      "Iteration 505, loss = 0.08547696\n",
      "Iteration 506, loss = 0.08614333\n",
      "Iteration 507, loss = 0.08490548\n",
      "Iteration 508, loss = 0.08411500\n",
      "Iteration 509, loss = 0.08402611\n",
      "Iteration 510, loss = 0.08471420\n",
      "Iteration 511, loss = 0.08441090\n",
      "Iteration 512, loss = 0.08373834\n",
      "Iteration 513, loss = 0.08295286\n",
      "Iteration 514, loss = 0.08234600\n",
      "Iteration 515, loss = 0.08202628\n",
      "Iteration 516, loss = 0.08202285\n",
      "Iteration 517, loss = 0.08156185\n",
      "Iteration 518, loss = 0.08111059\n",
      "Iteration 519, loss = 0.08082962\n",
      "Iteration 520, loss = 0.08150233\n",
      "Iteration 521, loss = 0.08033186\n",
      "Iteration 522, loss = 0.08014939\n",
      "Iteration 523, loss = 0.08004934\n",
      "Iteration 524, loss = 0.08016770\n",
      "Iteration 525, loss = 0.08032673\n",
      "Iteration 526, loss = 0.07909717\n",
      "Iteration 527, loss = 0.07875268\n",
      "Iteration 528, loss = 0.07856686\n",
      "Iteration 529, loss = 0.07911768\n",
      "Iteration 530, loss = 0.07795101\n",
      "Iteration 531, loss = 0.07780191\n",
      "Iteration 532, loss = 0.07784839\n",
      "Iteration 533, loss = 0.07750414\n",
      "Iteration 534, loss = 0.07754260\n",
      "Iteration 535, loss = 0.07827684\n",
      "Iteration 536, loss = 0.07688405\n",
      "Iteration 537, loss = 0.07645205\n",
      "Iteration 538, loss = 0.07645116\n",
      "Iteration 539, loss = 0.07560747\n",
      "Iteration 540, loss = 0.07552886\n",
      "Iteration 541, loss = 0.07606435\n",
      "Iteration 542, loss = 0.07626169\n",
      "Iteration 543, loss = 0.07488003\n",
      "Iteration 544, loss = 0.07454420\n",
      "Iteration 545, loss = 0.07477758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 546, loss = 0.07484726\n",
      "Iteration 547, loss = 0.07467288\n",
      "Iteration 548, loss = 0.07325488\n",
      "Iteration 549, loss = 0.07346246\n",
      "Iteration 550, loss = 0.07296801\n",
      "Iteration 551, loss = 0.07265871\n",
      "Iteration 552, loss = 0.07243334\n",
      "Iteration 553, loss = 0.07204565\n",
      "Iteration 554, loss = 0.07245138\n",
      "Iteration 555, loss = 0.07217009\n",
      "Iteration 556, loss = 0.07199197\n",
      "Iteration 557, loss = 0.07139386\n",
      "Iteration 558, loss = 0.07088644\n",
      "Iteration 559, loss = 0.07066259\n",
      "Iteration 560, loss = 0.07042106\n",
      "Iteration 561, loss = 0.07058559\n",
      "Iteration 562, loss = 0.07149476\n",
      "Iteration 563, loss = 0.07150359\n",
      "Iteration 564, loss = 0.07010741\n",
      "Iteration 565, loss = 0.06947501\n",
      "Iteration 566, loss = 0.06961389\n",
      "Iteration 567, loss = 0.06931242\n",
      "Iteration 568, loss = 0.06884470\n",
      "Iteration 569, loss = 0.06859929\n",
      "Iteration 570, loss = 0.06849330\n",
      "Iteration 571, loss = 0.06904599\n",
      "Iteration 572, loss = 0.06828959\n",
      "Iteration 573, loss = 0.06758330\n",
      "Iteration 574, loss = 0.06811572\n",
      "Iteration 575, loss = 0.06724238\n",
      "Iteration 576, loss = 0.06736591\n",
      "Iteration 577, loss = 0.06734731\n",
      "Iteration 578, loss = 0.06707734\n",
      "Iteration 579, loss = 0.06682795\n",
      "Iteration 580, loss = 0.06674987\n",
      "Iteration 581, loss = 0.06687823\n",
      "Iteration 582, loss = 0.06573930\n",
      "Iteration 583, loss = 0.06576866\n",
      "Iteration 584, loss = 0.06542523\n",
      "Iteration 585, loss = 0.06532095\n",
      "Iteration 586, loss = 0.06522850\n",
      "Iteration 587, loss = 0.06514123\n",
      "Iteration 588, loss = 0.06472704\n",
      "Iteration 589, loss = 0.06445442\n",
      "Iteration 590, loss = 0.06456741\n",
      "Iteration 591, loss = 0.06381575\n",
      "Iteration 592, loss = 0.06370865\n",
      "Iteration 593, loss = 0.06431751\n",
      "Iteration 594, loss = 0.06331314\n",
      "Iteration 595, loss = 0.06343320\n",
      "Iteration 596, loss = 0.06299606\n",
      "Iteration 597, loss = 0.06353295\n",
      "Iteration 598, loss = 0.06301054\n",
      "Iteration 599, loss = 0.06326849\n",
      "Iteration 600, loss = 0.06231645\n",
      "Iteration 601, loss = 0.06206779\n",
      "Iteration 602, loss = 0.06236913\n",
      "Iteration 603, loss = 0.06249232\n",
      "Iteration 604, loss = 0.06135192\n",
      "Iteration 605, loss = 0.06143866\n",
      "Iteration 606, loss = 0.06125353\n",
      "Iteration 607, loss = 0.06082468\n",
      "Iteration 608, loss = 0.06062597\n",
      "Iteration 609, loss = 0.06092176\n",
      "Iteration 610, loss = 0.06065806\n",
      "Iteration 611, loss = 0.06015172\n",
      "Iteration 612, loss = 0.06022883\n",
      "Iteration 613, loss = 0.06020098\n",
      "Iteration 614, loss = 0.06024588\n",
      "Iteration 615, loss = 0.05972607\n",
      "Iteration 616, loss = 0.05979019\n",
      "Iteration 617, loss = 0.05944387\n",
      "Iteration 618, loss = 0.05936898\n",
      "Iteration 619, loss = 0.05868840\n",
      "Iteration 620, loss = 0.05824635\n",
      "Iteration 621, loss = 0.05833382\n",
      "Iteration 622, loss = 0.05821925\n",
      "Iteration 623, loss = 0.05790211\n",
      "Iteration 624, loss = 0.05777793\n",
      "Iteration 625, loss = 0.05774895\n",
      "Iteration 626, loss = 0.05775071\n",
      "Iteration 627, loss = 0.05716320\n",
      "Iteration 628, loss = 0.05709562\n",
      "Iteration 629, loss = 0.05711474\n",
      "Iteration 630, loss = 0.05680522\n",
      "Iteration 631, loss = 0.05641922\n",
      "Iteration 632, loss = 0.05623092\n",
      "Iteration 633, loss = 0.05675918\n",
      "Iteration 634, loss = 0.05653749\n",
      "Iteration 635, loss = 0.05598717\n",
      "Iteration 636, loss = 0.05619436\n",
      "Iteration 637, loss = 0.05602185\n",
      "Iteration 638, loss = 0.05580778\n",
      "Iteration 639, loss = 0.05558440\n",
      "Iteration 640, loss = 0.05521423\n",
      "Iteration 641, loss = 0.05536136\n",
      "Iteration 642, loss = 0.05447668\n",
      "Iteration 643, loss = 0.05438646\n",
      "Iteration 644, loss = 0.05470307\n",
      "Iteration 645, loss = 0.05578735\n",
      "Iteration 646, loss = 0.05492991\n",
      "Iteration 647, loss = 0.05421102\n",
      "Iteration 648, loss = 0.05346289\n",
      "Iteration 649, loss = 0.05357483\n",
      "Iteration 650, loss = 0.05370573\n",
      "Iteration 651, loss = 0.05412710\n",
      "Iteration 652, loss = 0.05317492\n",
      "Iteration 653, loss = 0.05261421\n",
      "Iteration 654, loss = 0.05328032\n",
      "Iteration 655, loss = 0.05331414\n",
      "Iteration 656, loss = 0.05278720\n",
      "Iteration 657, loss = 0.05237961\n",
      "Iteration 658, loss = 0.05190600\n",
      "Iteration 659, loss = 0.05226369\n",
      "Iteration 660, loss = 0.05248028\n",
      "Iteration 661, loss = 0.05169680\n",
      "Iteration 662, loss = 0.05149415\n",
      "Iteration 663, loss = 0.05138549\n",
      "Iteration 664, loss = 0.05110752\n",
      "Iteration 665, loss = 0.05115568\n",
      "Iteration 666, loss = 0.05086197\n",
      "Iteration 667, loss = 0.05095974\n",
      "Iteration 668, loss = 0.05081001\n",
      "Iteration 669, loss = 0.05036378\n",
      "Iteration 670, loss = 0.04990797\n",
      "Iteration 671, loss = 0.04991842\n",
      "Iteration 672, loss = 0.05065039\n",
      "Iteration 673, loss = 0.05058843\n",
      "Iteration 674, loss = 0.04998709\n",
      "Iteration 675, loss = 0.04979701\n",
      "Iteration 676, loss = 0.04921844\n",
      "Iteration 677, loss = 0.04973431\n",
      "Iteration 678, loss = 0.05003944\n",
      "Iteration 679, loss = 0.04872474\n",
      "Iteration 680, loss = 0.04907191\n",
      "Iteration 681, loss = 0.04852107\n",
      "Iteration 682, loss = 0.04862195\n",
      "Iteration 683, loss = 0.04869868\n",
      "Iteration 684, loss = 0.04835425\n",
      "Iteration 685, loss = 0.04844723\n",
      "Iteration 686, loss = 0.04835694\n",
      "Iteration 687, loss = 0.04787924\n",
      "Iteration 688, loss = 0.04774884\n",
      "Iteration 689, loss = 0.04808993\n",
      "Iteration 690, loss = 0.04706514\n",
      "Iteration 691, loss = 0.04722438\n",
      "Iteration 692, loss = 0.04698904\n",
      "Iteration 693, loss = 0.04727745\n",
      "Iteration 694, loss = 0.04761065\n",
      "Iteration 695, loss = 0.04678607\n",
      "Iteration 696, loss = 0.04668339\n",
      "Iteration 697, loss = 0.04786965\n",
      "Iteration 698, loss = 0.04629400\n",
      "Iteration 699, loss = 0.04620941\n",
      "Iteration 700, loss = 0.04587342\n",
      "Iteration 701, loss = 0.04642757\n",
      "Iteration 702, loss = 0.04584398\n",
      "Iteration 703, loss = 0.04609163\n",
      "Iteration 704, loss = 0.04659803\n",
      "Iteration 705, loss = 0.04666710\n",
      "Iteration 706, loss = 0.04560174\n",
      "Iteration 707, loss = 0.04622722\n",
      "Iteration 708, loss = 0.04717368\n",
      "Iteration 709, loss = 0.04516421\n",
      "Iteration 710, loss = 0.04972784\n",
      "Iteration 711, loss = 0.05426390\n",
      "Iteration 712, loss = 0.04802635\n",
      "Iteration 713, loss = 0.04641660\n",
      "Iteration 714, loss = 0.04529589\n",
      "Iteration 715, loss = 0.04526017\n",
      "Iteration 716, loss = 0.04442328\n",
      "Iteration 717, loss = 0.04422503\n",
      "Iteration 718, loss = 0.04384737\n",
      "Iteration 719, loss = 0.04370533\n",
      "Iteration 720, loss = 0.04436531\n",
      "Iteration 721, loss = 0.04337689\n",
      "Iteration 722, loss = 0.04362322\n",
      "Iteration 723, loss = 0.04331263\n",
      "Iteration 724, loss = 0.04336350\n",
      "Iteration 725, loss = 0.04367119\n",
      "Iteration 726, loss = 0.04306358\n",
      "Iteration 727, loss = 0.04295545\n",
      "Iteration 728, loss = 0.04268053\n",
      "Iteration 729, loss = 0.04280579\n",
      "Iteration 730, loss = 0.04267908\n",
      "Iteration 731, loss = 0.04250474\n",
      "Iteration 732, loss = 0.04247744\n",
      "Iteration 733, loss = 0.04250812\n",
      "Iteration 734, loss = 0.04202355\n",
      "Iteration 735, loss = 0.04221261\n",
      "Iteration 736, loss = 0.04215583\n",
      "Iteration 737, loss = 0.04182962\n",
      "Iteration 738, loss = 0.04203157\n",
      "Iteration 739, loss = 0.04175792\n",
      "Iteration 740, loss = 0.04214201\n",
      "Iteration 741, loss = 0.04163244\n",
      "Iteration 742, loss = 0.04163783\n",
      "Iteration 743, loss = 0.04122361\n",
      "Iteration 744, loss = 0.04092057\n",
      "Iteration 745, loss = 0.04071648\n",
      "Iteration 746, loss = 0.04166973\n",
      "Iteration 747, loss = 0.04162810\n",
      "Iteration 748, loss = 0.04181191\n",
      "Iteration 749, loss = 0.04089951\n",
      "Iteration 750, loss = 0.04039046\n",
      "Iteration 751, loss = 0.04030772\n",
      "Iteration 752, loss = 0.04018515\n",
      "Iteration 753, loss = 0.04016859\n",
      "Iteration 754, loss = 0.04017652\n",
      "Iteration 755, loss = 0.04051446\n",
      "Iteration 756, loss = 0.03992158\n",
      "Iteration 757, loss = 0.03980195\n",
      "Iteration 758, loss = 0.04046333\n",
      "Iteration 759, loss = 0.03960696\n",
      "Iteration 760, loss = 0.03925689\n",
      "Iteration 761, loss = 0.03914427\n",
      "Iteration 762, loss = 0.03906044\n",
      "Iteration 763, loss = 0.03915421\n",
      "Iteration 764, loss = 0.03883225\n",
      "Iteration 765, loss = 0.03919907\n",
      "Iteration 766, loss = 0.03905276\n",
      "Iteration 767, loss = 0.03859679\n",
      "Iteration 768, loss = 0.03847081\n",
      "Iteration 769, loss = 0.03845694\n",
      "Iteration 770, loss = 0.03832306\n",
      "Iteration 771, loss = 0.03795377\n",
      "Iteration 772, loss = 0.03805930\n",
      "Iteration 773, loss = 0.03773299\n",
      "Iteration 774, loss = 0.03814329\n",
      "Iteration 775, loss = 0.03796329\n",
      "Iteration 776, loss = 0.03772517\n",
      "Iteration 777, loss = 0.03785688\n",
      "Iteration 778, loss = 0.03789601\n",
      "Iteration 779, loss = 0.03747719\n",
      "Iteration 780, loss = 0.03733408\n",
      "Iteration 781, loss = 0.03705368\n",
      "Iteration 782, loss = 0.03730508\n",
      "Iteration 783, loss = 0.03706811\n",
      "Iteration 784, loss = 0.03712738\n",
      "Iteration 785, loss = 0.03672888\n",
      "Iteration 786, loss = 0.03672719\n",
      "Iteration 787, loss = 0.03704627\n",
      "Iteration 788, loss = 0.03712422\n",
      "Iteration 789, loss = 0.03689794\n",
      "Iteration 790, loss = 0.03683963\n",
      "Iteration 791, loss = 0.03673153\n",
      "Iteration 792, loss = 0.03684542\n",
      "Iteration 793, loss = 0.03630913\n",
      "Iteration 794, loss = 0.03602830\n",
      "Iteration 795, loss = 0.03597197\n",
      "Iteration 796, loss = 0.03586030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 797, loss = 0.03595927\n",
      "Iteration 798, loss = 0.03659129\n",
      "Iteration 799, loss = 0.03569034\n",
      "Iteration 800, loss = 0.03559758\n",
      "Iteration 801, loss = 0.03521669\n",
      "Iteration 802, loss = 0.03551169\n",
      "Iteration 803, loss = 0.03589282\n",
      "Iteration 804, loss = 0.03539093\n",
      "Iteration 805, loss = 0.03479006\n",
      "Iteration 806, loss = 0.03508884\n",
      "Iteration 807, loss = 0.03456045\n",
      "Iteration 808, loss = 0.03504017\n",
      "Iteration 809, loss = 0.03470549\n",
      "Iteration 810, loss = 0.03482264\n",
      "Iteration 811, loss = 0.03499737\n",
      "Iteration 812, loss = 0.03569961\n",
      "Iteration 813, loss = 0.03473513\n",
      "Iteration 814, loss = 0.03437408\n",
      "Iteration 815, loss = 0.03448785\n",
      "Iteration 816, loss = 0.03458807\n",
      "Iteration 817, loss = 0.03445685\n",
      "Iteration 818, loss = 0.03420231\n",
      "Iteration 819, loss = 0.03381265\n",
      "Iteration 820, loss = 0.03378107\n",
      "Iteration 821, loss = 0.03387666\n",
      "Iteration 822, loss = 0.03406551\n",
      "Iteration 823, loss = 0.03439922\n",
      "Iteration 824, loss = 0.03404026\n",
      "Iteration 825, loss = 0.03345640\n",
      "Iteration 826, loss = 0.03358241\n",
      "Iteration 827, loss = 0.03326376\n",
      "Iteration 828, loss = 0.03300746\n",
      "Iteration 829, loss = 0.03370288\n",
      "Iteration 830, loss = 0.03403255\n",
      "Iteration 831, loss = 0.03385208\n",
      "Iteration 832, loss = 0.03272698\n",
      "Iteration 833, loss = 0.03274465\n",
      "Iteration 834, loss = 0.03267218\n",
      "Iteration 835, loss = 0.03257276\n",
      "Iteration 836, loss = 0.03303086\n",
      "Iteration 837, loss = 0.03319741\n",
      "Iteration 838, loss = 0.03344364\n",
      "Iteration 839, loss = 0.03269320\n",
      "Iteration 840, loss = 0.03264286\n",
      "Iteration 841, loss = 0.03268075\n",
      "Iteration 842, loss = 0.03201857\n",
      "Iteration 843, loss = 0.03181889\n",
      "Iteration 844, loss = 0.03184269\n",
      "Iteration 845, loss = 0.03180990\n",
      "Iteration 846, loss = 0.03196829\n",
      "Iteration 847, loss = 0.03251547\n",
      "Iteration 848, loss = 0.03224665\n",
      "Iteration 849, loss = 0.03188293\n",
      "Iteration 850, loss = 0.03161953\n",
      "Iteration 851, loss = 0.03159589\n",
      "Iteration 852, loss = 0.03168338\n",
      "Iteration 853, loss = 0.03143456\n",
      "Iteration 854, loss = 0.03113069\n",
      "Iteration 855, loss = 0.03122379\n",
      "Iteration 856, loss = 0.03138627\n",
      "Iteration 857, loss = 0.03122730\n",
      "Iteration 858, loss = 0.03065352\n",
      "Iteration 859, loss = 0.03080689\n",
      "Iteration 860, loss = 0.03055029\n",
      "Iteration 861, loss = 0.03133745\n",
      "Iteration 862, loss = 0.03088387\n",
      "Iteration 863, loss = 0.03043360\n",
      "Iteration 864, loss = 0.03030691\n",
      "Iteration 865, loss = 0.03042802\n",
      "Iteration 866, loss = 0.03006189\n",
      "Iteration 867, loss = 0.03039796\n",
      "Iteration 868, loss = 0.03057145\n",
      "Iteration 869, loss = 0.03044227\n",
      "Iteration 870, loss = 0.03000269\n",
      "Iteration 871, loss = 0.02999784\n",
      "Iteration 872, loss = 0.02973658\n",
      "Iteration 873, loss = 0.03011509\n",
      "Iteration 874, loss = 0.03009945\n",
      "Iteration 875, loss = 0.02966332\n",
      "Iteration 876, loss = 0.02947032\n",
      "Iteration 877, loss = 0.02939285\n",
      "Iteration 878, loss = 0.02967431\n",
      "Iteration 879, loss = 0.02922009\n",
      "Iteration 880, loss = 0.02908827\n",
      "Iteration 881, loss = 0.02936111\n",
      "Iteration 882, loss = 0.02924501\n",
      "Iteration 883, loss = 0.02906121\n",
      "Iteration 884, loss = 0.02897245\n",
      "Iteration 885, loss = 0.02889456\n",
      "Iteration 886, loss = 0.02929025\n",
      "Iteration 887, loss = 0.02971280\n",
      "Iteration 888, loss = 0.02873787\n",
      "Iteration 889, loss = 0.02895767\n",
      "Iteration 890, loss = 0.02871213\n",
      "Iteration 891, loss = 0.02884349\n",
      "Iteration 892, loss = 0.02867670\n",
      "Iteration 893, loss = 0.02852840\n",
      "Iteration 894, loss = 0.02854546\n",
      "Iteration 895, loss = 0.02837967\n",
      "Iteration 896, loss = 0.02846981\n",
      "Iteration 897, loss = 0.02827321\n",
      "Iteration 898, loss = 0.02823827\n",
      "Iteration 899, loss = 0.02831805\n",
      "Iteration 900, loss = 0.02876050\n",
      "Iteration 901, loss = 0.02792798\n",
      "Iteration 902, loss = 0.02811778\n",
      "Iteration 903, loss = 0.02844529\n",
      "Iteration 904, loss = 0.02821709\n",
      "Iteration 905, loss = 0.02792266\n",
      "Iteration 906, loss = 0.02763221\n",
      "Iteration 907, loss = 0.02745718\n",
      "Iteration 908, loss = 0.02892388\n",
      "Iteration 909, loss = 0.02788615\n",
      "Iteration 910, loss = 0.02758556\n",
      "Iteration 911, loss = 0.02742097\n",
      "Iteration 912, loss = 0.02762238\n",
      "Iteration 913, loss = 0.02766320\n",
      "Iteration 914, loss = 0.02734782\n",
      "Iteration 915, loss = 0.02721672\n",
      "Iteration 916, loss = 0.02675539\n",
      "Iteration 917, loss = 0.02712264\n",
      "Iteration 918, loss = 0.02718038\n",
      "Iteration 919, loss = 0.02684553\n",
      "Iteration 920, loss = 0.02706169\n",
      "Iteration 921, loss = 0.02738674\n",
      "Iteration 922, loss = 0.02719912\n",
      "Iteration 923, loss = 0.02711401\n",
      "Iteration 924, loss = 0.02724239\n",
      "Iteration 925, loss = 0.02743972\n",
      "Iteration 926, loss = 0.02757400\n",
      "Iteration 927, loss = 0.02813508\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 9 of 48...\n",
      "Iteration 1, loss = 0.68499572\n",
      "Iteration 2, loss = 0.66030703\n",
      "Iteration 3, loss = 0.64100112\n",
      "Iteration 4, loss = 0.62285805\n",
      "Iteration 5, loss = 0.60728999\n",
      "Iteration 6, loss = 0.59091625\n",
      "Iteration 7, loss = 0.57670389\n",
      "Iteration 8, loss = 0.56258563\n",
      "Iteration 9, loss = 0.54969111\n",
      "Iteration 10, loss = 0.53716452\n",
      "Iteration 11, loss = 0.52690453\n",
      "Iteration 12, loss = 0.51529883\n",
      "Iteration 13, loss = 0.50593345\n",
      "Iteration 14, loss = 0.49628734\n",
      "Iteration 15, loss = 0.48734695\n",
      "Iteration 16, loss = 0.47983348\n",
      "Iteration 17, loss = 0.47172945\n",
      "Iteration 18, loss = 0.46489144\n",
      "Iteration 19, loss = 0.45904160\n",
      "Iteration 20, loss = 0.45330489\n",
      "Iteration 21, loss = 0.44865131\n",
      "Iteration 22, loss = 0.44141543\n",
      "Iteration 23, loss = 0.43648222\n",
      "Iteration 24, loss = 0.43153438\n",
      "Iteration 25, loss = 0.42918159\n",
      "Iteration 26, loss = 0.42323182\n",
      "Iteration 27, loss = 0.41932574\n",
      "Iteration 28, loss = 0.41606069\n",
      "Iteration 29, loss = 0.41208406\n",
      "Iteration 30, loss = 0.40969469\n",
      "Iteration 31, loss = 0.40613440\n",
      "Iteration 32, loss = 0.40298180\n",
      "Iteration 33, loss = 0.40061414\n",
      "Iteration 34, loss = 0.39788362\n",
      "Iteration 35, loss = 0.39485273\n",
      "Iteration 36, loss = 0.39381368\n",
      "Iteration 37, loss = 0.38993728\n",
      "Iteration 38, loss = 0.39111898\n",
      "Iteration 39, loss = 0.38685670\n",
      "Iteration 40, loss = 0.38413606\n",
      "Iteration 41, loss = 0.38174784\n",
      "Iteration 42, loss = 0.37886263\n",
      "Iteration 43, loss = 0.37852970\n",
      "Iteration 44, loss = 0.37556862\n",
      "Iteration 45, loss = 0.37427017\n",
      "Iteration 46, loss = 0.37411238\n",
      "Iteration 47, loss = 0.37105137\n",
      "Iteration 48, loss = 0.36928554\n",
      "Iteration 49, loss = 0.36816236\n",
      "Iteration 50, loss = 0.36590253\n",
      "Iteration 51, loss = 0.36385544\n",
      "Iteration 52, loss = 0.36291894\n",
      "Iteration 53, loss = 0.36326688\n",
      "Iteration 54, loss = 0.36044380\n",
      "Iteration 55, loss = 0.35895598\n",
      "Iteration 56, loss = 0.35712392\n",
      "Iteration 57, loss = 0.35686877\n",
      "Iteration 58, loss = 0.35544372\n",
      "Iteration 59, loss = 0.35492575\n",
      "Iteration 60, loss = 0.35368651\n",
      "Iteration 61, loss = 0.35114314\n",
      "Iteration 62, loss = 0.35028427\n",
      "Iteration 63, loss = 0.34866867\n",
      "Iteration 64, loss = 0.34721216\n",
      "Iteration 65, loss = 0.34667490\n",
      "Iteration 66, loss = 0.34524130\n",
      "Iteration 67, loss = 0.34371006\n",
      "Iteration 68, loss = 0.34475102\n",
      "Iteration 69, loss = 0.34255369\n",
      "Iteration 70, loss = 0.34456300\n",
      "Iteration 71, loss = 0.34179727\n",
      "Iteration 72, loss = 0.34034780\n",
      "Iteration 73, loss = 0.33804254\n",
      "Iteration 74, loss = 0.33880699\n",
      "Iteration 75, loss = 0.33764795\n",
      "Iteration 76, loss = 0.33944120\n",
      "Iteration 77, loss = 0.33472708\n",
      "Iteration 78, loss = 0.33612705\n",
      "Iteration 79, loss = 0.33331879\n",
      "Iteration 80, loss = 0.33253441\n",
      "Iteration 81, loss = 0.33212346\n",
      "Iteration 82, loss = 0.33082634\n",
      "Iteration 83, loss = 0.33106352\n",
      "Iteration 84, loss = 0.32854777\n",
      "Iteration 85, loss = 0.32757269\n",
      "Iteration 86, loss = 0.32705955\n",
      "Iteration 87, loss = 0.32771391\n",
      "Iteration 88, loss = 0.32687454\n",
      "Iteration 89, loss = 0.32483660\n",
      "Iteration 90, loss = 0.32349001\n",
      "Iteration 91, loss = 0.32361883\n",
      "Iteration 92, loss = 0.32288771\n",
      "Iteration 93, loss = 0.32151319\n",
      "Iteration 94, loss = 0.32255979\n",
      "Iteration 95, loss = 0.32260648\n",
      "Iteration 96, loss = 0.31984391\n",
      "Iteration 97, loss = 0.31905689\n",
      "Iteration 98, loss = 0.31853224\n",
      "Iteration 99, loss = 0.31755890\n",
      "Iteration 100, loss = 0.31635747\n",
      "Iteration 101, loss = 0.31576383\n",
      "Iteration 102, loss = 0.31545703\n",
      "Iteration 103, loss = 0.31468036\n",
      "Iteration 104, loss = 0.31548267\n",
      "Iteration 105, loss = 0.31370823\n",
      "Iteration 106, loss = 0.31265717\n",
      "Iteration 107, loss = 0.31249065\n",
      "Iteration 108, loss = 0.31253060\n",
      "Iteration 109, loss = 0.31027594\n",
      "Iteration 110, loss = 0.31148806\n",
      "Iteration 111, loss = 0.30974645\n",
      "Iteration 112, loss = 0.30906195\n",
      "Iteration 113, loss = 0.30834107\n",
      "Iteration 114, loss = 0.30912573\n",
      "Iteration 115, loss = 0.30673268\n",
      "Iteration 116, loss = 0.30632818\n",
      "Iteration 117, loss = 0.30930702\n",
      "Iteration 118, loss = 0.30982497\n",
      "Iteration 119, loss = 0.30489857\n",
      "Iteration 120, loss = 0.30365087\n",
      "Iteration 121, loss = 0.30280120\n",
      "Iteration 122, loss = 0.30329467\n",
      "Iteration 123, loss = 0.30189255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 124, loss = 0.30174620\n",
      "Iteration 125, loss = 0.30323466\n",
      "Iteration 126, loss = 0.29990772\n",
      "Iteration 127, loss = 0.30175445\n",
      "Iteration 128, loss = 0.29949967\n",
      "Iteration 129, loss = 0.29816609\n",
      "Iteration 130, loss = 0.29926002\n",
      "Iteration 131, loss = 0.29690000\n",
      "Iteration 132, loss = 0.29872971\n",
      "Iteration 133, loss = 0.29763509\n",
      "Iteration 134, loss = 0.29603750\n",
      "Iteration 135, loss = 0.29899123\n",
      "Iteration 136, loss = 0.29411774\n",
      "Iteration 137, loss = 0.29509587\n",
      "Iteration 138, loss = 0.29310766\n",
      "Iteration 139, loss = 0.29264739\n",
      "Iteration 140, loss = 0.29373364\n",
      "Iteration 141, loss = 0.29435525\n",
      "Iteration 142, loss = 0.29380129\n",
      "Iteration 143, loss = 0.29480230\n",
      "Iteration 144, loss = 0.29316905\n",
      "Iteration 145, loss = 0.28995797\n",
      "Iteration 146, loss = 0.29070652\n",
      "Iteration 147, loss = 0.29080661\n",
      "Iteration 148, loss = 0.28977229\n",
      "Iteration 149, loss = 0.28861920\n",
      "Iteration 150, loss = 0.29026075\n",
      "Iteration 151, loss = 0.28697607\n",
      "Iteration 152, loss = 0.28708275\n",
      "Iteration 153, loss = 0.28773772\n",
      "Iteration 154, loss = 0.28567517\n",
      "Iteration 155, loss = 0.28551686\n",
      "Iteration 156, loss = 0.28502208\n",
      "Iteration 157, loss = 0.28396716\n",
      "Iteration 158, loss = 0.28384286\n",
      "Iteration 159, loss = 0.28310281\n",
      "Iteration 160, loss = 0.28409661\n",
      "Iteration 161, loss = 0.28314299\n",
      "Iteration 162, loss = 0.28400011\n",
      "Iteration 163, loss = 0.28162518\n",
      "Iteration 164, loss = 0.28148610\n",
      "Iteration 165, loss = 0.27918937\n",
      "Iteration 166, loss = 0.27902142\n",
      "Iteration 167, loss = 0.27827213\n",
      "Iteration 168, loss = 0.28165282\n",
      "Iteration 169, loss = 0.27802012\n",
      "Iteration 170, loss = 0.27758742\n",
      "Iteration 171, loss = 0.27758674\n",
      "Iteration 172, loss = 0.27942788\n",
      "Iteration 173, loss = 0.27706859\n",
      "Iteration 174, loss = 0.27629174\n",
      "Iteration 175, loss = 0.27452855\n",
      "Iteration 176, loss = 0.27524254\n",
      "Iteration 177, loss = 0.27321423\n",
      "Iteration 178, loss = 0.27286492\n",
      "Iteration 179, loss = 0.27263985\n",
      "Iteration 180, loss = 0.27336316\n",
      "Iteration 181, loss = 0.27241327\n",
      "Iteration 182, loss = 0.27136741\n",
      "Iteration 183, loss = 0.27248495\n",
      "Iteration 184, loss = 0.27133242\n",
      "Iteration 185, loss = 0.26963934\n",
      "Iteration 186, loss = 0.26970063\n",
      "Iteration 187, loss = 0.26931784\n",
      "Iteration 188, loss = 0.26993062\n",
      "Iteration 189, loss = 0.26967143\n",
      "Iteration 190, loss = 0.27203594\n",
      "Iteration 191, loss = 0.26985718\n",
      "Iteration 192, loss = 0.26637933\n",
      "Iteration 193, loss = 0.26788593\n",
      "Iteration 194, loss = 0.26642491\n",
      "Iteration 195, loss = 0.26577709\n",
      "Iteration 196, loss = 0.26506035\n",
      "Iteration 197, loss = 0.26478931\n",
      "Iteration 198, loss = 0.26651522\n",
      "Iteration 199, loss = 0.26571677\n",
      "Iteration 200, loss = 0.26421331\n",
      "Iteration 201, loss = 0.26315811\n",
      "Iteration 202, loss = 0.26253621\n",
      "Iteration 203, loss = 0.26247173\n",
      "Iteration 204, loss = 0.26132704\n",
      "Iteration 205, loss = 0.26021548\n",
      "Iteration 206, loss = 0.26172700\n",
      "Iteration 207, loss = 0.26098999\n",
      "Iteration 208, loss = 0.26024133\n",
      "Iteration 209, loss = 0.25887213\n",
      "Iteration 210, loss = 0.26330650\n",
      "Iteration 211, loss = 0.26247955\n",
      "Iteration 212, loss = 0.26030765\n",
      "Iteration 213, loss = 0.25822329\n",
      "Iteration 214, loss = 0.25768382\n",
      "Iteration 215, loss = 0.25630102\n",
      "Iteration 216, loss = 0.25678353\n",
      "Iteration 217, loss = 0.25822429\n",
      "Iteration 218, loss = 0.25680742\n",
      "Iteration 219, loss = 0.25512104\n",
      "Iteration 220, loss = 0.25513670\n",
      "Iteration 221, loss = 0.25341388\n",
      "Iteration 222, loss = 0.25248016\n",
      "Iteration 223, loss = 0.25584316\n",
      "Iteration 224, loss = 0.25505800\n",
      "Iteration 225, loss = 0.25182141\n",
      "Iteration 226, loss = 0.25092762\n",
      "Iteration 227, loss = 0.25313088\n",
      "Iteration 228, loss = 0.24974147\n",
      "Iteration 229, loss = 0.24906942\n",
      "Iteration 230, loss = 0.25118200\n",
      "Iteration 231, loss = 0.25015372\n",
      "Iteration 232, loss = 0.25174787\n",
      "Iteration 233, loss = 0.24831095\n",
      "Iteration 234, loss = 0.24722315\n",
      "Iteration 235, loss = 0.24670906\n",
      "Iteration 236, loss = 0.24583148\n",
      "Iteration 237, loss = 0.24636537\n",
      "Iteration 238, loss = 0.24772823\n",
      "Iteration 239, loss = 0.24488040\n",
      "Iteration 240, loss = 0.24410975\n",
      "Iteration 241, loss = 0.24499337\n",
      "Iteration 242, loss = 0.24342744\n",
      "Iteration 243, loss = 0.24272906\n",
      "Iteration 244, loss = 0.24302717\n",
      "Iteration 245, loss = 0.24226704\n",
      "Iteration 246, loss = 0.24161549\n",
      "Iteration 247, loss = 0.24116064\n",
      "Iteration 248, loss = 0.24059149\n",
      "Iteration 249, loss = 0.24117940\n",
      "Iteration 250, loss = 0.23948345\n",
      "Iteration 251, loss = 0.23863027\n",
      "Iteration 252, loss = 0.23810152\n",
      "Iteration 253, loss = 0.23824438\n",
      "Iteration 254, loss = 0.24143939\n",
      "Iteration 255, loss = 0.23894915\n",
      "Iteration 256, loss = 0.23618732\n",
      "Iteration 257, loss = 0.23755054\n",
      "Iteration 258, loss = 0.24012037\n",
      "Iteration 259, loss = 0.23661792\n",
      "Iteration 260, loss = 0.23479817\n",
      "Iteration 261, loss = 0.23395837\n",
      "Iteration 262, loss = 0.23547860\n",
      "Iteration 263, loss = 0.23351787\n",
      "Iteration 264, loss = 0.23345850\n",
      "Iteration 265, loss = 0.23289813\n",
      "Iteration 266, loss = 0.23183079\n",
      "Iteration 267, loss = 0.23081255\n",
      "Iteration 268, loss = 0.23085848\n",
      "Iteration 269, loss = 0.23039618\n",
      "Iteration 270, loss = 0.23037988\n",
      "Iteration 271, loss = 0.22968621\n",
      "Iteration 272, loss = 0.22999980\n",
      "Iteration 273, loss = 0.22969445\n",
      "Iteration 274, loss = 0.22842392\n",
      "Iteration 275, loss = 0.22819154\n",
      "Iteration 276, loss = 0.22802997\n",
      "Iteration 277, loss = 0.22548565\n",
      "Iteration 278, loss = 0.22877626\n",
      "Iteration 279, loss = 0.22691214\n",
      "Iteration 280, loss = 0.22642317\n",
      "Iteration 281, loss = 0.22671430\n",
      "Iteration 282, loss = 0.22474544\n",
      "Iteration 283, loss = 0.22547457\n",
      "Iteration 284, loss = 0.22282189\n",
      "Iteration 285, loss = 0.22251477\n",
      "Iteration 286, loss = 0.22240679\n",
      "Iteration 287, loss = 0.22279298\n",
      "Iteration 288, loss = 0.22187631\n",
      "Iteration 289, loss = 0.22037449\n",
      "Iteration 290, loss = 0.22029153\n",
      "Iteration 291, loss = 0.22156198\n",
      "Iteration 292, loss = 0.21929408\n",
      "Iteration 293, loss = 0.22013191\n",
      "Iteration 294, loss = 0.22093565\n",
      "Iteration 295, loss = 0.22251271\n",
      "Iteration 296, loss = 0.22157202\n",
      "Iteration 297, loss = 0.21748123\n",
      "Iteration 298, loss = 0.21845262\n",
      "Iteration 299, loss = 0.21649060\n",
      "Iteration 300, loss = 0.21484172\n",
      "Iteration 301, loss = 0.21499606\n",
      "Iteration 302, loss = 0.21445680\n",
      "Iteration 303, loss = 0.21386142\n",
      "Iteration 304, loss = 0.21473587\n",
      "Iteration 305, loss = 0.21465329\n",
      "Iteration 306, loss = 0.21526377\n",
      "Iteration 307, loss = 0.21283160\n",
      "Iteration 308, loss = 0.21160310\n",
      "Iteration 309, loss = 0.21170789\n",
      "Iteration 310, loss = 0.21075483\n",
      "Iteration 311, loss = 0.21025043\n",
      "Iteration 312, loss = 0.21084942\n",
      "Iteration 313, loss = 0.20893753\n",
      "Iteration 314, loss = 0.20915664\n",
      "Iteration 315, loss = 0.20883251\n",
      "Iteration 316, loss = 0.20955863\n",
      "Iteration 317, loss = 0.20723117\n",
      "Iteration 318, loss = 0.21031961\n",
      "Iteration 319, loss = 0.20793600\n",
      "Iteration 320, loss = 0.20583458\n",
      "Iteration 321, loss = 0.20736225\n",
      "Iteration 322, loss = 0.20803349\n",
      "Iteration 323, loss = 0.20608924\n",
      "Iteration 324, loss = 0.20460421\n",
      "Iteration 325, loss = 0.20406910\n",
      "Iteration 326, loss = 0.20355485\n",
      "Iteration 327, loss = 0.20234865\n",
      "Iteration 328, loss = 0.20225704\n",
      "Iteration 329, loss = 0.20761661\n",
      "Iteration 330, loss = 0.20228434\n",
      "Iteration 331, loss = 0.20082640\n",
      "Iteration 332, loss = 0.20124692\n",
      "Iteration 333, loss = 0.20479126\n",
      "Iteration 334, loss = 0.19985380\n",
      "Iteration 335, loss = 0.19955251\n",
      "Iteration 336, loss = 0.19961044\n",
      "Iteration 337, loss = 0.19966691\n",
      "Iteration 338, loss = 0.19860859\n",
      "Iteration 339, loss = 0.19825098\n",
      "Iteration 340, loss = 0.19758923\n",
      "Iteration 341, loss = 0.19583500\n",
      "Iteration 342, loss = 0.19568238\n",
      "Iteration 343, loss = 0.19530046\n",
      "Iteration 344, loss = 0.19608286\n",
      "Iteration 345, loss = 0.19560927\n",
      "Iteration 346, loss = 0.19580171\n",
      "Iteration 347, loss = 0.19445834\n",
      "Iteration 348, loss = 0.19272534\n",
      "Iteration 349, loss = 0.19219010\n",
      "Iteration 350, loss = 0.19222226\n",
      "Iteration 351, loss = 0.19206925\n",
      "Iteration 352, loss = 0.19082694\n",
      "Iteration 353, loss = 0.19056752\n",
      "Iteration 354, loss = 0.19023732\n",
      "Iteration 355, loss = 0.18957521\n",
      "Iteration 356, loss = 0.18928576\n",
      "Iteration 357, loss = 0.18922834\n",
      "Iteration 358, loss = 0.18922520\n",
      "Iteration 359, loss = 0.18899343\n",
      "Iteration 360, loss = 0.18879105\n",
      "Iteration 361, loss = 0.18786187\n",
      "Iteration 362, loss = 0.18692982\n",
      "Iteration 363, loss = 0.18604803\n",
      "Iteration 364, loss = 0.18549730\n",
      "Iteration 365, loss = 0.18580042\n",
      "Iteration 366, loss = 0.18447631\n",
      "Iteration 367, loss = 0.18477155\n",
      "Iteration 368, loss = 0.18642663\n",
      "Iteration 369, loss = 0.18437069\n",
      "Iteration 370, loss = 0.18422688\n",
      "Iteration 371, loss = 0.18308364\n",
      "Iteration 372, loss = 0.18436974\n",
      "Iteration 373, loss = 0.18249274\n",
      "Iteration 374, loss = 0.18156704\n",
      "Iteration 375, loss = 0.18042209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 376, loss = 0.18076433\n",
      "Iteration 377, loss = 0.18256081\n",
      "Iteration 378, loss = 0.18064153\n",
      "Iteration 379, loss = 0.17998920\n",
      "Iteration 380, loss = 0.17953312\n",
      "Iteration 381, loss = 0.17920271\n",
      "Iteration 382, loss = 0.18006409\n",
      "Iteration 383, loss = 0.17712133\n",
      "Iteration 384, loss = 0.17910279\n",
      "Iteration 385, loss = 0.17737301\n",
      "Iteration 386, loss = 0.17633486\n",
      "Iteration 387, loss = 0.17609053\n",
      "Iteration 388, loss = 0.17583724\n",
      "Iteration 389, loss = 0.17666181\n",
      "Iteration 390, loss = 0.17607236\n",
      "Iteration 391, loss = 0.17500989\n",
      "Iteration 392, loss = 0.17348755\n",
      "Iteration 393, loss = 0.17354369\n",
      "Iteration 394, loss = 0.17219186\n",
      "Iteration 395, loss = 0.17297887\n",
      "Iteration 396, loss = 0.17149303\n",
      "Iteration 397, loss = 0.17140104\n",
      "Iteration 398, loss = 0.17110672\n",
      "Iteration 399, loss = 0.17072030\n",
      "Iteration 400, loss = 0.17093698\n",
      "Iteration 401, loss = 0.17019480\n",
      "Iteration 402, loss = 0.16843255\n",
      "Iteration 403, loss = 0.17178114\n",
      "Iteration 404, loss = 0.17001188\n",
      "Iteration 405, loss = 0.16853010\n",
      "Iteration 406, loss = 0.16906205\n",
      "Iteration 407, loss = 0.16901419\n",
      "Iteration 408, loss = 0.16887224\n",
      "Iteration 409, loss = 0.16927542\n",
      "Iteration 410, loss = 0.16829729\n",
      "Iteration 411, loss = 0.16607674\n",
      "Iteration 412, loss = 0.16567515\n",
      "Iteration 413, loss = 0.16503480\n",
      "Iteration 414, loss = 0.16466695\n",
      "Iteration 415, loss = 0.16644722\n",
      "Iteration 416, loss = 0.16525342\n",
      "Iteration 417, loss = 0.16417435\n",
      "Iteration 418, loss = 0.16313648\n",
      "Iteration 419, loss = 0.16591630\n",
      "Iteration 420, loss = 0.16382014\n",
      "Iteration 421, loss = 0.16185435\n",
      "Iteration 422, loss = 0.16392458\n",
      "Iteration 423, loss = 0.16385417\n",
      "Iteration 424, loss = 0.16147346\n",
      "Iteration 425, loss = 0.16026069\n",
      "Iteration 426, loss = 0.16047614\n",
      "Iteration 427, loss = 0.16305363\n",
      "Iteration 428, loss = 0.16142764\n",
      "Iteration 429, loss = 0.16124586\n",
      "Iteration 430, loss = 0.16065235\n",
      "Iteration 431, loss = 0.16216791\n",
      "Iteration 432, loss = 0.15867458\n",
      "Iteration 433, loss = 0.15863831\n",
      "Iteration 434, loss = 0.15793946\n",
      "Iteration 435, loss = 0.15646204\n",
      "Iteration 436, loss = 0.15753876\n",
      "Iteration 437, loss = 0.15629001\n",
      "Iteration 438, loss = 0.15620468\n",
      "Iteration 439, loss = 0.15562001\n",
      "Iteration 440, loss = 0.15707885\n",
      "Iteration 441, loss = 0.15612097\n",
      "Iteration 442, loss = 0.15380003\n",
      "Iteration 443, loss = 0.15527877\n",
      "Iteration 444, loss = 0.15443670\n",
      "Iteration 445, loss = 0.15328516\n",
      "Iteration 446, loss = 0.15282557\n",
      "Iteration 447, loss = 0.15253195\n",
      "Iteration 448, loss = 0.15204738\n",
      "Iteration 449, loss = 0.15173791\n",
      "Iteration 450, loss = 0.15128260\n",
      "Iteration 451, loss = 0.15092238\n",
      "Iteration 452, loss = 0.15144994\n",
      "Iteration 453, loss = 0.15114398\n",
      "Iteration 454, loss = 0.15038029\n",
      "Iteration 455, loss = 0.14928392\n",
      "Iteration 456, loss = 0.14972257\n",
      "Iteration 457, loss = 0.15095345\n",
      "Iteration 458, loss = 0.15091463\n",
      "Iteration 459, loss = 0.14875502\n",
      "Iteration 460, loss = 0.14853090\n",
      "Iteration 461, loss = 0.14940704\n",
      "Iteration 462, loss = 0.14908759\n",
      "Iteration 463, loss = 0.15274007\n",
      "Iteration 464, loss = 0.15264283\n",
      "Iteration 465, loss = 0.14918403\n",
      "Iteration 466, loss = 0.14621951\n",
      "Iteration 467, loss = 0.14644069\n",
      "Iteration 468, loss = 0.14514141\n",
      "Iteration 469, loss = 0.14633376\n",
      "Iteration 470, loss = 0.14455959\n",
      "Iteration 471, loss = 0.14418907\n",
      "Iteration 472, loss = 0.14401670\n",
      "Iteration 473, loss = 0.14352814\n",
      "Iteration 474, loss = 0.14322537\n",
      "Iteration 475, loss = 0.14443882\n",
      "Iteration 476, loss = 0.14263556\n",
      "Iteration 477, loss = 0.14296930\n",
      "Iteration 478, loss = 0.14201488\n",
      "Iteration 479, loss = 0.14177404\n",
      "Iteration 480, loss = 0.14150102\n",
      "Iteration 481, loss = 0.14176923\n",
      "Iteration 482, loss = 0.14205842\n",
      "Iteration 483, loss = 0.14059382\n",
      "Iteration 484, loss = 0.14042401\n",
      "Iteration 485, loss = 0.13940337\n",
      "Iteration 486, loss = 0.13964470\n",
      "Iteration 487, loss = 0.13917912\n",
      "Iteration 488, loss = 0.13902451\n",
      "Iteration 489, loss = 0.13904941\n",
      "Iteration 490, loss = 0.13894433\n",
      "Iteration 491, loss = 0.13813341\n",
      "Iteration 492, loss = 0.13823222\n",
      "Iteration 493, loss = 0.13728377\n",
      "Iteration 494, loss = 0.13676032\n",
      "Iteration 495, loss = 0.13606834\n",
      "Iteration 496, loss = 0.13617167\n",
      "Iteration 497, loss = 0.13697161\n",
      "Iteration 498, loss = 0.13573929\n",
      "Iteration 499, loss = 0.13611864\n",
      "Iteration 500, loss = 0.13475336\n",
      "Iteration 501, loss = 0.13463684\n",
      "Iteration 502, loss = 0.13444769\n",
      "Iteration 503, loss = 0.13458997\n",
      "Iteration 504, loss = 0.13379714\n",
      "Iteration 505, loss = 0.13452489\n",
      "Iteration 506, loss = 0.13334774\n",
      "Iteration 507, loss = 0.13304319\n",
      "Iteration 508, loss = 0.13324986\n",
      "Iteration 509, loss = 0.13247683\n",
      "Iteration 510, loss = 0.13157420\n",
      "Iteration 511, loss = 0.13185671\n",
      "Iteration 512, loss = 0.13101432\n",
      "Iteration 513, loss = 0.13148795\n",
      "Iteration 514, loss = 0.13197424\n",
      "Iteration 515, loss = 0.13288455\n",
      "Iteration 516, loss = 0.13284232\n",
      "Iteration 517, loss = 0.13263139\n",
      "Iteration 518, loss = 0.12987557\n",
      "Iteration 519, loss = 0.13056504\n",
      "Iteration 520, loss = 0.12903208\n",
      "Iteration 521, loss = 0.12861558\n",
      "Iteration 522, loss = 0.13008367\n",
      "Iteration 523, loss = 0.12920171\n",
      "Iteration 524, loss = 0.12783694\n",
      "Iteration 525, loss = 0.12747408\n",
      "Iteration 526, loss = 0.12774467\n",
      "Iteration 527, loss = 0.12700845\n",
      "Iteration 528, loss = 0.12729904\n",
      "Iteration 529, loss = 0.12639321\n",
      "Iteration 530, loss = 0.12633923\n",
      "Iteration 531, loss = 0.12692972\n",
      "Iteration 532, loss = 0.12748677\n",
      "Iteration 533, loss = 0.12642415\n",
      "Iteration 534, loss = 0.12523274\n",
      "Iteration 535, loss = 0.12765591\n",
      "Iteration 536, loss = 0.12594431\n",
      "Iteration 537, loss = 0.12405215\n",
      "Iteration 538, loss = 0.12462472\n",
      "Iteration 539, loss = 0.12444237\n",
      "Iteration 540, loss = 0.12339382\n",
      "Iteration 541, loss = 0.12354232\n",
      "Iteration 542, loss = 0.12323609\n",
      "Iteration 543, loss = 0.12314996\n",
      "Iteration 544, loss = 0.12304257\n",
      "Iteration 545, loss = 0.12151118\n",
      "Iteration 546, loss = 0.12186625\n",
      "Iteration 547, loss = 0.12245753\n",
      "Iteration 548, loss = 0.12197341\n",
      "Iteration 549, loss = 0.12137983\n",
      "Iteration 550, loss = 0.12103062\n",
      "Iteration 551, loss = 0.12129858\n",
      "Iteration 552, loss = 0.12232712\n",
      "Iteration 553, loss = 0.12317552\n",
      "Iteration 554, loss = 0.12144771\n",
      "Iteration 555, loss = 0.12003512\n",
      "Iteration 556, loss = 0.11946038\n",
      "Iteration 557, loss = 0.11928176\n",
      "Iteration 558, loss = 0.11883936\n",
      "Iteration 559, loss = 0.11881051\n",
      "Iteration 560, loss = 0.11821137\n",
      "Iteration 561, loss = 0.11884458\n",
      "Iteration 562, loss = 0.11781495\n",
      "Iteration 563, loss = 0.11760106\n",
      "Iteration 564, loss = 0.11700066\n",
      "Iteration 565, loss = 0.11709328\n",
      "Iteration 566, loss = 0.11723409\n",
      "Iteration 567, loss = 0.11646895\n",
      "Iteration 568, loss = 0.11611154\n",
      "Iteration 569, loss = 0.11767316\n",
      "Iteration 570, loss = 0.11601523\n",
      "Iteration 571, loss = 0.11646139\n",
      "Iteration 572, loss = 0.11527363\n",
      "Iteration 573, loss = 0.11524671\n",
      "Iteration 574, loss = 0.11446002\n",
      "Iteration 575, loss = 0.11513016\n",
      "Iteration 576, loss = 0.11528426\n",
      "Iteration 577, loss = 0.11683135\n",
      "Iteration 578, loss = 0.11518730\n",
      "Iteration 579, loss = 0.11369402\n",
      "Iteration 580, loss = 0.11340764\n",
      "Iteration 581, loss = 0.11295066\n",
      "Iteration 582, loss = 0.11331607\n",
      "Iteration 583, loss = 0.11232194\n",
      "Iteration 584, loss = 0.11314903\n",
      "Iteration 585, loss = 0.11262771\n",
      "Iteration 586, loss = 0.11208130\n",
      "Iteration 587, loss = 0.11266450\n",
      "Iteration 588, loss = 0.11145271\n",
      "Iteration 589, loss = 0.11133985\n",
      "Iteration 590, loss = 0.11118394\n",
      "Iteration 591, loss = 0.11102759\n",
      "Iteration 592, loss = 0.11001776\n",
      "Iteration 593, loss = 0.11037614\n",
      "Iteration 594, loss = 0.11029886\n",
      "Iteration 595, loss = 0.11053562\n",
      "Iteration 596, loss = 0.11153266\n",
      "Iteration 597, loss = 0.11247438\n",
      "Iteration 598, loss = 0.11044789\n",
      "Iteration 599, loss = 0.11102591\n",
      "Iteration 600, loss = 0.10987614\n",
      "Iteration 601, loss = 0.10910616\n",
      "Iteration 602, loss = 0.10826734\n",
      "Iteration 603, loss = 0.10827500\n",
      "Iteration 604, loss = 0.10870881\n",
      "Iteration 605, loss = 0.10730019\n",
      "Iteration 606, loss = 0.10756544\n",
      "Iteration 607, loss = 0.10764164\n",
      "Iteration 608, loss = 0.10693290\n",
      "Iteration 609, loss = 0.10775150\n",
      "Iteration 610, loss = 0.10661962\n",
      "Iteration 611, loss = 0.10607914\n",
      "Iteration 612, loss = 0.10621752\n",
      "Iteration 613, loss = 0.10566945\n",
      "Iteration 614, loss = 0.10567423\n",
      "Iteration 615, loss = 0.10567270\n",
      "Iteration 616, loss = 0.10621216\n",
      "Iteration 617, loss = 0.10528448\n",
      "Iteration 618, loss = 0.10750989\n",
      "Iteration 619, loss = 0.10712329\n",
      "Iteration 620, loss = 0.10636738\n",
      "Iteration 621, loss = 0.10952039\n",
      "Iteration 622, loss = 0.10423415\n",
      "Iteration 623, loss = 0.10381276\n",
      "Iteration 624, loss = 0.10381049\n",
      "Iteration 625, loss = 0.10337456\n",
      "Iteration 626, loss = 0.10321199\n",
      "Iteration 627, loss = 0.10342493\n",
      "Iteration 628, loss = 0.10338463\n",
      "Iteration 629, loss = 0.10317998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 630, loss = 0.10291479\n",
      "Iteration 631, loss = 0.10226266\n",
      "Iteration 632, loss = 0.10175194\n",
      "Iteration 633, loss = 0.10214275\n",
      "Iteration 634, loss = 0.10154991\n",
      "Iteration 635, loss = 0.10280313\n",
      "Iteration 636, loss = 0.10391993\n",
      "Iteration 637, loss = 0.10067437\n",
      "Iteration 638, loss = 0.10315658\n",
      "Iteration 639, loss = 0.10143760\n",
      "Iteration 640, loss = 0.10083905\n",
      "Iteration 641, loss = 0.10105390\n",
      "Iteration 642, loss = 0.10053865\n",
      "Iteration 643, loss = 0.09971331\n",
      "Iteration 644, loss = 0.10016286\n",
      "Iteration 645, loss = 0.09984600\n",
      "Iteration 646, loss = 0.09937866\n",
      "Iteration 647, loss = 0.09890389\n",
      "Iteration 648, loss = 0.09966445\n",
      "Iteration 649, loss = 0.09873083\n",
      "Iteration 650, loss = 0.09978010\n",
      "Iteration 651, loss = 0.09892257\n",
      "Iteration 652, loss = 0.09890361\n",
      "Iteration 653, loss = 0.09901394\n",
      "Iteration 654, loss = 0.09792831\n",
      "Iteration 655, loss = 0.09754053\n",
      "Iteration 656, loss = 0.09759442\n",
      "Iteration 657, loss = 0.09732475\n",
      "Iteration 658, loss = 0.09680606\n",
      "Iteration 659, loss = 0.09779222\n",
      "Iteration 660, loss = 0.09740846\n",
      "Iteration 661, loss = 0.09700039\n",
      "Iteration 662, loss = 0.09641849\n",
      "Iteration 663, loss = 0.09637617\n",
      "Iteration 664, loss = 0.09640782\n",
      "Iteration 665, loss = 0.09601532\n",
      "Iteration 666, loss = 0.09559044\n",
      "Iteration 667, loss = 0.09556326\n",
      "Iteration 668, loss = 0.09506071\n",
      "Iteration 669, loss = 0.09569213\n",
      "Iteration 670, loss = 0.09535435\n",
      "Iteration 671, loss = 0.09575867\n",
      "Iteration 672, loss = 0.09472113\n",
      "Iteration 673, loss = 0.09450167\n",
      "Iteration 674, loss = 0.09395811\n",
      "Iteration 675, loss = 0.09387616\n",
      "Iteration 676, loss = 0.09468432\n",
      "Iteration 677, loss = 0.09581992\n",
      "Iteration 678, loss = 0.09379119\n",
      "Iteration 679, loss = 0.09680844\n",
      "Iteration 680, loss = 0.09413628\n",
      "Iteration 681, loss = 0.09526592\n",
      "Iteration 682, loss = 0.09485746\n",
      "Iteration 683, loss = 0.09384585\n",
      "Iteration 684, loss = 0.09579671\n",
      "Iteration 685, loss = 0.09324889\n",
      "Iteration 686, loss = 0.09286955\n",
      "Iteration 687, loss = 0.09237681\n",
      "Iteration 688, loss = 0.09264248\n",
      "Iteration 689, loss = 0.09140309\n",
      "Iteration 690, loss = 0.09194165\n",
      "Iteration 691, loss = 0.09146511\n",
      "Iteration 692, loss = 0.09227289\n",
      "Iteration 693, loss = 0.09152205\n",
      "Iteration 694, loss = 0.09110446\n",
      "Iteration 695, loss = 0.09115388\n",
      "Iteration 696, loss = 0.09092060\n",
      "Iteration 697, loss = 0.09081246\n",
      "Iteration 698, loss = 0.09054339\n",
      "Iteration 699, loss = 0.09028012\n",
      "Iteration 700, loss = 0.09045747\n",
      "Iteration 701, loss = 0.09010209\n",
      "Iteration 702, loss = 0.09003320\n",
      "Iteration 703, loss = 0.09075804\n",
      "Iteration 704, loss = 0.09013440\n",
      "Iteration 705, loss = 0.08965469\n",
      "Iteration 706, loss = 0.08951191\n",
      "Iteration 707, loss = 0.08930097\n",
      "Iteration 708, loss = 0.08977298\n",
      "Iteration 709, loss = 0.08911937\n",
      "Iteration 710, loss = 0.08965428\n",
      "Iteration 711, loss = 0.09004719\n",
      "Iteration 712, loss = 0.08953027\n",
      "Iteration 713, loss = 0.08826507\n",
      "Iteration 714, loss = 0.09008380\n",
      "Iteration 715, loss = 0.08879276\n",
      "Iteration 716, loss = 0.08788405\n",
      "Iteration 717, loss = 0.08775939\n",
      "Iteration 718, loss = 0.08751648\n",
      "Iteration 719, loss = 0.08716652\n",
      "Iteration 720, loss = 0.08713849\n",
      "Iteration 721, loss = 0.08752428\n",
      "Iteration 722, loss = 0.08777273\n",
      "Iteration 723, loss = 0.08865221\n",
      "Iteration 724, loss = 0.08776742\n",
      "Iteration 725, loss = 0.08660845\n",
      "Iteration 726, loss = 0.08658328\n",
      "Iteration 727, loss = 0.08771586\n",
      "Iteration 728, loss = 0.08692409\n",
      "Iteration 729, loss = 0.08584926\n",
      "Iteration 730, loss = 0.08585829\n",
      "Iteration 731, loss = 0.08604335\n",
      "Iteration 732, loss = 0.08606947\n",
      "Iteration 733, loss = 0.08620468\n",
      "Iteration 734, loss = 0.08587518\n",
      "Iteration 735, loss = 0.08561816\n",
      "Iteration 736, loss = 0.08540955\n",
      "Iteration 737, loss = 0.08515355\n",
      "Iteration 738, loss = 0.08562020\n",
      "Iteration 739, loss = 0.08537126\n",
      "Iteration 740, loss = 0.08476323\n",
      "Iteration 741, loss = 0.08498060\n",
      "Iteration 742, loss = 0.08471174\n",
      "Iteration 743, loss = 0.08404968\n",
      "Iteration 744, loss = 0.08381266\n",
      "Iteration 745, loss = 0.08382947\n",
      "Iteration 746, loss = 0.08321442\n",
      "Iteration 747, loss = 0.08400932\n",
      "Iteration 748, loss = 0.08417836\n",
      "Iteration 749, loss = 0.08427557\n",
      "Iteration 750, loss = 0.08297561\n",
      "Iteration 751, loss = 0.08334620\n",
      "Iteration 752, loss = 0.08324896\n",
      "Iteration 753, loss = 0.08354089\n",
      "Iteration 754, loss = 0.08331860\n",
      "Iteration 755, loss = 0.08339470\n",
      "Iteration 756, loss = 0.08343087\n",
      "Iteration 757, loss = 0.08362620\n",
      "Iteration 758, loss = 0.08295034\n",
      "Iteration 759, loss = 0.08290709\n",
      "Iteration 760, loss = 0.08306594\n",
      "Iteration 761, loss = 0.08327243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 10 of 48...\n",
      "Iteration 1, loss = 0.68098037\n",
      "Iteration 2, loss = 0.58496379\n",
      "Iteration 3, loss = 0.53935720\n",
      "Iteration 4, loss = 0.50655747\n",
      "Iteration 5, loss = 0.48215656\n",
      "Iteration 6, loss = 0.46201049\n",
      "Iteration 7, loss = 0.44573997\n",
      "Iteration 8, loss = 0.43279015\n",
      "Iteration 9, loss = 0.42329585\n",
      "Iteration 10, loss = 0.41466530\n",
      "Iteration 11, loss = 0.40434735\n",
      "Iteration 12, loss = 0.39454528\n",
      "Iteration 13, loss = 0.38752346\n",
      "Iteration 14, loss = 0.38047220\n",
      "Iteration 15, loss = 0.37532484\n",
      "Iteration 16, loss = 0.36950106\n",
      "Iteration 17, loss = 0.36222649\n",
      "Iteration 18, loss = 0.35830736\n",
      "Iteration 19, loss = 0.35307312\n",
      "Iteration 20, loss = 0.34913985\n",
      "Iteration 21, loss = 0.34828130\n",
      "Iteration 22, loss = 0.34492666\n",
      "Iteration 23, loss = 0.33798684\n",
      "Iteration 24, loss = 0.33417672\n",
      "Iteration 25, loss = 0.33275757\n",
      "Iteration 26, loss = 0.33281546\n",
      "Iteration 27, loss = 0.32464557\n",
      "Iteration 28, loss = 0.31970931\n",
      "Iteration 29, loss = 0.31858027\n",
      "Iteration 30, loss = 0.31824333\n",
      "Iteration 31, loss = 0.30866584\n",
      "Iteration 32, loss = 0.31429756\n",
      "Iteration 33, loss = 0.30747413\n",
      "Iteration 34, loss = 0.30068111\n",
      "Iteration 35, loss = 0.29855417\n",
      "Iteration 36, loss = 0.29535429\n",
      "Iteration 37, loss = 0.29441110\n",
      "Iteration 38, loss = 0.29142104\n",
      "Iteration 39, loss = 0.28817608\n",
      "Iteration 40, loss = 0.28537351\n",
      "Iteration 41, loss = 0.28437811\n",
      "Iteration 42, loss = 0.27943613\n",
      "Iteration 43, loss = 0.28200248\n",
      "Iteration 44, loss = 0.27708751\n",
      "Iteration 45, loss = 0.28149089\n",
      "Iteration 46, loss = 0.28199287\n",
      "Iteration 47, loss = 0.27307937\n",
      "Iteration 48, loss = 0.26623936\n",
      "Iteration 49, loss = 0.27963618\n",
      "Iteration 50, loss = 0.26463267\n",
      "Iteration 51, loss = 0.26151763\n",
      "Iteration 52, loss = 0.25902698\n",
      "Iteration 53, loss = 0.25945516\n",
      "Iteration 54, loss = 0.25376382\n",
      "Iteration 55, loss = 0.25057300\n",
      "Iteration 56, loss = 0.25232396\n",
      "Iteration 57, loss = 0.25134732\n",
      "Iteration 58, loss = 0.24695310\n",
      "Iteration 59, loss = 0.24736532\n",
      "Iteration 60, loss = 0.24354146\n",
      "Iteration 61, loss = 0.24106430\n",
      "Iteration 62, loss = 0.23866257\n",
      "Iteration 63, loss = 0.23900934\n",
      "Iteration 64, loss = 0.23434168\n",
      "Iteration 65, loss = 0.23269128\n",
      "Iteration 66, loss = 0.23701422\n",
      "Iteration 67, loss = 0.23190625\n",
      "Iteration 68, loss = 0.22810643\n",
      "Iteration 69, loss = 0.22779007\n",
      "Iteration 70, loss = 0.23329417\n",
      "Iteration 71, loss = 0.23022788\n",
      "Iteration 72, loss = 0.22640558\n",
      "Iteration 73, loss = 0.22342257\n",
      "Iteration 74, loss = 0.21592689\n",
      "Iteration 75, loss = 0.21957054\n",
      "Iteration 76, loss = 0.21614684\n",
      "Iteration 77, loss = 0.21407589\n",
      "Iteration 78, loss = 0.21172348\n",
      "Iteration 79, loss = 0.21062073\n",
      "Iteration 80, loss = 0.20846246\n",
      "Iteration 81, loss = 0.20673108\n",
      "Iteration 82, loss = 0.20450694\n",
      "Iteration 83, loss = 0.20424739\n",
      "Iteration 84, loss = 0.20101512\n",
      "Iteration 85, loss = 0.20001917\n",
      "Iteration 86, loss = 0.19749620\n",
      "Iteration 87, loss = 0.19624457\n",
      "Iteration 88, loss = 0.19786135\n",
      "Iteration 89, loss = 0.19364297\n",
      "Iteration 90, loss = 0.19512421\n",
      "Iteration 91, loss = 0.19165058\n",
      "Iteration 92, loss = 0.18776317\n",
      "Iteration 93, loss = 0.18617342\n",
      "Iteration 94, loss = 0.18765894\n",
      "Iteration 95, loss = 0.18723293\n",
      "Iteration 96, loss = 0.18481127\n",
      "Iteration 97, loss = 0.18351112\n",
      "Iteration 98, loss = 0.18034834\n",
      "Iteration 99, loss = 0.18435499\n",
      "Iteration 100, loss = 0.18214222\n",
      "Iteration 101, loss = 0.18008471\n",
      "Iteration 102, loss = 0.17477345\n",
      "Iteration 103, loss = 0.17278151\n",
      "Iteration 104, loss = 0.17050715\n",
      "Iteration 105, loss = 0.16886773\n",
      "Iteration 106, loss = 0.17028993\n",
      "Iteration 107, loss = 0.16849718\n",
      "Iteration 108, loss = 0.16608852\n",
      "Iteration 109, loss = 0.16584883\n",
      "Iteration 110, loss = 0.16339708\n",
      "Iteration 111, loss = 0.16264283\n",
      "Iteration 112, loss = 0.16075908\n",
      "Iteration 113, loss = 0.16187616\n",
      "Iteration 114, loss = 0.16045227\n",
      "Iteration 115, loss = 0.15734663\n",
      "Iteration 116, loss = 0.15596046\n",
      "Iteration 117, loss = 0.15418983\n",
      "Iteration 118, loss = 0.15259131\n",
      "Iteration 119, loss = 0.15283335\n",
      "Iteration 120, loss = 0.15142754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 121, loss = 0.15015910\n",
      "Iteration 122, loss = 0.14911097\n",
      "Iteration 123, loss = 0.14671904\n",
      "Iteration 124, loss = 0.14435795\n",
      "Iteration 125, loss = 0.14548438\n",
      "Iteration 126, loss = 0.14537951\n",
      "Iteration 127, loss = 0.14330957\n",
      "Iteration 128, loss = 0.14182018\n",
      "Iteration 129, loss = 0.14093230\n",
      "Iteration 130, loss = 0.14434178\n",
      "Iteration 131, loss = 0.14983202\n",
      "Iteration 132, loss = 0.14597224\n",
      "Iteration 133, loss = 0.14367070\n",
      "Iteration 134, loss = 0.13614056\n",
      "Iteration 135, loss = 0.13321350\n",
      "Iteration 136, loss = 0.13409925\n",
      "Iteration 137, loss = 0.13238848\n",
      "Iteration 138, loss = 0.13107853\n",
      "Iteration 139, loss = 0.12898490\n",
      "Iteration 140, loss = 0.12781275\n",
      "Iteration 141, loss = 0.12653272\n",
      "Iteration 142, loss = 0.12623987\n",
      "Iteration 143, loss = 0.12622990\n",
      "Iteration 144, loss = 0.12794227\n",
      "Iteration 145, loss = 0.12372629\n",
      "Iteration 146, loss = 0.12406493\n",
      "Iteration 147, loss = 0.12011749\n",
      "Iteration 148, loss = 0.12256818\n",
      "Iteration 149, loss = 0.12113865\n",
      "Iteration 150, loss = 0.12081868\n",
      "Iteration 151, loss = 0.12127544\n",
      "Iteration 152, loss = 0.11925393\n",
      "Iteration 153, loss = 0.11793030\n",
      "Iteration 154, loss = 0.11559244\n",
      "Iteration 155, loss = 0.11497985\n",
      "Iteration 156, loss = 0.11368878\n",
      "Iteration 157, loss = 0.11104528\n",
      "Iteration 158, loss = 0.11093495\n",
      "Iteration 159, loss = 0.10989185\n",
      "Iteration 160, loss = 0.11036429\n",
      "Iteration 161, loss = 0.11110648\n",
      "Iteration 162, loss = 0.10896629\n",
      "Iteration 163, loss = 0.11001012\n",
      "Iteration 164, loss = 0.10784617\n",
      "Iteration 165, loss = 0.10712322\n",
      "Iteration 166, loss = 0.10508101\n",
      "Iteration 167, loss = 0.10252874\n",
      "Iteration 168, loss = 0.10199153\n",
      "Iteration 169, loss = 0.10171433\n",
      "Iteration 170, loss = 0.09976433\n",
      "Iteration 171, loss = 0.09971559\n",
      "Iteration 172, loss = 0.09909801\n",
      "Iteration 173, loss = 0.09783613\n",
      "Iteration 174, loss = 0.09807977\n",
      "Iteration 175, loss = 0.09774921\n",
      "Iteration 176, loss = 0.09540370\n",
      "Iteration 177, loss = 0.09617713\n",
      "Iteration 178, loss = 0.09509026\n",
      "Iteration 179, loss = 0.09370763\n",
      "Iteration 180, loss = 0.09345625\n",
      "Iteration 181, loss = 0.09402989\n",
      "Iteration 182, loss = 0.09265205\n",
      "Iteration 183, loss = 0.09216107\n",
      "Iteration 184, loss = 0.08983958\n",
      "Iteration 185, loss = 0.08966416\n",
      "Iteration 186, loss = 0.08938072\n",
      "Iteration 187, loss = 0.08941583\n",
      "Iteration 188, loss = 0.08893320\n",
      "Iteration 189, loss = 0.08968380\n",
      "Iteration 190, loss = 0.09197774\n",
      "Iteration 191, loss = 0.08585133\n",
      "Iteration 192, loss = 0.08590297\n",
      "Iteration 193, loss = 0.08410344\n",
      "Iteration 194, loss = 0.08392490\n",
      "Iteration 195, loss = 0.08511927\n",
      "Iteration 196, loss = 0.08580514\n",
      "Iteration 197, loss = 0.08229700\n",
      "Iteration 198, loss = 0.08298574\n",
      "Iteration 199, loss = 0.08255496\n",
      "Iteration 200, loss = 0.07967871\n",
      "Iteration 201, loss = 0.07904740\n",
      "Iteration 202, loss = 0.07795377\n",
      "Iteration 203, loss = 0.07988900\n",
      "Iteration 204, loss = 0.07724387\n",
      "Iteration 205, loss = 0.07749067\n",
      "Iteration 206, loss = 0.07709685\n",
      "Iteration 207, loss = 0.07575842\n",
      "Iteration 208, loss = 0.07596592\n",
      "Iteration 209, loss = 0.07542264\n",
      "Iteration 210, loss = 0.07372062\n",
      "Iteration 211, loss = 0.07411173\n",
      "Iteration 212, loss = 0.07354309\n",
      "Iteration 213, loss = 0.07234017\n",
      "Iteration 214, loss = 0.07397801\n",
      "Iteration 215, loss = 0.07607100\n",
      "Iteration 216, loss = 0.07879763\n",
      "Iteration 217, loss = 0.07369318\n",
      "Iteration 218, loss = 0.06933102\n",
      "Iteration 219, loss = 0.07078650\n",
      "Iteration 220, loss = 0.06881636\n",
      "Iteration 221, loss = 0.06732089\n",
      "Iteration 222, loss = 0.06687562\n",
      "Iteration 223, loss = 0.06662310\n",
      "Iteration 224, loss = 0.06597604\n",
      "Iteration 225, loss = 0.06723529\n",
      "Iteration 226, loss = 0.06607823\n",
      "Iteration 227, loss = 0.06507799\n",
      "Iteration 228, loss = 0.06626432\n",
      "Iteration 229, loss = 0.06472572\n",
      "Iteration 230, loss = 0.06557405\n",
      "Iteration 231, loss = 0.06590385\n",
      "Iteration 232, loss = 0.06286909\n",
      "Iteration 233, loss = 0.06302147\n",
      "Iteration 234, loss = 0.06471934\n",
      "Iteration 235, loss = 0.06186812\n",
      "Iteration 236, loss = 0.06244934\n",
      "Iteration 237, loss = 0.06019197\n",
      "Iteration 238, loss = 0.06195869\n",
      "Iteration 239, loss = 0.05919324\n",
      "Iteration 240, loss = 0.05971055\n",
      "Iteration 241, loss = 0.05927235\n",
      "Iteration 242, loss = 0.05851629\n",
      "Iteration 243, loss = 0.05687144\n",
      "Iteration 244, loss = 0.05695681\n",
      "Iteration 245, loss = 0.05812015\n",
      "Iteration 246, loss = 0.05640450\n",
      "Iteration 247, loss = 0.05615280\n",
      "Iteration 248, loss = 0.05671788\n",
      "Iteration 249, loss = 0.05483630\n",
      "Iteration 250, loss = 0.05522807\n",
      "Iteration 251, loss = 0.05711267\n",
      "Iteration 252, loss = 0.05953437\n",
      "Iteration 253, loss = 0.05540602\n",
      "Iteration 254, loss = 0.05418230\n",
      "Iteration 255, loss = 0.05364851\n",
      "Iteration 256, loss = 0.05284805\n",
      "Iteration 257, loss = 0.05208003\n",
      "Iteration 258, loss = 0.05091883\n",
      "Iteration 259, loss = 0.05701111\n",
      "Iteration 260, loss = 0.05069985\n",
      "Iteration 261, loss = 0.05102950\n",
      "Iteration 262, loss = 0.05243580\n",
      "Iteration 263, loss = 0.05115233\n",
      "Iteration 264, loss = 0.05199436\n",
      "Iteration 265, loss = 0.04943644\n",
      "Iteration 266, loss = 0.04974229\n",
      "Iteration 267, loss = 0.04904831\n",
      "Iteration 268, loss = 0.05026010\n",
      "Iteration 269, loss = 0.04813617\n",
      "Iteration 270, loss = 0.04901956\n",
      "Iteration 271, loss = 0.04704605\n",
      "Iteration 272, loss = 0.04659883\n",
      "Iteration 273, loss = 0.04673152\n",
      "Iteration 274, loss = 0.04837894\n",
      "Iteration 275, loss = 0.04684879\n",
      "Iteration 276, loss = 0.04676837\n",
      "Iteration 277, loss = 0.04536437\n",
      "Iteration 278, loss = 0.04500274\n",
      "Iteration 279, loss = 0.04524491\n",
      "Iteration 280, loss = 0.04385103\n",
      "Iteration 281, loss = 0.04382057\n",
      "Iteration 282, loss = 0.04361933\n",
      "Iteration 283, loss = 0.04655775\n",
      "Iteration 284, loss = 0.04415902\n",
      "Iteration 285, loss = 0.04353077\n",
      "Iteration 286, loss = 0.04224968\n",
      "Iteration 287, loss = 0.04226129\n",
      "Iteration 288, loss = 0.04122231\n",
      "Iteration 289, loss = 0.04138028\n",
      "Iteration 290, loss = 0.04095750\n",
      "Iteration 291, loss = 0.04173617\n",
      "Iteration 292, loss = 0.04047294\n",
      "Iteration 293, loss = 0.04043336\n",
      "Iteration 294, loss = 0.04010120\n",
      "Iteration 295, loss = 0.04053846\n",
      "Iteration 296, loss = 0.03914664\n",
      "Iteration 297, loss = 0.03883238\n",
      "Iteration 298, loss = 0.03880879\n",
      "Iteration 299, loss = 0.03992203\n",
      "Iteration 300, loss = 0.03861215\n",
      "Iteration 301, loss = 0.03955933\n",
      "Iteration 302, loss = 0.03915485\n",
      "Iteration 303, loss = 0.03744564\n",
      "Iteration 304, loss = 0.03864306\n",
      "Iteration 305, loss = 0.03969755\n",
      "Iteration 306, loss = 0.03929707\n",
      "Iteration 307, loss = 0.03729685\n",
      "Iteration 308, loss = 0.03755426\n",
      "Iteration 309, loss = 0.03649956\n",
      "Iteration 310, loss = 0.03595751\n",
      "Iteration 311, loss = 0.03536921\n",
      "Iteration 312, loss = 0.03477288\n",
      "Iteration 313, loss = 0.03477592\n",
      "Iteration 314, loss = 0.03625721\n",
      "Iteration 315, loss = 0.03552212\n",
      "Iteration 316, loss = 0.03603324\n",
      "Iteration 317, loss = 0.03534358\n",
      "Iteration 318, loss = 0.03348884\n",
      "Iteration 319, loss = 0.03317488\n",
      "Iteration 320, loss = 0.03301074\n",
      "Iteration 321, loss = 0.03400699\n",
      "Iteration 322, loss = 0.03252711\n",
      "Iteration 323, loss = 0.03293548\n",
      "Iteration 324, loss = 0.03291135\n",
      "Iteration 325, loss = 0.03326200\n",
      "Iteration 326, loss = 0.03256415\n",
      "Iteration 327, loss = 0.03152273\n",
      "Iteration 328, loss = 0.03119875\n",
      "Iteration 329, loss = 0.03194862\n",
      "Iteration 330, loss = 0.03155828\n",
      "Iteration 331, loss = 0.03210625\n",
      "Iteration 332, loss = 0.03291790\n",
      "Iteration 333, loss = 0.03237458\n",
      "Iteration 334, loss = 0.03134060\n",
      "Iteration 335, loss = 0.03059801\n",
      "Iteration 336, loss = 0.02978882\n",
      "Iteration 337, loss = 0.03113722\n",
      "Iteration 338, loss = 0.02889728\n",
      "Iteration 339, loss = 0.02928568\n",
      "Iteration 340, loss = 0.02867142\n",
      "Iteration 341, loss = 0.02836889\n",
      "Iteration 342, loss = 0.02785587\n",
      "Iteration 343, loss = 0.02851583\n",
      "Iteration 344, loss = 0.02794915\n",
      "Iteration 345, loss = 0.02742474\n",
      "Iteration 346, loss = 0.02889842\n",
      "Iteration 347, loss = 0.02781613\n",
      "Iteration 348, loss = 0.02822848\n",
      "Iteration 349, loss = 0.02654890\n",
      "Iteration 350, loss = 0.02765377\n",
      "Iteration 351, loss = 0.02696948\n",
      "Iteration 352, loss = 0.02777324\n",
      "Iteration 353, loss = 0.02675908\n",
      "Iteration 354, loss = 0.02676838\n",
      "Iteration 355, loss = 0.02562680\n",
      "Iteration 356, loss = 0.02580636\n",
      "Iteration 357, loss = 0.02579005\n",
      "Iteration 358, loss = 0.02552176\n",
      "Iteration 359, loss = 0.02514188\n",
      "Iteration 360, loss = 0.02528205\n",
      "Iteration 361, loss = 0.02480801\n",
      "Iteration 362, loss = 0.02505678\n",
      "Iteration 363, loss = 0.02482662\n",
      "Iteration 364, loss = 0.02410864\n",
      "Iteration 365, loss = 0.02457799\n",
      "Iteration 366, loss = 0.02377563\n",
      "Iteration 367, loss = 0.02434918\n",
      "Iteration 368, loss = 0.02328929\n",
      "Iteration 369, loss = 0.02408718\n",
      "Iteration 370, loss = 0.02451296\n",
      "Iteration 371, loss = 0.02493668\n",
      "Iteration 372, loss = 0.02348652\n",
      "Iteration 373, loss = 0.02320388\n",
      "Iteration 374, loss = 0.02384706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 375, loss = 0.02301675\n",
      "Iteration 376, loss = 0.02256792\n",
      "Iteration 377, loss = 0.02266214\n",
      "Iteration 378, loss = 0.02228185\n",
      "Iteration 379, loss = 0.02199616\n",
      "Iteration 380, loss = 0.02157445\n",
      "Iteration 381, loss = 0.02224442\n",
      "Iteration 382, loss = 0.02143494\n",
      "Iteration 383, loss = 0.02134982\n",
      "Iteration 384, loss = 0.02190736\n",
      "Iteration 385, loss = 0.02233546\n",
      "Iteration 386, loss = 0.02170197\n",
      "Iteration 387, loss = 0.02166076\n",
      "Iteration 388, loss = 0.02043874\n",
      "Iteration 389, loss = 0.02029291\n",
      "Iteration 390, loss = 0.02100475\n",
      "Iteration 391, loss = 0.02031856\n",
      "Iteration 392, loss = 0.02023411\n",
      "Iteration 393, loss = 0.02136192\n",
      "Iteration 394, loss = 0.02016936\n",
      "Iteration 395, loss = 0.02017742\n",
      "Iteration 396, loss = 0.02021024\n",
      "Iteration 397, loss = 0.01916175\n",
      "Iteration 398, loss = 0.01900114\n",
      "Iteration 399, loss = 0.01888913\n",
      "Iteration 400, loss = 0.01875998\n",
      "Iteration 401, loss = 0.01926219\n",
      "Iteration 402, loss = 0.01842138\n",
      "Iteration 403, loss = 0.01909061\n",
      "Iteration 404, loss = 0.01936314\n",
      "Iteration 405, loss = 0.01873255\n",
      "Iteration 406, loss = 0.01813285\n",
      "Iteration 407, loss = 0.01812977\n",
      "Iteration 408, loss = 0.01812245\n",
      "Iteration 409, loss = 0.01790151\n",
      "Iteration 410, loss = 0.01776862\n",
      "Iteration 411, loss = 0.01745042\n",
      "Iteration 412, loss = 0.01808718\n",
      "Iteration 413, loss = 0.01706331\n",
      "Iteration 414, loss = 0.01761886\n",
      "Iteration 415, loss = 0.01690467\n",
      "Iteration 416, loss = 0.01714487\n",
      "Iteration 417, loss = 0.01636049\n",
      "Iteration 418, loss = 0.01639315\n",
      "Iteration 419, loss = 0.01608991\n",
      "Iteration 420, loss = 0.01600845\n",
      "Iteration 421, loss = 0.01628289\n",
      "Iteration 422, loss = 0.01584150\n",
      "Iteration 423, loss = 0.01613519\n",
      "Iteration 424, loss = 0.01676600\n",
      "Iteration 425, loss = 0.01588220\n",
      "Iteration 426, loss = 0.02646900\n",
      "Iteration 427, loss = 0.03060989\n",
      "Iteration 428, loss = 0.03850498\n",
      "Iteration 429, loss = 0.03246632\n",
      "Iteration 430, loss = 0.03233397\n",
      "Iteration 431, loss = 0.02410460\n",
      "Iteration 432, loss = 0.02169578\n",
      "Iteration 433, loss = 0.02055263\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 11 of 48...\n",
      "Iteration 1, loss = 0.64803333\n",
      "Iteration 2, loss = 0.56674285\n",
      "Iteration 3, loss = 0.52619676\n",
      "Iteration 4, loss = 0.49660281\n",
      "Iteration 5, loss = 0.47581670\n",
      "Iteration 6, loss = 0.45553188\n",
      "Iteration 7, loss = 0.44015066\n",
      "Iteration 8, loss = 0.42863390\n",
      "Iteration 9, loss = 0.42249844\n",
      "Iteration 10, loss = 0.41330394\n",
      "Iteration 11, loss = 0.40330879\n",
      "Iteration 12, loss = 0.39090138\n",
      "Iteration 13, loss = 0.38746978\n",
      "Iteration 14, loss = 0.37773192\n",
      "Iteration 15, loss = 0.37225492\n",
      "Iteration 16, loss = 0.36511913\n",
      "Iteration 17, loss = 0.36234129\n",
      "Iteration 18, loss = 0.36009570\n",
      "Iteration 19, loss = 0.36325243\n",
      "Iteration 20, loss = 0.34978084\n",
      "Iteration 21, loss = 0.34590763\n",
      "Iteration 22, loss = 0.34020103\n",
      "Iteration 23, loss = 0.33608362\n",
      "Iteration 24, loss = 0.33521934\n",
      "Iteration 25, loss = 0.33253421\n",
      "Iteration 26, loss = 0.33050209\n",
      "Iteration 27, loss = 0.32443329\n",
      "Iteration 28, loss = 0.32225686\n",
      "Iteration 29, loss = 0.31564228\n",
      "Iteration 30, loss = 0.31147241\n",
      "Iteration 31, loss = 0.30743894\n",
      "Iteration 32, loss = 0.30626803\n",
      "Iteration 33, loss = 0.30551676\n",
      "Iteration 34, loss = 0.30785218\n",
      "Iteration 35, loss = 0.30046148\n",
      "Iteration 36, loss = 0.29770379\n",
      "Iteration 37, loss = 0.29223496\n",
      "Iteration 38, loss = 0.28852239\n",
      "Iteration 39, loss = 0.29165910\n",
      "Iteration 40, loss = 0.28384042\n",
      "Iteration 41, loss = 0.28426820\n",
      "Iteration 42, loss = 0.28121269\n",
      "Iteration 43, loss = 0.27670354\n",
      "Iteration 44, loss = 0.27650461\n",
      "Iteration 45, loss = 0.27624212\n",
      "Iteration 46, loss = 0.27161141\n",
      "Iteration 47, loss = 0.26984992\n",
      "Iteration 48, loss = 0.27303050\n",
      "Iteration 49, loss = 0.26830916\n",
      "Iteration 50, loss = 0.27012943\n",
      "Iteration 51, loss = 0.26065815\n",
      "Iteration 52, loss = 0.25644005\n",
      "Iteration 53, loss = 0.25917674\n",
      "Iteration 54, loss = 0.25206571\n",
      "Iteration 55, loss = 0.25597023\n",
      "Iteration 56, loss = 0.24833351\n",
      "Iteration 57, loss = 0.24879215\n",
      "Iteration 58, loss = 0.25258175\n",
      "Iteration 59, loss = 0.24773575\n",
      "Iteration 60, loss = 0.24069351\n",
      "Iteration 61, loss = 0.23868942\n",
      "Iteration 62, loss = 0.23690204\n",
      "Iteration 63, loss = 0.23935932\n",
      "Iteration 64, loss = 0.23181362\n",
      "Iteration 65, loss = 0.23604449\n",
      "Iteration 66, loss = 0.23323048\n",
      "Iteration 67, loss = 0.22914986\n",
      "Iteration 68, loss = 0.22406366\n",
      "Iteration 69, loss = 0.22296962\n",
      "Iteration 70, loss = 0.22035657\n",
      "Iteration 71, loss = 0.22229551\n",
      "Iteration 72, loss = 0.21948648\n",
      "Iteration 73, loss = 0.21796878\n",
      "Iteration 74, loss = 0.21454335\n",
      "Iteration 75, loss = 0.21742506\n",
      "Iteration 76, loss = 0.21541431\n",
      "Iteration 77, loss = 0.21033350\n",
      "Iteration 78, loss = 0.20652337\n",
      "Iteration 79, loss = 0.20694601\n",
      "Iteration 80, loss = 0.20487129\n",
      "Iteration 81, loss = 0.20383873\n",
      "Iteration 82, loss = 0.20178036\n",
      "Iteration 83, loss = 0.20456970\n",
      "Iteration 84, loss = 0.19758906\n",
      "Iteration 85, loss = 0.19736491\n",
      "Iteration 86, loss = 0.19970726\n",
      "Iteration 87, loss = 0.19388829\n",
      "Iteration 88, loss = 0.19166222\n",
      "Iteration 89, loss = 0.19178337\n",
      "Iteration 90, loss = 0.19253191\n",
      "Iteration 91, loss = 0.18813562\n",
      "Iteration 92, loss = 0.18627201\n",
      "Iteration 93, loss = 0.18456025\n",
      "Iteration 94, loss = 0.18160607\n",
      "Iteration 95, loss = 0.18375740\n",
      "Iteration 96, loss = 0.17976037\n",
      "Iteration 97, loss = 0.17850992\n",
      "Iteration 98, loss = 0.17942856\n",
      "Iteration 99, loss = 0.17728405\n",
      "Iteration 100, loss = 0.18388602\n",
      "Iteration 101, loss = 0.18186580\n",
      "Iteration 102, loss = 0.17257705\n",
      "Iteration 103, loss = 0.16864845\n",
      "Iteration 104, loss = 0.16683922\n",
      "Iteration 105, loss = 0.16805139\n",
      "Iteration 106, loss = 0.16713154\n",
      "Iteration 107, loss = 0.16576485\n",
      "Iteration 108, loss = 0.16320125\n",
      "Iteration 109, loss = 0.16159558\n",
      "Iteration 110, loss = 0.15996637\n",
      "Iteration 111, loss = 0.16092269\n",
      "Iteration 112, loss = 0.16040082\n",
      "Iteration 113, loss = 0.15945426\n",
      "Iteration 114, loss = 0.15565854\n",
      "Iteration 115, loss = 0.15412658\n",
      "Iteration 116, loss = 0.15376327\n",
      "Iteration 117, loss = 0.15081201\n",
      "Iteration 118, loss = 0.14982296\n",
      "Iteration 119, loss = 0.14928711\n",
      "Iteration 120, loss = 0.14928330\n",
      "Iteration 121, loss = 0.14509805\n",
      "Iteration 122, loss = 0.14812530\n",
      "Iteration 123, loss = 0.14525227\n",
      "Iteration 124, loss = 0.14550792\n",
      "Iteration 125, loss = 0.14142746\n",
      "Iteration 126, loss = 0.14045108\n",
      "Iteration 127, loss = 0.13865496\n",
      "Iteration 128, loss = 0.13858059\n",
      "Iteration 129, loss = 0.13621878\n",
      "Iteration 130, loss = 0.13607831\n",
      "Iteration 131, loss = 0.13447246\n",
      "Iteration 132, loss = 0.13392179\n",
      "Iteration 133, loss = 0.13916388\n",
      "Iteration 134, loss = 0.13522862\n",
      "Iteration 135, loss = 0.13766835\n",
      "Iteration 136, loss = 0.12985973\n",
      "Iteration 137, loss = 0.12780969\n",
      "Iteration 138, loss = 0.12741186\n",
      "Iteration 139, loss = 0.12692727\n",
      "Iteration 140, loss = 0.12645221\n",
      "Iteration 141, loss = 0.12541283\n",
      "Iteration 142, loss = 0.12457631\n",
      "Iteration 143, loss = 0.12201388\n",
      "Iteration 144, loss = 0.12093357\n",
      "Iteration 145, loss = 0.12032495\n",
      "Iteration 146, loss = 0.12671530\n",
      "Iteration 147, loss = 0.12013088\n",
      "Iteration 148, loss = 0.11793176\n",
      "Iteration 149, loss = 0.11576283\n",
      "Iteration 150, loss = 0.11564594\n",
      "Iteration 151, loss = 0.11469655\n",
      "Iteration 152, loss = 0.11437047\n",
      "Iteration 153, loss = 0.11257371\n",
      "Iteration 154, loss = 0.11309510\n",
      "Iteration 155, loss = 0.11773435\n",
      "Iteration 156, loss = 0.11503403\n",
      "Iteration 157, loss = 0.11195932\n",
      "Iteration 158, loss = 0.11338373\n",
      "Iteration 159, loss = 0.11017474\n",
      "Iteration 160, loss = 0.10918200\n",
      "Iteration 161, loss = 0.10511984\n",
      "Iteration 162, loss = 0.10448026\n",
      "Iteration 163, loss = 0.10398594\n",
      "Iteration 164, loss = 0.10224941\n",
      "Iteration 165, loss = 0.10301971\n",
      "Iteration 166, loss = 0.10105866\n",
      "Iteration 167, loss = 0.10092748\n",
      "Iteration 168, loss = 0.10063365\n",
      "Iteration 169, loss = 0.09880156\n",
      "Iteration 170, loss = 0.09764035\n",
      "Iteration 171, loss = 0.09867181\n",
      "Iteration 172, loss = 0.09860281\n",
      "Iteration 173, loss = 0.09950562\n",
      "Iteration 174, loss = 0.09533798\n",
      "Iteration 175, loss = 0.09298186\n",
      "Iteration 176, loss = 0.09720365\n",
      "Iteration 177, loss = 0.09436036\n",
      "Iteration 178, loss = 0.09629124\n",
      "Iteration 179, loss = 0.09302864\n",
      "Iteration 180, loss = 0.09346838\n",
      "Iteration 181, loss = 0.09495649\n",
      "Iteration 182, loss = 0.09499882\n",
      "Iteration 183, loss = 0.08913770\n",
      "Iteration 184, loss = 0.08845398\n",
      "Iteration 185, loss = 0.08715449\n",
      "Iteration 186, loss = 0.08619217\n",
      "Iteration 187, loss = 0.08615888\n",
      "Iteration 188, loss = 0.08534979\n",
      "Iteration 189, loss = 0.08575758\n",
      "Iteration 190, loss = 0.08550338\n",
      "Iteration 191, loss = 0.08384470\n",
      "Iteration 192, loss = 0.08254455\n",
      "Iteration 193, loss = 0.08252468\n",
      "Iteration 194, loss = 0.08286789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 195, loss = 0.08596559\n",
      "Iteration 196, loss = 0.08365338\n",
      "Iteration 197, loss = 0.08115231\n",
      "Iteration 198, loss = 0.08059800\n",
      "Iteration 199, loss = 0.07945127\n",
      "Iteration 200, loss = 0.07906848\n",
      "Iteration 201, loss = 0.07757888\n",
      "Iteration 202, loss = 0.07647418\n",
      "Iteration 203, loss = 0.07543319\n",
      "Iteration 204, loss = 0.07640681\n",
      "Iteration 205, loss = 0.07674428\n",
      "Iteration 206, loss = 0.07486964\n",
      "Iteration 207, loss = 0.07504705\n",
      "Iteration 208, loss = 0.07448562\n",
      "Iteration 209, loss = 0.07211086\n",
      "Iteration 210, loss = 0.07230347\n",
      "Iteration 211, loss = 0.07386038\n",
      "Iteration 212, loss = 0.07137368\n",
      "Iteration 213, loss = 0.07023347\n",
      "Iteration 214, loss = 0.07084580\n",
      "Iteration 215, loss = 0.06892528\n",
      "Iteration 216, loss = 0.06966816\n",
      "Iteration 217, loss = 0.06877895\n",
      "Iteration 218, loss = 0.07088350\n",
      "Iteration 219, loss = 0.06934916\n",
      "Iteration 220, loss = 0.06669843\n",
      "Iteration 221, loss = 0.06636151\n",
      "Iteration 222, loss = 0.06561082\n",
      "Iteration 223, loss = 0.06598739\n",
      "Iteration 224, loss = 0.06491267\n",
      "Iteration 225, loss = 0.06508544\n",
      "Iteration 226, loss = 0.06422258\n",
      "Iteration 227, loss = 0.06551082\n",
      "Iteration 228, loss = 0.06487173\n",
      "Iteration 229, loss = 0.06261533\n",
      "Iteration 230, loss = 0.06151688\n",
      "Iteration 231, loss = 0.06206512\n",
      "Iteration 232, loss = 0.06147724\n",
      "Iteration 233, loss = 0.06204633\n",
      "Iteration 234, loss = 0.05951866\n",
      "Iteration 235, loss = 0.05943254\n",
      "Iteration 236, loss = 0.05871323\n",
      "Iteration 237, loss = 0.05909848\n",
      "Iteration 238, loss = 0.06028730\n",
      "Iteration 239, loss = 0.05841618\n",
      "Iteration 240, loss = 0.05831707\n",
      "Iteration 241, loss = 0.05905336\n",
      "Iteration 242, loss = 0.06268287\n",
      "Iteration 243, loss = 0.06131221\n",
      "Iteration 244, loss = 0.05635290\n",
      "Iteration 245, loss = 0.05566827\n",
      "Iteration 246, loss = 0.05566545\n",
      "Iteration 247, loss = 0.05700197\n",
      "Iteration 248, loss = 0.05517382\n",
      "Iteration 249, loss = 0.05393504\n",
      "Iteration 250, loss = 0.06112460\n",
      "Iteration 251, loss = 0.05985770\n",
      "Iteration 252, loss = 0.06168115\n",
      "Iteration 253, loss = 0.06692617\n",
      "Iteration 254, loss = 0.06242949\n",
      "Iteration 255, loss = 0.05494603\n",
      "Iteration 256, loss = 0.05410962\n",
      "Iteration 257, loss = 0.05180779\n",
      "Iteration 258, loss = 0.05212870\n",
      "Iteration 259, loss = 0.04983616\n",
      "Iteration 260, loss = 0.05006238\n",
      "Iteration 261, loss = 0.05056826\n",
      "Iteration 262, loss = 0.04969349\n",
      "Iteration 263, loss = 0.05056933\n",
      "Iteration 264, loss = 0.04948234\n",
      "Iteration 265, loss = 0.04849318\n",
      "Iteration 266, loss = 0.04820945\n",
      "Iteration 267, loss = 0.04891105\n",
      "Iteration 268, loss = 0.04808754\n",
      "Iteration 269, loss = 0.04745362\n",
      "Iteration 270, loss = 0.04640734\n",
      "Iteration 271, loss = 0.04886075\n",
      "Iteration 272, loss = 0.04646423\n",
      "Iteration 273, loss = 0.04617069\n",
      "Iteration 274, loss = 0.04592376\n",
      "Iteration 275, loss = 0.04650215\n",
      "Iteration 276, loss = 0.04718070\n",
      "Iteration 277, loss = 0.04544995\n",
      "Iteration 278, loss = 0.04467067\n",
      "Iteration 279, loss = 0.04371361\n",
      "Iteration 280, loss = 0.04362374\n",
      "Iteration 281, loss = 0.04352490\n",
      "Iteration 282, loss = 0.04220698\n",
      "Iteration 283, loss = 0.04229147\n",
      "Iteration 284, loss = 0.04161856\n",
      "Iteration 285, loss = 0.04156340\n",
      "Iteration 286, loss = 0.04092831\n",
      "Iteration 287, loss = 0.04218368\n",
      "Iteration 288, loss = 0.04391507\n",
      "Iteration 289, loss = 0.04243996\n",
      "Iteration 290, loss = 0.04217201\n",
      "Iteration 291, loss = 0.04158411\n",
      "Iteration 292, loss = 0.04133666\n",
      "Iteration 293, loss = 0.04150297\n",
      "Iteration 294, loss = 0.03984305\n",
      "Iteration 295, loss = 0.03980865\n",
      "Iteration 296, loss = 0.03969446\n",
      "Iteration 297, loss = 0.03804352\n",
      "Iteration 298, loss = 0.03862268\n",
      "Iteration 299, loss = 0.03730414\n",
      "Iteration 300, loss = 0.03780251\n",
      "Iteration 301, loss = 0.03744388\n",
      "Iteration 302, loss = 0.03747688\n",
      "Iteration 303, loss = 0.03762451\n",
      "Iteration 304, loss = 0.03680040\n",
      "Iteration 305, loss = 0.03634237\n",
      "Iteration 306, loss = 0.03707830\n",
      "Iteration 307, loss = 0.03725036\n",
      "Iteration 308, loss = 0.03631104\n",
      "Iteration 309, loss = 0.03555306\n",
      "Iteration 310, loss = 0.03653468\n",
      "Iteration 311, loss = 0.03533678\n",
      "Iteration 312, loss = 0.03441112\n",
      "Iteration 313, loss = 0.03515424\n",
      "Iteration 314, loss = 0.03406551\n",
      "Iteration 315, loss = 0.03412737\n",
      "Iteration 316, loss = 0.03428700\n",
      "Iteration 317, loss = 0.03387547\n",
      "Iteration 318, loss = 0.03365996\n",
      "Iteration 319, loss = 0.03405542\n",
      "Iteration 320, loss = 0.03453449\n",
      "Iteration 321, loss = 0.03320305\n",
      "Iteration 322, loss = 0.03303097\n",
      "Iteration 323, loss = 0.03281820\n",
      "Iteration 324, loss = 0.03249109\n",
      "Iteration 325, loss = 0.03162893\n",
      "Iteration 326, loss = 0.03197878\n",
      "Iteration 327, loss = 0.03318680\n",
      "Iteration 328, loss = 0.03240170\n",
      "Iteration 329, loss = 0.03150570\n",
      "Iteration 330, loss = 0.03096577\n",
      "Iteration 331, loss = 0.03096567\n",
      "Iteration 332, loss = 0.03118064\n",
      "Iteration 333, loss = 0.03095041\n",
      "Iteration 334, loss = 0.03021900\n",
      "Iteration 335, loss = 0.03109535\n",
      "Iteration 336, loss = 0.02985478\n",
      "Iteration 337, loss = 0.03033390\n",
      "Iteration 338, loss = 0.03008334\n",
      "Iteration 339, loss = 0.02902349\n",
      "Iteration 340, loss = 0.02895509\n",
      "Iteration 341, loss = 0.03132574\n",
      "Iteration 342, loss = 0.03062254\n",
      "Iteration 343, loss = 0.02967550\n",
      "Iteration 344, loss = 0.02811295\n",
      "Iteration 345, loss = 0.02824669\n",
      "Iteration 346, loss = 0.02802735\n",
      "Iteration 347, loss = 0.02758550\n",
      "Iteration 348, loss = 0.02757501\n",
      "Iteration 349, loss = 0.02741811\n",
      "Iteration 350, loss = 0.02785148\n",
      "Iteration 351, loss = 0.02675237\n",
      "Iteration 352, loss = 0.02857954\n",
      "Iteration 353, loss = 0.02819562\n",
      "Iteration 354, loss = 0.02743271\n",
      "Iteration 355, loss = 0.02815143\n",
      "Iteration 356, loss = 0.02684904\n",
      "Iteration 357, loss = 0.02916676\n",
      "Iteration 358, loss = 0.02699338\n",
      "Iteration 359, loss = 0.02571696\n",
      "Iteration 360, loss = 0.02623115\n",
      "Iteration 361, loss = 0.02594438\n",
      "Iteration 362, loss = 0.02518969\n",
      "Iteration 363, loss = 0.02573036\n",
      "Iteration 364, loss = 0.02787952\n",
      "Iteration 365, loss = 0.02513889\n",
      "Iteration 366, loss = 0.02437059\n",
      "Iteration 367, loss = 0.02421304\n",
      "Iteration 368, loss = 0.02412655\n",
      "Iteration 369, loss = 0.02376265\n",
      "Iteration 370, loss = 0.02370030\n",
      "Iteration 371, loss = 0.02337722\n",
      "Iteration 372, loss = 0.02384092\n",
      "Iteration 373, loss = 0.02329751\n",
      "Iteration 374, loss = 0.02332357\n",
      "Iteration 375, loss = 0.02336079\n",
      "Iteration 376, loss = 0.02364335\n",
      "Iteration 377, loss = 0.02341128\n",
      "Iteration 378, loss = 0.02276526\n",
      "Iteration 379, loss = 0.02400086\n",
      "Iteration 380, loss = 0.02397809\n",
      "Iteration 381, loss = 0.02327192\n",
      "Iteration 382, loss = 0.02246185\n",
      "Iteration 383, loss = 0.02171125\n",
      "Iteration 384, loss = 0.02202817\n",
      "Iteration 385, loss = 0.02173176\n",
      "Iteration 386, loss = 0.02138629\n",
      "Iteration 387, loss = 0.02460331\n",
      "Iteration 388, loss = 0.02218571\n",
      "Iteration 389, loss = 0.02314828\n",
      "Iteration 390, loss = 0.02260966\n",
      "Iteration 391, loss = 0.02226591\n",
      "Iteration 392, loss = 0.02115204\n",
      "Iteration 393, loss = 0.02201206\n",
      "Iteration 394, loss = 0.02252639\n",
      "Iteration 395, loss = 0.02479946\n",
      "Iteration 396, loss = 0.02176250\n",
      "Iteration 397, loss = 0.02228029\n",
      "Iteration 398, loss = 0.02049925\n",
      "Iteration 399, loss = 0.02005666\n",
      "Iteration 400, loss = 0.02168707\n",
      "Iteration 401, loss = 0.02074142\n",
      "Iteration 402, loss = 0.01955801\n",
      "Iteration 403, loss = 0.01933077\n",
      "Iteration 404, loss = 0.01909995\n",
      "Iteration 405, loss = 0.01929521\n",
      "Iteration 406, loss = 0.01903171\n",
      "Iteration 407, loss = 0.01892818\n",
      "Iteration 408, loss = 0.01974042\n",
      "Iteration 409, loss = 0.01941640\n",
      "Iteration 410, loss = 0.01845828\n",
      "Iteration 411, loss = 0.01842357\n",
      "Iteration 412, loss = 0.01854024\n",
      "Iteration 413, loss = 0.01915963\n",
      "Iteration 414, loss = 0.01901682\n",
      "Iteration 415, loss = 0.01876133\n",
      "Iteration 416, loss = 0.01799982\n",
      "Iteration 417, loss = 0.01802753\n",
      "Iteration 418, loss = 0.01825022\n",
      "Iteration 419, loss = 0.01826724\n",
      "Iteration 420, loss = 0.01790779\n",
      "Iteration 421, loss = 0.01780441\n",
      "Iteration 422, loss = 0.01833554\n",
      "Iteration 423, loss = 0.01780402\n",
      "Iteration 424, loss = 0.01791086\n",
      "Iteration 425, loss = 0.01715010\n",
      "Iteration 426, loss = 0.01719732\n",
      "Iteration 427, loss = 0.01791148\n",
      "Iteration 428, loss = 0.01841618\n",
      "Iteration 429, loss = 0.01780103\n",
      "Iteration 430, loss = 0.01814633\n",
      "Iteration 431, loss = 0.01779861\n",
      "Iteration 432, loss = 0.01689999\n",
      "Iteration 433, loss = 0.01650498\n",
      "Iteration 434, loss = 0.01633711\n",
      "Iteration 435, loss = 0.01648893\n",
      "Iteration 436, loss = 0.01595773\n",
      "Iteration 437, loss = 0.01616176\n",
      "Iteration 438, loss = 0.01607364\n",
      "Iteration 439, loss = 0.01595657\n",
      "Iteration 440, loss = 0.01611931\n",
      "Iteration 441, loss = 0.01615246\n",
      "Iteration 442, loss = 0.01612005\n",
      "Iteration 443, loss = 0.01587086\n",
      "Iteration 444, loss = 0.01641882\n",
      "Iteration 445, loss = 0.01730583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 446, loss = 0.01728139\n",
      "Iteration 447, loss = 0.01649309\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 12 of 48...\n",
      "Iteration 1, loss = 0.66402592\n",
      "Iteration 2, loss = 0.59063276\n",
      "Iteration 3, loss = 0.54612587\n",
      "Iteration 4, loss = 0.51469837\n",
      "Iteration 5, loss = 0.49032134\n",
      "Iteration 6, loss = 0.47157007\n",
      "Iteration 7, loss = 0.45613555\n",
      "Iteration 8, loss = 0.44365185\n",
      "Iteration 9, loss = 0.43930839\n",
      "Iteration 10, loss = 0.42109528\n",
      "Iteration 11, loss = 0.41461457\n",
      "Iteration 12, loss = 0.40404439\n",
      "Iteration 13, loss = 0.39526310\n",
      "Iteration 14, loss = 0.38960328\n",
      "Iteration 15, loss = 0.38223356\n",
      "Iteration 16, loss = 0.37691086\n",
      "Iteration 17, loss = 0.37345219\n",
      "Iteration 18, loss = 0.36869493\n",
      "Iteration 19, loss = 0.36172374\n",
      "Iteration 20, loss = 0.36021461\n",
      "Iteration 21, loss = 0.35210734\n",
      "Iteration 22, loss = 0.35327010\n",
      "Iteration 23, loss = 0.34482981\n",
      "Iteration 24, loss = 0.34147726\n",
      "Iteration 25, loss = 0.33725519\n",
      "Iteration 26, loss = 0.33525118\n",
      "Iteration 27, loss = 0.33669957\n",
      "Iteration 28, loss = 0.33578177\n",
      "Iteration 29, loss = 0.32721157\n",
      "Iteration 30, loss = 0.32178647\n",
      "Iteration 31, loss = 0.32464766\n",
      "Iteration 32, loss = 0.32072482\n",
      "Iteration 33, loss = 0.31279013\n",
      "Iteration 34, loss = 0.31491769\n",
      "Iteration 35, loss = 0.31566622\n",
      "Iteration 36, loss = 0.30787988\n",
      "Iteration 37, loss = 0.30284249\n",
      "Iteration 38, loss = 0.30154671\n",
      "Iteration 39, loss = 0.30274436\n",
      "Iteration 40, loss = 0.30176192\n",
      "Iteration 41, loss = 0.30818451\n",
      "Iteration 42, loss = 0.29412322\n",
      "Iteration 43, loss = 0.28938239\n",
      "Iteration 44, loss = 0.28447229\n",
      "Iteration 45, loss = 0.28697104\n",
      "Iteration 46, loss = 0.28269312\n",
      "Iteration 47, loss = 0.28133012\n",
      "Iteration 48, loss = 0.27947915\n",
      "Iteration 49, loss = 0.27718205\n",
      "Iteration 50, loss = 0.27443926\n",
      "Iteration 51, loss = 0.27314374\n",
      "Iteration 52, loss = 0.26968509\n",
      "Iteration 53, loss = 0.26894658\n",
      "Iteration 54, loss = 0.27250908\n",
      "Iteration 55, loss = 0.26527774\n",
      "Iteration 56, loss = 0.26271890\n",
      "Iteration 57, loss = 0.26122148\n",
      "Iteration 58, loss = 0.25855672\n",
      "Iteration 59, loss = 0.25797720\n",
      "Iteration 60, loss = 0.25984541\n",
      "Iteration 61, loss = 0.25291352\n",
      "Iteration 62, loss = 0.26241494\n",
      "Iteration 63, loss = 0.26329823\n",
      "Iteration 64, loss = 0.25638620\n",
      "Iteration 65, loss = 0.24832517\n",
      "Iteration 66, loss = 0.24509002\n",
      "Iteration 67, loss = 0.24234195\n",
      "Iteration 68, loss = 0.24731410\n",
      "Iteration 69, loss = 0.24382874\n",
      "Iteration 70, loss = 0.24131507\n",
      "Iteration 71, loss = 0.24450134\n",
      "Iteration 72, loss = 0.23730951\n",
      "Iteration 73, loss = 0.23597041\n",
      "Iteration 74, loss = 0.23001540\n",
      "Iteration 75, loss = 0.22974023\n",
      "Iteration 76, loss = 0.22880846\n",
      "Iteration 77, loss = 0.22741966\n",
      "Iteration 78, loss = 0.22526212\n",
      "Iteration 79, loss = 0.22302493\n",
      "Iteration 80, loss = 0.22321911\n",
      "Iteration 81, loss = 0.21904304\n",
      "Iteration 82, loss = 0.21771435\n",
      "Iteration 83, loss = 0.21607787\n",
      "Iteration 84, loss = 0.21735476\n",
      "Iteration 85, loss = 0.21471769\n",
      "Iteration 86, loss = 0.21380241\n",
      "Iteration 87, loss = 0.21046033\n",
      "Iteration 88, loss = 0.20782873\n",
      "Iteration 89, loss = 0.20875581\n",
      "Iteration 90, loss = 0.20908190\n",
      "Iteration 91, loss = 0.21353309\n",
      "Iteration 92, loss = 0.20671427\n",
      "Iteration 93, loss = 0.20783410\n",
      "Iteration 94, loss = 0.20233828\n",
      "Iteration 95, loss = 0.19852599\n",
      "Iteration 96, loss = 0.19874216\n",
      "Iteration 97, loss = 0.19690467\n",
      "Iteration 98, loss = 0.19507231\n",
      "Iteration 99, loss = 0.19286526\n",
      "Iteration 100, loss = 0.19342017\n",
      "Iteration 101, loss = 0.19237203\n",
      "Iteration 102, loss = 0.19653228\n",
      "Iteration 103, loss = 0.19665199\n",
      "Iteration 104, loss = 0.19379933\n",
      "Iteration 105, loss = 0.18746767\n",
      "Iteration 106, loss = 0.18667806\n",
      "Iteration 107, loss = 0.18798764\n",
      "Iteration 108, loss = 0.19478604\n",
      "Iteration 109, loss = 0.18673341\n",
      "Iteration 110, loss = 0.17906619\n",
      "Iteration 111, loss = 0.17889449\n",
      "Iteration 112, loss = 0.17607444\n",
      "Iteration 113, loss = 0.18227583\n",
      "Iteration 114, loss = 0.17769099\n",
      "Iteration 115, loss = 0.17520400\n",
      "Iteration 116, loss = 0.17721877\n",
      "Iteration 117, loss = 0.17567244\n",
      "Iteration 118, loss = 0.16959392\n",
      "Iteration 119, loss = 0.16791907\n",
      "Iteration 120, loss = 0.16840518\n",
      "Iteration 121, loss = 0.16628858\n",
      "Iteration 122, loss = 0.16623973\n",
      "Iteration 123, loss = 0.16301044\n",
      "Iteration 124, loss = 0.16077351\n",
      "Iteration 125, loss = 0.16060587\n",
      "Iteration 126, loss = 0.15937203\n",
      "Iteration 127, loss = 0.15773383\n",
      "Iteration 128, loss = 0.15736813\n",
      "Iteration 129, loss = 0.15591943\n",
      "Iteration 130, loss = 0.15497148\n",
      "Iteration 131, loss = 0.15395957\n",
      "Iteration 132, loss = 0.15194560\n",
      "Iteration 133, loss = 0.15238216\n",
      "Iteration 134, loss = 0.15152070\n",
      "Iteration 135, loss = 0.14921613\n",
      "Iteration 136, loss = 0.14824352\n",
      "Iteration 137, loss = 0.14694781\n",
      "Iteration 138, loss = 0.14842737\n",
      "Iteration 139, loss = 0.14477516\n",
      "Iteration 140, loss = 0.14377285\n",
      "Iteration 141, loss = 0.14253613\n",
      "Iteration 142, loss = 0.14219279\n",
      "Iteration 143, loss = 0.14178858\n",
      "Iteration 144, loss = 0.14223663\n",
      "Iteration 145, loss = 0.14270026\n",
      "Iteration 146, loss = 0.14218248\n",
      "Iteration 147, loss = 0.14096303\n",
      "Iteration 148, loss = 0.13771071\n",
      "Iteration 149, loss = 0.13464004\n",
      "Iteration 150, loss = 0.13498350\n",
      "Iteration 151, loss = 0.13675311\n",
      "Iteration 152, loss = 0.13176378\n",
      "Iteration 153, loss = 0.13112943\n",
      "Iteration 154, loss = 0.13302553\n",
      "Iteration 155, loss = 0.13425018\n",
      "Iteration 156, loss = 0.13083923\n",
      "Iteration 157, loss = 0.13206245\n",
      "Iteration 158, loss = 0.12869670\n",
      "Iteration 159, loss = 0.12735930\n",
      "Iteration 160, loss = 0.12654849\n",
      "Iteration 161, loss = 0.12493298\n",
      "Iteration 162, loss = 0.12296057\n",
      "Iteration 163, loss = 0.12509655\n",
      "Iteration 164, loss = 0.12442625\n",
      "Iteration 165, loss = 0.12056562\n",
      "Iteration 166, loss = 0.12057142\n",
      "Iteration 167, loss = 0.12016354\n",
      "Iteration 168, loss = 0.11983084\n",
      "Iteration 169, loss = 0.11911378\n",
      "Iteration 170, loss = 0.11635175\n",
      "Iteration 171, loss = 0.11754944\n",
      "Iteration 172, loss = 0.11657501\n",
      "Iteration 173, loss = 0.11451454\n",
      "Iteration 174, loss = 0.11385948\n",
      "Iteration 175, loss = 0.11283850\n",
      "Iteration 176, loss = 0.11352386\n",
      "Iteration 177, loss = 0.11231324\n",
      "Iteration 178, loss = 0.11116878\n",
      "Iteration 179, loss = 0.11103591\n",
      "Iteration 180, loss = 0.11163308\n",
      "Iteration 181, loss = 0.10865568\n",
      "Iteration 182, loss = 0.10858183\n",
      "Iteration 183, loss = 0.10648071\n",
      "Iteration 184, loss = 0.10577431\n",
      "Iteration 185, loss = 0.10570738\n",
      "Iteration 186, loss = 0.10681843\n",
      "Iteration 187, loss = 0.10388574\n",
      "Iteration 188, loss = 0.10540840\n",
      "Iteration 189, loss = 0.10500239\n",
      "Iteration 190, loss = 0.10271219\n",
      "Iteration 191, loss = 0.10137193\n",
      "Iteration 192, loss = 0.10031836\n",
      "Iteration 193, loss = 0.09972538\n",
      "Iteration 194, loss = 0.09922234\n",
      "Iteration 195, loss = 0.10170775\n",
      "Iteration 196, loss = 0.09993067\n",
      "Iteration 197, loss = 0.09754561\n",
      "Iteration 198, loss = 0.09789406\n",
      "Iteration 199, loss = 0.09688668\n",
      "Iteration 200, loss = 0.09625599\n",
      "Iteration 201, loss = 0.09573554\n",
      "Iteration 202, loss = 0.09362424\n",
      "Iteration 203, loss = 0.09376388\n",
      "Iteration 204, loss = 0.09381018\n",
      "Iteration 205, loss = 0.09168131\n",
      "Iteration 206, loss = 0.09128766\n",
      "Iteration 207, loss = 0.09202240\n",
      "Iteration 208, loss = 0.09082071\n",
      "Iteration 209, loss = 0.09092946\n",
      "Iteration 210, loss = 0.09019309\n",
      "Iteration 211, loss = 0.08941630\n",
      "Iteration 212, loss = 0.08797677\n",
      "Iteration 213, loss = 0.09202252\n",
      "Iteration 214, loss = 0.08857600\n",
      "Iteration 215, loss = 0.08869047\n",
      "Iteration 216, loss = 0.08636974\n",
      "Iteration 217, loss = 0.08566148\n",
      "Iteration 218, loss = 0.08538047\n",
      "Iteration 219, loss = 0.08597783\n",
      "Iteration 220, loss = 0.08385856\n",
      "Iteration 221, loss = 0.08257832\n",
      "Iteration 222, loss = 0.08416397\n",
      "Iteration 223, loss = 0.08300443\n",
      "Iteration 224, loss = 0.08449519\n",
      "Iteration 225, loss = 0.08252073\n",
      "Iteration 226, loss = 0.08047920\n",
      "Iteration 227, loss = 0.08016141\n",
      "Iteration 228, loss = 0.08025861\n",
      "Iteration 229, loss = 0.07955453\n",
      "Iteration 230, loss = 0.07866534\n",
      "Iteration 231, loss = 0.07798309\n",
      "Iteration 232, loss = 0.07898204\n",
      "Iteration 233, loss = 0.07759929\n",
      "Iteration 234, loss = 0.07789266\n",
      "Iteration 235, loss = 0.07723645\n",
      "Iteration 236, loss = 0.07855883\n",
      "Iteration 237, loss = 0.07880533\n",
      "Iteration 238, loss = 0.07606573\n",
      "Iteration 239, loss = 0.07586147\n",
      "Iteration 240, loss = 0.07440099\n",
      "Iteration 241, loss = 0.07428866\n",
      "Iteration 242, loss = 0.07424456\n",
      "Iteration 243, loss = 0.07366077\n",
      "Iteration 244, loss = 0.07356268\n",
      "Iteration 245, loss = 0.07232788\n",
      "Iteration 246, loss = 0.07531097\n",
      "Iteration 247, loss = 0.07235605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 248, loss = 0.07028113\n",
      "Iteration 249, loss = 0.07038563\n",
      "Iteration 250, loss = 0.07102012\n",
      "Iteration 251, loss = 0.07121401\n",
      "Iteration 252, loss = 0.06999069\n",
      "Iteration 253, loss = 0.06967700\n",
      "Iteration 254, loss = 0.06846928\n",
      "Iteration 255, loss = 0.06775683\n",
      "Iteration 256, loss = 0.06868919\n",
      "Iteration 257, loss = 0.06828681\n",
      "Iteration 258, loss = 0.06929448\n",
      "Iteration 259, loss = 0.06795320\n",
      "Iteration 260, loss = 0.06797120\n",
      "Iteration 261, loss = 0.06626789\n",
      "Iteration 262, loss = 0.06535498\n",
      "Iteration 263, loss = 0.06687716\n",
      "Iteration 264, loss = 0.07520339\n",
      "Iteration 265, loss = 0.07980074\n",
      "Iteration 266, loss = 0.07591720\n",
      "Iteration 267, loss = 0.06946639\n",
      "Iteration 268, loss = 0.06573812\n",
      "Iteration 269, loss = 0.06519737\n",
      "Iteration 270, loss = 0.06494976\n",
      "Iteration 271, loss = 0.06641594\n",
      "Iteration 272, loss = 0.06396428\n",
      "Iteration 273, loss = 0.06325105\n",
      "Iteration 274, loss = 0.06380441\n",
      "Iteration 275, loss = 0.06067065\n",
      "Iteration 276, loss = 0.06031480\n",
      "Iteration 277, loss = 0.06167162\n",
      "Iteration 278, loss = 0.06250940\n",
      "Iteration 279, loss = 0.06152978\n",
      "Iteration 280, loss = 0.06029421\n",
      "Iteration 281, loss = 0.06031728\n",
      "Iteration 282, loss = 0.05858139\n",
      "Iteration 283, loss = 0.05870123\n",
      "Iteration 284, loss = 0.05930093\n",
      "Iteration 285, loss = 0.05858348\n",
      "Iteration 286, loss = 0.05919255\n",
      "Iteration 287, loss = 0.05845460\n",
      "Iteration 288, loss = 0.05749559\n",
      "Iteration 289, loss = 0.05755748\n",
      "Iteration 290, loss = 0.05978892\n",
      "Iteration 291, loss = 0.05660434\n",
      "Iteration 292, loss = 0.05767101\n",
      "Iteration 293, loss = 0.05558790\n",
      "Iteration 294, loss = 0.05516426\n",
      "Iteration 295, loss = 0.05564940\n",
      "Iteration 296, loss = 0.05601693\n",
      "Iteration 297, loss = 0.05605092\n",
      "Iteration 298, loss = 0.05512829\n",
      "Iteration 299, loss = 0.05601526\n",
      "Iteration 300, loss = 0.05377687\n",
      "Iteration 301, loss = 0.05292621\n",
      "Iteration 302, loss = 0.05420575\n",
      "Iteration 303, loss = 0.05351467\n",
      "Iteration 304, loss = 0.05275351\n",
      "Iteration 305, loss = 0.05232932\n",
      "Iteration 306, loss = 0.05155209\n",
      "Iteration 307, loss = 0.05256296\n",
      "Iteration 308, loss = 0.05238131\n",
      "Iteration 309, loss = 0.05295755\n",
      "Iteration 310, loss = 0.05101567\n",
      "Iteration 311, loss = 0.05112929\n",
      "Iteration 312, loss = 0.05040236\n",
      "Iteration 313, loss = 0.05014048\n",
      "Iteration 314, loss = 0.05027551\n",
      "Iteration 315, loss = 0.04962945\n",
      "Iteration 316, loss = 0.04935507\n",
      "Iteration 317, loss = 0.05135853\n",
      "Iteration 318, loss = 0.05013593\n",
      "Iteration 319, loss = 0.04949434\n",
      "Iteration 320, loss = 0.04853770\n",
      "Iteration 321, loss = 0.04851041\n",
      "Iteration 322, loss = 0.04804083\n",
      "Iteration 323, loss = 0.04811708\n",
      "Iteration 324, loss = 0.04789508\n",
      "Iteration 325, loss = 0.04875351\n",
      "Iteration 326, loss = 0.04845124\n",
      "Iteration 327, loss = 0.04986053\n",
      "Iteration 328, loss = 0.05051435\n",
      "Iteration 329, loss = 0.04681264\n",
      "Iteration 330, loss = 0.04717751\n",
      "Iteration 331, loss = 0.04642415\n",
      "Iteration 332, loss = 0.04721546\n",
      "Iteration 333, loss = 0.04810083\n",
      "Iteration 334, loss = 0.04793564\n",
      "Iteration 335, loss = 0.04755638\n",
      "Iteration 336, loss = 0.04883396\n",
      "Iteration 337, loss = 0.04585552\n",
      "Iteration 338, loss = 0.04542862\n",
      "Iteration 339, loss = 0.04486099\n",
      "Iteration 340, loss = 0.04574423\n",
      "Iteration 341, loss = 0.04634491\n",
      "Iteration 342, loss = 0.04586330\n",
      "Iteration 343, loss = 0.04585707\n",
      "Iteration 344, loss = 0.04380515\n",
      "Iteration 345, loss = 0.04398272\n",
      "Iteration 346, loss = 0.04401310\n",
      "Iteration 347, loss = 0.04456522\n",
      "Iteration 348, loss = 0.04545242\n",
      "Iteration 349, loss = 0.04491054\n",
      "Iteration 350, loss = 0.04360078\n",
      "Iteration 351, loss = 0.04352637\n",
      "Iteration 352, loss = 0.04306609\n",
      "Iteration 353, loss = 0.05091937\n",
      "Iteration 354, loss = 0.05478889\n",
      "Iteration 355, loss = 0.04950839\n",
      "Iteration 356, loss = 0.04854584\n",
      "Iteration 357, loss = 0.05059087\n",
      "Iteration 358, loss = 0.04963232\n",
      "Iteration 359, loss = 0.04394639\n",
      "Iteration 360, loss = 0.04229511\n",
      "Iteration 361, loss = 0.04186271\n",
      "Iteration 362, loss = 0.04120161\n",
      "Iteration 363, loss = 0.04072397\n",
      "Iteration 364, loss = 0.04042205\n",
      "Iteration 365, loss = 0.04021362\n",
      "Iteration 366, loss = 0.03991556\n",
      "Iteration 367, loss = 0.04046261\n",
      "Iteration 368, loss = 0.04068081\n",
      "Iteration 369, loss = 0.04060943\n",
      "Iteration 370, loss = 0.04348544\n",
      "Iteration 371, loss = 0.04000494\n",
      "Iteration 372, loss = 0.04022921\n",
      "Iteration 373, loss = 0.03891158\n",
      "Iteration 374, loss = 0.04061773\n",
      "Iteration 375, loss = 0.03917408\n",
      "Iteration 376, loss = 0.04023719\n",
      "Iteration 377, loss = 0.03946882\n",
      "Iteration 378, loss = 0.03898914\n",
      "Iteration 379, loss = 0.03828473\n",
      "Iteration 380, loss = 0.03768418\n",
      "Iteration 381, loss = 0.03776561\n",
      "Iteration 382, loss = 0.03764582\n",
      "Iteration 383, loss = 0.03909236\n",
      "Iteration 384, loss = 0.03948002\n",
      "Iteration 385, loss = 0.04095705\n",
      "Iteration 386, loss = 0.03915025\n",
      "Iteration 387, loss = 0.03839936\n",
      "Iteration 388, loss = 0.03723441\n",
      "Iteration 389, loss = 0.03820833\n",
      "Iteration 390, loss = 0.03754044\n",
      "Iteration 391, loss = 0.03773853\n",
      "Iteration 392, loss = 0.03690422\n",
      "Iteration 393, loss = 0.03647802\n",
      "Iteration 394, loss = 0.03592693\n",
      "Iteration 395, loss = 0.03703852\n",
      "Iteration 396, loss = 0.03617632\n",
      "Iteration 397, loss = 0.03539033\n",
      "Iteration 398, loss = 0.03621253\n",
      "Iteration 399, loss = 0.03579700\n",
      "Iteration 400, loss = 0.03572442\n",
      "Iteration 401, loss = 0.03542601\n",
      "Iteration 402, loss = 0.03550452\n",
      "Iteration 403, loss = 0.03535869\n",
      "Iteration 404, loss = 0.03522642\n",
      "Iteration 405, loss = 0.03451765\n",
      "Iteration 406, loss = 0.03453775\n",
      "Iteration 407, loss = 0.03460007\n",
      "Iteration 408, loss = 0.03419504\n",
      "Iteration 409, loss = 0.03648576\n",
      "Iteration 410, loss = 0.03570105\n",
      "Iteration 411, loss = 0.03490024\n",
      "Iteration 412, loss = 0.03418892\n",
      "Iteration 413, loss = 0.03402830\n",
      "Iteration 414, loss = 0.03503799\n",
      "Iteration 415, loss = 0.03391179\n",
      "Iteration 416, loss = 0.03379104\n",
      "Iteration 417, loss = 0.03417682\n",
      "Iteration 418, loss = 0.03399289\n",
      "Iteration 419, loss = 0.03347024\n",
      "Iteration 420, loss = 0.03407862\n",
      "Iteration 421, loss = 0.03333740\n",
      "Iteration 422, loss = 0.03314092\n",
      "Iteration 423, loss = 0.03395928\n",
      "Iteration 424, loss = 0.03398526\n",
      "Iteration 425, loss = 0.03263442\n",
      "Iteration 426, loss = 0.03328026\n",
      "Iteration 427, loss = 0.03245273\n",
      "Iteration 428, loss = 0.03192703\n",
      "Iteration 429, loss = 0.03247573\n",
      "Iteration 430, loss = 0.03257196\n",
      "Iteration 431, loss = 0.03216717\n",
      "Iteration 432, loss = 0.03205131\n",
      "Iteration 433, loss = 0.03137241\n",
      "Iteration 434, loss = 0.03162501\n",
      "Iteration 435, loss = 0.03222343\n",
      "Iteration 436, loss = 0.03134232\n",
      "Iteration 437, loss = 0.03151392\n",
      "Iteration 438, loss = 0.03139738\n",
      "Iteration 439, loss = 0.03355719\n",
      "Iteration 440, loss = 0.03218941\n",
      "Iteration 441, loss = 0.03216388\n",
      "Iteration 442, loss = 0.03057263\n",
      "Iteration 443, loss = 0.03052685\n",
      "Iteration 444, loss = 0.03076839\n",
      "Iteration 445, loss = 0.03068573\n",
      "Iteration 446, loss = 0.03093848\n",
      "Iteration 447, loss = 0.03033570\n",
      "Iteration 448, loss = 0.03042680\n",
      "Iteration 449, loss = 0.03010543\n",
      "Iteration 450, loss = 0.03033216\n",
      "Iteration 451, loss = 0.03006273\n",
      "Iteration 452, loss = 0.02995380\n",
      "Iteration 453, loss = 0.02961318\n",
      "Iteration 454, loss = 0.02921848\n",
      "Iteration 455, loss = 0.03017295\n",
      "Iteration 456, loss = 0.03078023\n",
      "Iteration 457, loss = 0.02899174\n",
      "Iteration 458, loss = 0.02905303\n",
      "Iteration 459, loss = 0.02943480\n",
      "Iteration 460, loss = 0.03046247\n",
      "Iteration 461, loss = 0.02926960\n",
      "Iteration 462, loss = 0.02929155\n",
      "Iteration 463, loss = 0.02990501\n",
      "Iteration 464, loss = 0.02960048\n",
      "Iteration 465, loss = 0.02869134\n",
      "Iteration 466, loss = 0.02851857\n",
      "Iteration 467, loss = 0.02876914\n",
      "Iteration 468, loss = 0.02869267\n",
      "Iteration 469, loss = 0.02990695\n",
      "Iteration 470, loss = 0.02888460\n",
      "Iteration 471, loss = 0.02799568\n",
      "Iteration 472, loss = 0.02815694\n",
      "Iteration 473, loss = 0.02841757\n",
      "Iteration 474, loss = 0.02761670\n",
      "Iteration 475, loss = 0.02828204\n",
      "Iteration 476, loss = 0.02747477\n",
      "Iteration 477, loss = 0.02793085\n",
      "Iteration 478, loss = 0.02772702\n",
      "Iteration 479, loss = 0.02805007\n",
      "Iteration 480, loss = 0.02810242\n",
      "Iteration 481, loss = 0.02785859\n",
      "Iteration 482, loss = 0.02802969\n",
      "Iteration 483, loss = 0.02884902\n",
      "Iteration 484, loss = 0.02886042\n",
      "Iteration 485, loss = 0.02768673\n",
      "Iteration 486, loss = 0.02776450\n",
      "Iteration 487, loss = 0.02790637\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 13 of 48...\n",
      "Iteration 1, loss = 0.68323358\n",
      "Iteration 2, loss = 0.64689725\n",
      "Iteration 3, loss = 0.61988763\n",
      "Iteration 4, loss = 0.59782459\n",
      "Iteration 5, loss = 0.57616301\n",
      "Iteration 6, loss = 0.55695992\n",
      "Iteration 7, loss = 0.53993272\n",
      "Iteration 8, loss = 0.52297770\n",
      "Iteration 9, loss = 0.50842121\n",
      "Iteration 10, loss = 0.49448711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.48394975\n",
      "Iteration 12, loss = 0.47372378\n",
      "Iteration 13, loss = 0.46582828\n",
      "Iteration 14, loss = 0.45357785\n",
      "Iteration 15, loss = 0.44529508\n",
      "Iteration 16, loss = 0.43799733\n",
      "Iteration 17, loss = 0.43136783\n",
      "Iteration 18, loss = 0.42447458\n",
      "Iteration 19, loss = 0.41850351\n",
      "Iteration 20, loss = 0.41378759\n",
      "Iteration 21, loss = 0.40924558\n",
      "Iteration 22, loss = 0.40431791\n",
      "Iteration 23, loss = 0.39910408\n",
      "Iteration 24, loss = 0.39547890\n",
      "Iteration 25, loss = 0.39286637\n",
      "Iteration 26, loss = 0.39130619\n",
      "Iteration 27, loss = 0.38907964\n",
      "Iteration 28, loss = 0.38297372\n",
      "Iteration 29, loss = 0.37784819\n",
      "Iteration 30, loss = 0.37543576\n",
      "Iteration 31, loss = 0.37422323\n",
      "Iteration 32, loss = 0.37050896\n",
      "Iteration 33, loss = 0.36874344\n",
      "Iteration 34, loss = 0.36508560\n",
      "Iteration 35, loss = 0.36218144\n",
      "Iteration 36, loss = 0.36048927\n",
      "Iteration 37, loss = 0.35766308\n",
      "Iteration 38, loss = 0.35678755\n",
      "Iteration 39, loss = 0.35272711\n",
      "Iteration 40, loss = 0.35188560\n",
      "Iteration 41, loss = 0.34933828\n",
      "Iteration 42, loss = 0.34680461\n",
      "Iteration 43, loss = 0.34562745\n",
      "Iteration 44, loss = 0.34257483\n",
      "Iteration 45, loss = 0.34259877\n",
      "Iteration 46, loss = 0.34032168\n",
      "Iteration 47, loss = 0.33827819\n",
      "Iteration 48, loss = 0.33713085\n",
      "Iteration 49, loss = 0.33381248\n",
      "Iteration 50, loss = 0.33545397\n",
      "Iteration 51, loss = 0.33002584\n",
      "Iteration 52, loss = 0.32856593\n",
      "Iteration 53, loss = 0.32691727\n",
      "Iteration 54, loss = 0.32745972\n",
      "Iteration 55, loss = 0.32638810\n",
      "Iteration 56, loss = 0.32505932\n",
      "Iteration 57, loss = 0.32409211\n",
      "Iteration 58, loss = 0.32493958\n",
      "Iteration 59, loss = 0.31990001\n",
      "Iteration 60, loss = 0.31687739\n",
      "Iteration 61, loss = 0.31523555\n",
      "Iteration 62, loss = 0.31507935\n",
      "Iteration 63, loss = 0.31478612\n",
      "Iteration 64, loss = 0.31172271\n",
      "Iteration 65, loss = 0.31103297\n",
      "Iteration 66, loss = 0.30966602\n",
      "Iteration 67, loss = 0.30823686\n",
      "Iteration 68, loss = 0.30925106\n",
      "Iteration 69, loss = 0.30864465\n",
      "Iteration 70, loss = 0.30673544\n",
      "Iteration 71, loss = 0.30307534\n",
      "Iteration 72, loss = 0.30195137\n",
      "Iteration 73, loss = 0.30066613\n",
      "Iteration 74, loss = 0.29971436\n",
      "Iteration 75, loss = 0.29850981\n",
      "Iteration 76, loss = 0.29872049\n",
      "Iteration 77, loss = 0.29740206\n",
      "Iteration 78, loss = 0.29397347\n",
      "Iteration 79, loss = 0.29458304\n",
      "Iteration 80, loss = 0.29400130\n",
      "Iteration 81, loss = 0.29269887\n",
      "Iteration 82, loss = 0.29286010\n",
      "Iteration 83, loss = 0.28992054\n",
      "Iteration 84, loss = 0.28963618\n",
      "Iteration 85, loss = 0.28727798\n",
      "Iteration 86, loss = 0.28812153\n",
      "Iteration 87, loss = 0.28614848\n",
      "Iteration 88, loss = 0.28446051\n",
      "Iteration 89, loss = 0.28887307\n",
      "Iteration 90, loss = 0.28512412\n",
      "Iteration 91, loss = 0.28096488\n",
      "Iteration 92, loss = 0.28210384\n",
      "Iteration 93, loss = 0.27993908\n",
      "Iteration 94, loss = 0.28020664\n",
      "Iteration 95, loss = 0.27939835\n",
      "Iteration 96, loss = 0.27985140\n",
      "Iteration 97, loss = 0.27754408\n",
      "Iteration 98, loss = 0.28258598\n",
      "Iteration 99, loss = 0.27444101\n",
      "Iteration 100, loss = 0.27380424\n",
      "Iteration 101, loss = 0.27254174\n",
      "Iteration 102, loss = 0.27151410\n",
      "Iteration 103, loss = 0.27082233\n",
      "Iteration 104, loss = 0.27023642\n",
      "Iteration 105, loss = 0.26901933\n",
      "Iteration 106, loss = 0.26877099\n",
      "Iteration 107, loss = 0.26740683\n",
      "Iteration 108, loss = 0.27319498\n",
      "Iteration 109, loss = 0.26943898\n",
      "Iteration 110, loss = 0.26733831\n",
      "Iteration 111, loss = 0.26612102\n",
      "Iteration 112, loss = 0.26702264\n",
      "Iteration 113, loss = 0.26275801\n",
      "Iteration 114, loss = 0.26278080\n",
      "Iteration 115, loss = 0.26236007\n",
      "Iteration 116, loss = 0.27036701\n",
      "Iteration 117, loss = 0.26514419\n",
      "Iteration 118, loss = 0.26457711\n",
      "Iteration 119, loss = 0.26188901\n",
      "Iteration 120, loss = 0.25691632\n",
      "Iteration 121, loss = 0.25975034\n",
      "Iteration 122, loss = 0.25750932\n",
      "Iteration 123, loss = 0.25528169\n",
      "Iteration 124, loss = 0.25517795\n",
      "Iteration 125, loss = 0.25478116\n",
      "Iteration 126, loss = 0.25365890\n",
      "Iteration 127, loss = 0.25328759\n",
      "Iteration 128, loss = 0.25324770\n",
      "Iteration 129, loss = 0.25324649\n",
      "Iteration 130, loss = 0.25371306\n",
      "Iteration 131, loss = 0.25282401\n",
      "Iteration 132, loss = 0.24861484\n",
      "Iteration 133, loss = 0.24990338\n",
      "Iteration 134, loss = 0.24883724\n",
      "Iteration 135, loss = 0.24897626\n",
      "Iteration 136, loss = 0.24693561\n",
      "Iteration 137, loss = 0.24606338\n",
      "Iteration 138, loss = 0.24557926\n",
      "Iteration 139, loss = 0.24581946\n",
      "Iteration 140, loss = 0.24411669\n",
      "Iteration 141, loss = 0.24409636\n",
      "Iteration 142, loss = 0.24418591\n",
      "Iteration 143, loss = 0.24811589\n",
      "Iteration 144, loss = 0.24949772\n",
      "Iteration 145, loss = 0.24460112\n",
      "Iteration 146, loss = 0.24593832\n",
      "Iteration 147, loss = 0.24296899\n",
      "Iteration 148, loss = 0.24233762\n",
      "Iteration 149, loss = 0.24115842\n",
      "Iteration 150, loss = 0.24083162\n",
      "Iteration 151, loss = 0.23837116\n",
      "Iteration 152, loss = 0.23865512\n",
      "Iteration 153, loss = 0.23697046\n",
      "Iteration 154, loss = 0.23629915\n",
      "Iteration 155, loss = 0.23730116\n",
      "Iteration 156, loss = 0.23669198\n",
      "Iteration 157, loss = 0.23509560\n",
      "Iteration 158, loss = 0.23533652\n",
      "Iteration 159, loss = 0.23305289\n",
      "Iteration 160, loss = 0.23299739\n",
      "Iteration 161, loss = 0.23319105\n",
      "Iteration 162, loss = 0.23365878\n",
      "Iteration 163, loss = 0.23227572\n",
      "Iteration 164, loss = 0.23101782\n",
      "Iteration 165, loss = 0.22962845\n",
      "Iteration 166, loss = 0.22994112\n",
      "Iteration 167, loss = 0.23300295\n",
      "Iteration 168, loss = 0.23583201\n",
      "Iteration 169, loss = 0.23230578\n",
      "Iteration 170, loss = 0.22781509\n",
      "Iteration 171, loss = 0.22697586\n",
      "Iteration 172, loss = 0.22673711\n",
      "Iteration 173, loss = 0.22850756\n",
      "Iteration 174, loss = 0.23082356\n",
      "Iteration 175, loss = 0.22517895\n",
      "Iteration 176, loss = 0.22385932\n",
      "Iteration 177, loss = 0.22497755\n",
      "Iteration 178, loss = 0.22572676\n",
      "Iteration 179, loss = 0.22329805\n",
      "Iteration 180, loss = 0.22432489\n",
      "Iteration 181, loss = 0.22640723\n",
      "Iteration 182, loss = 0.23155085\n",
      "Iteration 183, loss = 0.22307497\n",
      "Iteration 184, loss = 0.22215980\n",
      "Iteration 185, loss = 0.22118804\n",
      "Iteration 186, loss = 0.22032667\n",
      "Iteration 187, loss = 0.22103876\n",
      "Iteration 188, loss = 0.21784071\n",
      "Iteration 189, loss = 0.21845566\n",
      "Iteration 190, loss = 0.21696980\n",
      "Iteration 191, loss = 0.21780979\n",
      "Iteration 192, loss = 0.21533002\n",
      "Iteration 193, loss = 0.22120204\n",
      "Iteration 194, loss = 0.21652396\n",
      "Iteration 195, loss = 0.21479864\n",
      "Iteration 196, loss = 0.21439652\n",
      "Iteration 197, loss = 0.21400177\n",
      "Iteration 198, loss = 0.21370863\n",
      "Iteration 199, loss = 0.21366397\n",
      "Iteration 200, loss = 0.21215962\n",
      "Iteration 201, loss = 0.21137871\n",
      "Iteration 202, loss = 0.21397189\n",
      "Iteration 203, loss = 0.21318476\n",
      "Iteration 204, loss = 0.21041145\n",
      "Iteration 205, loss = 0.21009651\n",
      "Iteration 206, loss = 0.21138818\n",
      "Iteration 207, loss = 0.20992621\n",
      "Iteration 208, loss = 0.20972326\n",
      "Iteration 209, loss = 0.20840197\n",
      "Iteration 210, loss = 0.20787880\n",
      "Iteration 211, loss = 0.20760330\n",
      "Iteration 212, loss = 0.20828716\n",
      "Iteration 213, loss = 0.20721609\n",
      "Iteration 214, loss = 0.20713200\n",
      "Iteration 215, loss = 0.21051105\n",
      "Iteration 216, loss = 0.20750899\n",
      "Iteration 217, loss = 0.20558585\n",
      "Iteration 218, loss = 0.20440079\n",
      "Iteration 219, loss = 0.20298533\n",
      "Iteration 220, loss = 0.20429931\n",
      "Iteration 221, loss = 0.20323579\n",
      "Iteration 222, loss = 0.20344471\n",
      "Iteration 223, loss = 0.20196992\n",
      "Iteration 224, loss = 0.20191083\n",
      "Iteration 225, loss = 0.20045923\n",
      "Iteration 226, loss = 0.20251624\n",
      "Iteration 227, loss = 0.19965254\n",
      "Iteration 228, loss = 0.20141997\n",
      "Iteration 229, loss = 0.19817610\n",
      "Iteration 230, loss = 0.19883103\n",
      "Iteration 231, loss = 0.19956191\n",
      "Iteration 232, loss = 0.19921891\n",
      "Iteration 233, loss = 0.19669286\n",
      "Iteration 234, loss = 0.19661028\n",
      "Iteration 235, loss = 0.19859596\n",
      "Iteration 236, loss = 0.19595198\n",
      "Iteration 237, loss = 0.19507303\n",
      "Iteration 238, loss = 0.19568577\n",
      "Iteration 239, loss = 0.19563034\n",
      "Iteration 240, loss = 0.19239113\n",
      "Iteration 241, loss = 0.19370101\n",
      "Iteration 242, loss = 0.19235102\n",
      "Iteration 243, loss = 0.19122227\n",
      "Iteration 244, loss = 0.19034951\n",
      "Iteration 245, loss = 0.18925403\n",
      "Iteration 246, loss = 0.19045671\n",
      "Iteration 247, loss = 0.18918339\n",
      "Iteration 248, loss = 0.18932512\n",
      "Iteration 249, loss = 0.19179355\n",
      "Iteration 250, loss = 0.18770141\n",
      "Iteration 251, loss = 0.18779929\n",
      "Iteration 252, loss = 0.18731509\n",
      "Iteration 253, loss = 0.18748473\n",
      "Iteration 254, loss = 0.18607765\n",
      "Iteration 255, loss = 0.18492690\n",
      "Iteration 256, loss = 0.18698267\n",
      "Iteration 257, loss = 0.18565914\n",
      "Iteration 258, loss = 0.18557533\n",
      "Iteration 259, loss = 0.18495585\n",
      "Iteration 260, loss = 0.18356749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 261, loss = 0.18397501\n",
      "Iteration 262, loss = 0.18328370\n",
      "Iteration 263, loss = 0.18143380\n",
      "Iteration 264, loss = 0.18180352\n",
      "Iteration 265, loss = 0.18305326\n",
      "Iteration 266, loss = 0.18533187\n",
      "Iteration 267, loss = 0.17864069\n",
      "Iteration 268, loss = 0.17979161\n",
      "Iteration 269, loss = 0.17838694\n",
      "Iteration 270, loss = 0.17750123\n",
      "Iteration 271, loss = 0.17769747\n",
      "Iteration 272, loss = 0.17843142\n",
      "Iteration 273, loss = 0.17826943\n",
      "Iteration 274, loss = 0.17795986\n",
      "Iteration 275, loss = 0.17695698\n",
      "Iteration 276, loss = 0.17597384\n",
      "Iteration 277, loss = 0.17602290\n",
      "Iteration 278, loss = 0.17434685\n",
      "Iteration 279, loss = 0.17257780\n",
      "Iteration 280, loss = 0.17327117\n",
      "Iteration 281, loss = 0.17451931\n",
      "Iteration 282, loss = 0.17733573\n",
      "Iteration 283, loss = 0.17180139\n",
      "Iteration 284, loss = 0.17186990\n",
      "Iteration 285, loss = 0.17063760\n",
      "Iteration 286, loss = 0.17012656\n",
      "Iteration 287, loss = 0.17005011\n",
      "Iteration 288, loss = 0.17160114\n",
      "Iteration 289, loss = 0.16944901\n",
      "Iteration 290, loss = 0.16742732\n",
      "Iteration 291, loss = 0.16738175\n",
      "Iteration 292, loss = 0.16878664\n",
      "Iteration 293, loss = 0.16565527\n",
      "Iteration 294, loss = 0.16589699\n",
      "Iteration 295, loss = 0.16627150\n",
      "Iteration 296, loss = 0.16511052\n",
      "Iteration 297, loss = 0.16486264\n",
      "Iteration 298, loss = 0.16518213\n",
      "Iteration 299, loss = 0.16434085\n",
      "Iteration 300, loss = 0.16212151\n",
      "Iteration 301, loss = 0.16192805\n",
      "Iteration 302, loss = 0.16202519\n",
      "Iteration 303, loss = 0.16161033\n",
      "Iteration 304, loss = 0.16311398\n",
      "Iteration 305, loss = 0.16281333\n",
      "Iteration 306, loss = 0.16072561\n",
      "Iteration 307, loss = 0.15942326\n",
      "Iteration 308, loss = 0.16005837\n",
      "Iteration 309, loss = 0.15998247\n",
      "Iteration 310, loss = 0.15810082\n",
      "Iteration 311, loss = 0.15960302\n",
      "Iteration 312, loss = 0.15818334\n",
      "Iteration 313, loss = 0.15761129\n",
      "Iteration 314, loss = 0.15878317\n",
      "Iteration 315, loss = 0.15659145\n",
      "Iteration 316, loss = 0.15535136\n",
      "Iteration 317, loss = 0.15388268\n",
      "Iteration 318, loss = 0.15432771\n",
      "Iteration 319, loss = 0.15273300\n",
      "Iteration 320, loss = 0.15287414\n",
      "Iteration 321, loss = 0.15283796\n",
      "Iteration 322, loss = 0.15273528\n",
      "Iteration 323, loss = 0.15668528\n",
      "Iteration 324, loss = 0.15005468\n",
      "Iteration 325, loss = 0.15059612\n",
      "Iteration 326, loss = 0.14978146\n",
      "Iteration 327, loss = 0.14970576\n",
      "Iteration 328, loss = 0.14849745\n",
      "Iteration 329, loss = 0.14811765\n",
      "Iteration 330, loss = 0.14790731\n",
      "Iteration 331, loss = 0.14699949\n",
      "Iteration 332, loss = 0.14773444\n",
      "Iteration 333, loss = 0.14860617\n",
      "Iteration 334, loss = 0.14706091\n",
      "Iteration 335, loss = 0.14570485\n",
      "Iteration 336, loss = 0.14530068\n",
      "Iteration 337, loss = 0.14462023\n",
      "Iteration 338, loss = 0.14354098\n",
      "Iteration 339, loss = 0.14519508\n",
      "Iteration 340, loss = 0.14582180\n",
      "Iteration 341, loss = 0.14340178\n",
      "Iteration 342, loss = 0.14238269\n",
      "Iteration 343, loss = 0.14083177\n",
      "Iteration 344, loss = 0.14202444\n",
      "Iteration 345, loss = 0.14246524\n",
      "Iteration 346, loss = 0.14440058\n",
      "Iteration 347, loss = 0.14066604\n",
      "Iteration 348, loss = 0.14040860\n",
      "Iteration 349, loss = 0.14156350\n",
      "Iteration 350, loss = 0.13986251\n",
      "Iteration 351, loss = 0.13737517\n",
      "Iteration 352, loss = 0.13767986\n",
      "Iteration 353, loss = 0.13668671\n",
      "Iteration 354, loss = 0.14023912\n",
      "Iteration 355, loss = 0.13892020\n",
      "Iteration 356, loss = 0.13640520\n",
      "Iteration 357, loss = 0.13483733\n",
      "Iteration 358, loss = 0.13466004\n",
      "Iteration 359, loss = 0.13383207\n",
      "Iteration 360, loss = 0.13334514\n",
      "Iteration 361, loss = 0.13350739\n",
      "Iteration 362, loss = 0.13330199\n",
      "Iteration 363, loss = 0.13170831\n",
      "Iteration 364, loss = 0.13145040\n",
      "Iteration 365, loss = 0.13135335\n",
      "Iteration 366, loss = 0.13009945\n",
      "Iteration 367, loss = 0.12976436\n",
      "Iteration 368, loss = 0.12942243\n",
      "Iteration 369, loss = 0.12915404\n",
      "Iteration 370, loss = 0.12992062\n",
      "Iteration 371, loss = 0.12905433\n",
      "Iteration 372, loss = 0.12856426\n",
      "Iteration 373, loss = 0.12703915\n",
      "Iteration 374, loss = 0.12702420\n",
      "Iteration 375, loss = 0.12762049\n",
      "Iteration 376, loss = 0.12682362\n",
      "Iteration 377, loss = 0.12465720\n",
      "Iteration 378, loss = 0.12482729\n",
      "Iteration 379, loss = 0.12490157\n",
      "Iteration 380, loss = 0.12444841\n",
      "Iteration 381, loss = 0.12390882\n",
      "Iteration 382, loss = 0.12290023\n",
      "Iteration 383, loss = 0.12382136\n",
      "Iteration 384, loss = 0.12198061\n",
      "Iteration 385, loss = 0.12232665\n",
      "Iteration 386, loss = 0.12222126\n",
      "Iteration 387, loss = 0.12072721\n",
      "Iteration 388, loss = 0.11993561\n",
      "Iteration 389, loss = 0.12059161\n",
      "Iteration 390, loss = 0.11859271\n",
      "Iteration 391, loss = 0.11844221\n",
      "Iteration 392, loss = 0.11895102\n",
      "Iteration 393, loss = 0.11784733\n",
      "Iteration 394, loss = 0.11795784\n",
      "Iteration 395, loss = 0.11857734\n",
      "Iteration 396, loss = 0.11857123\n",
      "Iteration 397, loss = 0.11652620\n",
      "Iteration 398, loss = 0.11716462\n",
      "Iteration 399, loss = 0.11690023\n",
      "Iteration 400, loss = 0.11507283\n",
      "Iteration 401, loss = 0.11787795\n",
      "Iteration 402, loss = 0.11478931\n",
      "Iteration 403, loss = 0.11310803\n",
      "Iteration 404, loss = 0.11345162\n",
      "Iteration 405, loss = 0.11358975\n",
      "Iteration 406, loss = 0.11285675\n",
      "Iteration 407, loss = 0.11223143\n",
      "Iteration 408, loss = 0.11110261\n",
      "Iteration 409, loss = 0.11104909\n",
      "Iteration 410, loss = 0.11143298\n",
      "Iteration 411, loss = 0.11087050\n",
      "Iteration 412, loss = 0.10912385\n",
      "Iteration 413, loss = 0.10874756\n",
      "Iteration 414, loss = 0.10935814\n",
      "Iteration 415, loss = 0.11060043\n",
      "Iteration 416, loss = 0.10849295\n",
      "Iteration 417, loss = 0.10717154\n",
      "Iteration 418, loss = 0.10657344\n",
      "Iteration 419, loss = 0.10723668\n",
      "Iteration 420, loss = 0.10563663\n",
      "Iteration 421, loss = 0.10622008\n",
      "Iteration 422, loss = 0.10628759\n",
      "Iteration 423, loss = 0.10467532\n",
      "Iteration 424, loss = 0.10461970\n",
      "Iteration 425, loss = 0.10433561\n",
      "Iteration 426, loss = 0.10464371\n",
      "Iteration 427, loss = 0.10369642\n",
      "Iteration 428, loss = 0.10244119\n",
      "Iteration 429, loss = 0.10227085\n",
      "Iteration 430, loss = 0.10317018\n",
      "Iteration 431, loss = 0.10191727\n",
      "Iteration 432, loss = 0.10149420\n",
      "Iteration 433, loss = 0.10078269\n",
      "Iteration 434, loss = 0.10092630\n",
      "Iteration 435, loss = 0.10004319\n",
      "Iteration 436, loss = 0.09987058\n",
      "Iteration 437, loss = 0.09913280\n",
      "Iteration 438, loss = 0.10222793\n",
      "Iteration 439, loss = 0.09944063\n",
      "Iteration 440, loss = 0.09850924\n",
      "Iteration 441, loss = 0.09760620\n",
      "Iteration 442, loss = 0.09730332\n",
      "Iteration 443, loss = 0.09713301\n",
      "Iteration 444, loss = 0.09602357\n",
      "Iteration 445, loss = 0.09698708\n",
      "Iteration 446, loss = 0.09743027\n",
      "Iteration 447, loss = 0.09527561\n",
      "Iteration 448, loss = 0.09470741\n",
      "Iteration 449, loss = 0.09475876\n",
      "Iteration 450, loss = 0.09579168\n",
      "Iteration 451, loss = 0.09308788\n",
      "Iteration 452, loss = 0.09289732\n",
      "Iteration 453, loss = 0.09274885\n",
      "Iteration 454, loss = 0.09288254\n",
      "Iteration 455, loss = 0.09384700\n",
      "Iteration 456, loss = 0.09333704\n",
      "Iteration 457, loss = 0.09108024\n",
      "Iteration 458, loss = 0.09140005\n",
      "Iteration 459, loss = 0.09015297\n",
      "Iteration 460, loss = 0.09082548\n",
      "Iteration 461, loss = 0.09084825\n",
      "Iteration 462, loss = 0.09120027\n",
      "Iteration 463, loss = 0.09222995\n",
      "Iteration 464, loss = 0.09095419\n",
      "Iteration 465, loss = 0.09073221\n",
      "Iteration 466, loss = 0.08721150\n",
      "Iteration 467, loss = 0.08778588\n",
      "Iteration 468, loss = 0.08782924\n",
      "Iteration 469, loss = 0.08763947\n",
      "Iteration 470, loss = 0.08738406\n",
      "Iteration 471, loss = 0.08828088\n",
      "Iteration 472, loss = 0.08780846\n",
      "Iteration 473, loss = 0.08539261\n",
      "Iteration 474, loss = 0.08497432\n",
      "Iteration 475, loss = 0.08519531\n",
      "Iteration 476, loss = 0.08436625\n",
      "Iteration 477, loss = 0.08390700\n",
      "Iteration 478, loss = 0.08366796\n",
      "Iteration 479, loss = 0.08334350\n",
      "Iteration 480, loss = 0.08300785\n",
      "Iteration 481, loss = 0.08266836\n",
      "Iteration 482, loss = 0.08190922\n",
      "Iteration 483, loss = 0.08300834\n",
      "Iteration 484, loss = 0.08179303\n",
      "Iteration 485, loss = 0.08097034\n",
      "Iteration 486, loss = 0.08074399\n",
      "Iteration 487, loss = 0.08018950\n",
      "Iteration 488, loss = 0.08149028\n",
      "Iteration 489, loss = 0.08006091\n",
      "Iteration 490, loss = 0.08004033\n",
      "Iteration 491, loss = 0.07874743\n",
      "Iteration 492, loss = 0.07836884\n",
      "Iteration 493, loss = 0.07775949\n",
      "Iteration 494, loss = 0.07881345\n",
      "Iteration 495, loss = 0.07801500\n",
      "Iteration 496, loss = 0.07731399\n",
      "Iteration 497, loss = 0.07659101\n",
      "Iteration 498, loss = 0.07843319\n",
      "Iteration 499, loss = 0.07771335\n",
      "Iteration 500, loss = 0.07846958\n",
      "Iteration 501, loss = 0.07731786\n",
      "Iteration 502, loss = 0.07496869\n",
      "Iteration 503, loss = 0.07561112\n",
      "Iteration 504, loss = 0.07499644\n",
      "Iteration 505, loss = 0.07560783\n",
      "Iteration 506, loss = 0.07578047\n",
      "Iteration 507, loss = 0.07545810\n",
      "Iteration 508, loss = 0.07430222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 509, loss = 0.07348925\n",
      "Iteration 510, loss = 0.07267074\n",
      "Iteration 511, loss = 0.07246517\n",
      "Iteration 512, loss = 0.07210517\n",
      "Iteration 513, loss = 0.07207252\n",
      "Iteration 514, loss = 0.07211285\n",
      "Iteration 515, loss = 0.07180654\n",
      "Iteration 516, loss = 0.07099186\n",
      "Iteration 517, loss = 0.07121116\n",
      "Iteration 518, loss = 0.07137042\n",
      "Iteration 519, loss = 0.06982871\n",
      "Iteration 520, loss = 0.06965424\n",
      "Iteration 521, loss = 0.07018165\n",
      "Iteration 522, loss = 0.07157007\n",
      "Iteration 523, loss = 0.06980147\n",
      "Iteration 524, loss = 0.06976434\n",
      "Iteration 525, loss = 0.06851033\n",
      "Iteration 526, loss = 0.07013590\n",
      "Iteration 527, loss = 0.07031084\n",
      "Iteration 528, loss = 0.06790457\n",
      "Iteration 529, loss = 0.06659613\n",
      "Iteration 530, loss = 0.07168745\n",
      "Iteration 531, loss = 0.07041362\n",
      "Iteration 532, loss = 0.06984153\n",
      "Iteration 533, loss = 0.06726969\n",
      "Iteration 534, loss = 0.06687595\n",
      "Iteration 535, loss = 0.06622176\n",
      "Iteration 536, loss = 0.06645925\n",
      "Iteration 537, loss = 0.06544493\n",
      "Iteration 538, loss = 0.06537889\n",
      "Iteration 539, loss = 0.06519036\n",
      "Iteration 540, loss = 0.06713415\n",
      "Iteration 541, loss = 0.06571177\n",
      "Iteration 542, loss = 0.06516833\n",
      "Iteration 543, loss = 0.06450072\n",
      "Iteration 544, loss = 0.06464667\n",
      "Iteration 545, loss = 0.06296790\n",
      "Iteration 546, loss = 0.06286188\n",
      "Iteration 547, loss = 0.06354605\n",
      "Iteration 548, loss = 0.06194285\n",
      "Iteration 549, loss = 0.06232922\n",
      "Iteration 550, loss = 0.06209189\n",
      "Iteration 551, loss = 0.06230834\n",
      "Iteration 552, loss = 0.06074516\n",
      "Iteration 553, loss = 0.06072966\n",
      "Iteration 554, loss = 0.06146926\n",
      "Iteration 555, loss = 0.06097113\n",
      "Iteration 556, loss = 0.06004557\n",
      "Iteration 557, loss = 0.06039697\n",
      "Iteration 558, loss = 0.06079659\n",
      "Iteration 559, loss = 0.06067759\n",
      "Iteration 560, loss = 0.05863568\n",
      "Iteration 561, loss = 0.05872718\n",
      "Iteration 562, loss = 0.05916225\n",
      "Iteration 563, loss = 0.05889643\n",
      "Iteration 564, loss = 0.05810350\n",
      "Iteration 565, loss = 0.05806800\n",
      "Iteration 566, loss = 0.05708651\n",
      "Iteration 567, loss = 0.05749345\n",
      "Iteration 568, loss = 0.05710934\n",
      "Iteration 569, loss = 0.05666931\n",
      "Iteration 570, loss = 0.05714020\n",
      "Iteration 571, loss = 0.05567837\n",
      "Iteration 572, loss = 0.05688024\n",
      "Iteration 573, loss = 0.05616567\n",
      "Iteration 574, loss = 0.05660748\n",
      "Iteration 575, loss = 0.05599793\n",
      "Iteration 576, loss = 0.05544929\n",
      "Iteration 577, loss = 0.05475838\n",
      "Iteration 578, loss = 0.05444967\n",
      "Iteration 579, loss = 0.05464393\n",
      "Iteration 580, loss = 0.05406049\n",
      "Iteration 581, loss = 0.05368062\n",
      "Iteration 582, loss = 0.05376822\n",
      "Iteration 583, loss = 0.05305918\n",
      "Iteration 584, loss = 0.05323653\n",
      "Iteration 585, loss = 0.05342799\n",
      "Iteration 586, loss = 0.05335660\n",
      "Iteration 587, loss = 0.05349997\n",
      "Iteration 588, loss = 0.05427385\n",
      "Iteration 589, loss = 0.05220647\n",
      "Iteration 590, loss = 0.05237161\n",
      "Iteration 591, loss = 0.05163283\n",
      "Iteration 592, loss = 0.05147477\n",
      "Iteration 593, loss = 0.05189236\n",
      "Iteration 594, loss = 0.05097161\n",
      "Iteration 595, loss = 0.05183915\n",
      "Iteration 596, loss = 0.05165677\n",
      "Iteration 597, loss = 0.05113334\n",
      "Iteration 598, loss = 0.05120581\n",
      "Iteration 599, loss = 0.05095863\n",
      "Iteration 600, loss = 0.05083007\n",
      "Iteration 601, loss = 0.04976398\n",
      "Iteration 602, loss = 0.04942585\n",
      "Iteration 603, loss = 0.04956696\n",
      "Iteration 604, loss = 0.04919622\n",
      "Iteration 605, loss = 0.04850336\n",
      "Iteration 606, loss = 0.04842607\n",
      "Iteration 607, loss = 0.04870227\n",
      "Iteration 608, loss = 0.04892959\n",
      "Iteration 609, loss = 0.05075189\n",
      "Iteration 610, loss = 0.04926759\n",
      "Iteration 611, loss = 0.04896435\n",
      "Iteration 612, loss = 0.04768230\n",
      "Iteration 613, loss = 0.04765564\n",
      "Iteration 614, loss = 0.04700884\n",
      "Iteration 615, loss = 0.04722234\n",
      "Iteration 616, loss = 0.04730662\n",
      "Iteration 617, loss = 0.04655411\n",
      "Iteration 618, loss = 0.04579234\n",
      "Iteration 619, loss = 0.04606806\n",
      "Iteration 620, loss = 0.04769307\n",
      "Iteration 621, loss = 0.04562316\n",
      "Iteration 622, loss = 0.04536822\n",
      "Iteration 623, loss = 0.04493203\n",
      "Iteration 624, loss = 0.04482617\n",
      "Iteration 625, loss = 0.04514708\n",
      "Iteration 626, loss = 0.04493085\n",
      "Iteration 627, loss = 0.04513058\n",
      "Iteration 628, loss = 0.04476735\n",
      "Iteration 629, loss = 0.04417415\n",
      "Iteration 630, loss = 0.04503910\n",
      "Iteration 631, loss = 0.04353132\n",
      "Iteration 632, loss = 0.04376300\n",
      "Iteration 633, loss = 0.04312635\n",
      "Iteration 634, loss = 0.04408405\n",
      "Iteration 635, loss = 0.04286171\n",
      "Iteration 636, loss = 0.04261513\n",
      "Iteration 637, loss = 0.04264795\n",
      "Iteration 638, loss = 0.04243869\n",
      "Iteration 639, loss = 0.04232842\n",
      "Iteration 640, loss = 0.04275259\n",
      "Iteration 641, loss = 0.04228948\n",
      "Iteration 642, loss = 0.04251939\n",
      "Iteration 643, loss = 0.04238901\n",
      "Iteration 644, loss = 0.04134949\n",
      "Iteration 645, loss = 0.04133795\n",
      "Iteration 646, loss = 0.04239250\n",
      "Iteration 647, loss = 0.04260071\n",
      "Iteration 648, loss = 0.04220008\n",
      "Iteration 649, loss = 0.04081758\n",
      "Iteration 650, loss = 0.04215469\n",
      "Iteration 651, loss = 0.04013536\n",
      "Iteration 652, loss = 0.04004586\n",
      "Iteration 653, loss = 0.04077176\n",
      "Iteration 654, loss = 0.04050221\n",
      "Iteration 655, loss = 0.03985753\n",
      "Iteration 656, loss = 0.03983605\n",
      "Iteration 657, loss = 0.03966231\n",
      "Iteration 658, loss = 0.03920574\n",
      "Iteration 659, loss = 0.03913860\n",
      "Iteration 660, loss = 0.03859628\n",
      "Iteration 661, loss = 0.03868719\n",
      "Iteration 662, loss = 0.03881959\n",
      "Iteration 663, loss = 0.03825276\n",
      "Iteration 664, loss = 0.03782518\n",
      "Iteration 665, loss = 0.03793509\n",
      "Iteration 666, loss = 0.03780881\n",
      "Iteration 667, loss = 0.03804024\n",
      "Iteration 668, loss = 0.03822931\n",
      "Iteration 669, loss = 0.03731379\n",
      "Iteration 670, loss = 0.03742978\n",
      "Iteration 671, loss = 0.03762134\n",
      "Iteration 672, loss = 0.03773472\n",
      "Iteration 673, loss = 0.03678802\n",
      "Iteration 674, loss = 0.03733739\n",
      "Iteration 675, loss = 0.03718699\n",
      "Iteration 676, loss = 0.03663922\n",
      "Iteration 677, loss = 0.03675525\n",
      "Iteration 678, loss = 0.03639012\n",
      "Iteration 679, loss = 0.03582574\n",
      "Iteration 680, loss = 0.03598817\n",
      "Iteration 681, loss = 0.03537394\n",
      "Iteration 682, loss = 0.03492569\n",
      "Iteration 683, loss = 0.03545504\n",
      "Iteration 684, loss = 0.03486257\n",
      "Iteration 685, loss = 0.03512972\n",
      "Iteration 686, loss = 0.03489849\n",
      "Iteration 687, loss = 0.03450278\n",
      "Iteration 688, loss = 0.03470772\n",
      "Iteration 689, loss = 0.03430880\n",
      "Iteration 690, loss = 0.03467548\n",
      "Iteration 691, loss = 0.03569332\n",
      "Iteration 692, loss = 0.03619719\n",
      "Iteration 693, loss = 0.03630262\n",
      "Iteration 694, loss = 0.03387455\n",
      "Iteration 695, loss = 0.03425921\n",
      "Iteration 696, loss = 0.03319994\n",
      "Iteration 697, loss = 0.03300554\n",
      "Iteration 698, loss = 0.03373969\n",
      "Iteration 699, loss = 0.03412831\n",
      "Iteration 700, loss = 0.03369803\n",
      "Iteration 701, loss = 0.03272738\n",
      "Iteration 702, loss = 0.03271477\n",
      "Iteration 703, loss = 0.03212213\n",
      "Iteration 704, loss = 0.03300239\n",
      "Iteration 705, loss = 0.03234324\n",
      "Iteration 706, loss = 0.03186905\n",
      "Iteration 707, loss = 0.03210619\n",
      "Iteration 708, loss = 0.03210602\n",
      "Iteration 709, loss = 0.03194823\n",
      "Iteration 710, loss = 0.03154118\n",
      "Iteration 711, loss = 0.03120161\n",
      "Iteration 712, loss = 0.03175338\n",
      "Iteration 713, loss = 0.03142924\n",
      "Iteration 714, loss = 0.03107305\n",
      "Iteration 715, loss = 0.03119019\n",
      "Iteration 716, loss = 0.03042691\n",
      "Iteration 717, loss = 0.03044983\n",
      "Iteration 718, loss = 0.03084390\n",
      "Iteration 719, loss = 0.03160995\n",
      "Iteration 720, loss = 0.03158885\n",
      "Iteration 721, loss = 0.03044190\n",
      "Iteration 722, loss = 0.02993831\n",
      "Iteration 723, loss = 0.02969042\n",
      "Iteration 724, loss = 0.02973210\n",
      "Iteration 725, loss = 0.02972580\n",
      "Iteration 726, loss = 0.02984786\n",
      "Iteration 727, loss = 0.02972673\n",
      "Iteration 728, loss = 0.02960093\n",
      "Iteration 729, loss = 0.03009993\n",
      "Iteration 730, loss = 0.02946294\n",
      "Iteration 731, loss = 0.02907087\n",
      "Iteration 732, loss = 0.02955076\n",
      "Iteration 733, loss = 0.02857683\n",
      "Iteration 734, loss = 0.02839369\n",
      "Iteration 735, loss = 0.02855981\n",
      "Iteration 736, loss = 0.02855496\n",
      "Iteration 737, loss = 0.02812525\n",
      "Iteration 738, loss = 0.02800762\n",
      "Iteration 739, loss = 0.02841904\n",
      "Iteration 740, loss = 0.02857040\n",
      "Iteration 741, loss = 0.02829296\n",
      "Iteration 742, loss = 0.02793326\n",
      "Iteration 743, loss = 0.02782251\n",
      "Iteration 744, loss = 0.02784245\n",
      "Iteration 745, loss = 0.02717331\n",
      "Iteration 746, loss = 0.02753290\n",
      "Iteration 747, loss = 0.02737421\n",
      "Iteration 748, loss = 0.02691001\n",
      "Iteration 749, loss = 0.02840583\n",
      "Iteration 750, loss = 0.02775470\n",
      "Iteration 751, loss = 0.02647643\n",
      "Iteration 752, loss = 0.02637741\n",
      "Iteration 753, loss = 0.02627747\n",
      "Iteration 754, loss = 0.02664858\n",
      "Iteration 755, loss = 0.02646693\n",
      "Iteration 756, loss = 0.02592067\n",
      "Iteration 757, loss = 0.02608841\n",
      "Iteration 758, loss = 0.02581186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 759, loss = 0.02576896\n",
      "Iteration 760, loss = 0.02572831\n",
      "Iteration 761, loss = 0.02580708\n",
      "Iteration 762, loss = 0.02565549\n",
      "Iteration 763, loss = 0.02568824\n",
      "Iteration 764, loss = 0.02560101\n",
      "Iteration 765, loss = 0.02568765\n",
      "Iteration 766, loss = 0.02555087\n",
      "Iteration 767, loss = 0.02507137\n",
      "Iteration 768, loss = 0.02501060\n",
      "Iteration 769, loss = 0.02538978\n",
      "Iteration 770, loss = 0.02696345\n",
      "Iteration 771, loss = 0.02515140\n",
      "Iteration 772, loss = 0.02500805\n",
      "Iteration 773, loss = 0.02486341\n",
      "Iteration 774, loss = 0.02451202\n",
      "Iteration 775, loss = 0.02411864\n",
      "Iteration 776, loss = 0.02409856\n",
      "Iteration 777, loss = 0.02368816\n",
      "Iteration 778, loss = 0.02453027\n",
      "Iteration 779, loss = 0.02414338\n",
      "Iteration 780, loss = 0.02364810\n",
      "Iteration 781, loss = 0.02340474\n",
      "Iteration 782, loss = 0.02376400\n",
      "Iteration 783, loss = 0.02374379\n",
      "Iteration 784, loss = 0.02361127\n",
      "Iteration 785, loss = 0.02357939\n",
      "Iteration 786, loss = 0.02281518\n",
      "Iteration 787, loss = 0.02332421\n",
      "Iteration 788, loss = 0.02331506\n",
      "Iteration 789, loss = 0.02352282\n",
      "Iteration 790, loss = 0.02443928\n",
      "Iteration 791, loss = 0.02352338\n",
      "Iteration 792, loss = 0.02348987\n",
      "Iteration 793, loss = 0.02264348\n",
      "Iteration 794, loss = 0.02195198\n",
      "Iteration 795, loss = 0.02373780\n",
      "Iteration 796, loss = 0.02252903\n",
      "Iteration 797, loss = 0.02248259\n",
      "Iteration 798, loss = 0.02185301\n",
      "Iteration 799, loss = 0.02194265\n",
      "Iteration 800, loss = 0.02199333\n",
      "Iteration 801, loss = 0.02145080\n",
      "Iteration 802, loss = 0.02136423\n",
      "Iteration 803, loss = 0.02238972\n",
      "Iteration 804, loss = 0.02146660\n",
      "Iteration 805, loss = 0.02125133\n",
      "Iteration 806, loss = 0.02120335\n",
      "Iteration 807, loss = 0.02112478\n",
      "Iteration 808, loss = 0.02100535\n",
      "Iteration 809, loss = 0.02107657\n",
      "Iteration 810, loss = 0.02076653\n",
      "Iteration 811, loss = 0.02127875\n",
      "Iteration 812, loss = 0.02076870\n",
      "Iteration 813, loss = 0.02056322\n",
      "Iteration 814, loss = 0.02035674\n",
      "Iteration 815, loss = 0.02036060\n",
      "Iteration 816, loss = 0.02024215\n",
      "Iteration 817, loss = 0.02069305\n",
      "Iteration 818, loss = 0.02031623\n",
      "Iteration 819, loss = 0.02038076\n",
      "Iteration 820, loss = 0.02050114\n",
      "Iteration 821, loss = 0.02153458\n",
      "Iteration 822, loss = 0.01978760\n",
      "Iteration 823, loss = 0.02011179\n",
      "Iteration 824, loss = 0.01993853\n",
      "Iteration 825, loss = 0.01974162\n",
      "Iteration 826, loss = 0.01960135\n",
      "Iteration 827, loss = 0.01936397\n",
      "Iteration 828, loss = 0.01969347\n",
      "Iteration 829, loss = 0.01966169\n",
      "Iteration 830, loss = 0.01964447\n",
      "Iteration 831, loss = 0.01920818\n",
      "Iteration 832, loss = 0.01936246\n",
      "Iteration 833, loss = 0.01915482\n",
      "Iteration 834, loss = 0.01909612\n",
      "Iteration 835, loss = 0.01901190\n",
      "Iteration 836, loss = 0.01885480\n",
      "Iteration 837, loss = 0.01884420\n",
      "Iteration 838, loss = 0.01865776\n",
      "Iteration 839, loss = 0.01851960\n",
      "Iteration 840, loss = 0.01831704\n",
      "Iteration 841, loss = 0.01852585\n",
      "Iteration 842, loss = 0.01816997\n",
      "Iteration 843, loss = 0.01835081\n",
      "Iteration 844, loss = 0.01815701\n",
      "Iteration 845, loss = 0.01819133\n",
      "Iteration 846, loss = 0.01829092\n",
      "Iteration 847, loss = 0.01896667\n",
      "Iteration 848, loss = 0.01896030\n",
      "Iteration 849, loss = 0.01826034\n",
      "Iteration 850, loss = 0.01892245\n",
      "Iteration 851, loss = 0.01808965\n",
      "Iteration 852, loss = 0.01832060\n",
      "Iteration 853, loss = 0.01776180\n",
      "Iteration 854, loss = 0.01779841\n",
      "Iteration 855, loss = 0.01782387\n",
      "Iteration 856, loss = 0.01754963\n",
      "Iteration 857, loss = 0.01765744\n",
      "Iteration 858, loss = 0.01860743\n",
      "Iteration 859, loss = 0.01741514\n",
      "Iteration 860, loss = 0.01747406\n",
      "Iteration 861, loss = 0.01762209\n",
      "Iteration 862, loss = 0.01699153\n",
      "Iteration 863, loss = 0.01661962\n",
      "Iteration 864, loss = 0.01673073\n",
      "Iteration 865, loss = 0.01659906\n",
      "Iteration 866, loss = 0.01640010\n",
      "Iteration 867, loss = 0.01691785\n",
      "Iteration 868, loss = 0.01687650\n",
      "Iteration 869, loss = 0.01647690\n",
      "Iteration 870, loss = 0.01650157\n",
      "Iteration 871, loss = 0.01648688\n",
      "Iteration 872, loss = 0.01644388\n",
      "Iteration 873, loss = 0.01701044\n",
      "Iteration 874, loss = 0.01746578\n",
      "Iteration 875, loss = 0.01665519\n",
      "Iteration 876, loss = 0.01656820\n",
      "Iteration 877, loss = 0.01623730\n",
      "Iteration 878, loss = 0.01603390\n",
      "Iteration 879, loss = 0.01557904\n",
      "Iteration 880, loss = 0.01601465\n",
      "Iteration 881, loss = 0.01584037\n",
      "Iteration 882, loss = 0.01568119\n",
      "Iteration 883, loss = 0.01552239\n",
      "Iteration 884, loss = 0.01606612\n",
      "Iteration 885, loss = 0.01586611\n",
      "Iteration 886, loss = 0.01570730\n",
      "Iteration 887, loss = 0.01569096\n",
      "Iteration 888, loss = 0.01511384\n",
      "Iteration 889, loss = 0.01510786\n",
      "Iteration 890, loss = 0.01531104\n",
      "Iteration 891, loss = 0.01533924\n",
      "Iteration 892, loss = 0.01595811\n",
      "Iteration 893, loss = 0.01481903\n",
      "Iteration 894, loss = 0.01487417\n",
      "Iteration 895, loss = 0.01477131\n",
      "Iteration 896, loss = 0.01470126\n",
      "Iteration 897, loss = 0.01488095\n",
      "Iteration 898, loss = 0.01479016\n",
      "Iteration 899, loss = 0.01467472\n",
      "Iteration 900, loss = 0.01486528\n",
      "Iteration 901, loss = 0.01456159\n",
      "Iteration 902, loss = 0.01451435\n",
      "Iteration 903, loss = 0.01460825\n",
      "Iteration 904, loss = 0.01425042\n",
      "Iteration 905, loss = 0.01433012\n",
      "Iteration 906, loss = 0.01446012\n",
      "Iteration 907, loss = 0.01429781\n",
      "Iteration 908, loss = 0.01449182\n",
      "Iteration 909, loss = 0.01433293\n",
      "Iteration 910, loss = 0.01418948\n",
      "Iteration 911, loss = 0.01397798\n",
      "Iteration 912, loss = 0.01423713\n",
      "Iteration 913, loss = 0.01390735\n",
      "Iteration 914, loss = 0.01400105\n",
      "Iteration 915, loss = 0.01413157\n",
      "Iteration 916, loss = 0.01366648\n",
      "Iteration 917, loss = 0.01358150\n",
      "Iteration 918, loss = 0.01399652\n",
      "Iteration 919, loss = 0.01428633\n",
      "Iteration 920, loss = 0.01612916\n",
      "Iteration 921, loss = 0.02962677\n",
      "Iteration 922, loss = 0.03182376\n",
      "Iteration 923, loss = 0.02349938\n",
      "Iteration 924, loss = 0.01750544\n",
      "Iteration 925, loss = 0.01445147\n",
      "Iteration 926, loss = 0.01405522\n",
      "Iteration 927, loss = 0.01387592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 14 of 48...\n",
      "Iteration 1, loss = 0.67155358\n",
      "Iteration 2, loss = 0.63497095\n",
      "Iteration 3, loss = 0.60889390\n",
      "Iteration 4, loss = 0.58625056\n",
      "Iteration 5, loss = 0.56562504\n",
      "Iteration 6, loss = 0.54908016\n",
      "Iteration 7, loss = 0.53288177\n",
      "Iteration 8, loss = 0.51588975\n",
      "Iteration 9, loss = 0.50234826\n",
      "Iteration 10, loss = 0.49147452\n",
      "Iteration 11, loss = 0.48188691\n",
      "Iteration 12, loss = 0.46990604\n",
      "Iteration 13, loss = 0.45906446\n",
      "Iteration 14, loss = 0.45125213\n",
      "Iteration 15, loss = 0.44371358\n",
      "Iteration 16, loss = 0.43609284\n",
      "Iteration 17, loss = 0.42967222\n",
      "Iteration 18, loss = 0.42423070\n",
      "Iteration 19, loss = 0.42630629\n",
      "Iteration 20, loss = 0.41474377\n",
      "Iteration 21, loss = 0.40867483\n",
      "Iteration 22, loss = 0.40448957\n",
      "Iteration 23, loss = 0.40081497\n",
      "Iteration 24, loss = 0.39716374\n",
      "Iteration 25, loss = 0.39357838\n",
      "Iteration 26, loss = 0.38983379\n",
      "Iteration 27, loss = 0.38579310\n",
      "Iteration 28, loss = 0.38262017\n",
      "Iteration 29, loss = 0.38044796\n",
      "Iteration 30, loss = 0.37694793\n",
      "Iteration 31, loss = 0.37551504\n",
      "Iteration 32, loss = 0.37239467\n",
      "Iteration 33, loss = 0.36886164\n",
      "Iteration 34, loss = 0.36758790\n",
      "Iteration 35, loss = 0.36454171\n",
      "Iteration 36, loss = 0.36333442\n",
      "Iteration 37, loss = 0.36095193\n",
      "Iteration 38, loss = 0.35676080\n",
      "Iteration 39, loss = 0.36052107\n",
      "Iteration 40, loss = 0.35426048\n",
      "Iteration 41, loss = 0.35208907\n",
      "Iteration 42, loss = 0.34943529\n",
      "Iteration 43, loss = 0.34779973\n",
      "Iteration 44, loss = 0.34572727\n",
      "Iteration 45, loss = 0.34362625\n",
      "Iteration 46, loss = 0.34125364\n",
      "Iteration 47, loss = 0.33971235\n",
      "Iteration 48, loss = 0.33797774\n",
      "Iteration 49, loss = 0.33669404\n",
      "Iteration 50, loss = 0.33791464\n",
      "Iteration 51, loss = 0.33331513\n",
      "Iteration 52, loss = 0.33251013\n",
      "Iteration 53, loss = 0.33070995\n",
      "Iteration 54, loss = 0.33254870\n",
      "Iteration 55, loss = 0.33486196\n",
      "Iteration 56, loss = 0.32829402\n",
      "Iteration 57, loss = 0.32437089\n",
      "Iteration 58, loss = 0.32420133\n",
      "Iteration 59, loss = 0.32250549\n",
      "Iteration 60, loss = 0.32268768\n",
      "Iteration 61, loss = 0.32464126\n",
      "Iteration 62, loss = 0.31822779\n",
      "Iteration 63, loss = 0.31645444\n",
      "Iteration 64, loss = 0.31566611\n",
      "Iteration 65, loss = 0.31512326\n",
      "Iteration 66, loss = 0.31375990\n",
      "Iteration 67, loss = 0.31496989\n",
      "Iteration 68, loss = 0.31302877\n",
      "Iteration 69, loss = 0.31016232\n",
      "Iteration 70, loss = 0.30849808\n",
      "Iteration 71, loss = 0.30806682\n",
      "Iteration 72, loss = 0.30871032\n",
      "Iteration 73, loss = 0.31282042\n",
      "Iteration 74, loss = 0.30540526\n",
      "Iteration 75, loss = 0.30661960\n",
      "Iteration 76, loss = 0.30507912\n",
      "Iteration 77, loss = 0.30326385\n",
      "Iteration 78, loss = 0.30195509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79, loss = 0.29982299\n",
      "Iteration 80, loss = 0.29713604\n",
      "Iteration 81, loss = 0.29620551\n",
      "Iteration 82, loss = 0.29652331\n",
      "Iteration 83, loss = 0.29524351\n",
      "Iteration 84, loss = 0.29561265\n",
      "Iteration 85, loss = 0.29299978\n",
      "Iteration 86, loss = 0.29161645\n",
      "Iteration 87, loss = 0.29025390\n",
      "Iteration 88, loss = 0.28955256\n",
      "Iteration 89, loss = 0.28993487\n",
      "Iteration 90, loss = 0.29016600\n",
      "Iteration 91, loss = 0.28814469\n",
      "Iteration 92, loss = 0.28711134\n",
      "Iteration 93, loss = 0.28480291\n",
      "Iteration 94, loss = 0.28459100\n",
      "Iteration 95, loss = 0.28398529\n",
      "Iteration 96, loss = 0.28533041\n",
      "Iteration 97, loss = 0.28252738\n",
      "Iteration 98, loss = 0.28491543\n",
      "Iteration 99, loss = 0.28068618\n",
      "Iteration 100, loss = 0.27968219\n",
      "Iteration 101, loss = 0.27814252\n",
      "Iteration 102, loss = 0.28187824\n",
      "Iteration 103, loss = 0.27789366\n",
      "Iteration 104, loss = 0.27609621\n",
      "Iteration 105, loss = 0.27615400\n",
      "Iteration 106, loss = 0.27569020\n",
      "Iteration 107, loss = 0.27681105\n",
      "Iteration 108, loss = 0.27726104\n",
      "Iteration 109, loss = 0.27436249\n",
      "Iteration 110, loss = 0.27453376\n",
      "Iteration 111, loss = 0.27355094\n",
      "Iteration 112, loss = 0.27231701\n",
      "Iteration 113, loss = 0.27067018\n",
      "Iteration 114, loss = 0.26988644\n",
      "Iteration 115, loss = 0.26902976\n",
      "Iteration 116, loss = 0.26897554\n",
      "Iteration 117, loss = 0.26761883\n",
      "Iteration 118, loss = 0.26719863\n",
      "Iteration 119, loss = 0.26596733\n",
      "Iteration 120, loss = 0.26712232\n",
      "Iteration 121, loss = 0.26682377\n",
      "Iteration 122, loss = 0.26474568\n",
      "Iteration 123, loss = 0.26730245\n",
      "Iteration 124, loss = 0.26905268\n",
      "Iteration 125, loss = 0.26158937\n",
      "Iteration 126, loss = 0.26138406\n",
      "Iteration 127, loss = 0.26262034\n",
      "Iteration 128, loss = 0.25942162\n",
      "Iteration 129, loss = 0.25858888\n",
      "Iteration 130, loss = 0.25838631\n",
      "Iteration 131, loss = 0.25946038\n",
      "Iteration 132, loss = 0.25719474\n",
      "Iteration 133, loss = 0.25607726\n",
      "Iteration 134, loss = 0.25534671\n",
      "Iteration 135, loss = 0.25573521\n",
      "Iteration 136, loss = 0.25813932\n",
      "Iteration 137, loss = 0.25572823\n",
      "Iteration 138, loss = 0.25557018\n",
      "Iteration 139, loss = 0.25524620\n",
      "Iteration 140, loss = 0.25228245\n",
      "Iteration 141, loss = 0.25219556\n",
      "Iteration 142, loss = 0.25214866\n",
      "Iteration 143, loss = 0.25331757\n",
      "Iteration 144, loss = 0.25024895\n",
      "Iteration 145, loss = 0.25074121\n",
      "Iteration 146, loss = 0.25172757\n",
      "Iteration 147, loss = 0.24868747\n",
      "Iteration 148, loss = 0.24865313\n",
      "Iteration 149, loss = 0.24806933\n",
      "Iteration 150, loss = 0.24761806\n",
      "Iteration 151, loss = 0.24775626\n",
      "Iteration 152, loss = 0.25129076\n",
      "Iteration 153, loss = 0.24972623\n",
      "Iteration 154, loss = 0.24618574\n",
      "Iteration 155, loss = 0.24476286\n",
      "Iteration 156, loss = 0.24490014\n",
      "Iteration 157, loss = 0.24363040\n",
      "Iteration 158, loss = 0.24248767\n",
      "Iteration 159, loss = 0.24256298\n",
      "Iteration 160, loss = 0.24307790\n",
      "Iteration 161, loss = 0.24303950\n",
      "Iteration 162, loss = 0.24247449\n",
      "Iteration 163, loss = 0.24172200\n",
      "Iteration 164, loss = 0.24030927\n",
      "Iteration 165, loss = 0.24035028\n",
      "Iteration 166, loss = 0.23984124\n",
      "Iteration 167, loss = 0.23996539\n",
      "Iteration 168, loss = 0.23796005\n",
      "Iteration 169, loss = 0.23890561\n",
      "Iteration 170, loss = 0.23702602\n",
      "Iteration 171, loss = 0.23634912\n",
      "Iteration 172, loss = 0.23977422\n",
      "Iteration 173, loss = 0.24187482\n",
      "Iteration 174, loss = 0.23584885\n",
      "Iteration 175, loss = 0.23752828\n",
      "Iteration 176, loss = 0.23487691\n",
      "Iteration 177, loss = 0.23599713\n",
      "Iteration 178, loss = 0.23542353\n",
      "Iteration 179, loss = 0.23580716\n",
      "Iteration 180, loss = 0.23956442\n",
      "Iteration 181, loss = 0.23771705\n",
      "Iteration 182, loss = 0.23305160\n",
      "Iteration 183, loss = 0.23222128\n",
      "Iteration 184, loss = 0.23171627\n",
      "Iteration 185, loss = 0.23153789\n",
      "Iteration 186, loss = 0.22993054\n",
      "Iteration 187, loss = 0.22985774\n",
      "Iteration 188, loss = 0.22882809\n",
      "Iteration 189, loss = 0.23027781\n",
      "Iteration 190, loss = 0.22863189\n",
      "Iteration 191, loss = 0.23113086\n",
      "Iteration 192, loss = 0.22627056\n",
      "Iteration 193, loss = 0.22770846\n",
      "Iteration 194, loss = 0.22758042\n",
      "Iteration 195, loss = 0.22464181\n",
      "Iteration 196, loss = 0.22681975\n",
      "Iteration 197, loss = 0.22462242\n",
      "Iteration 198, loss = 0.22638626\n",
      "Iteration 199, loss = 0.22500403\n",
      "Iteration 200, loss = 0.22528752\n",
      "Iteration 201, loss = 0.22859389\n",
      "Iteration 202, loss = 0.22433626\n",
      "Iteration 203, loss = 0.22291464\n",
      "Iteration 204, loss = 0.22247276\n",
      "Iteration 205, loss = 0.22218302\n",
      "Iteration 206, loss = 0.22103829\n",
      "Iteration 207, loss = 0.22057901\n",
      "Iteration 208, loss = 0.21960543\n",
      "Iteration 209, loss = 0.22064173\n",
      "Iteration 210, loss = 0.22040469\n",
      "Iteration 211, loss = 0.22011591\n",
      "Iteration 212, loss = 0.21806667\n",
      "Iteration 213, loss = 0.21819909\n",
      "Iteration 214, loss = 0.21774690\n",
      "Iteration 215, loss = 0.21704638\n",
      "Iteration 216, loss = 0.21597047\n",
      "Iteration 217, loss = 0.21682869\n",
      "Iteration 218, loss = 0.21537318\n",
      "Iteration 219, loss = 0.21540526\n",
      "Iteration 220, loss = 0.21556001\n",
      "Iteration 221, loss = 0.21441179\n",
      "Iteration 222, loss = 0.21409000\n",
      "Iteration 223, loss = 0.21319290\n",
      "Iteration 224, loss = 0.21282111\n",
      "Iteration 225, loss = 0.21345629\n",
      "Iteration 226, loss = 0.21246454\n",
      "Iteration 227, loss = 0.21203656\n",
      "Iteration 228, loss = 0.21214574\n",
      "Iteration 229, loss = 0.21609043\n",
      "Iteration 230, loss = 0.20955842\n",
      "Iteration 231, loss = 0.20989687\n",
      "Iteration 232, loss = 0.21183989\n",
      "Iteration 233, loss = 0.21501938\n",
      "Iteration 234, loss = 0.20914397\n",
      "Iteration 235, loss = 0.20917367\n",
      "Iteration 236, loss = 0.20852764\n",
      "Iteration 237, loss = 0.21369158\n",
      "Iteration 238, loss = 0.21172219\n",
      "Iteration 239, loss = 0.21032009\n",
      "Iteration 240, loss = 0.20834119\n",
      "Iteration 241, loss = 0.20597234\n",
      "Iteration 242, loss = 0.20437617\n",
      "Iteration 243, loss = 0.20410936\n",
      "Iteration 244, loss = 0.20544543\n",
      "Iteration 245, loss = 0.20397244\n",
      "Iteration 246, loss = 0.20388512\n",
      "Iteration 247, loss = 0.20245639\n",
      "Iteration 248, loss = 0.20239056\n",
      "Iteration 249, loss = 0.20112267\n",
      "Iteration 250, loss = 0.20257713\n",
      "Iteration 251, loss = 0.20328655\n",
      "Iteration 252, loss = 0.20287682\n",
      "Iteration 253, loss = 0.20296425\n",
      "Iteration 254, loss = 0.20381742\n",
      "Iteration 255, loss = 0.20068929\n",
      "Iteration 256, loss = 0.19961025\n",
      "Iteration 257, loss = 0.20085806\n",
      "Iteration 258, loss = 0.19821891\n",
      "Iteration 259, loss = 0.19696183\n",
      "Iteration 260, loss = 0.19773924\n",
      "Iteration 261, loss = 0.19590414\n",
      "Iteration 262, loss = 0.19836352\n",
      "Iteration 263, loss = 0.19793719\n",
      "Iteration 264, loss = 0.20104061\n",
      "Iteration 265, loss = 0.20020479\n",
      "Iteration 266, loss = 0.19631549\n",
      "Iteration 267, loss = 0.19456660\n",
      "Iteration 268, loss = 0.19345420\n",
      "Iteration 269, loss = 0.19381463\n",
      "Iteration 270, loss = 0.19325126\n",
      "Iteration 271, loss = 0.19355912\n",
      "Iteration 272, loss = 0.19242750\n",
      "Iteration 273, loss = 0.19232574\n",
      "Iteration 274, loss = 0.18942137\n",
      "Iteration 275, loss = 0.19008483\n",
      "Iteration 276, loss = 0.19056341\n",
      "Iteration 277, loss = 0.18862236\n",
      "Iteration 278, loss = 0.18937233\n",
      "Iteration 279, loss = 0.18758838\n",
      "Iteration 280, loss = 0.19116678\n",
      "Iteration 281, loss = 0.18880652\n",
      "Iteration 282, loss = 0.18821446\n",
      "Iteration 283, loss = 0.18908909\n",
      "Iteration 284, loss = 0.18918560\n",
      "Iteration 285, loss = 0.19253040\n",
      "Iteration 286, loss = 0.18603461\n",
      "Iteration 287, loss = 0.18405631\n",
      "Iteration 288, loss = 0.18404007\n",
      "Iteration 289, loss = 0.18575129\n",
      "Iteration 290, loss = 0.18438697\n",
      "Iteration 291, loss = 0.18465204\n",
      "Iteration 292, loss = 0.18267625\n",
      "Iteration 293, loss = 0.18212618\n",
      "Iteration 294, loss = 0.18290791\n",
      "Iteration 295, loss = 0.18346560\n",
      "Iteration 296, loss = 0.18070958\n",
      "Iteration 297, loss = 0.18041482\n",
      "Iteration 298, loss = 0.18222433\n",
      "Iteration 299, loss = 0.17877965\n",
      "Iteration 300, loss = 0.17885092\n",
      "Iteration 301, loss = 0.17767708\n",
      "Iteration 302, loss = 0.17752737\n",
      "Iteration 303, loss = 0.17672029\n",
      "Iteration 304, loss = 0.17662514\n",
      "Iteration 305, loss = 0.17584449\n",
      "Iteration 306, loss = 0.17640777\n",
      "Iteration 307, loss = 0.17603220\n",
      "Iteration 308, loss = 0.17566993\n",
      "Iteration 309, loss = 0.17466465\n",
      "Iteration 310, loss = 0.17451917\n",
      "Iteration 311, loss = 0.17420451\n",
      "Iteration 312, loss = 0.17301425\n",
      "Iteration 313, loss = 0.17351818\n",
      "Iteration 314, loss = 0.17279385\n",
      "Iteration 315, loss = 0.17167414\n",
      "Iteration 316, loss = 0.17181458\n",
      "Iteration 317, loss = 0.17178990\n",
      "Iteration 318, loss = 0.17010864\n",
      "Iteration 319, loss = 0.17028360\n",
      "Iteration 320, loss = 0.16942120\n",
      "Iteration 321, loss = 0.16846415\n",
      "Iteration 322, loss = 0.16798134\n",
      "Iteration 323, loss = 0.16883518\n",
      "Iteration 324, loss = 0.16751475\n",
      "Iteration 325, loss = 0.16721946\n",
      "Iteration 326, loss = 0.16693749\n",
      "Iteration 327, loss = 0.16944784\n",
      "Iteration 328, loss = 0.16615880\n",
      "Iteration 329, loss = 0.16592823\n",
      "Iteration 330, loss = 0.16640092\n",
      "Iteration 331, loss = 0.16866426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 332, loss = 0.16711374\n",
      "Iteration 333, loss = 0.16441784\n",
      "Iteration 334, loss = 0.16255932\n",
      "Iteration 335, loss = 0.16284124\n",
      "Iteration 336, loss = 0.16248271\n",
      "Iteration 337, loss = 0.16088502\n",
      "Iteration 338, loss = 0.16100486\n",
      "Iteration 339, loss = 0.16089222\n",
      "Iteration 340, loss = 0.16007615\n",
      "Iteration 341, loss = 0.16293247\n",
      "Iteration 342, loss = 0.16050495\n",
      "Iteration 343, loss = 0.15913997\n",
      "Iteration 344, loss = 0.15798599\n",
      "Iteration 345, loss = 0.15870992\n",
      "Iteration 346, loss = 0.15923336\n",
      "Iteration 347, loss = 0.15798908\n",
      "Iteration 348, loss = 0.15635147\n",
      "Iteration 349, loss = 0.16032128\n",
      "Iteration 350, loss = 0.16005107\n",
      "Iteration 351, loss = 0.16053491\n",
      "Iteration 352, loss = 0.15588588\n",
      "Iteration 353, loss = 0.15405065\n",
      "Iteration 354, loss = 0.15307290\n",
      "Iteration 355, loss = 0.15381434\n",
      "Iteration 356, loss = 0.15394578\n",
      "Iteration 357, loss = 0.15462324\n",
      "Iteration 358, loss = 0.15488281\n",
      "Iteration 359, loss = 0.15431489\n",
      "Iteration 360, loss = 0.15412441\n",
      "Iteration 361, loss = 0.14987168\n",
      "Iteration 362, loss = 0.14973490\n",
      "Iteration 363, loss = 0.14966825\n",
      "Iteration 364, loss = 0.15204517\n",
      "Iteration 365, loss = 0.15330253\n",
      "Iteration 366, loss = 0.14999274\n",
      "Iteration 367, loss = 0.14811821\n",
      "Iteration 368, loss = 0.14910856\n",
      "Iteration 369, loss = 0.14774537\n",
      "Iteration 370, loss = 0.14635598\n",
      "Iteration 371, loss = 0.14567109\n",
      "Iteration 372, loss = 0.14591769\n",
      "Iteration 373, loss = 0.14533762\n",
      "Iteration 374, loss = 0.14466582\n",
      "Iteration 375, loss = 0.14410232\n",
      "Iteration 376, loss = 0.14485033\n",
      "Iteration 377, loss = 0.14359255\n",
      "Iteration 378, loss = 0.14311173\n",
      "Iteration 379, loss = 0.14522614\n",
      "Iteration 380, loss = 0.14620606\n",
      "Iteration 381, loss = 0.14171949\n",
      "Iteration 382, loss = 0.14166997\n",
      "Iteration 383, loss = 0.14177604\n",
      "Iteration 384, loss = 0.14112494\n",
      "Iteration 385, loss = 0.14121302\n",
      "Iteration 386, loss = 0.14033110\n",
      "Iteration 387, loss = 0.13833116\n",
      "Iteration 388, loss = 0.14054090\n",
      "Iteration 389, loss = 0.13969858\n",
      "Iteration 390, loss = 0.13768624\n",
      "Iteration 391, loss = 0.13759046\n",
      "Iteration 392, loss = 0.13813726\n",
      "Iteration 393, loss = 0.13721399\n",
      "Iteration 394, loss = 0.13645932\n",
      "Iteration 395, loss = 0.13638584\n",
      "Iteration 396, loss = 0.13571598\n",
      "Iteration 397, loss = 0.13572194\n",
      "Iteration 398, loss = 0.13503709\n",
      "Iteration 399, loss = 0.13381189\n",
      "Iteration 400, loss = 0.13309741\n",
      "Iteration 401, loss = 0.13429477\n",
      "Iteration 402, loss = 0.13382409\n",
      "Iteration 403, loss = 0.13436988\n",
      "Iteration 404, loss = 0.13181081\n",
      "Iteration 405, loss = 0.13222033\n",
      "Iteration 406, loss = 0.13118967\n",
      "Iteration 407, loss = 0.13369282\n",
      "Iteration 408, loss = 0.14360138\n",
      "Iteration 409, loss = 0.13331653\n",
      "Iteration 410, loss = 0.12976092\n",
      "Iteration 411, loss = 0.13125178\n",
      "Iteration 412, loss = 0.12937662\n",
      "Iteration 413, loss = 0.13106614\n",
      "Iteration 414, loss = 0.13044858\n",
      "Iteration 415, loss = 0.12816033\n",
      "Iteration 416, loss = 0.12880956\n",
      "Iteration 417, loss = 0.12653784\n",
      "Iteration 418, loss = 0.12623320\n",
      "Iteration 419, loss = 0.12600456\n",
      "Iteration 420, loss = 0.12548829\n",
      "Iteration 421, loss = 0.12557800\n",
      "Iteration 422, loss = 0.12641601\n",
      "Iteration 423, loss = 0.12539076\n",
      "Iteration 424, loss = 0.12394932\n",
      "Iteration 425, loss = 0.12326528\n",
      "Iteration 426, loss = 0.12369499\n",
      "Iteration 427, loss = 0.12246133\n",
      "Iteration 428, loss = 0.12199971\n",
      "Iteration 429, loss = 0.12393470\n",
      "Iteration 430, loss = 0.12333904\n",
      "Iteration 431, loss = 0.12299012\n",
      "Iteration 432, loss = 0.12213617\n",
      "Iteration 433, loss = 0.12049358\n",
      "Iteration 434, loss = 0.12048003\n",
      "Iteration 435, loss = 0.12063126\n",
      "Iteration 436, loss = 0.11872332\n",
      "Iteration 437, loss = 0.11868377\n",
      "Iteration 438, loss = 0.11925985\n",
      "Iteration 439, loss = 0.12065578\n",
      "Iteration 440, loss = 0.11812353\n",
      "Iteration 441, loss = 0.11720007\n",
      "Iteration 442, loss = 0.11705026\n",
      "Iteration 443, loss = 0.11853908\n",
      "Iteration 444, loss = 0.11735387\n",
      "Iteration 445, loss = 0.11601741\n",
      "Iteration 446, loss = 0.11539236\n",
      "Iteration 447, loss = 0.11534294\n",
      "Iteration 448, loss = 0.11483466\n",
      "Iteration 449, loss = 0.11554836\n",
      "Iteration 450, loss = 0.11462745\n",
      "Iteration 451, loss = 0.11314340\n",
      "Iteration 452, loss = 0.11292803\n",
      "Iteration 453, loss = 0.11425290\n",
      "Iteration 454, loss = 0.11258517\n",
      "Iteration 455, loss = 0.11142751\n",
      "Iteration 456, loss = 0.11224838\n",
      "Iteration 457, loss = 0.11363181\n",
      "Iteration 458, loss = 0.11056166\n",
      "Iteration 459, loss = 0.11072982\n",
      "Iteration 460, loss = 0.11007529\n",
      "Iteration 461, loss = 0.11040645\n",
      "Iteration 462, loss = 0.11123770\n",
      "Iteration 463, loss = 0.10938280\n",
      "Iteration 464, loss = 0.10967017\n",
      "Iteration 465, loss = 0.10802600\n",
      "Iteration 466, loss = 0.11024717\n",
      "Iteration 467, loss = 0.10731246\n",
      "Iteration 468, loss = 0.10812154\n",
      "Iteration 469, loss = 0.10688883\n",
      "Iteration 470, loss = 0.10662330\n",
      "Iteration 471, loss = 0.10638833\n",
      "Iteration 472, loss = 0.10679929\n",
      "Iteration 473, loss = 0.10700648\n",
      "Iteration 474, loss = 0.10904180\n",
      "Iteration 475, loss = 0.10581958\n",
      "Iteration 476, loss = 0.10445133\n",
      "Iteration 477, loss = 0.10397365\n",
      "Iteration 478, loss = 0.10389163\n",
      "Iteration 479, loss = 0.10344917\n",
      "Iteration 480, loss = 0.10499990\n",
      "Iteration 481, loss = 0.10243174\n",
      "Iteration 482, loss = 0.10227969\n",
      "Iteration 483, loss = 0.10232375\n",
      "Iteration 484, loss = 0.10164413\n",
      "Iteration 485, loss = 0.10188342\n",
      "Iteration 486, loss = 0.10186713\n",
      "Iteration 487, loss = 0.10115908\n",
      "Iteration 488, loss = 0.10070862\n",
      "Iteration 489, loss = 0.10057696\n",
      "Iteration 490, loss = 0.10029058\n",
      "Iteration 491, loss = 0.09914535\n",
      "Iteration 492, loss = 0.09854663\n",
      "Iteration 493, loss = 0.09829010\n",
      "Iteration 494, loss = 0.09933497\n",
      "Iteration 495, loss = 0.09893030\n",
      "Iteration 496, loss = 0.09872357\n",
      "Iteration 497, loss = 0.09869546\n",
      "Iteration 498, loss = 0.09749433\n",
      "Iteration 499, loss = 0.09650438\n",
      "Iteration 500, loss = 0.09614612\n",
      "Iteration 501, loss = 0.09596597\n",
      "Iteration 502, loss = 0.09518992\n",
      "Iteration 503, loss = 0.09607511\n",
      "Iteration 504, loss = 0.09880329\n",
      "Iteration 505, loss = 0.09502760\n",
      "Iteration 506, loss = 0.09404653\n",
      "Iteration 507, loss = 0.09396569\n",
      "Iteration 508, loss = 0.09476076\n",
      "Iteration 509, loss = 0.09310431\n",
      "Iteration 510, loss = 0.09342626\n",
      "Iteration 511, loss = 0.09310729\n",
      "Iteration 512, loss = 0.09305843\n",
      "Iteration 513, loss = 0.09217382\n",
      "Iteration 514, loss = 0.09247206\n",
      "Iteration 515, loss = 0.09191127\n",
      "Iteration 516, loss = 0.09144870\n",
      "Iteration 517, loss = 0.09107188\n",
      "Iteration 518, loss = 0.09095613\n",
      "Iteration 519, loss = 0.08991188\n",
      "Iteration 520, loss = 0.09104092\n",
      "Iteration 521, loss = 0.09084334\n",
      "Iteration 522, loss = 0.09026495\n",
      "Iteration 523, loss = 0.08916283\n",
      "Iteration 524, loss = 0.08907800\n",
      "Iteration 525, loss = 0.08825610\n",
      "Iteration 526, loss = 0.08855044\n",
      "Iteration 527, loss = 0.08854528\n",
      "Iteration 528, loss = 0.08770907\n",
      "Iteration 529, loss = 0.08698607\n",
      "Iteration 530, loss = 0.08802408\n",
      "Iteration 531, loss = 0.08709580\n",
      "Iteration 532, loss = 0.08674168\n",
      "Iteration 533, loss = 0.08584125\n",
      "Iteration 534, loss = 0.08738626\n",
      "Iteration 535, loss = 0.08641066\n",
      "Iteration 536, loss = 0.08517645\n",
      "Iteration 537, loss = 0.08483143\n",
      "Iteration 538, loss = 0.08682562\n",
      "Iteration 539, loss = 0.08457787\n",
      "Iteration 540, loss = 0.08364292\n",
      "Iteration 541, loss = 0.08343135\n",
      "Iteration 542, loss = 0.08340796\n",
      "Iteration 543, loss = 0.08392930\n",
      "Iteration 544, loss = 0.08304415\n",
      "Iteration 545, loss = 0.08407696\n",
      "Iteration 546, loss = 0.08413802\n",
      "Iteration 547, loss = 0.08305393\n",
      "Iteration 548, loss = 0.08322798\n",
      "Iteration 549, loss = 0.08414936\n",
      "Iteration 550, loss = 0.08153991\n",
      "Iteration 551, loss = 0.08135673\n",
      "Iteration 552, loss = 0.08175712\n",
      "Iteration 553, loss = 0.08095933\n",
      "Iteration 554, loss = 0.08005263\n",
      "Iteration 555, loss = 0.07956831\n",
      "Iteration 556, loss = 0.08092188\n",
      "Iteration 557, loss = 0.07914671\n",
      "Iteration 558, loss = 0.07920793\n",
      "Iteration 559, loss = 0.08000875\n",
      "Iteration 560, loss = 0.07901891\n",
      "Iteration 561, loss = 0.07804649\n",
      "Iteration 562, loss = 0.07856541\n",
      "Iteration 563, loss = 0.07771593\n",
      "Iteration 564, loss = 0.07774910\n",
      "Iteration 565, loss = 0.07777327\n",
      "Iteration 566, loss = 0.07628185\n",
      "Iteration 567, loss = 0.07618051\n",
      "Iteration 568, loss = 0.07626192\n",
      "Iteration 569, loss = 0.07606612\n",
      "Iteration 570, loss = 0.07571302\n",
      "Iteration 571, loss = 0.07520553\n",
      "Iteration 572, loss = 0.07590943\n",
      "Iteration 573, loss = 0.07791542\n",
      "Iteration 574, loss = 0.07682858\n",
      "Iteration 575, loss = 0.07645731\n",
      "Iteration 576, loss = 0.07505074\n",
      "Iteration 577, loss = 0.07397409\n",
      "Iteration 578, loss = 0.07352358\n",
      "Iteration 579, loss = 0.07502710\n",
      "Iteration 580, loss = 0.07429239\n",
      "Iteration 581, loss = 0.07496902\n",
      "Iteration 582, loss = 0.07354244\n",
      "Iteration 583, loss = 0.07218401\n",
      "Iteration 584, loss = 0.07308771\n",
      "Iteration 585, loss = 0.07914245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 586, loss = 0.07407374\n",
      "Iteration 587, loss = 0.07379813\n",
      "Iteration 588, loss = 0.07172671\n",
      "Iteration 589, loss = 0.07093553\n",
      "Iteration 590, loss = 0.07057665\n",
      "Iteration 591, loss = 0.07020990\n",
      "Iteration 592, loss = 0.07032993\n",
      "Iteration 593, loss = 0.07075481\n",
      "Iteration 594, loss = 0.07228499\n",
      "Iteration 595, loss = 0.06926829\n",
      "Iteration 596, loss = 0.06953425\n",
      "Iteration 597, loss = 0.06897999\n",
      "Iteration 598, loss = 0.06876969\n",
      "Iteration 599, loss = 0.06888886\n",
      "Iteration 600, loss = 0.06830218\n",
      "Iteration 601, loss = 0.06868633\n",
      "Iteration 602, loss = 0.06845463\n",
      "Iteration 603, loss = 0.06797786\n",
      "Iteration 604, loss = 0.06778062\n",
      "Iteration 605, loss = 0.06753184\n",
      "Iteration 606, loss = 0.06753422\n",
      "Iteration 607, loss = 0.06789475\n",
      "Iteration 608, loss = 0.06666468\n",
      "Iteration 609, loss = 0.06680866\n",
      "Iteration 610, loss = 0.06615481\n",
      "Iteration 611, loss = 0.06691411\n",
      "Iteration 612, loss = 0.06590822\n",
      "Iteration 613, loss = 0.06570658\n",
      "Iteration 614, loss = 0.06669558\n",
      "Iteration 615, loss = 0.06533203\n",
      "Iteration 616, loss = 0.06499613\n",
      "Iteration 617, loss = 0.06500779\n",
      "Iteration 618, loss = 0.06571098\n",
      "Iteration 619, loss = 0.06426380\n",
      "Iteration 620, loss = 0.06601866\n",
      "Iteration 621, loss = 0.06415420\n",
      "Iteration 622, loss = 0.06427577\n",
      "Iteration 623, loss = 0.06356718\n",
      "Iteration 624, loss = 0.06316586\n",
      "Iteration 625, loss = 0.06274131\n",
      "Iteration 626, loss = 0.06282283\n",
      "Iteration 627, loss = 0.06320035\n",
      "Iteration 628, loss = 0.06238972\n",
      "Iteration 629, loss = 0.06203554\n",
      "Iteration 630, loss = 0.06272771\n",
      "Iteration 631, loss = 0.06389376\n",
      "Iteration 632, loss = 0.06269012\n",
      "Iteration 633, loss = 0.06159415\n",
      "Iteration 634, loss = 0.06102101\n",
      "Iteration 635, loss = 0.06137372\n",
      "Iteration 636, loss = 0.06128047\n",
      "Iteration 637, loss = 0.06031894\n",
      "Iteration 638, loss = 0.06067464\n",
      "Iteration 639, loss = 0.06213955\n",
      "Iteration 640, loss = 0.06156513\n",
      "Iteration 641, loss = 0.06076948\n",
      "Iteration 642, loss = 0.05948433\n",
      "Iteration 643, loss = 0.05927588\n",
      "Iteration 644, loss = 0.05889488\n",
      "Iteration 645, loss = 0.05904486\n",
      "Iteration 646, loss = 0.05896914\n",
      "Iteration 647, loss = 0.05947212\n",
      "Iteration 648, loss = 0.05811506\n",
      "Iteration 649, loss = 0.05837116\n",
      "Iteration 650, loss = 0.05827107\n",
      "Iteration 651, loss = 0.05771311\n",
      "Iteration 652, loss = 0.05738977\n",
      "Iteration 653, loss = 0.05844816\n",
      "Iteration 654, loss = 0.05717119\n",
      "Iteration 655, loss = 0.05799443\n",
      "Iteration 656, loss = 0.05720286\n",
      "Iteration 657, loss = 0.05649148\n",
      "Iteration 658, loss = 0.05707296\n",
      "Iteration 659, loss = 0.05617546\n",
      "Iteration 660, loss = 0.05619879\n",
      "Iteration 661, loss = 0.05653345\n",
      "Iteration 662, loss = 0.05627316\n",
      "Iteration 663, loss = 0.05662873\n",
      "Iteration 664, loss = 0.05616970\n",
      "Iteration 665, loss = 0.05531875\n",
      "Iteration 666, loss = 0.05967961\n",
      "Iteration 667, loss = 0.05995422\n",
      "Iteration 668, loss = 0.05732346\n",
      "Iteration 669, loss = 0.05662622\n",
      "Iteration 670, loss = 0.05497088\n",
      "Iteration 671, loss = 0.05448283\n",
      "Iteration 672, loss = 0.05421079\n",
      "Iteration 673, loss = 0.05488981\n",
      "Iteration 674, loss = 0.05429842\n",
      "Iteration 675, loss = 0.05425388\n",
      "Iteration 676, loss = 0.05388511\n",
      "Iteration 677, loss = 0.05330011\n",
      "Iteration 678, loss = 0.05279807\n",
      "Iteration 679, loss = 0.05288336\n",
      "Iteration 680, loss = 0.05370424\n",
      "Iteration 681, loss = 0.05307292\n",
      "Iteration 682, loss = 0.05357006\n",
      "Iteration 683, loss = 0.05246723\n",
      "Iteration 684, loss = 0.05276394\n",
      "Iteration 685, loss = 0.05166637\n",
      "Iteration 686, loss = 0.05241501\n",
      "Iteration 687, loss = 0.05215438\n",
      "Iteration 688, loss = 0.05154802\n",
      "Iteration 689, loss = 0.05143597\n",
      "Iteration 690, loss = 0.05144190\n",
      "Iteration 691, loss = 0.05120750\n",
      "Iteration 692, loss = 0.05115000\n",
      "Iteration 693, loss = 0.05092606\n",
      "Iteration 694, loss = 0.05076356\n",
      "Iteration 695, loss = 0.05004150\n",
      "Iteration 696, loss = 0.05066467\n",
      "Iteration 697, loss = 0.05007962\n",
      "Iteration 698, loss = 0.04980319\n",
      "Iteration 699, loss = 0.04968812\n",
      "Iteration 700, loss = 0.05044831\n",
      "Iteration 701, loss = 0.04989931\n",
      "Iteration 702, loss = 0.04922309\n",
      "Iteration 703, loss = 0.04963064\n",
      "Iteration 704, loss = 0.04976225\n",
      "Iteration 705, loss = 0.04901207\n",
      "Iteration 706, loss = 0.04864833\n",
      "Iteration 707, loss = 0.04804984\n",
      "Iteration 708, loss = 0.04788717\n",
      "Iteration 709, loss = 0.04964883\n",
      "Iteration 710, loss = 0.04756592\n",
      "Iteration 711, loss = 0.04759194\n",
      "Iteration 712, loss = 0.04822923\n",
      "Iteration 713, loss = 0.04767247\n",
      "Iteration 714, loss = 0.04833681\n",
      "Iteration 715, loss = 0.04750020\n",
      "Iteration 716, loss = 0.04809391\n",
      "Iteration 717, loss = 0.04700417\n",
      "Iteration 718, loss = 0.04667197\n",
      "Iteration 719, loss = 0.04706300\n",
      "Iteration 720, loss = 0.04739381\n",
      "Iteration 721, loss = 0.04739192\n",
      "Iteration 722, loss = 0.04722372\n",
      "Iteration 723, loss = 0.04739701\n",
      "Iteration 724, loss = 0.04640925\n",
      "Iteration 725, loss = 0.04587108\n",
      "Iteration 726, loss = 0.04565772\n",
      "Iteration 727, loss = 0.04548760\n",
      "Iteration 728, loss = 0.04614717\n",
      "Iteration 729, loss = 0.04581437\n",
      "Iteration 730, loss = 0.04575649\n",
      "Iteration 731, loss = 0.04541504\n",
      "Iteration 732, loss = 0.04446044\n",
      "Iteration 733, loss = 0.04464691\n",
      "Iteration 734, loss = 0.04431285\n",
      "Iteration 735, loss = 0.04411738\n",
      "Iteration 736, loss = 0.04443964\n",
      "Iteration 737, loss = 0.04373841\n",
      "Iteration 738, loss = 0.04399206\n",
      "Iteration 739, loss = 0.04395136\n",
      "Iteration 740, loss = 0.04350865\n",
      "Iteration 741, loss = 0.04363773\n",
      "Iteration 742, loss = 0.04355621\n",
      "Iteration 743, loss = 0.04335986\n",
      "Iteration 744, loss = 0.04306217\n",
      "Iteration 745, loss = 0.04395866\n",
      "Iteration 746, loss = 0.04275501\n",
      "Iteration 747, loss = 0.04293936\n",
      "Iteration 748, loss = 0.04272711\n",
      "Iteration 749, loss = 0.04257309\n",
      "Iteration 750, loss = 0.06041197\n",
      "Iteration 751, loss = 0.05559315\n",
      "Iteration 752, loss = 0.04741823\n",
      "Iteration 753, loss = 0.04340304\n",
      "Iteration 754, loss = 0.04220133\n",
      "Iteration 755, loss = 0.04210327\n",
      "Iteration 756, loss = 0.04233979\n",
      "Iteration 757, loss = 0.04173433\n",
      "Iteration 758, loss = 0.04227663\n",
      "Iteration 759, loss = 0.04172315\n",
      "Iteration 760, loss = 0.04204066\n",
      "Iteration 761, loss = 0.04246187\n",
      "Iteration 762, loss = 0.04348482\n",
      "Iteration 763, loss = 0.04228975\n",
      "Iteration 764, loss = 0.04240929\n",
      "Iteration 765, loss = 0.04132898\n",
      "Iteration 766, loss = 0.04080353\n",
      "Iteration 767, loss = 0.04129149\n",
      "Iteration 768, loss = 0.04095982\n",
      "Iteration 769, loss = 0.04088661\n",
      "Iteration 770, loss = 0.04006345\n",
      "Iteration 771, loss = 0.03975147\n",
      "Iteration 772, loss = 0.03990649\n",
      "Iteration 773, loss = 0.03966199\n",
      "Iteration 774, loss = 0.04028624\n",
      "Iteration 775, loss = 0.03963537\n",
      "Iteration 776, loss = 0.03932771\n",
      "Iteration 777, loss = 0.04009579\n",
      "Iteration 778, loss = 0.04063868\n",
      "Iteration 779, loss = 0.04084209\n",
      "Iteration 780, loss = 0.03948959\n",
      "Iteration 781, loss = 0.03909064\n",
      "Iteration 782, loss = 0.03909712\n",
      "Iteration 783, loss = 0.04026092\n",
      "Iteration 784, loss = 0.03890857\n",
      "Iteration 785, loss = 0.03835926\n",
      "Iteration 786, loss = 0.03850694\n",
      "Iteration 787, loss = 0.03912183\n",
      "Iteration 788, loss = 0.03897226\n",
      "Iteration 789, loss = 0.03824216\n",
      "Iteration 790, loss = 0.03759411\n",
      "Iteration 791, loss = 0.03767444\n",
      "Iteration 792, loss = 0.03787442\n",
      "Iteration 793, loss = 0.03813506\n",
      "Iteration 794, loss = 0.03772262\n",
      "Iteration 795, loss = 0.03722214\n",
      "Iteration 796, loss = 0.03696061\n",
      "Iteration 797, loss = 0.03715896\n",
      "Iteration 798, loss = 0.03705837\n",
      "Iteration 799, loss = 0.03682769\n",
      "Iteration 800, loss = 0.03678574\n",
      "Iteration 801, loss = 0.03736930\n",
      "Iteration 802, loss = 0.03758981\n",
      "Iteration 803, loss = 0.03711415\n",
      "Iteration 804, loss = 0.03629345\n",
      "Iteration 805, loss = 0.03643021\n",
      "Iteration 806, loss = 0.03657033\n",
      "Iteration 807, loss = 0.03590455\n",
      "Iteration 808, loss = 0.03664358\n",
      "Iteration 809, loss = 0.03626771\n",
      "Iteration 810, loss = 0.03588785\n",
      "Iteration 811, loss = 0.03607525\n",
      "Iteration 812, loss = 0.03620831\n",
      "Iteration 813, loss = 0.03593331\n",
      "Iteration 814, loss = 0.03546255\n",
      "Iteration 815, loss = 0.03531247\n",
      "Iteration 816, loss = 0.03507041\n",
      "Iteration 817, loss = 0.03502488\n",
      "Iteration 818, loss = 0.03528298\n",
      "Iteration 819, loss = 0.03573772\n",
      "Iteration 820, loss = 0.03570374\n",
      "Iteration 821, loss = 0.03479997\n",
      "Iteration 822, loss = 0.03497400\n",
      "Iteration 823, loss = 0.03478228\n",
      "Iteration 824, loss = 0.03416217\n",
      "Iteration 825, loss = 0.03421728\n",
      "Iteration 826, loss = 0.03405915\n",
      "Iteration 827, loss = 0.03389048\n",
      "Iteration 828, loss = 0.03405112\n",
      "Iteration 829, loss = 0.03423158\n",
      "Iteration 830, loss = 0.03410210\n",
      "Iteration 831, loss = 0.03510837\n",
      "Iteration 832, loss = 0.03387613\n",
      "Iteration 833, loss = 0.03384769\n",
      "Iteration 834, loss = 0.03384674\n",
      "Iteration 835, loss = 0.03390235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 836, loss = 0.03496190\n",
      "Iteration 837, loss = 0.03355372\n",
      "Iteration 838, loss = 0.03328255\n",
      "Iteration 839, loss = 0.03399830\n",
      "Iteration 840, loss = 0.03362654\n",
      "Iteration 841, loss = 0.03333871\n",
      "Iteration 842, loss = 0.03289543\n",
      "Iteration 843, loss = 0.03265618\n",
      "Iteration 844, loss = 0.03278047\n",
      "Iteration 845, loss = 0.03292912\n",
      "Iteration 846, loss = 0.03239375\n",
      "Iteration 847, loss = 0.03287767\n",
      "Iteration 848, loss = 0.03244354\n",
      "Iteration 849, loss = 0.03227476\n",
      "Iteration 850, loss = 0.03213306\n",
      "Iteration 851, loss = 0.03251738\n",
      "Iteration 852, loss = 0.03260906\n",
      "Iteration 853, loss = 0.03223942\n",
      "Iteration 854, loss = 0.03185302\n",
      "Iteration 855, loss = 0.03178834\n",
      "Iteration 856, loss = 0.03178021\n",
      "Iteration 857, loss = 0.03159469\n",
      "Iteration 858, loss = 0.03150635\n",
      "Iteration 859, loss = 0.03164694\n",
      "Iteration 860, loss = 0.03194254\n",
      "Iteration 861, loss = 0.03133558\n",
      "Iteration 862, loss = 0.03134625\n",
      "Iteration 863, loss = 0.03145262\n",
      "Iteration 864, loss = 0.03161044\n",
      "Iteration 865, loss = 0.03135811\n",
      "Iteration 866, loss = 0.03110200\n",
      "Iteration 867, loss = 0.03060004\n",
      "Iteration 868, loss = 0.03126892\n",
      "Iteration 869, loss = 0.03081306\n",
      "Iteration 870, loss = 0.03064820\n",
      "Iteration 871, loss = 0.03002679\n",
      "Iteration 872, loss = 0.03003642\n",
      "Iteration 873, loss = 0.03055460\n",
      "Iteration 874, loss = 0.03006656\n",
      "Iteration 875, loss = 0.03034685\n",
      "Iteration 876, loss = 0.02994848\n",
      "Iteration 877, loss = 0.03007515\n",
      "Iteration 878, loss = 0.02974211\n",
      "Iteration 879, loss = 0.03075556\n",
      "Iteration 880, loss = 0.02995503\n",
      "Iteration 881, loss = 0.02977976\n",
      "Iteration 882, loss = 0.02956565\n",
      "Iteration 883, loss = 0.02941482\n",
      "Iteration 884, loss = 0.02932369\n",
      "Iteration 885, loss = 0.02948125\n",
      "Iteration 886, loss = 0.02932077\n",
      "Iteration 887, loss = 0.02952054\n",
      "Iteration 888, loss = 0.02876660\n",
      "Iteration 889, loss = 0.02944709\n",
      "Iteration 890, loss = 0.02948276\n",
      "Iteration 891, loss = 0.02969756\n",
      "Iteration 892, loss = 0.02888741\n",
      "Iteration 893, loss = 0.02883045\n",
      "Iteration 894, loss = 0.02913792\n",
      "Iteration 895, loss = 0.02921523\n",
      "Iteration 896, loss = 0.02893993\n",
      "Iteration 897, loss = 0.02857629\n",
      "Iteration 898, loss = 0.02853260\n",
      "Iteration 899, loss = 0.02828592\n",
      "Iteration 900, loss = 0.02818747\n",
      "Iteration 901, loss = 0.02856649\n",
      "Iteration 902, loss = 0.02844661\n",
      "Iteration 903, loss = 0.02809833\n",
      "Iteration 904, loss = 0.02817539\n",
      "Iteration 905, loss = 0.02760790\n",
      "Iteration 906, loss = 0.02781659\n",
      "Iteration 907, loss = 0.02778864\n",
      "Iteration 908, loss = 0.02761346\n",
      "Iteration 909, loss = 0.02786603\n",
      "Iteration 910, loss = 0.02828825\n",
      "Iteration 911, loss = 0.02811673\n",
      "Iteration 912, loss = 0.02758090\n",
      "Iteration 913, loss = 0.02746780\n",
      "Iteration 914, loss = 0.02680355\n",
      "Iteration 915, loss = 0.02712820\n",
      "Iteration 916, loss = 0.02719150\n",
      "Iteration 917, loss = 0.02688198\n",
      "Iteration 918, loss = 0.02693819\n",
      "Iteration 919, loss = 0.02711439\n",
      "Iteration 920, loss = 0.02732657\n",
      "Iteration 921, loss = 0.02669812\n",
      "Iteration 922, loss = 0.02660034\n",
      "Iteration 923, loss = 0.02696773\n",
      "Iteration 924, loss = 0.02673323\n",
      "Iteration 925, loss = 0.02649712\n",
      "Iteration 926, loss = 0.02643483\n",
      "Iteration 927, loss = 0.02603377\n",
      "Iteration 928, loss = 0.02617465\n",
      "Iteration 929, loss = 0.02679093\n",
      "Iteration 930, loss = 0.02607107\n",
      "Iteration 931, loss = 0.02607646\n",
      "Iteration 932, loss = 0.02650917\n",
      "Iteration 933, loss = 0.02581466\n",
      "Iteration 934, loss = 0.02592306\n",
      "Iteration 935, loss = 0.02586895\n",
      "Iteration 936, loss = 0.02561107\n",
      "Iteration 937, loss = 0.02631208\n",
      "Iteration 938, loss = 0.02572441\n",
      "Iteration 939, loss = 0.02565199\n",
      "Iteration 940, loss = 0.02544416\n",
      "Iteration 941, loss = 0.02546316\n",
      "Iteration 942, loss = 0.02562882\n",
      "Iteration 943, loss = 0.02523982\n",
      "Iteration 944, loss = 0.02483886\n",
      "Iteration 945, loss = 0.02516622\n",
      "Iteration 946, loss = 0.02532165\n",
      "Iteration 947, loss = 0.02497046\n",
      "Iteration 948, loss = 0.02485003\n",
      "Iteration 949, loss = 0.02521955\n",
      "Iteration 950, loss = 0.02573571\n",
      "Iteration 951, loss = 0.02622720\n",
      "Iteration 952, loss = 0.02496409\n",
      "Iteration 953, loss = 0.02445082\n",
      "Iteration 954, loss = 0.02456363\n",
      "Iteration 955, loss = 0.02437516\n",
      "Iteration 956, loss = 0.02460397\n",
      "Iteration 957, loss = 0.02420212\n",
      "Iteration 958, loss = 0.02442604\n",
      "Iteration 959, loss = 0.02424542\n",
      "Iteration 960, loss = 0.02426440\n",
      "Iteration 961, loss = 0.02409575\n",
      "Iteration 962, loss = 0.02396326\n",
      "Iteration 963, loss = 0.02423728\n",
      "Iteration 964, loss = 0.02426292\n",
      "Iteration 965, loss = 0.02402155\n",
      "Iteration 966, loss = 0.02388136\n",
      "Iteration 967, loss = 0.02358952\n",
      "Iteration 968, loss = 0.02389973\n",
      "Iteration 969, loss = 0.02390301\n",
      "Iteration 970, loss = 0.02367388\n",
      "Iteration 971, loss = 0.02362359\n",
      "Iteration 972, loss = 0.02342500\n",
      "Iteration 973, loss = 0.02454611\n",
      "Iteration 974, loss = 0.02364151\n",
      "Iteration 975, loss = 0.02338594\n",
      "Iteration 976, loss = 0.02330434\n",
      "Iteration 977, loss = 0.02323873\n",
      "Iteration 978, loss = 0.02312149\n",
      "Iteration 979, loss = 0.02296064\n",
      "Iteration 980, loss = 0.02308133\n",
      "Iteration 981, loss = 0.02329787\n",
      "Iteration 982, loss = 0.02300434\n",
      "Iteration 983, loss = 0.02321975\n",
      "Iteration 984, loss = 0.02341516\n",
      "Iteration 985, loss = 0.02292957\n",
      "Iteration 986, loss = 0.02313030\n",
      "Iteration 987, loss = 0.02296298\n",
      "Iteration 988, loss = 0.02251233\n",
      "Iteration 989, loss = 0.02272777\n",
      "Iteration 990, loss = 0.02276202\n",
      "Iteration 991, loss = 0.02259991\n",
      "Iteration 992, loss = 0.02269056\n",
      "Iteration 993, loss = 0.02251920\n",
      "Iteration 994, loss = 0.02278837\n",
      "Iteration 995, loss = 0.02233171\n",
      "Iteration 996, loss = 0.02243622\n",
      "Iteration 997, loss = 0.02262440\n",
      "Iteration 998, loss = 0.02222868\n",
      "Iteration 999, loss = 0.02199928\n",
      "Iteration 1000, loss = 0.02200197\n",
      "Iteration 1001, loss = 0.02215331\n",
      "Iteration 1002, loss = 0.02167519\n",
      "Iteration 1003, loss = 0.02178031\n",
      "Iteration 1004, loss = 0.02181334\n",
      "Iteration 1005, loss = 0.02153752\n",
      "Iteration 1006, loss = 0.02178354\n",
      "Iteration 1007, loss = 0.02145135\n",
      "Iteration 1008, loss = 0.02178044\n",
      "Iteration 1009, loss = 0.02165485\n",
      "Iteration 1010, loss = 0.02159145\n",
      "Iteration 1011, loss = 0.02137554\n",
      "Iteration 1012, loss = 0.02130212\n",
      "Iteration 1013, loss = 0.02137214\n",
      "Iteration 1014, loss = 0.02244627\n",
      "Iteration 1015, loss = 0.02145839\n",
      "Iteration 1016, loss = 0.02149335\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 15 of 48...\n",
      "Iteration 1, loss = 0.69193399\n",
      "Iteration 2, loss = 0.66573342\n",
      "Iteration 3, loss = 0.64253583\n",
      "Iteration 4, loss = 0.62251451\n",
      "Iteration 5, loss = 0.60333608\n",
      "Iteration 6, loss = 0.58623121\n",
      "Iteration 7, loss = 0.56742247\n",
      "Iteration 8, loss = 0.55222545\n",
      "Iteration 9, loss = 0.53579080\n",
      "Iteration 10, loss = 0.52154488\n",
      "Iteration 11, loss = 0.50864708\n",
      "Iteration 12, loss = 0.49730027\n",
      "Iteration 13, loss = 0.48755343\n",
      "Iteration 14, loss = 0.47775050\n",
      "Iteration 15, loss = 0.46806099\n",
      "Iteration 16, loss = 0.46157586\n",
      "Iteration 17, loss = 0.45525491\n",
      "Iteration 18, loss = 0.44640763\n",
      "Iteration 19, loss = 0.43992792\n",
      "Iteration 20, loss = 0.43385876\n",
      "Iteration 21, loss = 0.42948875\n",
      "Iteration 22, loss = 0.42471377\n",
      "Iteration 23, loss = 0.42302939\n",
      "Iteration 24, loss = 0.41843717\n",
      "Iteration 25, loss = 0.41863978\n",
      "Iteration 26, loss = 0.41714627\n",
      "Iteration 27, loss = 0.40661841\n",
      "Iteration 28, loss = 0.40205347\n",
      "Iteration 29, loss = 0.39930166\n",
      "Iteration 30, loss = 0.39612934\n",
      "Iteration 31, loss = 0.39341627\n",
      "Iteration 32, loss = 0.39095933\n",
      "Iteration 33, loss = 0.38907922\n",
      "Iteration 34, loss = 0.38626503\n",
      "Iteration 35, loss = 0.38417440\n",
      "Iteration 36, loss = 0.38199853\n",
      "Iteration 37, loss = 0.38070774\n",
      "Iteration 38, loss = 0.37923068\n",
      "Iteration 39, loss = 0.37750995\n",
      "Iteration 40, loss = 0.37385423\n",
      "Iteration 41, loss = 0.37187631\n",
      "Iteration 42, loss = 0.37013821\n",
      "Iteration 43, loss = 0.36820514\n",
      "Iteration 44, loss = 0.36561638\n",
      "Iteration 45, loss = 0.36438531\n",
      "Iteration 46, loss = 0.36395429\n",
      "Iteration 47, loss = 0.36244742\n",
      "Iteration 48, loss = 0.36086487\n",
      "Iteration 49, loss = 0.36069180\n",
      "Iteration 50, loss = 0.35831942\n",
      "Iteration 51, loss = 0.35473625\n",
      "Iteration 52, loss = 0.35422016\n",
      "Iteration 53, loss = 0.35528748\n",
      "Iteration 54, loss = 0.35244604\n",
      "Iteration 55, loss = 0.35012195\n",
      "Iteration 56, loss = 0.34922787\n",
      "Iteration 57, loss = 0.34789444\n",
      "Iteration 58, loss = 0.34738895\n",
      "Iteration 59, loss = 0.34576821\n",
      "Iteration 60, loss = 0.34297481\n",
      "Iteration 61, loss = 0.34286293\n",
      "Iteration 62, loss = 0.34151504\n",
      "Iteration 63, loss = 0.34325327\n",
      "Iteration 64, loss = 0.33968248\n",
      "Iteration 65, loss = 0.33807588\n",
      "Iteration 66, loss = 0.33672136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67, loss = 0.33602696\n",
      "Iteration 68, loss = 0.33545993\n",
      "Iteration 69, loss = 0.33322946\n",
      "Iteration 70, loss = 0.33259974\n",
      "Iteration 71, loss = 0.33166698\n",
      "Iteration 72, loss = 0.33042911\n",
      "Iteration 73, loss = 0.32940965\n",
      "Iteration 74, loss = 0.33008567\n",
      "Iteration 75, loss = 0.32665023\n",
      "Iteration 76, loss = 0.32674565\n",
      "Iteration 77, loss = 0.32662092\n",
      "Iteration 78, loss = 0.32480718\n",
      "Iteration 79, loss = 0.32409051\n",
      "Iteration 80, loss = 0.32543206\n",
      "Iteration 81, loss = 0.32201314\n",
      "Iteration 82, loss = 0.32052126\n",
      "Iteration 83, loss = 0.32117616\n",
      "Iteration 84, loss = 0.32019839\n",
      "Iteration 85, loss = 0.31912662\n",
      "Iteration 86, loss = 0.31759029\n",
      "Iteration 87, loss = 0.31872073\n",
      "Iteration 88, loss = 0.31738285\n",
      "Iteration 89, loss = 0.31586625\n",
      "Iteration 90, loss = 0.31466422\n",
      "Iteration 91, loss = 0.31413287\n",
      "Iteration 92, loss = 0.31364573\n",
      "Iteration 93, loss = 0.31191445\n",
      "Iteration 94, loss = 0.31222880\n",
      "Iteration 95, loss = 0.31309884\n",
      "Iteration 96, loss = 0.31132651\n",
      "Iteration 97, loss = 0.31180240\n",
      "Iteration 98, loss = 0.31184160\n",
      "Iteration 99, loss = 0.30971517\n",
      "Iteration 100, loss = 0.31051540\n",
      "Iteration 101, loss = 0.31789183\n",
      "Iteration 102, loss = 0.30769580\n",
      "Iteration 103, loss = 0.30532296\n",
      "Iteration 104, loss = 0.30453474\n",
      "Iteration 105, loss = 0.30352513\n",
      "Iteration 106, loss = 0.30474934\n",
      "Iteration 107, loss = 0.30316540\n",
      "Iteration 108, loss = 0.30300149\n",
      "Iteration 109, loss = 0.30659877\n",
      "Iteration 110, loss = 0.30318684\n",
      "Iteration 111, loss = 0.29949215\n",
      "Iteration 112, loss = 0.29910005\n",
      "Iteration 113, loss = 0.29866101\n",
      "Iteration 114, loss = 0.29924549\n",
      "Iteration 115, loss = 0.29812614\n",
      "Iteration 116, loss = 0.29748302\n",
      "Iteration 117, loss = 0.29620243\n",
      "Iteration 118, loss = 0.29464913\n",
      "Iteration 119, loss = 0.29641385\n",
      "Iteration 120, loss = 0.29617253\n",
      "Iteration 121, loss = 0.29699187\n",
      "Iteration 122, loss = 0.29368311\n",
      "Iteration 123, loss = 0.29318370\n",
      "Iteration 124, loss = 0.29179347\n",
      "Iteration 125, loss = 0.29255994\n",
      "Iteration 126, loss = 0.29397445\n",
      "Iteration 127, loss = 0.29216743\n",
      "Iteration 128, loss = 0.28940600\n",
      "Iteration 129, loss = 0.28892942\n",
      "Iteration 130, loss = 0.28806708\n",
      "Iteration 131, loss = 0.28858499\n",
      "Iteration 132, loss = 0.28835456\n",
      "Iteration 133, loss = 0.28931562\n",
      "Iteration 134, loss = 0.28543982\n",
      "Iteration 135, loss = 0.28590102\n",
      "Iteration 136, loss = 0.28821737\n",
      "Iteration 137, loss = 0.28567553\n",
      "Iteration 138, loss = 0.28466039\n",
      "Iteration 139, loss = 0.28299398\n",
      "Iteration 140, loss = 0.28362463\n",
      "Iteration 141, loss = 0.28267044\n",
      "Iteration 142, loss = 0.28140213\n",
      "Iteration 143, loss = 0.28392817\n",
      "Iteration 144, loss = 0.28205405\n",
      "Iteration 145, loss = 0.28054812\n",
      "Iteration 146, loss = 0.27845117\n",
      "Iteration 147, loss = 0.28222541\n",
      "Iteration 148, loss = 0.27992678\n",
      "Iteration 149, loss = 0.27774306\n",
      "Iteration 150, loss = 0.27801153\n",
      "Iteration 151, loss = 0.27864564\n",
      "Iteration 152, loss = 0.27533564\n",
      "Iteration 153, loss = 0.27510052\n",
      "Iteration 154, loss = 0.27436356\n",
      "Iteration 155, loss = 0.27496119\n",
      "Iteration 156, loss = 0.27438604\n",
      "Iteration 157, loss = 0.27529439\n",
      "Iteration 158, loss = 0.27204879\n",
      "Iteration 159, loss = 0.27841939\n",
      "Iteration 160, loss = 0.27321157\n",
      "Iteration 161, loss = 0.27350506\n",
      "Iteration 162, loss = 0.27375192\n",
      "Iteration 163, loss = 0.27079011\n",
      "Iteration 164, loss = 0.27352351\n",
      "Iteration 165, loss = 0.27048921\n",
      "Iteration 166, loss = 0.26883184\n",
      "Iteration 167, loss = 0.27005618\n",
      "Iteration 168, loss = 0.27090321\n",
      "Iteration 169, loss = 0.26967721\n",
      "Iteration 170, loss = 0.26622397\n",
      "Iteration 171, loss = 0.26529650\n",
      "Iteration 172, loss = 0.26651304\n",
      "Iteration 173, loss = 0.26532263\n",
      "Iteration 174, loss = 0.26445963\n",
      "Iteration 175, loss = 0.26645775\n",
      "Iteration 176, loss = 0.26514632\n",
      "Iteration 177, loss = 0.26283858\n",
      "Iteration 178, loss = 0.26207056\n",
      "Iteration 179, loss = 0.26076842\n",
      "Iteration 180, loss = 0.26185723\n",
      "Iteration 181, loss = 0.25938482\n",
      "Iteration 182, loss = 0.26124430\n",
      "Iteration 183, loss = 0.26026380\n",
      "Iteration 184, loss = 0.26142272\n",
      "Iteration 185, loss = 0.25985786\n",
      "Iteration 186, loss = 0.25755078\n",
      "Iteration 187, loss = 0.25872995\n",
      "Iteration 188, loss = 0.25903442\n",
      "Iteration 189, loss = 0.25711794\n",
      "Iteration 190, loss = 0.25580052\n",
      "Iteration 191, loss = 0.25554520\n",
      "Iteration 192, loss = 0.25741342\n",
      "Iteration 193, loss = 0.25490596\n",
      "Iteration 194, loss = 0.25535803\n",
      "Iteration 195, loss = 0.25441431\n",
      "Iteration 196, loss = 0.25569750\n",
      "Iteration 197, loss = 0.25323624\n",
      "Iteration 198, loss = 0.25082210\n",
      "Iteration 199, loss = 0.24976038\n",
      "Iteration 200, loss = 0.25131687\n",
      "Iteration 201, loss = 0.25097574\n",
      "Iteration 202, loss = 0.25771539\n",
      "Iteration 203, loss = 0.25024491\n",
      "Iteration 204, loss = 0.25152064\n",
      "Iteration 205, loss = 0.24902263\n",
      "Iteration 206, loss = 0.24852803\n",
      "Iteration 207, loss = 0.24607896\n",
      "Iteration 208, loss = 0.24549724\n",
      "Iteration 209, loss = 0.24594547\n",
      "Iteration 210, loss = 0.24521217\n",
      "Iteration 211, loss = 0.24479534\n",
      "Iteration 212, loss = 0.24359428\n",
      "Iteration 213, loss = 0.24278990\n",
      "Iteration 214, loss = 0.24340072\n",
      "Iteration 215, loss = 0.24635347\n",
      "Iteration 216, loss = 0.24210188\n",
      "Iteration 217, loss = 0.24302323\n",
      "Iteration 218, loss = 0.24097873\n",
      "Iteration 219, loss = 0.24194899\n",
      "Iteration 220, loss = 0.24065278\n",
      "Iteration 221, loss = 0.23994305\n",
      "Iteration 222, loss = 0.23911822\n",
      "Iteration 223, loss = 0.23755619\n",
      "Iteration 224, loss = 0.23800101\n",
      "Iteration 225, loss = 0.23578046\n",
      "Iteration 226, loss = 0.23608709\n",
      "Iteration 227, loss = 0.23780187\n",
      "Iteration 228, loss = 0.23604552\n",
      "Iteration 229, loss = 0.23423890\n",
      "Iteration 230, loss = 0.23383903\n",
      "Iteration 231, loss = 0.23305256\n",
      "Iteration 232, loss = 0.23294752\n",
      "Iteration 233, loss = 0.23210029\n",
      "Iteration 234, loss = 0.23345572\n",
      "Iteration 235, loss = 0.23247072\n",
      "Iteration 236, loss = 0.23096481\n",
      "Iteration 237, loss = 0.23072474\n",
      "Iteration 238, loss = 0.23076731\n",
      "Iteration 239, loss = 0.23336764\n",
      "Iteration 240, loss = 0.23041435\n",
      "Iteration 241, loss = 0.22927620\n",
      "Iteration 242, loss = 0.22697160\n",
      "Iteration 243, loss = 0.22791500\n",
      "Iteration 244, loss = 0.22602775\n",
      "Iteration 245, loss = 0.23047252\n",
      "Iteration 246, loss = 0.22636776\n",
      "Iteration 247, loss = 0.22451826\n",
      "Iteration 248, loss = 0.22321347\n",
      "Iteration 249, loss = 0.22309926\n",
      "Iteration 250, loss = 0.22230728\n",
      "Iteration 251, loss = 0.22248706\n",
      "Iteration 252, loss = 0.22425946\n",
      "Iteration 253, loss = 0.22189051\n",
      "Iteration 254, loss = 0.22253087\n",
      "Iteration 255, loss = 0.22189170\n",
      "Iteration 256, loss = 0.22002614\n",
      "Iteration 257, loss = 0.22004916\n",
      "Iteration 258, loss = 0.21889987\n",
      "Iteration 259, loss = 0.21777309\n",
      "Iteration 260, loss = 0.21872725\n",
      "Iteration 261, loss = 0.21658410\n",
      "Iteration 262, loss = 0.21643922\n",
      "Iteration 263, loss = 0.21768246\n",
      "Iteration 264, loss = 0.21579270\n",
      "Iteration 265, loss = 0.21520965\n",
      "Iteration 266, loss = 0.21397754\n",
      "Iteration 267, loss = 0.21349272\n",
      "Iteration 268, loss = 0.21395495\n",
      "Iteration 269, loss = 0.21415701\n",
      "Iteration 270, loss = 0.21414260\n",
      "Iteration 271, loss = 0.21195191\n",
      "Iteration 272, loss = 0.21026302\n",
      "Iteration 273, loss = 0.21169084\n",
      "Iteration 274, loss = 0.21144081\n",
      "Iteration 275, loss = 0.21023616\n",
      "Iteration 276, loss = 0.20938956\n",
      "Iteration 277, loss = 0.20703616\n",
      "Iteration 278, loss = 0.20766702\n",
      "Iteration 279, loss = 0.20831035\n",
      "Iteration 280, loss = 0.20685886\n",
      "Iteration 281, loss = 0.21362138\n",
      "Iteration 282, loss = 0.20569954\n",
      "Iteration 283, loss = 0.20570335\n",
      "Iteration 284, loss = 0.20432116\n",
      "Iteration 285, loss = 0.20297370\n",
      "Iteration 286, loss = 0.20516990\n",
      "Iteration 287, loss = 0.20307675\n",
      "Iteration 288, loss = 0.20307674\n",
      "Iteration 289, loss = 0.20190131\n",
      "Iteration 290, loss = 0.20084451\n",
      "Iteration 291, loss = 0.20091825\n",
      "Iteration 292, loss = 0.19880314\n",
      "Iteration 293, loss = 0.19937427\n",
      "Iteration 294, loss = 0.19939590\n",
      "Iteration 295, loss = 0.19973321\n",
      "Iteration 296, loss = 0.19818918\n",
      "Iteration 297, loss = 0.19675099\n",
      "Iteration 298, loss = 0.19859864\n",
      "Iteration 299, loss = 0.19694589\n",
      "Iteration 300, loss = 0.19658088\n",
      "Iteration 301, loss = 0.19528798\n",
      "Iteration 302, loss = 0.19433162\n",
      "Iteration 303, loss = 0.19347901\n",
      "Iteration 304, loss = 0.19369036\n",
      "Iteration 305, loss = 0.19236848\n",
      "Iteration 306, loss = 0.19390040\n",
      "Iteration 307, loss = 0.19183074\n",
      "Iteration 308, loss = 0.19262314\n",
      "Iteration 309, loss = 0.19143669\n",
      "Iteration 310, loss = 0.18995379\n",
      "Iteration 311, loss = 0.19064090\n",
      "Iteration 312, loss = 0.19144464\n",
      "Iteration 313, loss = 0.18806337\n",
      "Iteration 314, loss = 0.18794399\n",
      "Iteration 315, loss = 0.18692828\n",
      "Iteration 316, loss = 0.18612342\n",
      "Iteration 317, loss = 0.18647666\n",
      "Iteration 318, loss = 0.18494590\n",
      "Iteration 319, loss = 0.18480597\n",
      "Iteration 320, loss = 0.18466668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 321, loss = 0.18371819\n",
      "Iteration 322, loss = 0.18432893\n",
      "Iteration 323, loss = 0.18538454\n",
      "Iteration 324, loss = 0.18440348\n",
      "Iteration 325, loss = 0.18588949\n",
      "Iteration 326, loss = 0.18487885\n",
      "Iteration 327, loss = 0.18113524\n",
      "Iteration 328, loss = 0.18195760\n",
      "Iteration 329, loss = 0.18012407\n",
      "Iteration 330, loss = 0.17852365\n",
      "Iteration 331, loss = 0.17893000\n",
      "Iteration 332, loss = 0.18140819\n",
      "Iteration 333, loss = 0.18049452\n",
      "Iteration 334, loss = 0.17773141\n",
      "Iteration 335, loss = 0.17690913\n",
      "Iteration 336, loss = 0.17863700\n",
      "Iteration 337, loss = 0.17719887\n",
      "Iteration 338, loss = 0.17628474\n",
      "Iteration 339, loss = 0.17559255\n",
      "Iteration 340, loss = 0.17472215\n",
      "Iteration 341, loss = 0.17465804\n",
      "Iteration 342, loss = 0.17316866\n",
      "Iteration 343, loss = 0.17265915\n",
      "Iteration 344, loss = 0.17527152\n",
      "Iteration 345, loss = 0.17404150\n",
      "Iteration 346, loss = 0.17217744\n",
      "Iteration 347, loss = 0.17166535\n",
      "Iteration 348, loss = 0.17118654\n",
      "Iteration 349, loss = 0.17106949\n",
      "Iteration 350, loss = 0.17032363\n",
      "Iteration 351, loss = 0.16914941\n",
      "Iteration 352, loss = 0.16931369\n",
      "Iteration 353, loss = 0.16990758\n",
      "Iteration 354, loss = 0.17036119\n",
      "Iteration 355, loss = 0.16909061\n",
      "Iteration 356, loss = 0.16759748\n",
      "Iteration 357, loss = 0.16842683\n",
      "Iteration 358, loss = 0.16980446\n",
      "Iteration 359, loss = 0.16787536\n",
      "Iteration 360, loss = 0.16641657\n",
      "Iteration 361, loss = 0.16621112\n",
      "Iteration 362, loss = 0.16639166\n",
      "Iteration 363, loss = 0.16475627\n",
      "Iteration 364, loss = 0.16537515\n",
      "Iteration 365, loss = 0.16475933\n",
      "Iteration 366, loss = 0.16259302\n",
      "Iteration 367, loss = 0.16133563\n",
      "Iteration 368, loss = 0.16197196\n",
      "Iteration 369, loss = 0.16136612\n",
      "Iteration 370, loss = 0.16148719\n",
      "Iteration 371, loss = 0.16071434\n",
      "Iteration 372, loss = 0.15964094\n",
      "Iteration 373, loss = 0.15864041\n",
      "Iteration 374, loss = 0.15833097\n",
      "Iteration 375, loss = 0.15767213\n",
      "Iteration 376, loss = 0.15687390\n",
      "Iteration 377, loss = 0.15697928\n",
      "Iteration 378, loss = 0.15644224\n",
      "Iteration 379, loss = 0.15670790\n",
      "Iteration 380, loss = 0.15622135\n",
      "Iteration 381, loss = 0.15629012\n",
      "Iteration 382, loss = 0.15593769\n",
      "Iteration 383, loss = 0.15687832\n",
      "Iteration 384, loss = 0.15671002\n",
      "Iteration 385, loss = 0.15676856\n",
      "Iteration 386, loss = 0.15575434\n",
      "Iteration 387, loss = 0.15584361\n",
      "Iteration 388, loss = 0.15446240\n",
      "Iteration 389, loss = 0.15400995\n",
      "Iteration 390, loss = 0.15336931\n",
      "Iteration 391, loss = 0.15227519\n",
      "Iteration 392, loss = 0.15208299\n",
      "Iteration 393, loss = 0.15249483\n",
      "Iteration 394, loss = 0.15032456\n",
      "Iteration 395, loss = 0.15029091\n",
      "Iteration 396, loss = 0.14907914\n",
      "Iteration 397, loss = 0.15058464\n",
      "Iteration 398, loss = 0.15392081\n",
      "Iteration 399, loss = 0.15275111\n",
      "Iteration 400, loss = 0.14951414\n",
      "Iteration 401, loss = 0.14738646\n",
      "Iteration 402, loss = 0.14786826\n",
      "Iteration 403, loss = 0.14816409\n",
      "Iteration 404, loss = 0.14646642\n",
      "Iteration 405, loss = 0.14670250\n",
      "Iteration 406, loss = 0.14620203\n",
      "Iteration 407, loss = 0.14655534\n",
      "Iteration 408, loss = 0.14566077\n",
      "Iteration 409, loss = 0.14457526\n",
      "Iteration 410, loss = 0.14363528\n",
      "Iteration 411, loss = 0.14258398\n",
      "Iteration 412, loss = 0.14238082\n",
      "Iteration 413, loss = 0.14242957\n",
      "Iteration 414, loss = 0.14229770\n",
      "Iteration 415, loss = 0.14226974\n",
      "Iteration 416, loss = 0.14297031\n",
      "Iteration 417, loss = 0.14259464\n",
      "Iteration 418, loss = 0.14284414\n",
      "Iteration 419, loss = 0.14204386\n",
      "Iteration 420, loss = 0.13955635\n",
      "Iteration 421, loss = 0.14068303\n",
      "Iteration 422, loss = 0.14105959\n",
      "Iteration 423, loss = 0.14104256\n",
      "Iteration 424, loss = 0.14023675\n",
      "Iteration 425, loss = 0.13809460\n",
      "Iteration 426, loss = 0.13886686\n",
      "Iteration 427, loss = 0.13684626\n",
      "Iteration 428, loss = 0.13696882\n",
      "Iteration 429, loss = 0.13569556\n",
      "Iteration 430, loss = 0.13566284\n",
      "Iteration 431, loss = 0.13693136\n",
      "Iteration 432, loss = 0.13578714\n",
      "Iteration 433, loss = 0.13582492\n",
      "Iteration 434, loss = 0.13475988\n",
      "Iteration 435, loss = 0.13605687\n",
      "Iteration 436, loss = 0.13695487\n",
      "Iteration 437, loss = 0.13619128\n",
      "Iteration 438, loss = 0.13404445\n",
      "Iteration 439, loss = 0.13388504\n",
      "Iteration 440, loss = 0.13408393\n",
      "Iteration 441, loss = 0.13307550\n",
      "Iteration 442, loss = 0.13317260\n",
      "Iteration 443, loss = 0.13285351\n",
      "Iteration 444, loss = 0.13417088\n",
      "Iteration 445, loss = 0.13338874\n",
      "Iteration 446, loss = 0.13293241\n",
      "Iteration 447, loss = 0.13136525\n",
      "Iteration 448, loss = 0.12994088\n",
      "Iteration 449, loss = 0.13012360\n",
      "Iteration 450, loss = 0.12949361\n",
      "Iteration 451, loss = 0.12943610\n",
      "Iteration 452, loss = 0.12873856\n",
      "Iteration 453, loss = 0.13130257\n",
      "Iteration 454, loss = 0.13355582\n",
      "Iteration 455, loss = 0.12996100\n",
      "Iteration 456, loss = 0.12988517\n",
      "Iteration 457, loss = 0.13133824\n",
      "Iteration 458, loss = 0.12802826\n",
      "Iteration 459, loss = 0.12772648\n",
      "Iteration 460, loss = 0.12906478\n",
      "Iteration 461, loss = 0.12763815\n",
      "Iteration 462, loss = 0.12913550\n",
      "Iteration 463, loss = 0.12662632\n",
      "Iteration 464, loss = 0.12717814\n",
      "Iteration 465, loss = 0.12568610\n",
      "Iteration 466, loss = 0.12580267\n",
      "Iteration 467, loss = 0.12465303\n",
      "Iteration 468, loss = 0.12366542\n",
      "Iteration 469, loss = 0.12359874\n",
      "Iteration 470, loss = 0.12315239\n",
      "Iteration 471, loss = 0.12282293\n",
      "Iteration 472, loss = 0.12234483\n",
      "Iteration 473, loss = 0.12207879\n",
      "Iteration 474, loss = 0.12369496\n",
      "Iteration 475, loss = 0.12172413\n",
      "Iteration 476, loss = 0.12146612\n",
      "Iteration 477, loss = 0.12155107\n",
      "Iteration 478, loss = 0.12169986\n",
      "Iteration 479, loss = 0.12100879\n",
      "Iteration 480, loss = 0.11987806\n",
      "Iteration 481, loss = 0.12002185\n",
      "Iteration 482, loss = 0.11983336\n",
      "Iteration 483, loss = 0.12069424\n",
      "Iteration 484, loss = 0.11887190\n",
      "Iteration 485, loss = 0.11843043\n",
      "Iteration 486, loss = 0.11869515\n",
      "Iteration 487, loss = 0.11807083\n",
      "Iteration 488, loss = 0.11908013\n",
      "Iteration 489, loss = 0.11807271\n",
      "Iteration 490, loss = 0.11940776\n",
      "Iteration 491, loss = 0.11827327\n",
      "Iteration 492, loss = 0.11703898\n",
      "Iteration 493, loss = 0.11693332\n",
      "Iteration 494, loss = 0.11706368\n",
      "Iteration 495, loss = 0.11608619\n",
      "Iteration 496, loss = 0.11636184\n",
      "Iteration 497, loss = 0.11642507\n",
      "Iteration 498, loss = 0.11736748\n",
      "Iteration 499, loss = 0.11629489\n",
      "Iteration 500, loss = 0.11555393\n",
      "Iteration 501, loss = 0.11811864\n",
      "Iteration 502, loss = 0.11556754\n",
      "Iteration 503, loss = 0.11606095\n",
      "Iteration 504, loss = 0.11373349\n",
      "Iteration 505, loss = 0.11461956\n",
      "Iteration 506, loss = 0.11379713\n",
      "Iteration 507, loss = 0.11299960\n",
      "Iteration 508, loss = 0.11295337\n",
      "Iteration 509, loss = 0.11478710\n",
      "Iteration 510, loss = 0.11178710\n",
      "Iteration 511, loss = 0.11279213\n",
      "Iteration 512, loss = 0.11181120\n",
      "Iteration 513, loss = 0.11161463\n",
      "Iteration 514, loss = 0.11133921\n",
      "Iteration 515, loss = 0.11216681\n",
      "Iteration 516, loss = 0.11194906\n",
      "Iteration 517, loss = 0.11161434\n",
      "Iteration 518, loss = 0.10987315\n",
      "Iteration 519, loss = 0.10995465\n",
      "Iteration 520, loss = 0.11020732\n",
      "Iteration 521, loss = 0.10947228\n",
      "Iteration 522, loss = 0.10867212\n",
      "Iteration 523, loss = 0.10865410\n",
      "Iteration 524, loss = 0.10972501\n",
      "Iteration 525, loss = 0.10849315\n",
      "Iteration 526, loss = 0.10852931\n",
      "Iteration 527, loss = 0.10876391\n",
      "Iteration 528, loss = 0.10797386\n",
      "Iteration 529, loss = 0.10755571\n",
      "Iteration 530, loss = 0.10693041\n",
      "Iteration 531, loss = 0.10741508\n",
      "Iteration 532, loss = 0.10663891\n",
      "Iteration 533, loss = 0.10692479\n",
      "Iteration 534, loss = 0.10694415\n",
      "Iteration 535, loss = 0.10696607\n",
      "Iteration 536, loss = 0.10688504\n",
      "Iteration 537, loss = 0.10723182\n",
      "Iteration 538, loss = 0.10539901\n",
      "Iteration 539, loss = 0.10663801\n",
      "Iteration 540, loss = 0.10525273\n",
      "Iteration 541, loss = 0.10556401\n",
      "Iteration 542, loss = 0.10451577\n",
      "Iteration 543, loss = 0.10496709\n",
      "Iteration 544, loss = 0.10355240\n",
      "Iteration 545, loss = 0.10335993\n",
      "Iteration 546, loss = 0.10283702\n",
      "Iteration 547, loss = 0.10345942\n",
      "Iteration 548, loss = 0.10393500\n",
      "Iteration 549, loss = 0.10301702\n",
      "Iteration 550, loss = 0.10307027\n",
      "Iteration 551, loss = 0.10344124\n",
      "Iteration 552, loss = 0.10248247\n",
      "Iteration 553, loss = 0.10192574\n",
      "Iteration 554, loss = 0.10147657\n",
      "Iteration 555, loss = 0.10169231\n",
      "Iteration 556, loss = 0.10108484\n",
      "Iteration 557, loss = 0.10052941\n",
      "Iteration 558, loss = 0.10135041\n",
      "Iteration 559, loss = 0.10139298\n",
      "Iteration 560, loss = 0.10076447\n",
      "Iteration 561, loss = 0.09995392\n",
      "Iteration 562, loss = 0.09966928\n",
      "Iteration 563, loss = 0.09996228\n",
      "Iteration 564, loss = 0.09986495\n",
      "Iteration 565, loss = 0.09917021\n",
      "Iteration 566, loss = 0.09938015\n",
      "Iteration 567, loss = 0.10040232\n",
      "Iteration 568, loss = 0.09916848\n",
      "Iteration 569, loss = 0.09860659\n",
      "Iteration 570, loss = 0.09833238\n",
      "Iteration 571, loss = 0.09833931\n",
      "Iteration 572, loss = 0.09850496\n",
      "Iteration 573, loss = 0.09800383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 574, loss = 0.09793488\n",
      "Iteration 575, loss = 0.09690092\n",
      "Iteration 576, loss = 0.09831808\n",
      "Iteration 577, loss = 0.09765938\n",
      "Iteration 578, loss = 0.09706017\n",
      "Iteration 579, loss = 0.09746156\n",
      "Iteration 580, loss = 0.09689657\n",
      "Iteration 581, loss = 0.09659957\n",
      "Iteration 582, loss = 0.09572139\n",
      "Iteration 583, loss = 0.09621372\n",
      "Iteration 584, loss = 0.10031308\n",
      "Iteration 585, loss = 0.10039092\n",
      "Iteration 586, loss = 0.09724459\n",
      "Iteration 587, loss = 0.10065543\n",
      "Iteration 588, loss = 0.09629736\n",
      "Iteration 589, loss = 0.09506753\n",
      "Iteration 590, loss = 0.09638148\n",
      "Iteration 591, loss = 0.09524087\n",
      "Iteration 592, loss = 0.09498221\n",
      "Iteration 593, loss = 0.09576184\n",
      "Iteration 594, loss = 0.09389892\n",
      "Iteration 595, loss = 0.09312002\n",
      "Iteration 596, loss = 0.09406551\n",
      "Iteration 597, loss = 0.09306143\n",
      "Iteration 598, loss = 0.09276792\n",
      "Iteration 599, loss = 0.09466096\n",
      "Iteration 600, loss = 0.09315571\n",
      "Iteration 601, loss = 0.09214286\n",
      "Iteration 602, loss = 0.09235031\n",
      "Iteration 603, loss = 0.09556481\n",
      "Iteration 604, loss = 0.09390213\n",
      "Iteration 605, loss = 0.09231668\n",
      "Iteration 606, loss = 0.09134762\n",
      "Iteration 607, loss = 0.09172582\n",
      "Iteration 608, loss = 0.09024934\n",
      "Iteration 609, loss = 0.09050671\n",
      "Iteration 610, loss = 0.09140612\n",
      "Iteration 611, loss = 0.09383335\n",
      "Iteration 612, loss = 0.09588367\n",
      "Iteration 613, loss = 0.09295121\n",
      "Iteration 614, loss = 0.09052735\n",
      "Iteration 615, loss = 0.08976918\n",
      "Iteration 616, loss = 0.08929092\n",
      "Iteration 617, loss = 0.08940919\n",
      "Iteration 618, loss = 0.08969541\n",
      "Iteration 619, loss = 0.08958398\n",
      "Iteration 620, loss = 0.08960901\n",
      "Iteration 621, loss = 0.09028725\n",
      "Iteration 622, loss = 0.08996630\n",
      "Iteration 623, loss = 0.08881453\n",
      "Iteration 624, loss = 0.08881509\n",
      "Iteration 625, loss = 0.08793637\n",
      "Iteration 626, loss = 0.08774024\n",
      "Iteration 627, loss = 0.08902462\n",
      "Iteration 628, loss = 0.08886388\n",
      "Iteration 629, loss = 0.08821287\n",
      "Iteration 630, loss = 0.08874293\n",
      "Iteration 631, loss = 0.08933657\n",
      "Iteration 632, loss = 0.08844595\n",
      "Iteration 633, loss = 0.08828519\n",
      "Iteration 634, loss = 0.08997799\n",
      "Iteration 635, loss = 0.08938948\n",
      "Iteration 636, loss = 0.08888871\n",
      "Iteration 637, loss = 0.08929564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 16 of 48...\n",
      "Iteration 1, loss = 0.67382513\n",
      "Iteration 2, loss = 0.55471378\n",
      "Iteration 3, loss = 0.50533686\n",
      "Iteration 4, loss = 0.47646761\n",
      "Iteration 5, loss = 0.44914723\n",
      "Iteration 6, loss = 0.43344625\n",
      "Iteration 7, loss = 0.42362930\n",
      "Iteration 8, loss = 0.40743854\n",
      "Iteration 9, loss = 0.39814171\n",
      "Iteration 10, loss = 0.39976612\n",
      "Iteration 11, loss = 0.38906021\n",
      "Iteration 12, loss = 0.37979737\n",
      "Iteration 13, loss = 0.36976101\n",
      "Iteration 14, loss = 0.36334314\n",
      "Iteration 15, loss = 0.35634764\n",
      "Iteration 16, loss = 0.34876699\n",
      "Iteration 17, loss = 0.34434627\n",
      "Iteration 18, loss = 0.34273827\n",
      "Iteration 19, loss = 0.33729641\n",
      "Iteration 20, loss = 0.33270831\n",
      "Iteration 21, loss = 0.32715783\n",
      "Iteration 22, loss = 0.32140984\n",
      "Iteration 23, loss = 0.31763262\n",
      "Iteration 24, loss = 0.31943247\n",
      "Iteration 25, loss = 0.31309507\n",
      "Iteration 26, loss = 0.30949479\n",
      "Iteration 27, loss = 0.30674116\n",
      "Iteration 28, loss = 0.30337988\n",
      "Iteration 29, loss = 0.30414736\n",
      "Iteration 30, loss = 0.29581797\n",
      "Iteration 31, loss = 0.29441867\n",
      "Iteration 32, loss = 0.28780146\n",
      "Iteration 33, loss = 0.28601615\n",
      "Iteration 34, loss = 0.28639735\n",
      "Iteration 35, loss = 0.27685520\n",
      "Iteration 36, loss = 0.27662324\n",
      "Iteration 37, loss = 0.27438561\n",
      "Iteration 38, loss = 0.26909376\n",
      "Iteration 39, loss = 0.26685078\n",
      "Iteration 40, loss = 0.26189199\n",
      "Iteration 41, loss = 0.26388635\n",
      "Iteration 42, loss = 0.25894954\n",
      "Iteration 43, loss = 0.25292987\n",
      "Iteration 44, loss = 0.25186234\n",
      "Iteration 45, loss = 0.25691864\n",
      "Iteration 46, loss = 0.25165565\n",
      "Iteration 47, loss = 0.24497077\n",
      "Iteration 48, loss = 0.24603080\n",
      "Iteration 49, loss = 0.23936963\n",
      "Iteration 50, loss = 0.24158427\n",
      "Iteration 51, loss = 0.23506243\n",
      "Iteration 52, loss = 0.23318455\n",
      "Iteration 53, loss = 0.23204573\n",
      "Iteration 54, loss = 0.23233125\n",
      "Iteration 55, loss = 0.22776860\n",
      "Iteration 56, loss = 0.22304477\n",
      "Iteration 57, loss = 0.22943094\n",
      "Iteration 58, loss = 0.22493850\n",
      "Iteration 59, loss = 0.21821893\n",
      "Iteration 60, loss = 0.21596471\n",
      "Iteration 61, loss = 0.22842631\n",
      "Iteration 62, loss = 0.22223297\n",
      "Iteration 63, loss = 0.21711511\n",
      "Iteration 64, loss = 0.21046276\n",
      "Iteration 65, loss = 0.20467606\n",
      "Iteration 66, loss = 0.20273091\n",
      "Iteration 67, loss = 0.20086480\n",
      "Iteration 68, loss = 0.19876002\n",
      "Iteration 69, loss = 0.19864654\n",
      "Iteration 70, loss = 0.19713154\n",
      "Iteration 71, loss = 0.19517868\n",
      "Iteration 72, loss = 0.19211497\n",
      "Iteration 73, loss = 0.19137533\n",
      "Iteration 74, loss = 0.18774694\n",
      "Iteration 75, loss = 0.19403264\n",
      "Iteration 76, loss = 0.18522343\n",
      "Iteration 77, loss = 0.18273136\n",
      "Iteration 78, loss = 0.18006069\n",
      "Iteration 79, loss = 0.18070342\n",
      "Iteration 80, loss = 0.18091454\n",
      "Iteration 81, loss = 0.17867420\n",
      "Iteration 82, loss = 0.17645891\n",
      "Iteration 83, loss = 0.17544171\n",
      "Iteration 84, loss = 0.16956104\n",
      "Iteration 85, loss = 0.16900350\n",
      "Iteration 86, loss = 0.16947152\n",
      "Iteration 87, loss = 0.16908007\n",
      "Iteration 88, loss = 0.16172661\n",
      "Iteration 89, loss = 0.16281460\n",
      "Iteration 90, loss = 0.16337540\n",
      "Iteration 91, loss = 0.16212744\n",
      "Iteration 92, loss = 0.15502122\n",
      "Iteration 93, loss = 0.15597264\n",
      "Iteration 94, loss = 0.15566807\n",
      "Iteration 95, loss = 0.15204704\n",
      "Iteration 96, loss = 0.14967039\n",
      "Iteration 97, loss = 0.14859672\n",
      "Iteration 98, loss = 0.15133460\n",
      "Iteration 99, loss = 0.14631940\n",
      "Iteration 100, loss = 0.14649828\n",
      "Iteration 101, loss = 0.14315397\n",
      "Iteration 102, loss = 0.14115067\n",
      "Iteration 103, loss = 0.14299617\n",
      "Iteration 104, loss = 0.14709198\n",
      "Iteration 105, loss = 0.14495974\n",
      "Iteration 106, loss = 0.14325531\n",
      "Iteration 107, loss = 0.13615809\n",
      "Iteration 108, loss = 0.13280787\n",
      "Iteration 109, loss = 0.13437926\n",
      "Iteration 110, loss = 0.13980424\n",
      "Iteration 111, loss = 0.13433641\n",
      "Iteration 112, loss = 0.13239884\n",
      "Iteration 113, loss = 0.13076242\n",
      "Iteration 114, loss = 0.12919030\n",
      "Iteration 115, loss = 0.12815090\n",
      "Iteration 116, loss = 0.13137062\n",
      "Iteration 117, loss = 0.12777408\n",
      "Iteration 118, loss = 0.12358914\n",
      "Iteration 119, loss = 0.11904576\n",
      "Iteration 120, loss = 0.12129483\n",
      "Iteration 121, loss = 0.11923695\n",
      "Iteration 122, loss = 0.11851347\n",
      "Iteration 123, loss = 0.11516666\n",
      "Iteration 124, loss = 0.11309700\n",
      "Iteration 125, loss = 0.11435270\n",
      "Iteration 126, loss = 0.11291375\n",
      "Iteration 127, loss = 0.11132367\n",
      "Iteration 128, loss = 0.11268848\n",
      "Iteration 129, loss = 0.11059253\n",
      "Iteration 130, loss = 0.11211149\n",
      "Iteration 131, loss = 0.10874480\n",
      "Iteration 132, loss = 0.10580391\n",
      "Iteration 133, loss = 0.10718817\n",
      "Iteration 134, loss = 0.10931247\n",
      "Iteration 135, loss = 0.10316236\n",
      "Iteration 136, loss = 0.10378691\n",
      "Iteration 137, loss = 0.10155579\n",
      "Iteration 138, loss = 0.10190877\n",
      "Iteration 139, loss = 0.10030368\n",
      "Iteration 140, loss = 0.09780208\n",
      "Iteration 141, loss = 0.09602601\n",
      "Iteration 142, loss = 0.10299616\n",
      "Iteration 143, loss = 0.09662193\n",
      "Iteration 144, loss = 0.09379748\n",
      "Iteration 145, loss = 0.09594036\n",
      "Iteration 146, loss = 0.09233645\n",
      "Iteration 147, loss = 0.09235533\n",
      "Iteration 148, loss = 0.09292057\n",
      "Iteration 149, loss = 0.08869755\n",
      "Iteration 150, loss = 0.09081720\n",
      "Iteration 151, loss = 0.09018852\n",
      "Iteration 152, loss = 0.09050487\n",
      "Iteration 153, loss = 0.08989863\n",
      "Iteration 154, loss = 0.08659062\n",
      "Iteration 155, loss = 0.08451733\n",
      "Iteration 156, loss = 0.08756025\n",
      "Iteration 157, loss = 0.08408577\n",
      "Iteration 158, loss = 0.08394150\n",
      "Iteration 159, loss = 0.08161468\n",
      "Iteration 160, loss = 0.08127772\n",
      "Iteration 161, loss = 0.07970533\n",
      "Iteration 162, loss = 0.07930895\n",
      "Iteration 163, loss = 0.08080221\n",
      "Iteration 164, loss = 0.08141191\n",
      "Iteration 165, loss = 0.07828214\n",
      "Iteration 166, loss = 0.07538844\n",
      "Iteration 167, loss = 0.07509482\n",
      "Iteration 168, loss = 0.07867266\n",
      "Iteration 169, loss = 0.07428250\n",
      "Iteration 170, loss = 0.07431072\n",
      "Iteration 171, loss = 0.07941295\n",
      "Iteration 172, loss = 0.07359263\n",
      "Iteration 173, loss = 0.07133695\n",
      "Iteration 174, loss = 0.07222902\n",
      "Iteration 175, loss = 0.06945737\n",
      "Iteration 176, loss = 0.06881078\n",
      "Iteration 177, loss = 0.06784008\n",
      "Iteration 178, loss = 0.06770722\n",
      "Iteration 179, loss = 0.07214483\n",
      "Iteration 180, loss = 0.07106437\n",
      "Iteration 181, loss = 0.06882838\n",
      "Iteration 182, loss = 0.06780793\n",
      "Iteration 183, loss = 0.06830303\n",
      "Iteration 184, loss = 0.07172872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 185, loss = 0.06648708\n",
      "Iteration 186, loss = 0.06540728\n",
      "Iteration 187, loss = 0.06429569\n",
      "Iteration 188, loss = 0.06774347\n",
      "Iteration 189, loss = 0.06189565\n",
      "Iteration 190, loss = 0.06130961\n",
      "Iteration 191, loss = 0.06072005\n",
      "Iteration 192, loss = 0.06010847\n",
      "Iteration 193, loss = 0.06109022\n",
      "Iteration 194, loss = 0.05946212\n",
      "Iteration 195, loss = 0.05963566\n",
      "Iteration 196, loss = 0.05903893\n",
      "Iteration 197, loss = 0.05718889\n",
      "Iteration 198, loss = 0.05726220\n",
      "Iteration 199, loss = 0.05541190\n",
      "Iteration 200, loss = 0.05658233\n",
      "Iteration 201, loss = 0.05533194\n",
      "Iteration 202, loss = 0.05568900\n",
      "Iteration 203, loss = 0.05666238\n",
      "Iteration 204, loss = 0.05365110\n",
      "Iteration 205, loss = 0.05343867\n",
      "Iteration 206, loss = 0.05202012\n",
      "Iteration 207, loss = 0.05231962\n",
      "Iteration 208, loss = 0.05279187\n",
      "Iteration 209, loss = 0.05427691\n",
      "Iteration 210, loss = 0.05454906\n",
      "Iteration 211, loss = 0.05053861\n",
      "Iteration 212, loss = 0.05035703\n",
      "Iteration 213, loss = 0.04976015\n",
      "Iteration 214, loss = 0.04942819\n",
      "Iteration 215, loss = 0.04842086\n",
      "Iteration 216, loss = 0.04749695\n",
      "Iteration 217, loss = 0.04789959\n",
      "Iteration 218, loss = 0.04658278\n",
      "Iteration 219, loss = 0.04700464\n",
      "Iteration 220, loss = 0.04750521\n",
      "Iteration 221, loss = 0.04632969\n",
      "Iteration 222, loss = 0.04730076\n",
      "Iteration 223, loss = 0.04687622\n",
      "Iteration 224, loss = 0.04620710\n",
      "Iteration 225, loss = 0.04507455\n",
      "Iteration 226, loss = 0.04344687\n",
      "Iteration 227, loss = 0.04401715\n",
      "Iteration 228, loss = 0.04336445\n",
      "Iteration 229, loss = 0.04319863\n",
      "Iteration 230, loss = 0.04220163\n",
      "Iteration 231, loss = 0.04331197\n",
      "Iteration 232, loss = 0.04177387\n",
      "Iteration 233, loss = 0.04160618\n",
      "Iteration 234, loss = 0.04087586\n",
      "Iteration 235, loss = 0.04126778\n",
      "Iteration 236, loss = 0.04116178\n",
      "Iteration 237, loss = 0.03932169\n",
      "Iteration 238, loss = 0.03945662\n",
      "Iteration 239, loss = 0.03976917\n",
      "Iteration 240, loss = 0.03889933\n",
      "Iteration 241, loss = 0.03829420\n",
      "Iteration 242, loss = 0.03863198\n",
      "Iteration 243, loss = 0.04010992\n",
      "Iteration 244, loss = 0.03937812\n",
      "Iteration 245, loss = 0.03762267\n",
      "Iteration 246, loss = 0.03762984\n",
      "Iteration 247, loss = 0.03677470\n",
      "Iteration 248, loss = 0.03608604\n",
      "Iteration 249, loss = 0.03609078\n",
      "Iteration 250, loss = 0.03563458\n",
      "Iteration 251, loss = 0.03486731\n",
      "Iteration 252, loss = 0.03688282\n",
      "Iteration 253, loss = 0.03669768\n",
      "Iteration 254, loss = 0.03459459\n",
      "Iteration 255, loss = 0.03677992\n",
      "Iteration 256, loss = 0.03451927\n",
      "Iteration 257, loss = 0.03515164\n",
      "Iteration 258, loss = 0.03355473\n",
      "Iteration 259, loss = 0.03256368\n",
      "Iteration 260, loss = 0.03310660\n",
      "Iteration 261, loss = 0.03369947\n",
      "Iteration 262, loss = 0.03299881\n",
      "Iteration 263, loss = 0.03308035\n",
      "Iteration 264, loss = 0.03290118\n",
      "Iteration 265, loss = 0.03214914\n",
      "Iteration 266, loss = 0.03241359\n",
      "Iteration 267, loss = 0.03292957\n",
      "Iteration 268, loss = 0.03338352\n",
      "Iteration 269, loss = 0.03488718\n",
      "Iteration 270, loss = 0.03101658\n",
      "Iteration 271, loss = 0.03081920\n",
      "Iteration 272, loss = 0.02989183\n",
      "Iteration 273, loss = 0.02994314\n",
      "Iteration 274, loss = 0.03014206\n",
      "Iteration 275, loss = 0.02836518\n",
      "Iteration 276, loss = 0.02940507\n",
      "Iteration 277, loss = 0.02940838\n",
      "Iteration 278, loss = 0.02892104\n",
      "Iteration 279, loss = 0.02766315\n",
      "Iteration 280, loss = 0.02905574\n",
      "Iteration 281, loss = 0.02779953\n",
      "Iteration 282, loss = 0.02806300\n",
      "Iteration 283, loss = 0.03398047\n",
      "Iteration 284, loss = 0.03218196\n",
      "Iteration 285, loss = 0.04263190\n",
      "Iteration 286, loss = 0.04863289\n",
      "Iteration 287, loss = 0.04723174\n",
      "Iteration 288, loss = 0.03863121\n",
      "Iteration 289, loss = 0.03099277\n",
      "Iteration 290, loss = 0.03011979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 17 of 48...\n",
      "Iteration 1, loss = 0.66686932\n",
      "Iteration 2, loss = 0.54927581\n",
      "Iteration 3, loss = 0.49839390\n",
      "Iteration 4, loss = 0.46620642\n",
      "Iteration 5, loss = 0.44387972\n",
      "Iteration 6, loss = 0.42988932\n",
      "Iteration 7, loss = 0.41695432\n",
      "Iteration 8, loss = 0.40559684\n",
      "Iteration 9, loss = 0.39185735\n",
      "Iteration 10, loss = 0.38930949\n",
      "Iteration 11, loss = 0.37629838\n",
      "Iteration 12, loss = 0.36851826\n",
      "Iteration 13, loss = 0.35893522\n",
      "Iteration 14, loss = 0.35824387\n",
      "Iteration 15, loss = 0.34851381\n",
      "Iteration 16, loss = 0.34538630\n",
      "Iteration 17, loss = 0.33937191\n",
      "Iteration 18, loss = 0.33423903\n",
      "Iteration 19, loss = 0.32817982\n",
      "Iteration 20, loss = 0.33124936\n",
      "Iteration 21, loss = 0.33619746\n",
      "Iteration 22, loss = 0.32537005\n",
      "Iteration 23, loss = 0.32007071\n",
      "Iteration 24, loss = 0.31371187\n",
      "Iteration 25, loss = 0.30764750\n",
      "Iteration 26, loss = 0.30462816\n",
      "Iteration 27, loss = 0.29870942\n",
      "Iteration 28, loss = 0.30107703\n",
      "Iteration 29, loss = 0.29353038\n",
      "Iteration 30, loss = 0.28793428\n",
      "Iteration 31, loss = 0.28534815\n",
      "Iteration 32, loss = 0.28604657\n",
      "Iteration 33, loss = 0.28260949\n",
      "Iteration 34, loss = 0.28443537\n",
      "Iteration 35, loss = 0.27807025\n",
      "Iteration 36, loss = 0.27793381\n",
      "Iteration 37, loss = 0.28034330\n",
      "Iteration 38, loss = 0.27575829\n",
      "Iteration 39, loss = 0.27095940\n",
      "Iteration 40, loss = 0.26226008\n",
      "Iteration 41, loss = 0.25833141\n",
      "Iteration 42, loss = 0.25598212\n",
      "Iteration 43, loss = 0.25747153\n",
      "Iteration 44, loss = 0.25242986\n",
      "Iteration 45, loss = 0.25293838\n",
      "Iteration 46, loss = 0.24632665\n",
      "Iteration 47, loss = 0.24487400\n",
      "Iteration 48, loss = 0.24378417\n",
      "Iteration 49, loss = 0.24313635\n",
      "Iteration 50, loss = 0.24482741\n",
      "Iteration 51, loss = 0.24915586\n",
      "Iteration 52, loss = 0.23853896\n",
      "Iteration 53, loss = 0.23444107\n",
      "Iteration 54, loss = 0.23149990\n",
      "Iteration 55, loss = 0.23076409\n",
      "Iteration 56, loss = 0.22753084\n",
      "Iteration 57, loss = 0.22784243\n",
      "Iteration 58, loss = 0.22514870\n",
      "Iteration 59, loss = 0.21889911\n",
      "Iteration 60, loss = 0.22083225\n",
      "Iteration 61, loss = 0.21890148\n",
      "Iteration 62, loss = 0.21589238\n",
      "Iteration 63, loss = 0.21400560\n",
      "Iteration 64, loss = 0.21005089\n",
      "Iteration 65, loss = 0.21330418\n",
      "Iteration 66, loss = 0.21504582\n",
      "Iteration 67, loss = 0.20807918\n",
      "Iteration 68, loss = 0.20655501\n",
      "Iteration 69, loss = 0.20917540\n",
      "Iteration 70, loss = 0.19950715\n",
      "Iteration 71, loss = 0.20538721\n",
      "Iteration 72, loss = 0.19971744\n",
      "Iteration 73, loss = 0.19371160\n",
      "Iteration 74, loss = 0.19884013\n",
      "Iteration 75, loss = 0.18974265\n",
      "Iteration 76, loss = 0.18933466\n",
      "Iteration 77, loss = 0.18726974\n",
      "Iteration 78, loss = 0.18501294\n",
      "Iteration 79, loss = 0.18310764\n",
      "Iteration 80, loss = 0.18186722\n",
      "Iteration 81, loss = 0.18119173\n",
      "Iteration 82, loss = 0.18062926\n",
      "Iteration 83, loss = 0.17568963\n",
      "Iteration 84, loss = 0.17677253\n",
      "Iteration 85, loss = 0.17368850\n",
      "Iteration 86, loss = 0.17507803\n",
      "Iteration 87, loss = 0.17154837\n",
      "Iteration 88, loss = 0.17224363\n",
      "Iteration 89, loss = 0.16921691\n",
      "Iteration 90, loss = 0.16698040\n",
      "Iteration 91, loss = 0.16524899\n",
      "Iteration 92, loss = 0.16365959\n",
      "Iteration 93, loss = 0.17542022\n",
      "Iteration 94, loss = 0.16576434\n",
      "Iteration 95, loss = 0.15811048\n",
      "Iteration 96, loss = 0.15794516\n",
      "Iteration 97, loss = 0.16060749\n",
      "Iteration 98, loss = 0.16057618\n",
      "Iteration 99, loss = 0.15147378\n",
      "Iteration 100, loss = 0.15309451\n",
      "Iteration 101, loss = 0.15233902\n",
      "Iteration 102, loss = 0.15176472\n",
      "Iteration 103, loss = 0.15335249\n",
      "Iteration 104, loss = 0.14750094\n",
      "Iteration 105, loss = 0.15421455\n",
      "Iteration 106, loss = 0.15077878\n",
      "Iteration 107, loss = 0.14505727\n",
      "Iteration 108, loss = 0.14760116\n",
      "Iteration 109, loss = 0.13941965\n",
      "Iteration 110, loss = 0.13706603\n",
      "Iteration 111, loss = 0.13638633\n",
      "Iteration 112, loss = 0.13565214\n",
      "Iteration 113, loss = 0.13355547\n",
      "Iteration 114, loss = 0.13730164\n",
      "Iteration 115, loss = 0.13431379\n",
      "Iteration 116, loss = 0.13052354\n",
      "Iteration 117, loss = 0.12979245\n",
      "Iteration 118, loss = 0.12912145\n",
      "Iteration 119, loss = 0.12781366\n",
      "Iteration 120, loss = 0.12735912\n",
      "Iteration 121, loss = 0.12673640\n",
      "Iteration 122, loss = 0.12227856\n",
      "Iteration 123, loss = 0.12634772\n",
      "Iteration 124, loss = 0.12036226\n",
      "Iteration 125, loss = 0.12970044\n",
      "Iteration 126, loss = 0.12323665\n",
      "Iteration 127, loss = 0.11881404\n",
      "Iteration 128, loss = 0.11912918\n",
      "Iteration 129, loss = 0.11745135\n",
      "Iteration 130, loss = 0.11506855\n",
      "Iteration 131, loss = 0.11767021\n",
      "Iteration 132, loss = 0.11316421\n",
      "Iteration 133, loss = 0.11279308\n",
      "Iteration 134, loss = 0.11278330\n",
      "Iteration 135, loss = 0.11158470\n",
      "Iteration 136, loss = 0.10919135\n",
      "Iteration 137, loss = 0.10887401\n",
      "Iteration 138, loss = 0.10617373\n",
      "Iteration 139, loss = 0.10646838\n",
      "Iteration 140, loss = 0.10660212\n",
      "Iteration 141, loss = 0.10527456\n",
      "Iteration 142, loss = 0.10428571\n",
      "Iteration 143, loss = 0.10376495\n",
      "Iteration 144, loss = 0.10180199\n",
      "Iteration 145, loss = 0.10151178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 146, loss = 0.09937269\n",
      "Iteration 147, loss = 0.10014260\n",
      "Iteration 148, loss = 0.09833193\n",
      "Iteration 149, loss = 0.09797173\n",
      "Iteration 150, loss = 0.09481339\n",
      "Iteration 151, loss = 0.09584581\n",
      "Iteration 152, loss = 0.09629770\n",
      "Iteration 153, loss = 0.09459915\n",
      "Iteration 154, loss = 0.09421808\n",
      "Iteration 155, loss = 0.09486320\n",
      "Iteration 156, loss = 0.10215246\n",
      "Iteration 157, loss = 0.09348000\n",
      "Iteration 158, loss = 0.08884014\n",
      "Iteration 159, loss = 0.08943553\n",
      "Iteration 160, loss = 0.09406509\n",
      "Iteration 161, loss = 0.09084177\n",
      "Iteration 162, loss = 0.08930340\n",
      "Iteration 163, loss = 0.08624042\n",
      "Iteration 164, loss = 0.08805214\n",
      "Iteration 165, loss = 0.08440735\n",
      "Iteration 166, loss = 0.08273940\n",
      "Iteration 167, loss = 0.08324520\n",
      "Iteration 168, loss = 0.08283710\n",
      "Iteration 169, loss = 0.08133223\n",
      "Iteration 170, loss = 0.08559809\n",
      "Iteration 171, loss = 0.08411074\n",
      "Iteration 172, loss = 0.08343458\n",
      "Iteration 173, loss = 0.07968814\n",
      "Iteration 174, loss = 0.07706280\n",
      "Iteration 175, loss = 0.07672285\n",
      "Iteration 176, loss = 0.07588540\n",
      "Iteration 177, loss = 0.07665379\n",
      "Iteration 178, loss = 0.07582549\n",
      "Iteration 179, loss = 0.07456121\n",
      "Iteration 180, loss = 0.07279347\n",
      "Iteration 181, loss = 0.07458033\n",
      "Iteration 182, loss = 0.07481011\n",
      "Iteration 183, loss = 0.07295548\n",
      "Iteration 184, loss = 0.07159115\n",
      "Iteration 185, loss = 0.07108906\n",
      "Iteration 186, loss = 0.07064718\n",
      "Iteration 187, loss = 0.07402944\n",
      "Iteration 188, loss = 0.07028244\n",
      "Iteration 189, loss = 0.06876652\n",
      "Iteration 190, loss = 0.06872626\n",
      "Iteration 191, loss = 0.06914967\n",
      "Iteration 192, loss = 0.06947204\n",
      "Iteration 193, loss = 0.06591982\n",
      "Iteration 194, loss = 0.06684015\n",
      "Iteration 195, loss = 0.06656159\n",
      "Iteration 196, loss = 0.06432296\n",
      "Iteration 197, loss = 0.06659618\n",
      "Iteration 198, loss = 0.06716167\n",
      "Iteration 199, loss = 0.06289987\n",
      "Iteration 200, loss = 0.06305295\n",
      "Iteration 201, loss = 0.06220474\n",
      "Iteration 202, loss = 0.06123045\n",
      "Iteration 203, loss = 0.06102797\n",
      "Iteration 204, loss = 0.06080736\n",
      "Iteration 205, loss = 0.05965653\n",
      "Iteration 206, loss = 0.05976859\n",
      "Iteration 207, loss = 0.05861298\n",
      "Iteration 208, loss = 0.05863812\n",
      "Iteration 209, loss = 0.05986201\n",
      "Iteration 210, loss = 0.06140286\n",
      "Iteration 211, loss = 0.06342804\n",
      "Iteration 212, loss = 0.06347386\n",
      "Iteration 213, loss = 0.06142705\n",
      "Iteration 214, loss = 0.05866633\n",
      "Iteration 215, loss = 0.05492366\n",
      "Iteration 216, loss = 0.05596422\n",
      "Iteration 217, loss = 0.05474313\n",
      "Iteration 218, loss = 0.05564235\n",
      "Iteration 219, loss = 0.05551169\n",
      "Iteration 220, loss = 0.05378000\n",
      "Iteration 221, loss = 0.05375701\n",
      "Iteration 222, loss = 0.05173525\n",
      "Iteration 223, loss = 0.05354154\n",
      "Iteration 224, loss = 0.05360772\n",
      "Iteration 225, loss = 0.05198354\n",
      "Iteration 226, loss = 0.05197848\n",
      "Iteration 227, loss = 0.05143095\n",
      "Iteration 228, loss = 0.04992396\n",
      "Iteration 229, loss = 0.05029232\n",
      "Iteration 230, loss = 0.05185405\n",
      "Iteration 231, loss = 0.05099655\n",
      "Iteration 232, loss = 0.04916603\n",
      "Iteration 233, loss = 0.04897751\n",
      "Iteration 234, loss = 0.04934367\n",
      "Iteration 235, loss = 0.04727683\n",
      "Iteration 236, loss = 0.04714105\n",
      "Iteration 237, loss = 0.04653738\n",
      "Iteration 238, loss = 0.04792103\n",
      "Iteration 239, loss = 0.04615262\n",
      "Iteration 240, loss = 0.04709384\n",
      "Iteration 241, loss = 0.04510419\n",
      "Iteration 242, loss = 0.04422093\n",
      "Iteration 243, loss = 0.04411137\n",
      "Iteration 244, loss = 0.04492030\n",
      "Iteration 245, loss = 0.04620990\n",
      "Iteration 246, loss = 0.04413959\n",
      "Iteration 247, loss = 0.04223495\n",
      "Iteration 248, loss = 0.04452571\n",
      "Iteration 249, loss = 0.04303477\n",
      "Iteration 250, loss = 0.04164252\n",
      "Iteration 251, loss = 0.04170576\n",
      "Iteration 252, loss = 0.04083058\n",
      "Iteration 253, loss = 0.04136533\n",
      "Iteration 254, loss = 0.04113656\n",
      "Iteration 255, loss = 0.04214327\n",
      "Iteration 256, loss = 0.04264477\n",
      "Iteration 257, loss = 0.04002343\n",
      "Iteration 258, loss = 0.04012125\n",
      "Iteration 259, loss = 0.04024558\n",
      "Iteration 260, loss = 0.04021281\n",
      "Iteration 261, loss = 0.04072844\n",
      "Iteration 262, loss = 0.04026948\n",
      "Iteration 263, loss = 0.04080016\n",
      "Iteration 264, loss = 0.04057049\n",
      "Iteration 265, loss = 0.03895774\n",
      "Iteration 266, loss = 0.03767126\n",
      "Iteration 267, loss = 0.03782424\n",
      "Iteration 268, loss = 0.03871704\n",
      "Iteration 269, loss = 0.03763226\n",
      "Iteration 270, loss = 0.03593437\n",
      "Iteration 271, loss = 0.03706236\n",
      "Iteration 272, loss = 0.03539778\n",
      "Iteration 273, loss = 0.03614800\n",
      "Iteration 274, loss = 0.03690797\n",
      "Iteration 275, loss = 0.03667109\n",
      "Iteration 276, loss = 0.03408948\n",
      "Iteration 277, loss = 0.03482290\n",
      "Iteration 278, loss = 0.03429774\n",
      "Iteration 279, loss = 0.03406795\n",
      "Iteration 280, loss = 0.03430501\n",
      "Iteration 281, loss = 0.03285011\n",
      "Iteration 282, loss = 0.03512992\n",
      "Iteration 283, loss = 0.03195248\n",
      "Iteration 284, loss = 0.03276908\n",
      "Iteration 285, loss = 0.03603293\n",
      "Iteration 286, loss = 0.03354713\n",
      "Iteration 287, loss = 0.03265702\n",
      "Iteration 288, loss = 0.03329321\n",
      "Iteration 289, loss = 0.03173945\n",
      "Iteration 290, loss = 0.03094700\n",
      "Iteration 291, loss = 0.03073663\n",
      "Iteration 292, loss = 0.03174072\n",
      "Iteration 293, loss = 0.03261438\n",
      "Iteration 294, loss = 0.03090219\n",
      "Iteration 295, loss = 0.03042298\n",
      "Iteration 296, loss = 0.03024579\n",
      "Iteration 297, loss = 0.03172048\n",
      "Iteration 298, loss = 0.03012244\n",
      "Iteration 299, loss = 0.03048508\n",
      "Iteration 300, loss = 0.02918447\n",
      "Iteration 301, loss = 0.02916944\n",
      "Iteration 302, loss = 0.02909659\n",
      "Iteration 303, loss = 0.02904290\n",
      "Iteration 304, loss = 0.02851452\n",
      "Iteration 305, loss = 0.02847771\n",
      "Iteration 306, loss = 0.02884305\n",
      "Iteration 307, loss = 0.02796388\n",
      "Iteration 308, loss = 0.02721262\n",
      "Iteration 309, loss = 0.02680655\n",
      "Iteration 310, loss = 0.02806496\n",
      "Iteration 311, loss = 0.02913275\n",
      "Iteration 312, loss = 0.02709645\n",
      "Iteration 313, loss = 0.02738758\n",
      "Iteration 314, loss = 0.02584262\n",
      "Iteration 315, loss = 0.02774368\n",
      "Iteration 316, loss = 0.02784467\n",
      "Iteration 317, loss = 0.02620926\n",
      "Iteration 318, loss = 0.02771316\n",
      "Iteration 319, loss = 0.02804141\n",
      "Iteration 320, loss = 0.02662655\n",
      "Iteration 321, loss = 0.02537482\n",
      "Iteration 322, loss = 0.02677823\n",
      "Iteration 323, loss = 0.02610975\n",
      "Iteration 324, loss = 0.02484128\n",
      "Iteration 325, loss = 0.02397233\n",
      "Iteration 326, loss = 0.02486349\n",
      "Iteration 327, loss = 0.02361753\n",
      "Iteration 328, loss = 0.02459945\n",
      "Iteration 329, loss = 0.02449009\n",
      "Iteration 330, loss = 0.02505618\n",
      "Iteration 331, loss = 0.02352751\n",
      "Iteration 332, loss = 0.02291499\n",
      "Iteration 333, loss = 0.02301144\n",
      "Iteration 334, loss = 0.02309936\n",
      "Iteration 335, loss = 0.02259339\n",
      "Iteration 336, loss = 0.02214451\n",
      "Iteration 337, loss = 0.02245521\n",
      "Iteration 338, loss = 0.02291927\n",
      "Iteration 339, loss = 0.02248398\n",
      "Iteration 340, loss = 0.02190912\n",
      "Iteration 341, loss = 0.02192468\n",
      "Iteration 342, loss = 0.02230171\n",
      "Iteration 343, loss = 0.02313380\n",
      "Iteration 344, loss = 0.02213007\n",
      "Iteration 345, loss = 0.02128516\n",
      "Iteration 346, loss = 0.02092750\n",
      "Iteration 347, loss = 0.02088335\n",
      "Iteration 348, loss = 0.02099831\n",
      "Iteration 349, loss = 0.02066586\n",
      "Iteration 350, loss = 0.02035705\n",
      "Iteration 351, loss = 0.02043854\n",
      "Iteration 352, loss = 0.02034899\n",
      "Iteration 353, loss = 0.02048703\n",
      "Iteration 354, loss = 0.02043173\n",
      "Iteration 355, loss = 0.02095622\n",
      "Iteration 356, loss = 0.02124201\n",
      "Iteration 357, loss = 0.02005489\n",
      "Iteration 358, loss = 0.01959811\n",
      "Iteration 359, loss = 0.01904821\n",
      "Iteration 360, loss = 0.01961288\n",
      "Iteration 361, loss = 0.01999763\n",
      "Iteration 362, loss = 0.02002585\n",
      "Iteration 363, loss = 0.02106138\n",
      "Iteration 364, loss = 0.03803594\n",
      "Iteration 365, loss = 0.07268078\n",
      "Iteration 366, loss = 0.06021066\n",
      "Iteration 367, loss = 0.04095127\n",
      "Iteration 368, loss = 0.03392330\n",
      "Iteration 369, loss = 0.02721819\n",
      "Iteration 370, loss = 0.02346598\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 18 of 48...\n",
      "Iteration 1, loss = 0.66292166\n",
      "Iteration 2, loss = 0.55825531\n",
      "Iteration 3, loss = 0.51278231\n",
      "Iteration 4, loss = 0.47960842\n",
      "Iteration 5, loss = 0.46363892\n",
      "Iteration 6, loss = 0.43753849\n",
      "Iteration 7, loss = 0.42691683\n",
      "Iteration 8, loss = 0.41804118\n",
      "Iteration 9, loss = 0.40223923\n",
      "Iteration 10, loss = 0.40086889\n",
      "Iteration 11, loss = 0.38342885\n",
      "Iteration 12, loss = 0.38139471\n",
      "Iteration 13, loss = 0.37545423\n",
      "Iteration 14, loss = 0.37141583\n",
      "Iteration 15, loss = 0.36202787\n",
      "Iteration 16, loss = 0.36155607\n",
      "Iteration 17, loss = 0.35623246\n",
      "Iteration 18, loss = 0.34806373\n",
      "Iteration 19, loss = 0.34173030\n",
      "Iteration 20, loss = 0.33862374\n",
      "Iteration 21, loss = 0.33277056\n",
      "Iteration 22, loss = 0.33003751\n",
      "Iteration 23, loss = 0.32544137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.32049530\n",
      "Iteration 25, loss = 0.31647528\n",
      "Iteration 26, loss = 0.31498245\n",
      "Iteration 27, loss = 0.30827359\n",
      "Iteration 28, loss = 0.30771296\n",
      "Iteration 29, loss = 0.31170122\n",
      "Iteration 30, loss = 0.30431497\n",
      "Iteration 31, loss = 0.29846117\n",
      "Iteration 32, loss = 0.29628154\n",
      "Iteration 33, loss = 0.29291810\n",
      "Iteration 34, loss = 0.28665076\n",
      "Iteration 35, loss = 0.28462785\n",
      "Iteration 36, loss = 0.28458839\n",
      "Iteration 37, loss = 0.28164714\n",
      "Iteration 38, loss = 0.27758122\n",
      "Iteration 39, loss = 0.28061899\n",
      "Iteration 40, loss = 0.27601487\n",
      "Iteration 41, loss = 0.27253826\n",
      "Iteration 42, loss = 0.26869524\n",
      "Iteration 43, loss = 0.26401285\n",
      "Iteration 44, loss = 0.26236788\n",
      "Iteration 45, loss = 0.26137750\n",
      "Iteration 46, loss = 0.26371165\n",
      "Iteration 47, loss = 0.25785096\n",
      "Iteration 48, loss = 0.25536043\n",
      "Iteration 49, loss = 0.25058527\n",
      "Iteration 50, loss = 0.24604455\n",
      "Iteration 51, loss = 0.24798922\n",
      "Iteration 52, loss = 0.24840009\n",
      "Iteration 53, loss = 0.24345736\n",
      "Iteration 54, loss = 0.24250625\n",
      "Iteration 55, loss = 0.23885151\n",
      "Iteration 56, loss = 0.23672528\n",
      "Iteration 57, loss = 0.23444504\n",
      "Iteration 58, loss = 0.23211302\n",
      "Iteration 59, loss = 0.22734720\n",
      "Iteration 60, loss = 0.23426756\n",
      "Iteration 61, loss = 0.22594163\n",
      "Iteration 62, loss = 0.22253549\n",
      "Iteration 63, loss = 0.22022650\n",
      "Iteration 64, loss = 0.21915083\n",
      "Iteration 65, loss = 0.21579951\n",
      "Iteration 66, loss = 0.21352222\n",
      "Iteration 67, loss = 0.21631241\n",
      "Iteration 68, loss = 0.21344832\n",
      "Iteration 69, loss = 0.20798716\n",
      "Iteration 70, loss = 0.20661514\n",
      "Iteration 71, loss = 0.20514460\n",
      "Iteration 72, loss = 0.20587301\n",
      "Iteration 73, loss = 0.20615858\n",
      "Iteration 74, loss = 0.20994447\n",
      "Iteration 75, loss = 0.22207136\n",
      "Iteration 76, loss = 0.20396086\n",
      "Iteration 77, loss = 0.20901281\n",
      "Iteration 78, loss = 0.19700863\n",
      "Iteration 79, loss = 0.19806253\n",
      "Iteration 80, loss = 0.18994014\n",
      "Iteration 81, loss = 0.18734887\n",
      "Iteration 82, loss = 0.18393278\n",
      "Iteration 83, loss = 0.18373242\n",
      "Iteration 84, loss = 0.18359618\n",
      "Iteration 85, loss = 0.18160708\n",
      "Iteration 86, loss = 0.17877346\n",
      "Iteration 87, loss = 0.17611549\n",
      "Iteration 88, loss = 0.17489133\n",
      "Iteration 89, loss = 0.17587866\n",
      "Iteration 90, loss = 0.17778618\n",
      "Iteration 91, loss = 0.17579442\n",
      "Iteration 92, loss = 0.17438590\n",
      "Iteration 93, loss = 0.16814263\n",
      "Iteration 94, loss = 0.16909588\n",
      "Iteration 95, loss = 0.16622687\n",
      "Iteration 96, loss = 0.16586041\n",
      "Iteration 97, loss = 0.16553465\n",
      "Iteration 98, loss = 0.16286701\n",
      "Iteration 99, loss = 0.16561341\n",
      "Iteration 100, loss = 0.15947328\n",
      "Iteration 101, loss = 0.16059993\n",
      "Iteration 102, loss = 0.16168084\n",
      "Iteration 103, loss = 0.15497124\n",
      "Iteration 104, loss = 0.15247126\n",
      "Iteration 105, loss = 0.15187162\n",
      "Iteration 106, loss = 0.15982985\n",
      "Iteration 107, loss = 0.15865439\n",
      "Iteration 108, loss = 0.15990806\n",
      "Iteration 109, loss = 0.15323820\n",
      "Iteration 110, loss = 0.14727606\n",
      "Iteration 111, loss = 0.14564689\n",
      "Iteration 112, loss = 0.14417074\n",
      "Iteration 113, loss = 0.14253234\n",
      "Iteration 114, loss = 0.14352790\n",
      "Iteration 115, loss = 0.14136462\n",
      "Iteration 116, loss = 0.14663174\n",
      "Iteration 117, loss = 0.14603177\n",
      "Iteration 118, loss = 0.13664915\n",
      "Iteration 119, loss = 0.14195183\n",
      "Iteration 120, loss = 0.14086940\n",
      "Iteration 121, loss = 0.14174405\n",
      "Iteration 122, loss = 0.13540806\n",
      "Iteration 123, loss = 0.13608195\n",
      "Iteration 124, loss = 0.13333328\n",
      "Iteration 125, loss = 0.12986656\n",
      "Iteration 126, loss = 0.12728634\n",
      "Iteration 127, loss = 0.12797155\n",
      "Iteration 128, loss = 0.12567902\n",
      "Iteration 129, loss = 0.12521834\n",
      "Iteration 130, loss = 0.12524924\n",
      "Iteration 131, loss = 0.12375921\n",
      "Iteration 132, loss = 0.12093955\n",
      "Iteration 133, loss = 0.12172799\n",
      "Iteration 134, loss = 0.11957283\n",
      "Iteration 135, loss = 0.12069840\n",
      "Iteration 136, loss = 0.11781184\n",
      "Iteration 137, loss = 0.12133906\n",
      "Iteration 138, loss = 0.11716509\n",
      "Iteration 139, loss = 0.11544510\n",
      "Iteration 140, loss = 0.11817571\n",
      "Iteration 141, loss = 0.11358303\n",
      "Iteration 142, loss = 0.11249533\n",
      "Iteration 143, loss = 0.11202485\n",
      "Iteration 144, loss = 0.11046691\n",
      "Iteration 145, loss = 0.11108702\n",
      "Iteration 146, loss = 0.10922589\n",
      "Iteration 147, loss = 0.10794329\n",
      "Iteration 148, loss = 0.10922696\n",
      "Iteration 149, loss = 0.10656508\n",
      "Iteration 150, loss = 0.10736954\n",
      "Iteration 151, loss = 0.10571744\n",
      "Iteration 152, loss = 0.10503287\n",
      "Iteration 153, loss = 0.10540180\n",
      "Iteration 154, loss = 0.10628468\n",
      "Iteration 155, loss = 0.10325101\n",
      "Iteration 156, loss = 0.10469250\n",
      "Iteration 157, loss = 0.10578752\n",
      "Iteration 158, loss = 0.10232665\n",
      "Iteration 159, loss = 0.10091797\n",
      "Iteration 160, loss = 0.10105350\n",
      "Iteration 161, loss = 0.10043396\n",
      "Iteration 162, loss = 0.09749522\n",
      "Iteration 163, loss = 0.09723220\n",
      "Iteration 164, loss = 0.09612007\n",
      "Iteration 165, loss = 0.09943094\n",
      "Iteration 166, loss = 0.10581231\n",
      "Iteration 167, loss = 0.10124481\n",
      "Iteration 168, loss = 0.09854492\n",
      "Iteration 169, loss = 0.09600568\n",
      "Iteration 170, loss = 0.09744269\n",
      "Iteration 171, loss = 0.09309264\n",
      "Iteration 172, loss = 0.09299051\n",
      "Iteration 173, loss = 0.09104158\n",
      "Iteration 174, loss = 0.08975464\n",
      "Iteration 175, loss = 0.08957077\n",
      "Iteration 176, loss = 0.08817556\n",
      "Iteration 177, loss = 0.08805982\n",
      "Iteration 178, loss = 0.09130621\n",
      "Iteration 179, loss = 0.08843314\n",
      "Iteration 180, loss = 0.08600805\n",
      "Iteration 181, loss = 0.08633319\n",
      "Iteration 182, loss = 0.08834980\n",
      "Iteration 183, loss = 0.08676245\n",
      "Iteration 184, loss = 0.08683969\n",
      "Iteration 185, loss = 0.08435567\n",
      "Iteration 186, loss = 0.08177973\n",
      "Iteration 187, loss = 0.08270674\n",
      "Iteration 188, loss = 0.08260712\n",
      "Iteration 189, loss = 0.08647676\n",
      "Iteration 190, loss = 0.08812188\n",
      "Iteration 191, loss = 0.09021040\n",
      "Iteration 192, loss = 0.08591570\n",
      "Iteration 193, loss = 0.08520118\n",
      "Iteration 194, loss = 0.08672495\n",
      "Iteration 195, loss = 0.08663975\n",
      "Iteration 196, loss = 0.08562200\n",
      "Iteration 197, loss = 0.07974629\n",
      "Iteration 198, loss = 0.07867760\n",
      "Iteration 199, loss = 0.07886380\n",
      "Iteration 200, loss = 0.07847279\n",
      "Iteration 201, loss = 0.07619217\n",
      "Iteration 202, loss = 0.07604624\n",
      "Iteration 203, loss = 0.07627387\n",
      "Iteration 204, loss = 0.07638095\n",
      "Iteration 205, loss = 0.07816692\n",
      "Iteration 206, loss = 0.07311697\n",
      "Iteration 207, loss = 0.07291971\n",
      "Iteration 208, loss = 0.07281178\n",
      "Iteration 209, loss = 0.07344482\n",
      "Iteration 210, loss = 0.07448187\n",
      "Iteration 211, loss = 0.07284476\n",
      "Iteration 212, loss = 0.07161754\n",
      "Iteration 213, loss = 0.07446522\n",
      "Iteration 214, loss = 0.07392110\n",
      "Iteration 215, loss = 0.07055850\n",
      "Iteration 216, loss = 0.07004417\n",
      "Iteration 217, loss = 0.06911968\n",
      "Iteration 218, loss = 0.06972650\n",
      "Iteration 219, loss = 0.06771490\n",
      "Iteration 220, loss = 0.06817671\n",
      "Iteration 221, loss = 0.06675519\n",
      "Iteration 222, loss = 0.06774498\n",
      "Iteration 223, loss = 0.06764018\n",
      "Iteration 224, loss = 0.06693576\n",
      "Iteration 225, loss = 0.06814520\n",
      "Iteration 226, loss = 0.06626684\n",
      "Iteration 227, loss = 0.06525677\n",
      "Iteration 228, loss = 0.06486789\n",
      "Iteration 229, loss = 0.06433478\n",
      "Iteration 230, loss = 0.06481314\n",
      "Iteration 231, loss = 0.06370860\n",
      "Iteration 232, loss = 0.06284491\n",
      "Iteration 233, loss = 0.06662077\n",
      "Iteration 234, loss = 0.06331072\n",
      "Iteration 235, loss = 0.06917773\n",
      "Iteration 236, loss = 0.06867351\n",
      "Iteration 237, loss = 0.06198461\n",
      "Iteration 238, loss = 0.06360923\n",
      "Iteration 239, loss = 0.06282447\n",
      "Iteration 240, loss = 0.06292630\n",
      "Iteration 241, loss = 0.06042461\n",
      "Iteration 242, loss = 0.06299068\n",
      "Iteration 243, loss = 0.06103967\n",
      "Iteration 244, loss = 0.06109902\n",
      "Iteration 245, loss = 0.05988269\n",
      "Iteration 246, loss = 0.05929524\n",
      "Iteration 247, loss = 0.05923837\n",
      "Iteration 248, loss = 0.05858435\n",
      "Iteration 249, loss = 0.05816071\n",
      "Iteration 250, loss = 0.05839365\n",
      "Iteration 251, loss = 0.05766927\n",
      "Iteration 252, loss = 0.05849711\n",
      "Iteration 253, loss = 0.05740744\n",
      "Iteration 254, loss = 0.05622030\n",
      "Iteration 255, loss = 0.05566389\n",
      "Iteration 256, loss = 0.05597899\n",
      "Iteration 257, loss = 0.05508690\n",
      "Iteration 258, loss = 0.05493668\n",
      "Iteration 259, loss = 0.05617349\n",
      "Iteration 260, loss = 0.05654143\n",
      "Iteration 261, loss = 0.05468179\n",
      "Iteration 262, loss = 0.05348402\n",
      "Iteration 263, loss = 0.05308476\n",
      "Iteration 264, loss = 0.05309705\n",
      "Iteration 265, loss = 0.05306893\n",
      "Iteration 266, loss = 0.05277309\n",
      "Iteration 267, loss = 0.05390814\n",
      "Iteration 268, loss = 0.05286558\n",
      "Iteration 269, loss = 0.05410293\n",
      "Iteration 270, loss = 0.05169992\n",
      "Iteration 271, loss = 0.05192134\n",
      "Iteration 272, loss = 0.05257424\n",
      "Iteration 273, loss = 0.05296838\n",
      "Iteration 274, loss = 0.05662345\n",
      "Iteration 275, loss = 0.05174946\n",
      "Iteration 276, loss = 0.05406561\n",
      "Iteration 277, loss = 0.05219891\n",
      "Iteration 278, loss = 0.05342184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 279, loss = 0.05058399\n",
      "Iteration 280, loss = 0.05113813\n",
      "Iteration 281, loss = 0.05030119\n",
      "Iteration 282, loss = 0.05047898\n",
      "Iteration 283, loss = 0.04943450\n",
      "Iteration 284, loss = 0.04945469\n",
      "Iteration 285, loss = 0.04934575\n",
      "Iteration 286, loss = 0.04803857\n",
      "Iteration 287, loss = 0.04854642\n",
      "Iteration 288, loss = 0.04748540\n",
      "Iteration 289, loss = 0.04808602\n",
      "Iteration 290, loss = 0.04775394\n",
      "Iteration 291, loss = 0.04616447\n",
      "Iteration 292, loss = 0.04792355\n",
      "Iteration 293, loss = 0.04760347\n",
      "Iteration 294, loss = 0.04711433\n",
      "Iteration 295, loss = 0.04733957\n",
      "Iteration 296, loss = 0.04884572\n",
      "Iteration 297, loss = 0.04733816\n",
      "Iteration 298, loss = 0.04578543\n",
      "Iteration 299, loss = 0.05126678\n",
      "Iteration 300, loss = 0.06350737\n",
      "Iteration 301, loss = 0.05550341\n",
      "Iteration 302, loss = 0.05590209\n",
      "Iteration 303, loss = 0.05971010\n",
      "Iteration 304, loss = 0.05154997\n",
      "Iteration 305, loss = 0.04837513\n",
      "Iteration 306, loss = 0.04797978\n",
      "Iteration 307, loss = 0.04741026\n",
      "Iteration 308, loss = 0.04863365\n",
      "Iteration 309, loss = 0.04624387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 19 of 48...\n",
      "Iteration 1, loss = 0.67499447\n",
      "Iteration 2, loss = 0.62603913\n",
      "Iteration 3, loss = 0.59168053\n",
      "Iteration 4, loss = 0.56869190\n",
      "Iteration 5, loss = 0.54481581\n",
      "Iteration 6, loss = 0.52585604\n",
      "Iteration 7, loss = 0.50849244\n",
      "Iteration 8, loss = 0.49367642\n",
      "Iteration 9, loss = 0.47788850\n",
      "Iteration 10, loss = 0.46679745\n",
      "Iteration 11, loss = 0.45524109\n",
      "Iteration 12, loss = 0.44491581\n",
      "Iteration 13, loss = 0.43935852\n",
      "Iteration 14, loss = 0.42864599\n",
      "Iteration 15, loss = 0.42225006\n",
      "Iteration 16, loss = 0.41490646\n",
      "Iteration 17, loss = 0.40785206\n",
      "Iteration 18, loss = 0.40300689\n",
      "Iteration 19, loss = 0.39835616\n",
      "Iteration 20, loss = 0.39407428\n",
      "Iteration 21, loss = 0.38982276\n",
      "Iteration 22, loss = 0.38529095\n",
      "Iteration 23, loss = 0.37970439\n",
      "Iteration 24, loss = 0.37746101\n",
      "Iteration 25, loss = 0.37419524\n",
      "Iteration 26, loss = 0.36912157\n",
      "Iteration 27, loss = 0.36675458\n",
      "Iteration 28, loss = 0.36476816\n",
      "Iteration 29, loss = 0.36191607\n",
      "Iteration 30, loss = 0.35847876\n",
      "Iteration 31, loss = 0.35435484\n",
      "Iteration 32, loss = 0.35308549\n",
      "Iteration 33, loss = 0.34917009\n",
      "Iteration 34, loss = 0.35347211\n",
      "Iteration 35, loss = 0.34650925\n",
      "Iteration 36, loss = 0.34269590\n",
      "Iteration 37, loss = 0.34163008\n",
      "Iteration 38, loss = 0.33796809\n",
      "Iteration 39, loss = 0.33660967\n",
      "Iteration 40, loss = 0.33362995\n",
      "Iteration 41, loss = 0.33367423\n",
      "Iteration 42, loss = 0.33254762\n",
      "Iteration 43, loss = 0.33141048\n",
      "Iteration 44, loss = 0.33105054\n",
      "Iteration 45, loss = 0.32661444\n",
      "Iteration 46, loss = 0.32495590\n",
      "Iteration 47, loss = 0.32263988\n",
      "Iteration 48, loss = 0.32051260\n",
      "Iteration 49, loss = 0.31700039\n",
      "Iteration 50, loss = 0.31627526\n",
      "Iteration 51, loss = 0.31520725\n",
      "Iteration 52, loss = 0.31283257\n",
      "Iteration 53, loss = 0.31191711\n",
      "Iteration 54, loss = 0.31705626\n",
      "Iteration 55, loss = 0.31142919\n",
      "Iteration 56, loss = 0.30776639\n",
      "Iteration 57, loss = 0.30441175\n",
      "Iteration 58, loss = 0.30307415\n",
      "Iteration 59, loss = 0.30167079\n",
      "Iteration 60, loss = 0.30310256\n",
      "Iteration 61, loss = 0.30067072\n",
      "Iteration 62, loss = 0.30223211\n",
      "Iteration 63, loss = 0.29820894\n",
      "Iteration 64, loss = 0.29587464\n",
      "Iteration 65, loss = 0.29434589\n",
      "Iteration 66, loss = 0.29191307\n",
      "Iteration 67, loss = 0.29182117\n",
      "Iteration 68, loss = 0.29066021\n",
      "Iteration 69, loss = 0.28943159\n",
      "Iteration 70, loss = 0.28727819\n",
      "Iteration 71, loss = 0.28606501\n",
      "Iteration 72, loss = 0.28637451\n",
      "Iteration 73, loss = 0.28754872\n",
      "Iteration 74, loss = 0.28429995\n",
      "Iteration 75, loss = 0.28349230\n",
      "Iteration 76, loss = 0.28140661\n",
      "Iteration 77, loss = 0.28074952\n",
      "Iteration 78, loss = 0.28043953\n",
      "Iteration 79, loss = 0.27746830\n",
      "Iteration 80, loss = 0.27605567\n",
      "Iteration 81, loss = 0.27661578\n",
      "Iteration 82, loss = 0.27386930\n",
      "Iteration 83, loss = 0.27341854\n",
      "Iteration 84, loss = 0.27430309\n",
      "Iteration 85, loss = 0.27238339\n",
      "Iteration 86, loss = 0.27055904\n",
      "Iteration 87, loss = 0.27314279\n",
      "Iteration 88, loss = 0.26867195\n",
      "Iteration 89, loss = 0.26757640\n",
      "Iteration 90, loss = 0.26555716\n",
      "Iteration 91, loss = 0.26728860\n",
      "Iteration 92, loss = 0.26772862\n",
      "Iteration 93, loss = 0.26741164\n",
      "Iteration 94, loss = 0.26524583\n",
      "Iteration 95, loss = 0.26459828\n",
      "Iteration 96, loss = 0.26333372\n",
      "Iteration 97, loss = 0.26463872\n",
      "Iteration 98, loss = 0.26307991\n",
      "Iteration 99, loss = 0.26046025\n",
      "Iteration 100, loss = 0.26197581\n",
      "Iteration 101, loss = 0.26171944\n",
      "Iteration 102, loss = 0.25710958\n",
      "Iteration 103, loss = 0.26046981\n",
      "Iteration 104, loss = 0.25423878\n",
      "Iteration 105, loss = 0.25909048\n",
      "Iteration 106, loss = 0.25529084\n",
      "Iteration 107, loss = 0.25634695\n",
      "Iteration 108, loss = 0.25259684\n",
      "Iteration 109, loss = 0.25210765\n",
      "Iteration 110, loss = 0.25135000\n",
      "Iteration 111, loss = 0.25095563\n",
      "Iteration 112, loss = 0.24885665\n",
      "Iteration 113, loss = 0.24995053\n",
      "Iteration 114, loss = 0.24864503\n",
      "Iteration 115, loss = 0.24699702\n",
      "Iteration 116, loss = 0.24603525\n",
      "Iteration 117, loss = 0.24998924\n",
      "Iteration 118, loss = 0.24710634\n",
      "Iteration 119, loss = 0.24314790\n",
      "Iteration 120, loss = 0.24424362\n",
      "Iteration 121, loss = 0.24263932\n",
      "Iteration 122, loss = 0.24315620\n",
      "Iteration 123, loss = 0.24224423\n",
      "Iteration 124, loss = 0.24104092\n",
      "Iteration 125, loss = 0.24026632\n",
      "Iteration 126, loss = 0.23949486\n",
      "Iteration 127, loss = 0.24025360\n",
      "Iteration 128, loss = 0.24052514\n",
      "Iteration 129, loss = 0.24079910\n",
      "Iteration 130, loss = 0.24615928\n",
      "Iteration 131, loss = 0.23777555\n",
      "Iteration 132, loss = 0.24020813\n",
      "Iteration 133, loss = 0.23762971\n",
      "Iteration 134, loss = 0.23789784\n",
      "Iteration 135, loss = 0.24133418\n",
      "Iteration 136, loss = 0.24318101\n",
      "Iteration 137, loss = 0.23870315\n",
      "Iteration 138, loss = 0.23696714\n",
      "Iteration 139, loss = 0.24473674\n",
      "Iteration 140, loss = 0.23269634\n",
      "Iteration 141, loss = 0.23301306\n",
      "Iteration 142, loss = 0.22977309\n",
      "Iteration 143, loss = 0.22900710\n",
      "Iteration 144, loss = 0.23000228\n",
      "Iteration 145, loss = 0.22789957\n",
      "Iteration 146, loss = 0.22882250\n",
      "Iteration 147, loss = 0.22876206\n",
      "Iteration 148, loss = 0.23287718\n",
      "Iteration 149, loss = 0.22824154\n",
      "Iteration 150, loss = 0.22665835\n",
      "Iteration 151, loss = 0.22613916\n",
      "Iteration 152, loss = 0.22443454\n",
      "Iteration 153, loss = 0.22473451\n",
      "Iteration 154, loss = 0.22364834\n",
      "Iteration 155, loss = 0.22354375\n",
      "Iteration 156, loss = 0.22472796\n",
      "Iteration 157, loss = 0.22191034\n",
      "Iteration 158, loss = 0.22145179\n",
      "Iteration 159, loss = 0.22241669\n",
      "Iteration 160, loss = 0.21974102\n",
      "Iteration 161, loss = 0.22163979\n",
      "Iteration 162, loss = 0.22261689\n",
      "Iteration 163, loss = 0.22324757\n",
      "Iteration 164, loss = 0.22021433\n",
      "Iteration 165, loss = 0.21846739\n",
      "Iteration 166, loss = 0.21814121\n",
      "Iteration 167, loss = 0.21712595\n",
      "Iteration 168, loss = 0.21669952\n",
      "Iteration 169, loss = 0.21739569\n",
      "Iteration 170, loss = 0.21807529\n",
      "Iteration 171, loss = 0.21835360\n",
      "Iteration 172, loss = 0.21722438\n",
      "Iteration 173, loss = 0.21403591\n",
      "Iteration 174, loss = 0.21403769\n",
      "Iteration 175, loss = 0.21452815\n",
      "Iteration 176, loss = 0.21513538\n",
      "Iteration 177, loss = 0.21281439\n",
      "Iteration 178, loss = 0.21157449\n",
      "Iteration 179, loss = 0.21160152\n",
      "Iteration 180, loss = 0.21337319\n",
      "Iteration 181, loss = 0.21447970\n",
      "Iteration 182, loss = 0.21344836\n",
      "Iteration 183, loss = 0.21154816\n",
      "Iteration 184, loss = 0.21175715\n",
      "Iteration 185, loss = 0.21305797\n",
      "Iteration 186, loss = 0.21324880\n",
      "Iteration 187, loss = 0.21290263\n",
      "Iteration 188, loss = 0.20948758\n",
      "Iteration 189, loss = 0.21143238\n",
      "Iteration 190, loss = 0.20654782\n",
      "Iteration 191, loss = 0.20790250\n",
      "Iteration 192, loss = 0.20849868\n",
      "Iteration 193, loss = 0.21269754\n",
      "Iteration 194, loss = 0.20478210\n",
      "Iteration 195, loss = 0.20632302\n",
      "Iteration 196, loss = 0.20711864\n",
      "Iteration 197, loss = 0.20386569\n",
      "Iteration 198, loss = 0.20313823\n",
      "Iteration 199, loss = 0.20510513\n",
      "Iteration 200, loss = 0.20420165\n",
      "Iteration 201, loss = 0.20682047\n",
      "Iteration 202, loss = 0.20242802\n",
      "Iteration 203, loss = 0.20003009\n",
      "Iteration 204, loss = 0.19968569\n",
      "Iteration 205, loss = 0.20018250\n",
      "Iteration 206, loss = 0.19885866\n",
      "Iteration 207, loss = 0.20173503\n",
      "Iteration 208, loss = 0.20436082\n",
      "Iteration 209, loss = 0.19763540\n",
      "Iteration 210, loss = 0.19693207\n",
      "Iteration 211, loss = 0.19602315\n",
      "Iteration 212, loss = 0.19820633\n",
      "Iteration 213, loss = 0.19508935\n",
      "Iteration 214, loss = 0.19623920\n",
      "Iteration 215, loss = 0.19635452\n",
      "Iteration 216, loss = 0.20202156\n",
      "Iteration 217, loss = 0.19712709\n",
      "Iteration 218, loss = 0.19392254\n",
      "Iteration 219, loss = 0.19835584\n",
      "Iteration 220, loss = 0.19262560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 221, loss = 0.19538929\n",
      "Iteration 222, loss = 0.19698294\n",
      "Iteration 223, loss = 0.19048771\n",
      "Iteration 224, loss = 0.19119328\n",
      "Iteration 225, loss = 0.19082179\n",
      "Iteration 226, loss = 0.19089251\n",
      "Iteration 227, loss = 0.19008737\n",
      "Iteration 228, loss = 0.19186389\n",
      "Iteration 229, loss = 0.18979012\n",
      "Iteration 230, loss = 0.18982030\n",
      "Iteration 231, loss = 0.18642687\n",
      "Iteration 232, loss = 0.18802206\n",
      "Iteration 233, loss = 0.18661376\n",
      "Iteration 234, loss = 0.18550110\n",
      "Iteration 235, loss = 0.18559408\n",
      "Iteration 236, loss = 0.18442893\n",
      "Iteration 237, loss = 0.18433369\n",
      "Iteration 238, loss = 0.18463727\n",
      "Iteration 239, loss = 0.18877941\n",
      "Iteration 240, loss = 0.19370878\n",
      "Iteration 241, loss = 0.18363903\n",
      "Iteration 242, loss = 0.18731708\n",
      "Iteration 243, loss = 0.18606187\n",
      "Iteration 244, loss = 0.18522158\n",
      "Iteration 245, loss = 0.18249074\n",
      "Iteration 246, loss = 0.17961945\n",
      "Iteration 247, loss = 0.18285772\n",
      "Iteration 248, loss = 0.18003864\n",
      "Iteration 249, loss = 0.17923310\n",
      "Iteration 250, loss = 0.17826426\n",
      "Iteration 251, loss = 0.17725387\n",
      "Iteration 252, loss = 0.17732644\n",
      "Iteration 253, loss = 0.17734262\n",
      "Iteration 254, loss = 0.17614641\n",
      "Iteration 255, loss = 0.17656490\n",
      "Iteration 256, loss = 0.17540886\n",
      "Iteration 257, loss = 0.17531091\n",
      "Iteration 258, loss = 0.17440284\n",
      "Iteration 259, loss = 0.17833781\n",
      "Iteration 260, loss = 0.17620675\n",
      "Iteration 261, loss = 0.17284530\n",
      "Iteration 262, loss = 0.17115670\n",
      "Iteration 263, loss = 0.17426529\n",
      "Iteration 264, loss = 0.17159560\n",
      "Iteration 265, loss = 0.16957898\n",
      "Iteration 266, loss = 0.16892292\n",
      "Iteration 267, loss = 0.17216755\n",
      "Iteration 268, loss = 0.17084526\n",
      "Iteration 269, loss = 0.16975839\n",
      "Iteration 270, loss = 0.16803673\n",
      "Iteration 271, loss = 0.16874406\n",
      "Iteration 272, loss = 0.17217015\n",
      "Iteration 273, loss = 0.17236775\n",
      "Iteration 274, loss = 0.16724127\n",
      "Iteration 275, loss = 0.16612613\n",
      "Iteration 276, loss = 0.16581283\n",
      "Iteration 277, loss = 0.16449065\n",
      "Iteration 278, loss = 0.16491828\n",
      "Iteration 279, loss = 0.16476155\n",
      "Iteration 280, loss = 0.16332273\n",
      "Iteration 281, loss = 0.16750191\n",
      "Iteration 282, loss = 0.16248041\n",
      "Iteration 283, loss = 0.16478337\n",
      "Iteration 284, loss = 0.16054686\n",
      "Iteration 285, loss = 0.16258825\n",
      "Iteration 286, loss = 0.16264457\n",
      "Iteration 287, loss = 0.16422495\n",
      "Iteration 288, loss = 0.16528597\n",
      "Iteration 289, loss = 0.16389485\n",
      "Iteration 290, loss = 0.15891491\n",
      "Iteration 291, loss = 0.15867643\n",
      "Iteration 292, loss = 0.15829083\n",
      "Iteration 293, loss = 0.15673205\n",
      "Iteration 294, loss = 0.15727671\n",
      "Iteration 295, loss = 0.15869074\n",
      "Iteration 296, loss = 0.15607901\n",
      "Iteration 297, loss = 0.15478874\n",
      "Iteration 298, loss = 0.15457484\n",
      "Iteration 299, loss = 0.16413057\n",
      "Iteration 300, loss = 0.16366812\n",
      "Iteration 301, loss = 0.15518970\n",
      "Iteration 302, loss = 0.15320021\n",
      "Iteration 303, loss = 0.15220767\n",
      "Iteration 304, loss = 0.15259460\n",
      "Iteration 305, loss = 0.15100881\n",
      "Iteration 306, loss = 0.15257360\n",
      "Iteration 307, loss = 0.15092482\n",
      "Iteration 308, loss = 0.15313965\n",
      "Iteration 309, loss = 0.14975831\n",
      "Iteration 310, loss = 0.15041829\n",
      "Iteration 311, loss = 0.14719468\n",
      "Iteration 312, loss = 0.14802447\n",
      "Iteration 313, loss = 0.14708983\n",
      "Iteration 314, loss = 0.14729241\n",
      "Iteration 315, loss = 0.14663526\n",
      "Iteration 316, loss = 0.14728139\n",
      "Iteration 317, loss = 0.14685755\n",
      "Iteration 318, loss = 0.14740527\n",
      "Iteration 319, loss = 0.14446683\n",
      "Iteration 320, loss = 0.14573175\n",
      "Iteration 321, loss = 0.14881494\n",
      "Iteration 322, loss = 0.14578825\n",
      "Iteration 323, loss = 0.14311350\n",
      "Iteration 324, loss = 0.14283094\n",
      "Iteration 325, loss = 0.14241351\n",
      "Iteration 326, loss = 0.14091592\n",
      "Iteration 327, loss = 0.14155338\n",
      "Iteration 328, loss = 0.13972734\n",
      "Iteration 329, loss = 0.13881393\n",
      "Iteration 330, loss = 0.13855561\n",
      "Iteration 331, loss = 0.13836914\n",
      "Iteration 332, loss = 0.13863335\n",
      "Iteration 333, loss = 0.13793721\n",
      "Iteration 334, loss = 0.13746502\n",
      "Iteration 335, loss = 0.13663409\n",
      "Iteration 336, loss = 0.13622339\n",
      "Iteration 337, loss = 0.13584910\n",
      "Iteration 338, loss = 0.13904088\n",
      "Iteration 339, loss = 0.13359359\n",
      "Iteration 340, loss = 0.13446017\n",
      "Iteration 341, loss = 0.13593399\n",
      "Iteration 342, loss = 0.13326992\n",
      "Iteration 343, loss = 0.13282717\n",
      "Iteration 344, loss = 0.13195270\n",
      "Iteration 345, loss = 0.13333661\n",
      "Iteration 346, loss = 0.13242030\n",
      "Iteration 347, loss = 0.13281170\n",
      "Iteration 348, loss = 0.13282539\n",
      "Iteration 349, loss = 0.13221525\n",
      "Iteration 350, loss = 0.13052522\n",
      "Iteration 351, loss = 0.13048456\n",
      "Iteration 352, loss = 0.12828008\n",
      "Iteration 353, loss = 0.13245664\n",
      "Iteration 354, loss = 0.12937983\n",
      "Iteration 355, loss = 0.12740829\n",
      "Iteration 356, loss = 0.12820025\n",
      "Iteration 357, loss = 0.12675298\n",
      "Iteration 358, loss = 0.12503569\n",
      "Iteration 359, loss = 0.12650462\n",
      "Iteration 360, loss = 0.12514545\n",
      "Iteration 361, loss = 0.12478003\n",
      "Iteration 362, loss = 0.12507217\n",
      "Iteration 363, loss = 0.12530169\n",
      "Iteration 364, loss = 0.12316833\n",
      "Iteration 365, loss = 0.12240358\n",
      "Iteration 366, loss = 0.12261914\n",
      "Iteration 367, loss = 0.12132348\n",
      "Iteration 368, loss = 0.12166389\n",
      "Iteration 369, loss = 0.12319496\n",
      "Iteration 370, loss = 0.12174153\n",
      "Iteration 371, loss = 0.12212494\n",
      "Iteration 372, loss = 0.12251728\n",
      "Iteration 373, loss = 0.11965373\n",
      "Iteration 374, loss = 0.11812483\n",
      "Iteration 375, loss = 0.11826843\n",
      "Iteration 376, loss = 0.11695505\n",
      "Iteration 377, loss = 0.11695572\n",
      "Iteration 378, loss = 0.11586327\n",
      "Iteration 379, loss = 0.11736569\n",
      "Iteration 380, loss = 0.11612960\n",
      "Iteration 381, loss = 0.11557997\n",
      "Iteration 382, loss = 0.11497141\n",
      "Iteration 383, loss = 0.11557945\n",
      "Iteration 384, loss = 0.11400838\n",
      "Iteration 385, loss = 0.11425978\n",
      "Iteration 386, loss = 0.11484726\n",
      "Iteration 387, loss = 0.11358347\n",
      "Iteration 388, loss = 0.11306545\n",
      "Iteration 389, loss = 0.11229633\n",
      "Iteration 390, loss = 0.11145160\n",
      "Iteration 391, loss = 0.11289718\n",
      "Iteration 392, loss = 0.11409564\n",
      "Iteration 393, loss = 0.11049890\n",
      "Iteration 394, loss = 0.11166231\n",
      "Iteration 395, loss = 0.10970206\n",
      "Iteration 396, loss = 0.10962282\n",
      "Iteration 397, loss = 0.10931930\n",
      "Iteration 398, loss = 0.10872473\n",
      "Iteration 399, loss = 0.10794606\n",
      "Iteration 400, loss = 0.10755010\n",
      "Iteration 401, loss = 0.10663087\n",
      "Iteration 402, loss = 0.10529689\n",
      "Iteration 403, loss = 0.10575934\n",
      "Iteration 404, loss = 0.10736487\n",
      "Iteration 405, loss = 0.10977372\n",
      "Iteration 406, loss = 0.10758812\n",
      "Iteration 407, loss = 0.10469575\n",
      "Iteration 408, loss = 0.10467731\n",
      "Iteration 409, loss = 0.10363449\n",
      "Iteration 410, loss = 0.10502028\n",
      "Iteration 411, loss = 0.10258163\n",
      "Iteration 412, loss = 0.10251582\n",
      "Iteration 413, loss = 0.10165259\n",
      "Iteration 414, loss = 0.10228317\n",
      "Iteration 415, loss = 0.10171333\n",
      "Iteration 416, loss = 0.10065445\n",
      "Iteration 417, loss = 0.10117805\n",
      "Iteration 418, loss = 0.10062976\n",
      "Iteration 419, loss = 0.09897957\n",
      "Iteration 420, loss = 0.09809390\n",
      "Iteration 421, loss = 0.09892076\n",
      "Iteration 422, loss = 0.09888311\n",
      "Iteration 423, loss = 0.09747144\n",
      "Iteration 424, loss = 0.09656259\n",
      "Iteration 425, loss = 0.09718149\n",
      "Iteration 426, loss = 0.09566077\n",
      "Iteration 427, loss = 0.09587425\n",
      "Iteration 428, loss = 0.09556847\n",
      "Iteration 429, loss = 0.09538228\n",
      "Iteration 430, loss = 0.09494903\n",
      "Iteration 431, loss = 0.09504741\n",
      "Iteration 432, loss = 0.09422912\n",
      "Iteration 433, loss = 0.09420759\n",
      "Iteration 434, loss = 0.09393165\n",
      "Iteration 435, loss = 0.09369459\n",
      "Iteration 436, loss = 0.09406839\n",
      "Iteration 437, loss = 0.09439385\n",
      "Iteration 438, loss = 0.09153431\n",
      "Iteration 439, loss = 0.09171833\n",
      "Iteration 440, loss = 0.09075236\n",
      "Iteration 441, loss = 0.09252216\n",
      "Iteration 442, loss = 0.08994916\n",
      "Iteration 443, loss = 0.08962257\n",
      "Iteration 444, loss = 0.08956948\n",
      "Iteration 445, loss = 0.08777115\n",
      "Iteration 446, loss = 0.08889113\n",
      "Iteration 447, loss = 0.08820798\n",
      "Iteration 448, loss = 0.08792489\n",
      "Iteration 449, loss = 0.08743716\n",
      "Iteration 450, loss = 0.08848807\n",
      "Iteration 451, loss = 0.08665546\n",
      "Iteration 452, loss = 0.08657052\n",
      "Iteration 453, loss = 0.08554361\n",
      "Iteration 454, loss = 0.08694966\n",
      "Iteration 455, loss = 0.08548899\n",
      "Iteration 456, loss = 0.08504077\n",
      "Iteration 457, loss = 0.08432715\n",
      "Iteration 458, loss = 0.08513778\n",
      "Iteration 459, loss = 0.08368185\n",
      "Iteration 460, loss = 0.08318789\n",
      "Iteration 461, loss = 0.08408403\n",
      "Iteration 462, loss = 0.08385691\n",
      "Iteration 463, loss = 0.08188100\n",
      "Iteration 464, loss = 0.08448202\n",
      "Iteration 465, loss = 0.08418251\n",
      "Iteration 466, loss = 0.08230304\n",
      "Iteration 467, loss = 0.08141582\n",
      "Iteration 468, loss = 0.08045385\n",
      "Iteration 469, loss = 0.07989148\n",
      "Iteration 470, loss = 0.08905197\n",
      "Iteration 471, loss = 0.08217959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 472, loss = 0.07943605\n",
      "Iteration 473, loss = 0.07950942\n",
      "Iteration 474, loss = 0.07940232\n",
      "Iteration 475, loss = 0.07815996\n",
      "Iteration 476, loss = 0.07880351\n",
      "Iteration 477, loss = 0.07742460\n",
      "Iteration 478, loss = 0.07811474\n",
      "Iteration 479, loss = 0.07822942\n",
      "Iteration 480, loss = 0.07697607\n",
      "Iteration 481, loss = 0.07839408\n",
      "Iteration 482, loss = 0.07756195\n",
      "Iteration 483, loss = 0.07658409\n",
      "Iteration 484, loss = 0.07770714\n",
      "Iteration 485, loss = 0.07853150\n",
      "Iteration 486, loss = 0.07439939\n",
      "Iteration 487, loss = 0.07429804\n",
      "Iteration 488, loss = 0.07416490\n",
      "Iteration 489, loss = 0.07495685\n",
      "Iteration 490, loss = 0.07510212\n",
      "Iteration 491, loss = 0.07355388\n",
      "Iteration 492, loss = 0.07242977\n",
      "Iteration 493, loss = 0.07230885\n",
      "Iteration 494, loss = 0.07245312\n",
      "Iteration 495, loss = 0.07146648\n",
      "Iteration 496, loss = 0.07300681\n",
      "Iteration 497, loss = 0.07161311\n",
      "Iteration 498, loss = 0.07236157\n",
      "Iteration 499, loss = 0.07072874\n",
      "Iteration 500, loss = 0.07144833\n",
      "Iteration 501, loss = 0.07124056\n",
      "Iteration 502, loss = 0.07057502\n",
      "Iteration 503, loss = 0.06947262\n",
      "Iteration 504, loss = 0.06892083\n",
      "Iteration 505, loss = 0.06790985\n",
      "Iteration 506, loss = 0.06974370\n",
      "Iteration 507, loss = 0.06957756\n",
      "Iteration 508, loss = 0.06782486\n",
      "Iteration 509, loss = 0.06786589\n",
      "Iteration 510, loss = 0.06859039\n",
      "Iteration 511, loss = 0.06776625\n",
      "Iteration 512, loss = 0.06606750\n",
      "Iteration 513, loss = 0.06665682\n",
      "Iteration 514, loss = 0.06649189\n",
      "Iteration 515, loss = 0.06556110\n",
      "Iteration 516, loss = 0.06514178\n",
      "Iteration 517, loss = 0.06565834\n",
      "Iteration 518, loss = 0.06468937\n",
      "Iteration 519, loss = 0.06425359\n",
      "Iteration 520, loss = 0.06484433\n",
      "Iteration 521, loss = 0.06382652\n",
      "Iteration 522, loss = 0.06361980\n",
      "Iteration 523, loss = 0.06376806\n",
      "Iteration 524, loss = 0.06359036\n",
      "Iteration 525, loss = 0.06395809\n",
      "Iteration 526, loss = 0.06225320\n",
      "Iteration 527, loss = 0.06350284\n",
      "Iteration 528, loss = 0.06643348\n",
      "Iteration 529, loss = 0.06387232\n",
      "Iteration 530, loss = 0.06305241\n",
      "Iteration 531, loss = 0.06389758\n",
      "Iteration 532, loss = 0.06234484\n",
      "Iteration 533, loss = 0.06326288\n",
      "Iteration 534, loss = 0.05962031\n",
      "Iteration 535, loss = 0.05987051\n",
      "Iteration 536, loss = 0.06054529\n",
      "Iteration 537, loss = 0.06191645\n",
      "Iteration 538, loss = 0.06066977\n",
      "Iteration 539, loss = 0.05954151\n",
      "Iteration 540, loss = 0.05911602\n",
      "Iteration 541, loss = 0.05814621\n",
      "Iteration 542, loss = 0.05789025\n",
      "Iteration 543, loss = 0.05815790\n",
      "Iteration 544, loss = 0.05715278\n",
      "Iteration 545, loss = 0.05797801\n",
      "Iteration 546, loss = 0.05875515\n",
      "Iteration 547, loss = 0.05858845\n",
      "Iteration 548, loss = 0.05739094\n",
      "Iteration 549, loss = 0.05638926\n",
      "Iteration 550, loss = 0.05743395\n",
      "Iteration 551, loss = 0.05670670\n",
      "Iteration 552, loss = 0.05613785\n",
      "Iteration 553, loss = 0.05542190\n",
      "Iteration 554, loss = 0.05781797\n",
      "Iteration 555, loss = 0.05636557\n",
      "Iteration 556, loss = 0.05611990\n",
      "Iteration 557, loss = 0.05652238\n",
      "Iteration 558, loss = 0.05474902\n",
      "Iteration 559, loss = 0.05341809\n",
      "Iteration 560, loss = 0.05551636\n",
      "Iteration 561, loss = 0.05666806\n",
      "Iteration 562, loss = 0.05552933\n",
      "Iteration 563, loss = 0.05352211\n",
      "Iteration 564, loss = 0.05294466\n",
      "Iteration 565, loss = 0.05210368\n",
      "Iteration 566, loss = 0.05537025\n",
      "Iteration 567, loss = 0.05218323\n",
      "Iteration 568, loss = 0.05185808\n",
      "Iteration 569, loss = 0.05342590\n",
      "Iteration 570, loss = 0.05251311\n",
      "Iteration 571, loss = 0.05158124\n",
      "Iteration 572, loss = 0.05219104\n",
      "Iteration 573, loss = 0.05057546\n",
      "Iteration 574, loss = 0.05125847\n",
      "Iteration 575, loss = 0.05074129\n",
      "Iteration 576, loss = 0.04985411\n",
      "Iteration 577, loss = 0.05031455\n",
      "Iteration 578, loss = 0.05012543\n",
      "Iteration 579, loss = 0.05082305\n",
      "Iteration 580, loss = 0.04991123\n",
      "Iteration 581, loss = 0.04907643\n",
      "Iteration 582, loss = 0.05174295\n",
      "Iteration 583, loss = 0.04993783\n",
      "Iteration 584, loss = 0.05008856\n",
      "Iteration 585, loss = 0.05041907\n",
      "Iteration 586, loss = 0.04994549\n",
      "Iteration 587, loss = 0.04884309\n",
      "Iteration 588, loss = 0.04829714\n",
      "Iteration 589, loss = 0.04768799\n",
      "Iteration 590, loss = 0.04842583\n",
      "Iteration 591, loss = 0.04728853\n",
      "Iteration 592, loss = 0.04634527\n",
      "Iteration 593, loss = 0.04700848\n",
      "Iteration 594, loss = 0.04753832\n",
      "Iteration 595, loss = 0.04778323\n",
      "Iteration 596, loss = 0.04666425\n",
      "Iteration 597, loss = 0.04705092\n",
      "Iteration 598, loss = 0.04589031\n",
      "Iteration 599, loss = 0.04586436\n",
      "Iteration 600, loss = 0.04492442\n",
      "Iteration 601, loss = 0.04460466\n",
      "Iteration 602, loss = 0.04462289\n",
      "Iteration 603, loss = 0.04456420\n",
      "Iteration 604, loss = 0.04445869\n",
      "Iteration 605, loss = 0.04448569\n",
      "Iteration 606, loss = 0.04442564\n",
      "Iteration 607, loss = 0.04431129\n",
      "Iteration 608, loss = 0.04446145\n",
      "Iteration 609, loss = 0.04408612\n",
      "Iteration 610, loss = 0.04423667\n",
      "Iteration 611, loss = 0.04395148\n",
      "Iteration 612, loss = 0.04340977\n",
      "Iteration 613, loss = 0.04346638\n",
      "Iteration 614, loss = 0.04245753\n",
      "Iteration 615, loss = 0.04282586\n",
      "Iteration 616, loss = 0.04282220\n",
      "Iteration 617, loss = 0.04243505\n",
      "Iteration 618, loss = 0.04161220\n",
      "Iteration 619, loss = 0.04142497\n",
      "Iteration 620, loss = 0.04220866\n",
      "Iteration 621, loss = 0.04150132\n",
      "Iteration 622, loss = 0.04135434\n",
      "Iteration 623, loss = 0.04200350\n",
      "Iteration 624, loss = 0.04176699\n",
      "Iteration 625, loss = 0.04060065\n",
      "Iteration 626, loss = 0.04033570\n",
      "Iteration 627, loss = 0.04007074\n",
      "Iteration 628, loss = 0.04108405\n",
      "Iteration 629, loss = 0.03963508\n",
      "Iteration 630, loss = 0.03954550\n",
      "Iteration 631, loss = 0.03996401\n",
      "Iteration 632, loss = 0.04048328\n",
      "Iteration 633, loss = 0.03931662\n",
      "Iteration 634, loss = 0.03935232\n",
      "Iteration 635, loss = 0.03884925\n",
      "Iteration 636, loss = 0.03902827\n",
      "Iteration 637, loss = 0.03851092\n",
      "Iteration 638, loss = 0.04018989\n",
      "Iteration 639, loss = 0.03956989\n",
      "Iteration 640, loss = 0.03960238\n",
      "Iteration 641, loss = 0.03946815\n",
      "Iteration 642, loss = 0.03978398\n",
      "Iteration 643, loss = 0.03765065\n",
      "Iteration 644, loss = 0.03779880\n",
      "Iteration 645, loss = 0.03757101\n",
      "Iteration 646, loss = 0.03743200\n",
      "Iteration 647, loss = 0.03642317\n",
      "Iteration 648, loss = 0.03666762\n",
      "Iteration 649, loss = 0.03703994\n",
      "Iteration 650, loss = 0.03630307\n",
      "Iteration 651, loss = 0.03698352\n",
      "Iteration 652, loss = 0.03581990\n",
      "Iteration 653, loss = 0.03596819\n",
      "Iteration 654, loss = 0.03723186\n",
      "Iteration 655, loss = 0.03695912\n",
      "Iteration 656, loss = 0.03638976\n",
      "Iteration 657, loss = 0.03588863\n",
      "Iteration 658, loss = 0.03658770\n",
      "Iteration 659, loss = 0.03525436\n",
      "Iteration 660, loss = 0.03476537\n",
      "Iteration 661, loss = 0.03422039\n",
      "Iteration 662, loss = 0.03470621\n",
      "Iteration 663, loss = 0.03501103\n",
      "Iteration 664, loss = 0.03554752\n",
      "Iteration 665, loss = 0.03463129\n",
      "Iteration 666, loss = 0.03481379\n",
      "Iteration 667, loss = 0.03646965\n",
      "Iteration 668, loss = 0.03571136\n",
      "Iteration 669, loss = 0.03384939\n",
      "Iteration 670, loss = 0.03445481\n",
      "Iteration 671, loss = 0.03568116\n",
      "Iteration 672, loss = 0.03307847\n",
      "Iteration 673, loss = 0.03266370\n",
      "Iteration 674, loss = 0.03307840\n",
      "Iteration 675, loss = 0.03261430\n",
      "Iteration 676, loss = 0.03571014\n",
      "Iteration 677, loss = 0.03491126\n",
      "Iteration 678, loss = 0.03512804\n",
      "Iteration 679, loss = 0.03316303\n",
      "Iteration 680, loss = 0.03230511\n",
      "Iteration 681, loss = 0.03301837\n",
      "Iteration 682, loss = 0.03219718\n",
      "Iteration 683, loss = 0.03334761\n",
      "Iteration 684, loss = 0.03209942\n",
      "Iteration 685, loss = 0.03177681\n",
      "Iteration 686, loss = 0.03103367\n",
      "Iteration 687, loss = 0.03098097\n",
      "Iteration 688, loss = 0.03135925\n",
      "Iteration 689, loss = 0.03144456\n",
      "Iteration 690, loss = 0.03109593\n",
      "Iteration 691, loss = 0.03095525\n",
      "Iteration 692, loss = 0.03008643\n",
      "Iteration 693, loss = 0.03088338\n",
      "Iteration 694, loss = 0.03013594\n",
      "Iteration 695, loss = 0.03007840\n",
      "Iteration 696, loss = 0.03020512\n",
      "Iteration 697, loss = 0.02997433\n",
      "Iteration 698, loss = 0.02981418\n",
      "Iteration 699, loss = 0.02969425\n",
      "Iteration 700, loss = 0.02947951\n",
      "Iteration 701, loss = 0.02979831\n",
      "Iteration 702, loss = 0.02997197\n",
      "Iteration 703, loss = 0.02937425\n",
      "Iteration 704, loss = 0.02974669\n",
      "Iteration 705, loss = 0.03077766\n",
      "Iteration 706, loss = 0.02991952\n",
      "Iteration 707, loss = 0.03066342\n",
      "Iteration 708, loss = 0.03042683\n",
      "Iteration 709, loss = 0.02971379\n",
      "Iteration 710, loss = 0.03028583\n",
      "Iteration 711, loss = 0.02930067\n",
      "Iteration 712, loss = 0.02901226\n",
      "Iteration 713, loss = 0.02887554\n",
      "Iteration 714, loss = 0.02962236\n",
      "Iteration 715, loss = 0.02835075\n",
      "Iteration 716, loss = 0.02814738\n",
      "Iteration 717, loss = 0.02820466\n",
      "Iteration 718, loss = 0.02828281\n",
      "Iteration 719, loss = 0.02725460\n",
      "Iteration 720, loss = 0.02726862\n",
      "Iteration 721, loss = 0.02666217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 722, loss = 0.02668706\n",
      "Iteration 723, loss = 0.02794071\n",
      "Iteration 724, loss = 0.02690551\n",
      "Iteration 725, loss = 0.02702357\n",
      "Iteration 726, loss = 0.02616782\n",
      "Iteration 727, loss = 0.02728211\n",
      "Iteration 728, loss = 0.02719172\n",
      "Iteration 729, loss = 0.02655532\n",
      "Iteration 730, loss = 0.02626407\n",
      "Iteration 731, loss = 0.02600324\n",
      "Iteration 732, loss = 0.02667785\n",
      "Iteration 733, loss = 0.02608807\n",
      "Iteration 734, loss = 0.02527319\n",
      "Iteration 735, loss = 0.02547438\n",
      "Iteration 736, loss = 0.02593986\n",
      "Iteration 737, loss = 0.02589727\n",
      "Iteration 738, loss = 0.02494095\n",
      "Iteration 739, loss = 0.02500006\n",
      "Iteration 740, loss = 0.02499583\n",
      "Iteration 741, loss = 0.02501318\n",
      "Iteration 742, loss = 0.02438482\n",
      "Iteration 743, loss = 0.02457212\n",
      "Iteration 744, loss = 0.02513346\n",
      "Iteration 745, loss = 0.02462598\n",
      "Iteration 746, loss = 0.02467963\n",
      "Iteration 747, loss = 0.02402565\n",
      "Iteration 748, loss = 0.02491094\n",
      "Iteration 749, loss = 0.02479131\n",
      "Iteration 750, loss = 0.02444276\n",
      "Iteration 751, loss = 0.02419872\n",
      "Iteration 752, loss = 0.02480964\n",
      "Iteration 753, loss = 0.02446683\n",
      "Iteration 754, loss = 0.02405125\n",
      "Iteration 755, loss = 0.02513940\n",
      "Iteration 756, loss = 0.02857979\n",
      "Iteration 757, loss = 0.02490660\n",
      "Iteration 758, loss = 0.02483601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 20 of 48...\n",
      "Iteration 1, loss = 0.66681401\n",
      "Iteration 2, loss = 0.62083065\n",
      "Iteration 3, loss = 0.58854639\n",
      "Iteration 4, loss = 0.56361276\n",
      "Iteration 5, loss = 0.54304994\n",
      "Iteration 6, loss = 0.52066435\n",
      "Iteration 7, loss = 0.50426695\n",
      "Iteration 8, loss = 0.48856046\n",
      "Iteration 9, loss = 0.47500084\n",
      "Iteration 10, loss = 0.46215859\n",
      "Iteration 11, loss = 0.45128665\n",
      "Iteration 12, loss = 0.44206567\n",
      "Iteration 13, loss = 0.43494094\n",
      "Iteration 14, loss = 0.43209722\n",
      "Iteration 15, loss = 0.41952228\n",
      "Iteration 16, loss = 0.41338806\n",
      "Iteration 17, loss = 0.40663862\n",
      "Iteration 18, loss = 0.40334933\n",
      "Iteration 19, loss = 0.39640785\n",
      "Iteration 20, loss = 0.39207051\n",
      "Iteration 21, loss = 0.38787890\n",
      "Iteration 22, loss = 0.38392082\n",
      "Iteration 23, loss = 0.38043912\n",
      "Iteration 24, loss = 0.37644579\n",
      "Iteration 25, loss = 0.37286619\n",
      "Iteration 26, loss = 0.36920873\n",
      "Iteration 27, loss = 0.36621853\n",
      "Iteration 28, loss = 0.36363576\n",
      "Iteration 29, loss = 0.36102291\n",
      "Iteration 30, loss = 0.36587738\n",
      "Iteration 31, loss = 0.35840315\n",
      "Iteration 32, loss = 0.35374914\n",
      "Iteration 33, loss = 0.35152425\n",
      "Iteration 34, loss = 0.34834904\n",
      "Iteration 35, loss = 0.34759133\n",
      "Iteration 36, loss = 0.34421644\n",
      "Iteration 37, loss = 0.34230847\n",
      "Iteration 38, loss = 0.34118560\n",
      "Iteration 39, loss = 0.33785696\n",
      "Iteration 40, loss = 0.33624448\n",
      "Iteration 41, loss = 0.33341597\n",
      "Iteration 42, loss = 0.33320732\n",
      "Iteration 43, loss = 0.33005086\n",
      "Iteration 44, loss = 0.32900736\n",
      "Iteration 45, loss = 0.32767585\n",
      "Iteration 46, loss = 0.32699760\n",
      "Iteration 47, loss = 0.32348244\n",
      "Iteration 48, loss = 0.32205747\n",
      "Iteration 49, loss = 0.31922600\n",
      "Iteration 50, loss = 0.31836788\n",
      "Iteration 51, loss = 0.31703380\n",
      "Iteration 52, loss = 0.31464054\n",
      "Iteration 53, loss = 0.31643540\n",
      "Iteration 54, loss = 0.31207106\n",
      "Iteration 55, loss = 0.31272822\n",
      "Iteration 56, loss = 0.31271872\n",
      "Iteration 57, loss = 0.30925334\n",
      "Iteration 58, loss = 0.30796382\n",
      "Iteration 59, loss = 0.30436363\n",
      "Iteration 60, loss = 0.30497824\n",
      "Iteration 61, loss = 0.30597968\n",
      "Iteration 62, loss = 0.30527657\n",
      "Iteration 63, loss = 0.30144200\n",
      "Iteration 64, loss = 0.29772273\n",
      "Iteration 65, loss = 0.29701105\n",
      "Iteration 66, loss = 0.29529274\n",
      "Iteration 67, loss = 0.29492598\n",
      "Iteration 68, loss = 0.29434557\n",
      "Iteration 69, loss = 0.29184092\n",
      "Iteration 70, loss = 0.29244265\n",
      "Iteration 71, loss = 0.29263993\n",
      "Iteration 72, loss = 0.29119913\n",
      "Iteration 73, loss = 0.29046676\n",
      "Iteration 74, loss = 0.28882249\n",
      "Iteration 75, loss = 0.28818529\n",
      "Iteration 76, loss = 0.28751890\n",
      "Iteration 77, loss = 0.28428522\n",
      "Iteration 78, loss = 0.28382675\n",
      "Iteration 79, loss = 0.28220468\n",
      "Iteration 80, loss = 0.28372529\n",
      "Iteration 81, loss = 0.28238056\n",
      "Iteration 82, loss = 0.28466442\n",
      "Iteration 83, loss = 0.28034938\n",
      "Iteration 84, loss = 0.27860154\n",
      "Iteration 85, loss = 0.27706799\n",
      "Iteration 86, loss = 0.27710713\n",
      "Iteration 87, loss = 0.27376730\n",
      "Iteration 88, loss = 0.27327402\n",
      "Iteration 89, loss = 0.27539645\n",
      "Iteration 90, loss = 0.27524600\n",
      "Iteration 91, loss = 0.27429059\n",
      "Iteration 92, loss = 0.27079184\n",
      "Iteration 93, loss = 0.27041819\n",
      "Iteration 94, loss = 0.26917495\n",
      "Iteration 95, loss = 0.26767656\n",
      "Iteration 96, loss = 0.27092734\n",
      "Iteration 97, loss = 0.26669737\n",
      "Iteration 98, loss = 0.27039735\n",
      "Iteration 99, loss = 0.26530418\n",
      "Iteration 100, loss = 0.26342179\n",
      "Iteration 101, loss = 0.26325373\n",
      "Iteration 102, loss = 0.26287998\n",
      "Iteration 103, loss = 0.26188101\n",
      "Iteration 104, loss = 0.26452917\n",
      "Iteration 105, loss = 0.26513730\n",
      "Iteration 106, loss = 0.26299718\n",
      "Iteration 107, loss = 0.26394404\n",
      "Iteration 108, loss = 0.26135150\n",
      "Iteration 109, loss = 0.25946305\n",
      "Iteration 110, loss = 0.26015721\n",
      "Iteration 111, loss = 0.25803073\n",
      "Iteration 112, loss = 0.25757953\n",
      "Iteration 113, loss = 0.25683439\n",
      "Iteration 114, loss = 0.25630345\n",
      "Iteration 115, loss = 0.25590573\n",
      "Iteration 116, loss = 0.25509550\n",
      "Iteration 117, loss = 0.25472658\n",
      "Iteration 118, loss = 0.25263010\n",
      "Iteration 119, loss = 0.25291558\n",
      "Iteration 120, loss = 0.25604210\n",
      "Iteration 121, loss = 0.25535917\n",
      "Iteration 122, loss = 0.25257507\n",
      "Iteration 123, loss = 0.25191360\n",
      "Iteration 124, loss = 0.25212872\n",
      "Iteration 125, loss = 0.24828073\n",
      "Iteration 126, loss = 0.24994161\n",
      "Iteration 127, loss = 0.25160669\n",
      "Iteration 128, loss = 0.24964824\n",
      "Iteration 129, loss = 0.24654948\n",
      "Iteration 130, loss = 0.24781965\n",
      "Iteration 131, loss = 0.24481032\n",
      "Iteration 132, loss = 0.24426997\n",
      "Iteration 133, loss = 0.24300985\n",
      "Iteration 134, loss = 0.24325426\n",
      "Iteration 135, loss = 0.24350121\n",
      "Iteration 136, loss = 0.24254407\n",
      "Iteration 137, loss = 0.24266862\n",
      "Iteration 138, loss = 0.24748297\n",
      "Iteration 139, loss = 0.24261731\n",
      "Iteration 140, loss = 0.24306874\n",
      "Iteration 141, loss = 0.24003127\n",
      "Iteration 142, loss = 0.23911886\n",
      "Iteration 143, loss = 0.23996286\n",
      "Iteration 144, loss = 0.24169224\n",
      "Iteration 145, loss = 0.24425536\n",
      "Iteration 146, loss = 0.23969357\n",
      "Iteration 147, loss = 0.23677526\n",
      "Iteration 148, loss = 0.23570052\n",
      "Iteration 149, loss = 0.23580200\n",
      "Iteration 150, loss = 0.23773679\n",
      "Iteration 151, loss = 0.23689670\n",
      "Iteration 152, loss = 0.23390280\n",
      "Iteration 153, loss = 0.23472602\n",
      "Iteration 154, loss = 0.23453704\n",
      "Iteration 155, loss = 0.23300855\n",
      "Iteration 156, loss = 0.23279238\n",
      "Iteration 157, loss = 0.23574507\n",
      "Iteration 158, loss = 0.24419684\n",
      "Iteration 159, loss = 0.24105703\n",
      "Iteration 160, loss = 0.23259561\n",
      "Iteration 161, loss = 0.23264090\n",
      "Iteration 162, loss = 0.23054067\n",
      "Iteration 163, loss = 0.23399324\n",
      "Iteration 164, loss = 0.23316154\n",
      "Iteration 165, loss = 0.23091184\n",
      "Iteration 166, loss = 0.22898050\n",
      "Iteration 167, loss = 0.22723471\n",
      "Iteration 168, loss = 0.22768570\n",
      "Iteration 169, loss = 0.22946811\n",
      "Iteration 170, loss = 0.22579816\n",
      "Iteration 171, loss = 0.22901941\n",
      "Iteration 172, loss = 0.22858781\n",
      "Iteration 173, loss = 0.22920773\n",
      "Iteration 174, loss = 0.22729604\n",
      "Iteration 175, loss = 0.22850840\n",
      "Iteration 176, loss = 0.22664496\n",
      "Iteration 177, loss = 0.22504650\n",
      "Iteration 178, loss = 0.22420694\n",
      "Iteration 179, loss = 0.22443720\n",
      "Iteration 180, loss = 0.22616165\n",
      "Iteration 181, loss = 0.22561118\n",
      "Iteration 182, loss = 0.22128360\n",
      "Iteration 183, loss = 0.22398737\n",
      "Iteration 184, loss = 0.22274551\n",
      "Iteration 185, loss = 0.22003460\n",
      "Iteration 186, loss = 0.21932032\n",
      "Iteration 187, loss = 0.21988061\n",
      "Iteration 188, loss = 0.22097037\n",
      "Iteration 189, loss = 0.21966688\n",
      "Iteration 190, loss = 0.22045614\n",
      "Iteration 191, loss = 0.21923004\n",
      "Iteration 192, loss = 0.21916640\n",
      "Iteration 193, loss = 0.21845254\n",
      "Iteration 194, loss = 0.21806189\n",
      "Iteration 195, loss = 0.21948115\n",
      "Iteration 196, loss = 0.21600498\n",
      "Iteration 197, loss = 0.21842729\n",
      "Iteration 198, loss = 0.22224939\n",
      "Iteration 199, loss = 0.21716356\n",
      "Iteration 200, loss = 0.21396660\n",
      "Iteration 201, loss = 0.21466185\n",
      "Iteration 202, loss = 0.21520956\n",
      "Iteration 203, loss = 0.21331886\n",
      "Iteration 204, loss = 0.21162259\n",
      "Iteration 205, loss = 0.21202125\n",
      "Iteration 206, loss = 0.21363945\n",
      "Iteration 207, loss = 0.21187865\n",
      "Iteration 208, loss = 0.21169107\n",
      "Iteration 209, loss = 0.21612522\n",
      "Iteration 210, loss = 0.21199583\n",
      "Iteration 211, loss = 0.20988862\n",
      "Iteration 212, loss = 0.20950679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 213, loss = 0.20902458\n",
      "Iteration 214, loss = 0.20910085\n",
      "Iteration 215, loss = 0.21261723\n",
      "Iteration 216, loss = 0.21109771\n",
      "Iteration 217, loss = 0.20997535\n",
      "Iteration 218, loss = 0.20811487\n",
      "Iteration 219, loss = 0.20914735\n",
      "Iteration 220, loss = 0.20546844\n",
      "Iteration 221, loss = 0.20817273\n",
      "Iteration 222, loss = 0.21081269\n",
      "Iteration 223, loss = 0.20629154\n",
      "Iteration 224, loss = 0.20592268\n",
      "Iteration 225, loss = 0.20392154\n",
      "Iteration 226, loss = 0.20396342\n",
      "Iteration 227, loss = 0.20358923\n",
      "Iteration 228, loss = 0.20396753\n",
      "Iteration 229, loss = 0.20350160\n",
      "Iteration 230, loss = 0.20721527\n",
      "Iteration 231, loss = 0.20186239\n",
      "Iteration 232, loss = 0.20434852\n",
      "Iteration 233, loss = 0.20555158\n",
      "Iteration 234, loss = 0.20226771\n",
      "Iteration 235, loss = 0.20275816\n",
      "Iteration 236, loss = 0.20206241\n",
      "Iteration 237, loss = 0.20644062\n",
      "Iteration 238, loss = 0.19891892\n",
      "Iteration 239, loss = 0.20686004\n",
      "Iteration 240, loss = 0.20060457\n",
      "Iteration 241, loss = 0.20244190\n",
      "Iteration 242, loss = 0.19831223\n",
      "Iteration 243, loss = 0.20025262\n",
      "Iteration 244, loss = 0.19910521\n",
      "Iteration 245, loss = 0.19955001\n",
      "Iteration 246, loss = 0.19685923\n",
      "Iteration 247, loss = 0.19491066\n",
      "Iteration 248, loss = 0.19813461\n",
      "Iteration 249, loss = 0.19406653\n",
      "Iteration 250, loss = 0.19252007\n",
      "Iteration 251, loss = 0.19129303\n",
      "Iteration 252, loss = 0.19578251\n",
      "Iteration 253, loss = 0.20267778\n",
      "Iteration 254, loss = 0.19904180\n",
      "Iteration 255, loss = 0.19387064\n",
      "Iteration 256, loss = 0.19401837\n",
      "Iteration 257, loss = 0.19004786\n",
      "Iteration 258, loss = 0.19081567\n",
      "Iteration 259, loss = 0.18919631\n",
      "Iteration 260, loss = 0.18933174\n",
      "Iteration 261, loss = 0.18894366\n",
      "Iteration 262, loss = 0.18814543\n",
      "Iteration 263, loss = 0.18715525\n",
      "Iteration 264, loss = 0.18892648\n",
      "Iteration 265, loss = 0.18705969\n",
      "Iteration 266, loss = 0.18774644\n",
      "Iteration 267, loss = 0.18638820\n",
      "Iteration 268, loss = 0.18471760\n",
      "Iteration 269, loss = 0.18739206\n",
      "Iteration 270, loss = 0.18861351\n",
      "Iteration 271, loss = 0.18547706\n",
      "Iteration 272, loss = 0.18710658\n",
      "Iteration 273, loss = 0.19508814\n",
      "Iteration 274, loss = 0.18510565\n",
      "Iteration 275, loss = 0.18394682\n",
      "Iteration 276, loss = 0.18167727\n",
      "Iteration 277, loss = 0.18116175\n",
      "Iteration 278, loss = 0.18278893\n",
      "Iteration 279, loss = 0.18519540\n",
      "Iteration 280, loss = 0.18225001\n",
      "Iteration 281, loss = 0.17978467\n",
      "Iteration 282, loss = 0.17881305\n",
      "Iteration 283, loss = 0.17939972\n",
      "Iteration 284, loss = 0.17725523\n",
      "Iteration 285, loss = 0.17688004\n",
      "Iteration 286, loss = 0.17659840\n",
      "Iteration 287, loss = 0.17515216\n",
      "Iteration 288, loss = 0.17708275\n",
      "Iteration 289, loss = 0.18249082\n",
      "Iteration 290, loss = 0.17952107\n",
      "Iteration 291, loss = 0.17486727\n",
      "Iteration 292, loss = 0.17554307\n",
      "Iteration 293, loss = 0.17426994\n",
      "Iteration 294, loss = 0.17441409\n",
      "Iteration 295, loss = 0.17339557\n",
      "Iteration 296, loss = 0.17421843\n",
      "Iteration 297, loss = 0.17170803\n",
      "Iteration 298, loss = 0.17068603\n",
      "Iteration 299, loss = 0.17208325\n",
      "Iteration 300, loss = 0.17114125\n",
      "Iteration 301, loss = 0.17107664\n",
      "Iteration 302, loss = 0.17690398\n",
      "Iteration 303, loss = 0.17081745\n",
      "Iteration 304, loss = 0.17199319\n",
      "Iteration 305, loss = 0.16821079\n",
      "Iteration 306, loss = 0.17135884\n",
      "Iteration 307, loss = 0.16788214\n",
      "Iteration 308, loss = 0.16846060\n",
      "Iteration 309, loss = 0.16671436\n",
      "Iteration 310, loss = 0.16905085\n",
      "Iteration 311, loss = 0.16645762\n",
      "Iteration 312, loss = 0.16412919\n",
      "Iteration 313, loss = 0.16358299\n",
      "Iteration 314, loss = 0.16652571\n",
      "Iteration 315, loss = 0.16509936\n",
      "Iteration 316, loss = 0.16394335\n",
      "Iteration 317, loss = 0.16326375\n",
      "Iteration 318, loss = 0.16211922\n",
      "Iteration 319, loss = 0.16195504\n",
      "Iteration 320, loss = 0.15976089\n",
      "Iteration 321, loss = 0.15945461\n",
      "Iteration 322, loss = 0.15946337\n",
      "Iteration 323, loss = 0.16034437\n",
      "Iteration 324, loss = 0.15993892\n",
      "Iteration 325, loss = 0.15908940\n",
      "Iteration 326, loss = 0.15798252\n",
      "Iteration 327, loss = 0.15694018\n",
      "Iteration 328, loss = 0.15787367\n",
      "Iteration 329, loss = 0.15865401\n",
      "Iteration 330, loss = 0.15672578\n",
      "Iteration 331, loss = 0.15746197\n",
      "Iteration 332, loss = 0.15467722\n",
      "Iteration 333, loss = 0.15459266\n",
      "Iteration 334, loss = 0.15285493\n",
      "Iteration 335, loss = 0.15460742\n",
      "Iteration 336, loss = 0.15262342\n",
      "Iteration 337, loss = 0.15300159\n",
      "Iteration 338, loss = 0.15222275\n",
      "Iteration 339, loss = 0.15177612\n",
      "Iteration 340, loss = 0.15166899\n",
      "Iteration 341, loss = 0.15220680\n",
      "Iteration 342, loss = 0.15122138\n",
      "Iteration 343, loss = 0.15036279\n",
      "Iteration 344, loss = 0.14880309\n",
      "Iteration 345, loss = 0.14822655\n",
      "Iteration 346, loss = 0.14883266\n",
      "Iteration 347, loss = 0.14976061\n",
      "Iteration 348, loss = 0.14871455\n",
      "Iteration 349, loss = 0.14748590\n",
      "Iteration 350, loss = 0.14525660\n",
      "Iteration 351, loss = 0.14596597\n",
      "Iteration 352, loss = 0.14708728\n",
      "Iteration 353, loss = 0.14639609\n",
      "Iteration 354, loss = 0.14513277\n",
      "Iteration 355, loss = 0.14408852\n",
      "Iteration 356, loss = 0.14390682\n",
      "Iteration 357, loss = 0.14564873\n",
      "Iteration 358, loss = 0.14266952\n",
      "Iteration 359, loss = 0.14176186\n",
      "Iteration 360, loss = 0.14427910\n",
      "Iteration 361, loss = 0.14240014\n",
      "Iteration 362, loss = 0.13912459\n",
      "Iteration 363, loss = 0.14243211\n",
      "Iteration 364, loss = 0.13999106\n",
      "Iteration 365, loss = 0.13990333\n",
      "Iteration 366, loss = 0.14032181\n",
      "Iteration 367, loss = 0.13784099\n",
      "Iteration 368, loss = 0.13769443\n",
      "Iteration 369, loss = 0.13676131\n",
      "Iteration 370, loss = 0.13702405\n",
      "Iteration 371, loss = 0.13784601\n",
      "Iteration 372, loss = 0.13517679\n",
      "Iteration 373, loss = 0.13636714\n",
      "Iteration 374, loss = 0.13383763\n",
      "Iteration 375, loss = 0.13588296\n",
      "Iteration 376, loss = 0.13310625\n",
      "Iteration 377, loss = 0.13351035\n",
      "Iteration 378, loss = 0.13286300\n",
      "Iteration 379, loss = 0.13417105\n",
      "Iteration 380, loss = 0.13406830\n",
      "Iteration 381, loss = 0.13501853\n",
      "Iteration 382, loss = 0.13104406\n",
      "Iteration 383, loss = 0.13069373\n",
      "Iteration 384, loss = 0.12985975\n",
      "Iteration 385, loss = 0.13117888\n",
      "Iteration 386, loss = 0.13628571\n",
      "Iteration 387, loss = 0.13354300\n",
      "Iteration 388, loss = 0.12770463\n",
      "Iteration 389, loss = 0.12724770\n",
      "Iteration 390, loss = 0.12728489\n",
      "Iteration 391, loss = 0.12865870\n",
      "Iteration 392, loss = 0.13046345\n",
      "Iteration 393, loss = 0.12795763\n",
      "Iteration 394, loss = 0.12706721\n",
      "Iteration 395, loss = 0.12677493\n",
      "Iteration 396, loss = 0.12479464\n",
      "Iteration 397, loss = 0.12639051\n",
      "Iteration 398, loss = 0.12457871\n",
      "Iteration 399, loss = 0.12350021\n",
      "Iteration 400, loss = 0.12450273\n",
      "Iteration 401, loss = 0.12631558\n",
      "Iteration 402, loss = 0.12183069\n",
      "Iteration 403, loss = 0.12339116\n",
      "Iteration 404, loss = 0.12204542\n",
      "Iteration 405, loss = 0.12085798\n",
      "Iteration 406, loss = 0.12190624\n",
      "Iteration 407, loss = 0.12244793\n",
      "Iteration 408, loss = 0.11931747\n",
      "Iteration 409, loss = 0.11846934\n",
      "Iteration 410, loss = 0.12008745\n",
      "Iteration 411, loss = 0.11859157\n",
      "Iteration 412, loss = 0.11820271\n",
      "Iteration 413, loss = 0.11924583\n",
      "Iteration 414, loss = 0.11817845\n",
      "Iteration 415, loss = 0.11674002\n",
      "Iteration 416, loss = 0.11854843\n",
      "Iteration 417, loss = 0.12131707\n",
      "Iteration 418, loss = 0.11679665\n",
      "Iteration 419, loss = 0.11596439\n",
      "Iteration 420, loss = 0.11327703\n",
      "Iteration 421, loss = 0.11612295\n",
      "Iteration 422, loss = 0.11417136\n",
      "Iteration 423, loss = 0.11415308\n",
      "Iteration 424, loss = 0.11274449\n",
      "Iteration 425, loss = 0.11542193\n",
      "Iteration 426, loss = 0.11202992\n",
      "Iteration 427, loss = 0.11294438\n",
      "Iteration 428, loss = 0.11138177\n",
      "Iteration 429, loss = 0.11227081\n",
      "Iteration 430, loss = 0.10969133\n",
      "Iteration 431, loss = 0.10969053\n",
      "Iteration 432, loss = 0.11004212\n",
      "Iteration 433, loss = 0.10873046\n",
      "Iteration 434, loss = 0.10897006\n",
      "Iteration 435, loss = 0.10805549\n",
      "Iteration 436, loss = 0.10715444\n",
      "Iteration 437, loss = 0.10685652\n",
      "Iteration 438, loss = 0.10629658\n",
      "Iteration 439, loss = 0.10571887\n",
      "Iteration 440, loss = 0.10638387\n",
      "Iteration 441, loss = 0.10479746\n",
      "Iteration 442, loss = 0.10739862\n",
      "Iteration 443, loss = 0.10621992\n",
      "Iteration 444, loss = 0.10474242\n",
      "Iteration 445, loss = 0.10465864\n",
      "Iteration 446, loss = 0.10303667\n",
      "Iteration 447, loss = 0.10260564\n",
      "Iteration 448, loss = 0.10221362\n",
      "Iteration 449, loss = 0.10172005\n",
      "Iteration 450, loss = 0.10180951\n",
      "Iteration 451, loss = 0.10250453\n",
      "Iteration 452, loss = 0.10105842\n",
      "Iteration 453, loss = 0.10049577\n",
      "Iteration 454, loss = 0.10124160\n",
      "Iteration 455, loss = 0.10444494\n",
      "Iteration 456, loss = 0.10399005\n",
      "Iteration 457, loss = 0.10675605\n",
      "Iteration 458, loss = 0.09896101\n",
      "Iteration 459, loss = 0.10140609\n",
      "Iteration 460, loss = 0.10017375\n",
      "Iteration 461, loss = 0.10143471\n",
      "Iteration 462, loss = 0.09818057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 463, loss = 0.09786392\n",
      "Iteration 464, loss = 0.09724015\n",
      "Iteration 465, loss = 0.09626076\n",
      "Iteration 466, loss = 0.09832707\n",
      "Iteration 467, loss = 0.09522314\n",
      "Iteration 468, loss = 0.09473097\n",
      "Iteration 469, loss = 0.09616297\n",
      "Iteration 470, loss = 0.09565251\n",
      "Iteration 471, loss = 0.09447730\n",
      "Iteration 472, loss = 0.09346590\n",
      "Iteration 473, loss = 0.09262342\n",
      "Iteration 474, loss = 0.09264094\n",
      "Iteration 475, loss = 0.09332547\n",
      "Iteration 476, loss = 0.09159525\n",
      "Iteration 477, loss = 0.09255456\n",
      "Iteration 478, loss = 0.09150028\n",
      "Iteration 479, loss = 0.09070196\n",
      "Iteration 480, loss = 0.09139451\n",
      "Iteration 481, loss = 0.09192935\n",
      "Iteration 482, loss = 0.09053545\n",
      "Iteration 483, loss = 0.08972259\n",
      "Iteration 484, loss = 0.09082184\n",
      "Iteration 485, loss = 0.09090380\n",
      "Iteration 486, loss = 0.09202651\n",
      "Iteration 487, loss = 0.08982483\n",
      "Iteration 488, loss = 0.08856501\n",
      "Iteration 489, loss = 0.08828746\n",
      "Iteration 490, loss = 0.08788208\n",
      "Iteration 491, loss = 0.08716720\n",
      "Iteration 492, loss = 0.08709282\n",
      "Iteration 493, loss = 0.08653909\n",
      "Iteration 494, loss = 0.08859553\n",
      "Iteration 495, loss = 0.09050792\n",
      "Iteration 496, loss = 0.08766213\n",
      "Iteration 497, loss = 0.08802843\n",
      "Iteration 498, loss = 0.08439545\n",
      "Iteration 499, loss = 0.08489404\n",
      "Iteration 500, loss = 0.08520213\n",
      "Iteration 501, loss = 0.08396131\n",
      "Iteration 502, loss = 0.08376980\n",
      "Iteration 503, loss = 0.08335231\n",
      "Iteration 504, loss = 0.08364770\n",
      "Iteration 505, loss = 0.08287676\n",
      "Iteration 506, loss = 0.08331817\n",
      "Iteration 507, loss = 0.08231680\n",
      "Iteration 508, loss = 0.08343462\n",
      "Iteration 509, loss = 0.08166663\n",
      "Iteration 510, loss = 0.08161231\n",
      "Iteration 511, loss = 0.08038809\n",
      "Iteration 512, loss = 0.08047496\n",
      "Iteration 513, loss = 0.08065779\n",
      "Iteration 514, loss = 0.07936330\n",
      "Iteration 515, loss = 0.07935743\n",
      "Iteration 516, loss = 0.07957691\n",
      "Iteration 517, loss = 0.07963951\n",
      "Iteration 518, loss = 0.07895543\n",
      "Iteration 519, loss = 0.08010464\n",
      "Iteration 520, loss = 0.08043825\n",
      "Iteration 521, loss = 0.07807233\n",
      "Iteration 522, loss = 0.07792573\n",
      "Iteration 523, loss = 0.07694060\n",
      "Iteration 524, loss = 0.07755230\n",
      "Iteration 525, loss = 0.07662446\n",
      "Iteration 526, loss = 0.07658701\n",
      "Iteration 527, loss = 0.07573684\n",
      "Iteration 528, loss = 0.07646764\n",
      "Iteration 529, loss = 0.07722871\n",
      "Iteration 530, loss = 0.07899647\n",
      "Iteration 531, loss = 0.07710461\n",
      "Iteration 532, loss = 0.07534941\n",
      "Iteration 533, loss = 0.07643450\n",
      "Iteration 534, loss = 0.07559306\n",
      "Iteration 535, loss = 0.07585932\n",
      "Iteration 536, loss = 0.07496817\n",
      "Iteration 537, loss = 0.07510417\n",
      "Iteration 538, loss = 0.07345041\n",
      "Iteration 539, loss = 0.07503225\n",
      "Iteration 540, loss = 0.07378440\n",
      "Iteration 541, loss = 0.07298354\n",
      "Iteration 542, loss = 0.07194839\n",
      "Iteration 543, loss = 0.07171240\n",
      "Iteration 544, loss = 0.07124713\n",
      "Iteration 545, loss = 0.07230235\n",
      "Iteration 546, loss = 0.07113843\n",
      "Iteration 547, loss = 0.07093714\n",
      "Iteration 548, loss = 0.07026790\n",
      "Iteration 549, loss = 0.07030488\n",
      "Iteration 550, loss = 0.07220763\n",
      "Iteration 551, loss = 0.07023145\n",
      "Iteration 552, loss = 0.06999141\n",
      "Iteration 553, loss = 0.07261207\n",
      "Iteration 554, loss = 0.07119077\n",
      "Iteration 555, loss = 0.07119389\n",
      "Iteration 556, loss = 0.06851193\n",
      "Iteration 557, loss = 0.06946662\n",
      "Iteration 558, loss = 0.07048752\n",
      "Iteration 559, loss = 0.06886689\n",
      "Iteration 560, loss = 0.06712109\n",
      "Iteration 561, loss = 0.06785232\n",
      "Iteration 562, loss = 0.07062375\n",
      "Iteration 563, loss = 0.07106950\n",
      "Iteration 564, loss = 0.06900773\n",
      "Iteration 565, loss = 0.06721670\n",
      "Iteration 566, loss = 0.06715337\n",
      "Iteration 567, loss = 0.06604543\n",
      "Iteration 568, loss = 0.06650309\n",
      "Iteration 569, loss = 0.06527222\n",
      "Iteration 570, loss = 0.06552220\n",
      "Iteration 571, loss = 0.06468515\n",
      "Iteration 572, loss = 0.06470261\n",
      "Iteration 573, loss = 0.06485779\n",
      "Iteration 574, loss = 0.06582535\n",
      "Iteration 575, loss = 0.06515881\n",
      "Iteration 576, loss = 0.06377324\n",
      "Iteration 577, loss = 0.06385499\n",
      "Iteration 578, loss = 0.06367016\n",
      "Iteration 579, loss = 0.06311989\n",
      "Iteration 580, loss = 0.06424716\n",
      "Iteration 581, loss = 0.06426014\n",
      "Iteration 582, loss = 0.06348801\n",
      "Iteration 583, loss = 0.06305810\n",
      "Iteration 584, loss = 0.06284111\n",
      "Iteration 585, loss = 0.06309883\n",
      "Iteration 586, loss = 0.06183307\n",
      "Iteration 587, loss = 0.06098254\n",
      "Iteration 588, loss = 0.06161034\n",
      "Iteration 589, loss = 0.06157240\n",
      "Iteration 590, loss = 0.06069687\n",
      "Iteration 591, loss = 0.06054391\n",
      "Iteration 592, loss = 0.06025651\n",
      "Iteration 593, loss = 0.06120809\n",
      "Iteration 594, loss = 0.06282507\n",
      "Iteration 595, loss = 0.05981857\n",
      "Iteration 596, loss = 0.06034913\n",
      "Iteration 597, loss = 0.06039577\n",
      "Iteration 598, loss = 0.06527028\n",
      "Iteration 599, loss = 0.08029623\n",
      "Iteration 600, loss = 0.07100461\n",
      "Iteration 601, loss = 0.06694179\n",
      "Iteration 602, loss = 0.06088809\n",
      "Iteration 603, loss = 0.06066952\n",
      "Iteration 604, loss = 0.05860965\n",
      "Iteration 605, loss = 0.05856603\n",
      "Iteration 606, loss = 0.05789945\n",
      "Iteration 607, loss = 0.05829428\n",
      "Iteration 608, loss = 0.05843327\n",
      "Iteration 609, loss = 0.05962542\n",
      "Iteration 610, loss = 0.05786906\n",
      "Iteration 611, loss = 0.05912631\n",
      "Iteration 612, loss = 0.05652972\n",
      "Iteration 613, loss = 0.05586903\n",
      "Iteration 614, loss = 0.05684297\n",
      "Iteration 615, loss = 0.05583419\n",
      "Iteration 616, loss = 0.05584865\n",
      "Iteration 617, loss = 0.05544079\n",
      "Iteration 618, loss = 0.05526714\n",
      "Iteration 619, loss = 0.05588792\n",
      "Iteration 620, loss = 0.05500158\n",
      "Iteration 621, loss = 0.05493325\n",
      "Iteration 622, loss = 0.05443286\n",
      "Iteration 623, loss = 0.05812285\n",
      "Iteration 624, loss = 0.05491419\n",
      "Iteration 625, loss = 0.05388209\n",
      "Iteration 626, loss = 0.05395392\n",
      "Iteration 627, loss = 0.05419525\n",
      "Iteration 628, loss = 0.05398068\n",
      "Iteration 629, loss = 0.05453487\n",
      "Iteration 630, loss = 0.05394960\n",
      "Iteration 631, loss = 0.05411709\n",
      "Iteration 632, loss = 0.05495976\n",
      "Iteration 633, loss = 0.05405762\n",
      "Iteration 634, loss = 0.05236708\n",
      "Iteration 635, loss = 0.05245618\n",
      "Iteration 636, loss = 0.05247378\n",
      "Iteration 637, loss = 0.05309149\n",
      "Iteration 638, loss = 0.05143437\n",
      "Iteration 639, loss = 0.05169895\n",
      "Iteration 640, loss = 0.05130144\n",
      "Iteration 641, loss = 0.05155369\n",
      "Iteration 642, loss = 0.05115487\n",
      "Iteration 643, loss = 0.05141200\n",
      "Iteration 644, loss = 0.05324132\n",
      "Iteration 645, loss = 0.05117713\n",
      "Iteration 646, loss = 0.05089432\n",
      "Iteration 647, loss = 0.05002086\n",
      "Iteration 648, loss = 0.04995360\n",
      "Iteration 649, loss = 0.05387270\n",
      "Iteration 650, loss = 0.04993349\n",
      "Iteration 651, loss = 0.04992694\n",
      "Iteration 652, loss = 0.04972554\n",
      "Iteration 653, loss = 0.04963926\n",
      "Iteration 654, loss = 0.04914899\n",
      "Iteration 655, loss = 0.04891065\n",
      "Iteration 656, loss = 0.05161243\n",
      "Iteration 657, loss = 0.04904795\n",
      "Iteration 658, loss = 0.04911284\n",
      "Iteration 659, loss = 0.04833193\n",
      "Iteration 660, loss = 0.04840546\n",
      "Iteration 661, loss = 0.04834519\n",
      "Iteration 662, loss = 0.04792122\n",
      "Iteration 663, loss = 0.04882308\n",
      "Iteration 664, loss = 0.04751558\n",
      "Iteration 665, loss = 0.04755804\n",
      "Iteration 666, loss = 0.04835180\n",
      "Iteration 667, loss = 0.04863155\n",
      "Iteration 668, loss = 0.04734416\n",
      "Iteration 669, loss = 0.04680555\n",
      "Iteration 670, loss = 0.04669845\n",
      "Iteration 671, loss = 0.04730845\n",
      "Iteration 672, loss = 0.04777213\n",
      "Iteration 673, loss = 0.04701254\n",
      "Iteration 674, loss = 0.04599999\n",
      "Iteration 675, loss = 0.04612872\n",
      "Iteration 676, loss = 0.04565088\n",
      "Iteration 677, loss = 0.04600139\n",
      "Iteration 678, loss = 0.04495217\n",
      "Iteration 679, loss = 0.04581222\n",
      "Iteration 680, loss = 0.04517658\n",
      "Iteration 681, loss = 0.04638969\n",
      "Iteration 682, loss = 0.04545981\n",
      "Iteration 683, loss = 0.04543573\n",
      "Iteration 684, loss = 0.04531553\n",
      "Iteration 685, loss = 0.04483246\n",
      "Iteration 686, loss = 0.04468260\n",
      "Iteration 687, loss = 0.04485960\n",
      "Iteration 688, loss = 0.04493213\n",
      "Iteration 689, loss = 0.04429226\n",
      "Iteration 690, loss = 0.04430510\n",
      "Iteration 691, loss = 0.04426707\n",
      "Iteration 692, loss = 0.04321572\n",
      "Iteration 693, loss = 0.04370758\n",
      "Iteration 694, loss = 0.04314123\n",
      "Iteration 695, loss = 0.04446088\n",
      "Iteration 696, loss = 0.04288980\n",
      "Iteration 697, loss = 0.04292210\n",
      "Iteration 698, loss = 0.04262545\n",
      "Iteration 699, loss = 0.04245595\n",
      "Iteration 700, loss = 0.04216902\n",
      "Iteration 701, loss = 0.04180767\n",
      "Iteration 702, loss = 0.04329853\n",
      "Iteration 703, loss = 0.04170262\n",
      "Iteration 704, loss = 0.04210178\n",
      "Iteration 705, loss = 0.04189747\n",
      "Iteration 706, loss = 0.04225745\n",
      "Iteration 707, loss = 0.04282775\n",
      "Iteration 708, loss = 0.04177881\n",
      "Iteration 709, loss = 0.04145928\n",
      "Iteration 710, loss = 0.04116296\n",
      "Iteration 711, loss = 0.04142336\n",
      "Iteration 712, loss = 0.04099790\n",
      "Iteration 713, loss = 0.04092022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 714, loss = 0.04204265\n",
      "Iteration 715, loss = 0.04050883\n",
      "Iteration 716, loss = 0.04099228\n",
      "Iteration 717, loss = 0.04098573\n",
      "Iteration 718, loss = 0.04138073\n",
      "Iteration 719, loss = 0.04322020\n",
      "Iteration 720, loss = 0.04028670\n",
      "Iteration 721, loss = 0.03976400\n",
      "Iteration 722, loss = 0.03994678\n",
      "Iteration 723, loss = 0.03955615\n",
      "Iteration 724, loss = 0.04002399\n",
      "Iteration 725, loss = 0.03909686\n",
      "Iteration 726, loss = 0.03909327\n",
      "Iteration 727, loss = 0.03937981\n",
      "Iteration 728, loss = 0.03848227\n",
      "Iteration 729, loss = 0.03982260\n",
      "Iteration 730, loss = 0.03893483\n",
      "Iteration 731, loss = 0.03838915\n",
      "Iteration 732, loss = 0.03859456\n",
      "Iteration 733, loss = 0.03886781\n",
      "Iteration 734, loss = 0.03865055\n",
      "Iteration 735, loss = 0.03902462\n",
      "Iteration 736, loss = 0.03824213\n",
      "Iteration 737, loss = 0.03871913\n",
      "Iteration 738, loss = 0.03818067\n",
      "Iteration 739, loss = 0.03864222\n",
      "Iteration 740, loss = 0.03853724\n",
      "Iteration 741, loss = 0.03721197\n",
      "Iteration 742, loss = 0.03701220\n",
      "Iteration 743, loss = 0.03687533\n",
      "Iteration 744, loss = 0.03696429\n",
      "Iteration 745, loss = 0.03715520\n",
      "Iteration 746, loss = 0.03748736\n",
      "Iteration 747, loss = 0.03727300\n",
      "Iteration 748, loss = 0.03881599\n",
      "Iteration 749, loss = 0.04204652\n",
      "Iteration 750, loss = 0.04282886\n",
      "Iteration 751, loss = 0.04255358\n",
      "Iteration 752, loss = 0.03915485\n",
      "Iteration 753, loss = 0.03909667\n",
      "Iteration 754, loss = 0.03887264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 21 of 48...\n",
      "Iteration 1, loss = 0.68266287\n",
      "Iteration 2, loss = 0.64410285\n",
      "Iteration 3, loss = 0.61404676\n",
      "Iteration 4, loss = 0.58822545\n",
      "Iteration 5, loss = 0.56566147\n",
      "Iteration 6, loss = 0.54334758\n",
      "Iteration 7, loss = 0.52341514\n",
      "Iteration 8, loss = 0.50728708\n",
      "Iteration 9, loss = 0.49124571\n",
      "Iteration 10, loss = 0.47911662\n",
      "Iteration 11, loss = 0.46642120\n",
      "Iteration 12, loss = 0.45717754\n",
      "Iteration 13, loss = 0.44752258\n",
      "Iteration 14, loss = 0.44013783\n",
      "Iteration 15, loss = 0.43354518\n",
      "Iteration 16, loss = 0.42685311\n",
      "Iteration 17, loss = 0.42142732\n",
      "Iteration 18, loss = 0.41511008\n",
      "Iteration 19, loss = 0.41053029\n",
      "Iteration 20, loss = 0.40569645\n",
      "Iteration 21, loss = 0.40099698\n",
      "Iteration 22, loss = 0.39760185\n",
      "Iteration 23, loss = 0.39501184\n",
      "Iteration 24, loss = 0.39149713\n",
      "Iteration 25, loss = 0.39165764\n",
      "Iteration 26, loss = 0.38480394\n",
      "Iteration 27, loss = 0.38228647\n",
      "Iteration 28, loss = 0.37975362\n",
      "Iteration 29, loss = 0.37903019\n",
      "Iteration 30, loss = 0.37423371\n",
      "Iteration 31, loss = 0.37275509\n",
      "Iteration 32, loss = 0.37029585\n",
      "Iteration 33, loss = 0.36856461\n",
      "Iteration 34, loss = 0.36716791\n",
      "Iteration 35, loss = 0.36381245\n",
      "Iteration 36, loss = 0.36384696\n",
      "Iteration 37, loss = 0.36205694\n",
      "Iteration 38, loss = 0.35842672\n",
      "Iteration 39, loss = 0.35648125\n",
      "Iteration 40, loss = 0.35652222\n",
      "Iteration 41, loss = 0.35671498\n",
      "Iteration 42, loss = 0.35250975\n",
      "Iteration 43, loss = 0.34995008\n",
      "Iteration 44, loss = 0.34719074\n",
      "Iteration 45, loss = 0.34693114\n",
      "Iteration 46, loss = 0.35130911\n",
      "Iteration 47, loss = 0.34532286\n",
      "Iteration 48, loss = 0.34410205\n",
      "Iteration 49, loss = 0.34230875\n",
      "Iteration 50, loss = 0.34036966\n",
      "Iteration 51, loss = 0.33967804\n",
      "Iteration 52, loss = 0.33811268\n",
      "Iteration 53, loss = 0.33949019\n",
      "Iteration 54, loss = 0.33575686\n",
      "Iteration 55, loss = 0.33350085\n",
      "Iteration 56, loss = 0.33289366\n",
      "Iteration 57, loss = 0.33285949\n",
      "Iteration 58, loss = 0.32969693\n",
      "Iteration 59, loss = 0.32844430\n",
      "Iteration 60, loss = 0.32842872\n",
      "Iteration 61, loss = 0.32677248\n",
      "Iteration 62, loss = 0.32594311\n",
      "Iteration 63, loss = 0.32580651\n",
      "Iteration 64, loss = 0.32419302\n",
      "Iteration 65, loss = 0.32766929\n",
      "Iteration 66, loss = 0.32952728\n",
      "Iteration 67, loss = 0.32522179\n",
      "Iteration 68, loss = 0.32301570\n",
      "Iteration 69, loss = 0.31951591\n",
      "Iteration 70, loss = 0.31908582\n",
      "Iteration 71, loss = 0.31878555\n",
      "Iteration 72, loss = 0.31675743\n",
      "Iteration 73, loss = 0.31707838\n",
      "Iteration 74, loss = 0.31542361\n",
      "Iteration 75, loss = 0.31699636\n",
      "Iteration 76, loss = 0.31594663\n",
      "Iteration 77, loss = 0.31546893\n",
      "Iteration 78, loss = 0.31155027\n",
      "Iteration 79, loss = 0.31115423\n",
      "Iteration 80, loss = 0.31274808\n",
      "Iteration 81, loss = 0.31210081\n",
      "Iteration 82, loss = 0.30958318\n",
      "Iteration 83, loss = 0.30901539\n",
      "Iteration 84, loss = 0.30745178\n",
      "Iteration 85, loss = 0.30852291\n",
      "Iteration 86, loss = 0.30569732\n",
      "Iteration 87, loss = 0.31068762\n",
      "Iteration 88, loss = 0.30664099\n",
      "Iteration 89, loss = 0.30954541\n",
      "Iteration 90, loss = 0.31115822\n",
      "Iteration 91, loss = 0.30526922\n",
      "Iteration 92, loss = 0.30392400\n",
      "Iteration 93, loss = 0.30298355\n",
      "Iteration 94, loss = 0.30158618\n",
      "Iteration 95, loss = 0.30402382\n",
      "Iteration 96, loss = 0.30364992\n",
      "Iteration 97, loss = 0.30020581\n",
      "Iteration 98, loss = 0.30489708\n",
      "Iteration 99, loss = 0.30182430\n",
      "Iteration 100, loss = 0.29884172\n",
      "Iteration 101, loss = 0.29870828\n",
      "Iteration 102, loss = 0.30082676\n",
      "Iteration 103, loss = 0.29704937\n",
      "Iteration 104, loss = 0.29583043\n",
      "Iteration 105, loss = 0.29947721\n",
      "Iteration 106, loss = 0.29582225\n",
      "Iteration 107, loss = 0.29462356\n",
      "Iteration 108, loss = 0.29363284\n",
      "Iteration 109, loss = 0.29392715\n",
      "Iteration 110, loss = 0.29377778\n",
      "Iteration 111, loss = 0.29170786\n",
      "Iteration 112, loss = 0.29251647\n",
      "Iteration 113, loss = 0.29265763\n",
      "Iteration 114, loss = 0.29320449\n",
      "Iteration 115, loss = 0.29338965\n",
      "Iteration 116, loss = 0.29356530\n",
      "Iteration 117, loss = 0.29053039\n",
      "Iteration 118, loss = 0.28936531\n",
      "Iteration 119, loss = 0.29500602\n",
      "Iteration 120, loss = 0.29207034\n",
      "Iteration 121, loss = 0.28857654\n",
      "Iteration 122, loss = 0.29561048\n",
      "Iteration 123, loss = 0.29525572\n",
      "Iteration 124, loss = 0.29009912\n",
      "Iteration 125, loss = 0.28828285\n",
      "Iteration 126, loss = 0.28755882\n",
      "Iteration 127, loss = 0.28592853\n",
      "Iteration 128, loss = 0.28526881\n",
      "Iteration 129, loss = 0.28569159\n",
      "Iteration 130, loss = 0.28695484\n",
      "Iteration 131, loss = 0.28845523\n",
      "Iteration 132, loss = 0.28901312\n",
      "Iteration 133, loss = 0.28222541\n",
      "Iteration 134, loss = 0.28221088\n",
      "Iteration 135, loss = 0.28338259\n",
      "Iteration 136, loss = 0.28081499\n",
      "Iteration 137, loss = 0.28097028\n",
      "Iteration 138, loss = 0.28122407\n",
      "Iteration 139, loss = 0.28175709\n",
      "Iteration 140, loss = 0.28226910\n",
      "Iteration 141, loss = 0.27810562\n",
      "Iteration 142, loss = 0.28095286\n",
      "Iteration 143, loss = 0.27916130\n",
      "Iteration 144, loss = 0.27785192\n",
      "Iteration 145, loss = 0.28048128\n",
      "Iteration 146, loss = 0.28074869\n",
      "Iteration 147, loss = 0.27968461\n",
      "Iteration 148, loss = 0.27774940\n",
      "Iteration 149, loss = 0.27535931\n",
      "Iteration 150, loss = 0.27831972\n",
      "Iteration 151, loss = 0.28065690\n",
      "Iteration 152, loss = 0.28492243\n",
      "Iteration 153, loss = 0.27775664\n",
      "Iteration 154, loss = 0.27590937\n",
      "Iteration 155, loss = 0.27406090\n",
      "Iteration 156, loss = 0.27414872\n",
      "Iteration 157, loss = 0.27438231\n",
      "Iteration 158, loss = 0.27383269\n",
      "Iteration 159, loss = 0.27566356\n",
      "Iteration 160, loss = 0.27885275\n",
      "Iteration 161, loss = 0.27980220\n",
      "Iteration 162, loss = 0.27380066\n",
      "Iteration 163, loss = 0.27353435\n",
      "Iteration 164, loss = 0.27196934\n",
      "Iteration 165, loss = 0.27620881\n",
      "Iteration 166, loss = 0.27223134\n",
      "Iteration 167, loss = 0.27081714\n",
      "Iteration 168, loss = 0.27395973\n",
      "Iteration 169, loss = 0.27043626\n",
      "Iteration 170, loss = 0.26995195\n",
      "Iteration 171, loss = 0.27043957\n",
      "Iteration 172, loss = 0.27249902\n",
      "Iteration 173, loss = 0.26900990\n",
      "Iteration 174, loss = 0.27014422\n",
      "Iteration 175, loss = 0.27250633\n",
      "Iteration 176, loss = 0.27106139\n",
      "Iteration 177, loss = 0.27049484\n",
      "Iteration 178, loss = 0.26732400\n",
      "Iteration 179, loss = 0.27128806\n",
      "Iteration 180, loss = 0.26669016\n",
      "Iteration 181, loss = 0.26669220\n",
      "Iteration 182, loss = 0.26695926\n",
      "Iteration 183, loss = 0.26643602\n",
      "Iteration 184, loss = 0.26422773\n",
      "Iteration 185, loss = 0.26509025\n",
      "Iteration 186, loss = 0.26360032\n",
      "Iteration 187, loss = 0.26443264\n",
      "Iteration 188, loss = 0.26444128\n",
      "Iteration 189, loss = 0.26246955\n",
      "Iteration 190, loss = 0.26129742\n",
      "Iteration 191, loss = 0.26511301\n",
      "Iteration 192, loss = 0.26119485\n",
      "Iteration 193, loss = 0.26806024\n",
      "Iteration 194, loss = 0.26340453\n",
      "Iteration 195, loss = 0.26191328\n",
      "Iteration 196, loss = 0.26421074\n",
      "Iteration 197, loss = 0.25955701\n",
      "Iteration 198, loss = 0.25973706\n",
      "Iteration 199, loss = 0.26112070\n",
      "Iteration 200, loss = 0.26217086\n",
      "Iteration 201, loss = 0.25843158\n",
      "Iteration 202, loss = 0.25950893\n",
      "Iteration 203, loss = 0.25967976\n",
      "Iteration 204, loss = 0.25767358\n",
      "Iteration 205, loss = 0.25931998\n",
      "Iteration 206, loss = 0.25812105\n",
      "Iteration 207, loss = 0.25792251\n",
      "Iteration 208, loss = 0.25770247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 209, loss = 0.25571405\n",
      "Iteration 210, loss = 0.25570394\n",
      "Iteration 211, loss = 0.25440466\n",
      "Iteration 212, loss = 0.25424826\n",
      "Iteration 213, loss = 0.25582774\n",
      "Iteration 214, loss = 0.25815148\n",
      "Iteration 215, loss = 0.25591612\n",
      "Iteration 216, loss = 0.25508352\n",
      "Iteration 217, loss = 0.25631211\n",
      "Iteration 218, loss = 0.25418507\n",
      "Iteration 219, loss = 0.25241992\n",
      "Iteration 220, loss = 0.25604680\n",
      "Iteration 221, loss = 0.25646286\n",
      "Iteration 222, loss = 0.25181682\n",
      "Iteration 223, loss = 0.25087241\n",
      "Iteration 224, loss = 0.25145208\n",
      "Iteration 225, loss = 0.25126414\n",
      "Iteration 226, loss = 0.25113309\n",
      "Iteration 227, loss = 0.24975356\n",
      "Iteration 228, loss = 0.25197677\n",
      "Iteration 229, loss = 0.24943710\n",
      "Iteration 230, loss = 0.24764462\n",
      "Iteration 231, loss = 0.24868335\n",
      "Iteration 232, loss = 0.24950651\n",
      "Iteration 233, loss = 0.24996623\n",
      "Iteration 234, loss = 0.25632057\n",
      "Iteration 235, loss = 0.25446818\n",
      "Iteration 236, loss = 0.24642565\n",
      "Iteration 237, loss = 0.24693752\n",
      "Iteration 238, loss = 0.24571654\n",
      "Iteration 239, loss = 0.24728406\n",
      "Iteration 240, loss = 0.24464388\n",
      "Iteration 241, loss = 0.24251556\n",
      "Iteration 242, loss = 0.24340964\n",
      "Iteration 243, loss = 0.24297030\n",
      "Iteration 244, loss = 0.24107163\n",
      "Iteration 245, loss = 0.24312964\n",
      "Iteration 246, loss = 0.24709503\n",
      "Iteration 247, loss = 0.24319763\n",
      "Iteration 248, loss = 0.24514473\n",
      "Iteration 249, loss = 0.24461819\n",
      "Iteration 250, loss = 0.24728514\n",
      "Iteration 251, loss = 0.24500812\n",
      "Iteration 252, loss = 0.24416851\n",
      "Iteration 253, loss = 0.24121329\n",
      "Iteration 254, loss = 0.24170975\n",
      "Iteration 255, loss = 0.24087969\n",
      "Iteration 256, loss = 0.24031409\n",
      "Iteration 257, loss = 0.23642700\n",
      "Iteration 258, loss = 0.23869238\n",
      "Iteration 259, loss = 0.23659960\n",
      "Iteration 260, loss = 0.24191648\n",
      "Iteration 261, loss = 0.24431987\n",
      "Iteration 262, loss = 0.23716795\n",
      "Iteration 263, loss = 0.23527284\n",
      "Iteration 264, loss = 0.23573850\n",
      "Iteration 265, loss = 0.23538125\n",
      "Iteration 266, loss = 0.23297332\n",
      "Iteration 267, loss = 0.23341492\n",
      "Iteration 268, loss = 0.23261988\n",
      "Iteration 269, loss = 0.23528864\n",
      "Iteration 270, loss = 0.23070511\n",
      "Iteration 271, loss = 0.23238884\n",
      "Iteration 272, loss = 0.23499558\n",
      "Iteration 273, loss = 0.23146467\n",
      "Iteration 274, loss = 0.23295175\n",
      "Iteration 275, loss = 0.22863253\n",
      "Iteration 276, loss = 0.22829725\n",
      "Iteration 277, loss = 0.23160099\n",
      "Iteration 278, loss = 0.22998172\n",
      "Iteration 279, loss = 0.22637432\n",
      "Iteration 280, loss = 0.22963841\n",
      "Iteration 281, loss = 0.22595730\n",
      "Iteration 282, loss = 0.22636176\n",
      "Iteration 283, loss = 0.22551396\n",
      "Iteration 284, loss = 0.22657512\n",
      "Iteration 285, loss = 0.22663927\n",
      "Iteration 286, loss = 0.22488279\n",
      "Iteration 287, loss = 0.22283844\n",
      "Iteration 288, loss = 0.22197965\n",
      "Iteration 289, loss = 0.22443859\n",
      "Iteration 290, loss = 0.22245380\n",
      "Iteration 291, loss = 0.22422591\n",
      "Iteration 292, loss = 0.22315267\n",
      "Iteration 293, loss = 0.22417662\n",
      "Iteration 294, loss = 0.22053580\n",
      "Iteration 295, loss = 0.22002916\n",
      "Iteration 296, loss = 0.22180897\n",
      "Iteration 297, loss = 0.21874806\n",
      "Iteration 298, loss = 0.21928582\n",
      "Iteration 299, loss = 0.22037442\n",
      "Iteration 300, loss = 0.21825002\n",
      "Iteration 301, loss = 0.22349903\n",
      "Iteration 302, loss = 0.21831056\n",
      "Iteration 303, loss = 0.21625729\n",
      "Iteration 304, loss = 0.21611188\n",
      "Iteration 305, loss = 0.21761333\n",
      "Iteration 306, loss = 0.21476768\n",
      "Iteration 307, loss = 0.21351729\n",
      "Iteration 308, loss = 0.21222030\n",
      "Iteration 309, loss = 0.21356312\n",
      "Iteration 310, loss = 0.21317113\n",
      "Iteration 311, loss = 0.21299944\n",
      "Iteration 312, loss = 0.21692023\n",
      "Iteration 313, loss = 0.21166919\n",
      "Iteration 314, loss = 0.21206849\n",
      "Iteration 315, loss = 0.21545612\n",
      "Iteration 316, loss = 0.20971689\n",
      "Iteration 317, loss = 0.21040995\n",
      "Iteration 318, loss = 0.21229724\n",
      "Iteration 319, loss = 0.21124626\n",
      "Iteration 320, loss = 0.20732498\n",
      "Iteration 321, loss = 0.20784727\n",
      "Iteration 322, loss = 0.20747322\n",
      "Iteration 323, loss = 0.21116546\n",
      "Iteration 324, loss = 0.20925947\n",
      "Iteration 325, loss = 0.20992603\n",
      "Iteration 326, loss = 0.21239425\n",
      "Iteration 327, loss = 0.20659646\n",
      "Iteration 328, loss = 0.20478737\n",
      "Iteration 329, loss = 0.20504305\n",
      "Iteration 330, loss = 0.20414803\n",
      "Iteration 331, loss = 0.20375533\n",
      "Iteration 332, loss = 0.20197925\n",
      "Iteration 333, loss = 0.20223790\n",
      "Iteration 334, loss = 0.20042578\n",
      "Iteration 335, loss = 0.20472906\n",
      "Iteration 336, loss = 0.19924853\n",
      "Iteration 337, loss = 0.20104678\n",
      "Iteration 338, loss = 0.20537146\n",
      "Iteration 339, loss = 0.20412435\n",
      "Iteration 340, loss = 0.20136198\n",
      "Iteration 341, loss = 0.19873415\n",
      "Iteration 342, loss = 0.19686048\n",
      "Iteration 343, loss = 0.20064715\n",
      "Iteration 344, loss = 0.20044327\n",
      "Iteration 345, loss = 0.19770135\n",
      "Iteration 346, loss = 0.19957161\n",
      "Iteration 347, loss = 0.19569265\n",
      "Iteration 348, loss = 0.19475199\n",
      "Iteration 349, loss = 0.19527557\n",
      "Iteration 350, loss = 0.19409700\n",
      "Iteration 351, loss = 0.19396337\n",
      "Iteration 352, loss = 0.19533221\n",
      "Iteration 353, loss = 0.19323364\n",
      "Iteration 354, loss = 0.19584386\n",
      "Iteration 355, loss = 0.19167108\n",
      "Iteration 356, loss = 0.19014784\n",
      "Iteration 357, loss = 0.20026054\n",
      "Iteration 358, loss = 0.19154953\n",
      "Iteration 359, loss = 0.18869032\n",
      "Iteration 360, loss = 0.19061755\n",
      "Iteration 361, loss = 0.18928959\n",
      "Iteration 362, loss = 0.18830524\n",
      "Iteration 363, loss = 0.18907487\n",
      "Iteration 364, loss = 0.18932237\n",
      "Iteration 365, loss = 0.19103108\n",
      "Iteration 366, loss = 0.19356310\n",
      "Iteration 367, loss = 0.18729412\n",
      "Iteration 368, loss = 0.18752718\n",
      "Iteration 369, loss = 0.18481563\n",
      "Iteration 370, loss = 0.18412126\n",
      "Iteration 371, loss = 0.18434451\n",
      "Iteration 372, loss = 0.18466989\n",
      "Iteration 373, loss = 0.18191722\n",
      "Iteration 374, loss = 0.18224987\n",
      "Iteration 375, loss = 0.18371156\n",
      "Iteration 376, loss = 0.18397305\n",
      "Iteration 377, loss = 0.18464310\n",
      "Iteration 378, loss = 0.18711361\n",
      "Iteration 379, loss = 0.18539629\n",
      "Iteration 380, loss = 0.18166767\n",
      "Iteration 381, loss = 0.18157330\n",
      "Iteration 382, loss = 0.18059733\n",
      "Iteration 383, loss = 0.17882572\n",
      "Iteration 384, loss = 0.17980833\n",
      "Iteration 385, loss = 0.18047468\n",
      "Iteration 386, loss = 0.18000122\n",
      "Iteration 387, loss = 0.18020480\n",
      "Iteration 388, loss = 0.17818626\n",
      "Iteration 389, loss = 0.17616878\n",
      "Iteration 390, loss = 0.17488954\n",
      "Iteration 391, loss = 0.17484982\n",
      "Iteration 392, loss = 0.17431014\n",
      "Iteration 393, loss = 0.17629959\n",
      "Iteration 394, loss = 0.18140603\n",
      "Iteration 395, loss = 0.18009119\n",
      "Iteration 396, loss = 0.17554108\n",
      "Iteration 397, loss = 0.17492188\n",
      "Iteration 398, loss = 0.17458850\n",
      "Iteration 399, loss = 0.17257673\n",
      "Iteration 400, loss = 0.17429584\n",
      "Iteration 401, loss = 0.17118991\n",
      "Iteration 402, loss = 0.17209825\n",
      "Iteration 403, loss = 0.17382341\n",
      "Iteration 404, loss = 0.17240577\n",
      "Iteration 405, loss = 0.16877329\n",
      "Iteration 406, loss = 0.17054281\n",
      "Iteration 407, loss = 0.16960485\n",
      "Iteration 408, loss = 0.16838189\n",
      "Iteration 409, loss = 0.17078999\n",
      "Iteration 410, loss = 0.16993920\n",
      "Iteration 411, loss = 0.16772948\n",
      "Iteration 412, loss = 0.17036518\n",
      "Iteration 413, loss = 0.16790499\n",
      "Iteration 414, loss = 0.16732464\n",
      "Iteration 415, loss = 0.16676441\n",
      "Iteration 416, loss = 0.16508942\n",
      "Iteration 417, loss = 0.16540064\n",
      "Iteration 418, loss = 0.16696191\n",
      "Iteration 419, loss = 0.16463644\n",
      "Iteration 420, loss = 0.16503583\n",
      "Iteration 421, loss = 0.16373995\n",
      "Iteration 422, loss = 0.16329756\n",
      "Iteration 423, loss = 0.16313928\n",
      "Iteration 424, loss = 0.16248994\n",
      "Iteration 425, loss = 0.16329928\n",
      "Iteration 426, loss = 0.16077261\n",
      "Iteration 427, loss = 0.16669266\n",
      "Iteration 428, loss = 0.16449713\n",
      "Iteration 429, loss = 0.16078051\n",
      "Iteration 430, loss = 0.16313568\n",
      "Iteration 431, loss = 0.16155898\n",
      "Iteration 432, loss = 0.15812008\n",
      "Iteration 433, loss = 0.15751196\n",
      "Iteration 434, loss = 0.15860343\n",
      "Iteration 435, loss = 0.15782575\n",
      "Iteration 436, loss = 0.15892588\n",
      "Iteration 437, loss = 0.15965827\n",
      "Iteration 438, loss = 0.15887630\n",
      "Iteration 439, loss = 0.15957064\n",
      "Iteration 440, loss = 0.15765657\n",
      "Iteration 441, loss = 0.15513259\n",
      "Iteration 442, loss = 0.15538419\n",
      "Iteration 443, loss = 0.15590785\n",
      "Iteration 444, loss = 0.15413723\n",
      "Iteration 445, loss = 0.15633081\n",
      "Iteration 446, loss = 0.15428881\n",
      "Iteration 447, loss = 0.15221792\n",
      "Iteration 448, loss = 0.15256116\n",
      "Iteration 449, loss = 0.15278062\n",
      "Iteration 450, loss = 0.15147504\n",
      "Iteration 451, loss = 0.15121104\n",
      "Iteration 452, loss = 0.15152380\n",
      "Iteration 453, loss = 0.15286883\n",
      "Iteration 454, loss = 0.15149883\n",
      "Iteration 455, loss = 0.15046940\n",
      "Iteration 456, loss = 0.15062495\n",
      "Iteration 457, loss = 0.14900336\n",
      "Iteration 458, loss = 0.14897856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 459, loss = 0.14879858\n",
      "Iteration 460, loss = 0.14741176\n",
      "Iteration 461, loss = 0.14798965\n",
      "Iteration 462, loss = 0.14865810\n",
      "Iteration 463, loss = 0.14709575\n",
      "Iteration 464, loss = 0.14741752\n",
      "Iteration 465, loss = 0.14771589\n",
      "Iteration 466, loss = 0.14712160\n",
      "Iteration 467, loss = 0.14713317\n",
      "Iteration 468, loss = 0.14592596\n",
      "Iteration 469, loss = 0.14602480\n",
      "Iteration 470, loss = 0.14629706\n",
      "Iteration 471, loss = 0.14597932\n",
      "Iteration 472, loss = 0.14453492\n",
      "Iteration 473, loss = 0.14466371\n",
      "Iteration 474, loss = 0.14397536\n",
      "Iteration 475, loss = 0.14910949\n",
      "Iteration 476, loss = 0.15189961\n",
      "Iteration 477, loss = 0.14649316\n",
      "Iteration 478, loss = 0.14492690\n",
      "Iteration 479, loss = 0.14481933\n",
      "Iteration 480, loss = 0.14271450\n",
      "Iteration 481, loss = 0.14075932\n",
      "Iteration 482, loss = 0.14181566\n",
      "Iteration 483, loss = 0.14502198\n",
      "Iteration 484, loss = 0.14061365\n",
      "Iteration 485, loss = 0.14057721\n",
      "Iteration 486, loss = 0.13988242\n",
      "Iteration 487, loss = 0.14110378\n",
      "Iteration 488, loss = 0.13865960\n",
      "Iteration 489, loss = 0.13825995\n",
      "Iteration 490, loss = 0.13707374\n",
      "Iteration 491, loss = 0.13735057\n",
      "Iteration 492, loss = 0.13762679\n",
      "Iteration 493, loss = 0.13843136\n",
      "Iteration 494, loss = 0.13667396\n",
      "Iteration 495, loss = 0.13765847\n",
      "Iteration 496, loss = 0.13734287\n",
      "Iteration 497, loss = 0.13699321\n",
      "Iteration 498, loss = 0.13779159\n",
      "Iteration 499, loss = 0.13553968\n",
      "Iteration 500, loss = 0.13569724\n",
      "Iteration 501, loss = 0.13432757\n",
      "Iteration 502, loss = 0.13419163\n",
      "Iteration 503, loss = 0.13461157\n",
      "Iteration 504, loss = 0.13564175\n",
      "Iteration 505, loss = 0.13915009\n",
      "Iteration 506, loss = 0.13972770\n",
      "Iteration 507, loss = 0.13743914\n",
      "Iteration 508, loss = 0.13307135\n",
      "Iteration 509, loss = 0.13288055\n",
      "Iteration 510, loss = 0.13103090\n",
      "Iteration 511, loss = 0.13228899\n",
      "Iteration 512, loss = 0.13276162\n",
      "Iteration 513, loss = 0.13146667\n",
      "Iteration 514, loss = 0.12977332\n",
      "Iteration 515, loss = 0.13088646\n",
      "Iteration 516, loss = 0.13198175\n",
      "Iteration 517, loss = 0.13069078\n",
      "Iteration 518, loss = 0.12908562\n",
      "Iteration 519, loss = 0.13039018\n",
      "Iteration 520, loss = 0.12868581\n",
      "Iteration 521, loss = 0.12946560\n",
      "Iteration 522, loss = 0.13188093\n",
      "Iteration 523, loss = 0.12912986\n",
      "Iteration 524, loss = 0.12878660\n",
      "Iteration 525, loss = 0.12713005\n",
      "Iteration 526, loss = 0.12730232\n",
      "Iteration 527, loss = 0.12711003\n",
      "Iteration 528, loss = 0.12941194\n",
      "Iteration 529, loss = 0.12854119\n",
      "Iteration 530, loss = 0.12694021\n",
      "Iteration 531, loss = 0.12562629\n",
      "Iteration 532, loss = 0.12557523\n",
      "Iteration 533, loss = 0.12471852\n",
      "Iteration 534, loss = 0.12462756\n",
      "Iteration 535, loss = 0.12456648\n",
      "Iteration 536, loss = 0.12401369\n",
      "Iteration 537, loss = 0.12511436\n",
      "Iteration 538, loss = 0.12521704\n",
      "Iteration 539, loss = 0.12491529\n",
      "Iteration 540, loss = 0.12332725\n",
      "Iteration 541, loss = 0.12255067\n",
      "Iteration 542, loss = 0.12161225\n",
      "Iteration 543, loss = 0.12172654\n",
      "Iteration 544, loss = 0.12565934\n",
      "Iteration 545, loss = 0.12302765\n",
      "Iteration 546, loss = 0.12336624\n",
      "Iteration 547, loss = 0.12349203\n",
      "Iteration 548, loss = 0.12372108\n",
      "Iteration 549, loss = 0.12400949\n",
      "Iteration 550, loss = 0.12647038\n",
      "Iteration 551, loss = 0.12668632\n",
      "Iteration 552, loss = 0.12333878\n",
      "Iteration 553, loss = 0.12120532\n",
      "Iteration 554, loss = 0.11941565\n",
      "Iteration 555, loss = 0.12143730\n",
      "Iteration 556, loss = 0.11961500\n",
      "Iteration 557, loss = 0.11856656\n",
      "Iteration 558, loss = 0.11932870\n",
      "Iteration 559, loss = 0.11809253\n",
      "Iteration 560, loss = 0.11745478\n",
      "Iteration 561, loss = 0.11704421\n",
      "Iteration 562, loss = 0.11658219\n",
      "Iteration 563, loss = 0.11703211\n",
      "Iteration 564, loss = 0.11775067\n",
      "Iteration 565, loss = 0.11671768\n",
      "Iteration 566, loss = 0.11674568\n",
      "Iteration 567, loss = 0.11602371\n",
      "Iteration 568, loss = 0.11622004\n",
      "Iteration 569, loss = 0.11741071\n",
      "Iteration 570, loss = 0.11832697\n",
      "Iteration 571, loss = 0.11636948\n",
      "Iteration 572, loss = 0.11528489\n",
      "Iteration 573, loss = 0.11433293\n",
      "Iteration 574, loss = 0.11446753\n",
      "Iteration 575, loss = 0.11909522\n",
      "Iteration 576, loss = 0.11770524\n",
      "Iteration 577, loss = 0.11676620\n",
      "Iteration 578, loss = 0.11563771\n",
      "Iteration 579, loss = 0.11555711\n",
      "Iteration 580, loss = 0.11799203\n",
      "Iteration 581, loss = 0.11462049\n",
      "Iteration 582, loss = 0.11556440\n",
      "Iteration 583, loss = 0.11616836\n",
      "Iteration 584, loss = 0.11346319\n",
      "Iteration 585, loss = 0.11261829\n",
      "Iteration 586, loss = 0.11296874\n",
      "Iteration 587, loss = 0.11029958\n",
      "Iteration 588, loss = 0.11123665\n",
      "Iteration 589, loss = 0.11166678\n",
      "Iteration 590, loss = 0.10984163\n",
      "Iteration 591, loss = 0.10937306\n",
      "Iteration 592, loss = 0.11096241\n",
      "Iteration 593, loss = 0.11106841\n",
      "Iteration 594, loss = 0.11224719\n",
      "Iteration 595, loss = 0.11087176\n",
      "Iteration 596, loss = 0.10999414\n",
      "Iteration 597, loss = 0.10868135\n",
      "Iteration 598, loss = 0.11016754\n",
      "Iteration 599, loss = 0.10954877\n",
      "Iteration 600, loss = 0.10820567\n",
      "Iteration 601, loss = 0.10919102\n",
      "Iteration 602, loss = 0.10727956\n",
      "Iteration 603, loss = 0.10811870\n",
      "Iteration 604, loss = 0.11016213\n",
      "Iteration 605, loss = 0.11057780\n",
      "Iteration 606, loss = 0.10722037\n",
      "Iteration 607, loss = 0.10606304\n",
      "Iteration 608, loss = 0.10629508\n",
      "Iteration 609, loss = 0.10933348\n",
      "Iteration 610, loss = 0.10878154\n",
      "Iteration 611, loss = 0.10684313\n",
      "Iteration 612, loss = 0.10624489\n",
      "Iteration 613, loss = 0.10640931\n",
      "Iteration 614, loss = 0.10770721\n",
      "Iteration 615, loss = 0.10583608\n",
      "Iteration 616, loss = 0.10603921\n",
      "Iteration 617, loss = 0.11211565\n",
      "Iteration 618, loss = 0.10998482\n",
      "Iteration 619, loss = 0.10765845\n",
      "Iteration 620, loss = 0.10540712\n",
      "Iteration 621, loss = 0.10495176\n",
      "Iteration 622, loss = 0.10417840\n",
      "Iteration 623, loss = 0.10509005\n",
      "Iteration 624, loss = 0.10440410\n",
      "Iteration 625, loss = 0.10268546\n",
      "Iteration 626, loss = 0.10347724\n",
      "Iteration 627, loss = 0.10554133\n",
      "Iteration 628, loss = 0.10280446\n",
      "Iteration 629, loss = 0.10445781\n",
      "Iteration 630, loss = 0.10583750\n",
      "Iteration 631, loss = 0.10221559\n",
      "Iteration 632, loss = 0.10202064\n",
      "Iteration 633, loss = 0.10237798\n",
      "Iteration 634, loss = 0.10167919\n",
      "Iteration 635, loss = 0.10252481\n",
      "Iteration 636, loss = 0.10239737\n",
      "Iteration 637, loss = 0.10114753\n",
      "Iteration 638, loss = 0.10037225\n",
      "Iteration 639, loss = 0.10061171\n",
      "Iteration 640, loss = 0.10015622\n",
      "Iteration 641, loss = 0.10053894\n",
      "Iteration 642, loss = 0.10260896\n",
      "Iteration 643, loss = 0.10422231\n",
      "Iteration 644, loss = 0.10380061\n",
      "Iteration 645, loss = 0.09977570\n",
      "Iteration 646, loss = 0.10157326\n",
      "Iteration 647, loss = 0.10602501\n",
      "Iteration 648, loss = 0.10485277\n",
      "Iteration 649, loss = 0.10611629\n",
      "Iteration 650, loss = 0.10636785\n",
      "Iteration 651, loss = 0.10341694\n",
      "Iteration 652, loss = 0.10120576\n",
      "Iteration 653, loss = 0.09852940\n",
      "Iteration 654, loss = 0.09888889\n",
      "Iteration 655, loss = 0.09794439\n",
      "Iteration 656, loss = 0.09794976\n",
      "Iteration 657, loss = 0.09863981\n",
      "Iteration 658, loss = 0.09764691\n",
      "Iteration 659, loss = 0.09746593\n",
      "Iteration 660, loss = 0.09762481\n",
      "Iteration 661, loss = 0.09638438\n",
      "Iteration 662, loss = 0.09762004\n",
      "Iteration 663, loss = 0.09651616\n",
      "Iteration 664, loss = 0.09746585\n",
      "Iteration 665, loss = 0.09684404\n",
      "Iteration 666, loss = 0.09719385\n",
      "Iteration 667, loss = 0.09845989\n",
      "Iteration 668, loss = 0.09924933\n",
      "Iteration 669, loss = 0.09865056\n",
      "Iteration 670, loss = 0.09806476\n",
      "Iteration 671, loss = 0.09709936\n",
      "Iteration 672, loss = 0.09566201\n",
      "Iteration 673, loss = 0.09656129\n",
      "Iteration 674, loss = 0.09758755\n",
      "Iteration 675, loss = 0.09705491\n",
      "Iteration 676, loss = 0.09448726\n",
      "Iteration 677, loss = 0.09785845\n",
      "Iteration 678, loss = 0.09633539\n",
      "Iteration 679, loss = 0.09498346\n",
      "Iteration 680, loss = 0.09313642\n",
      "Iteration 681, loss = 0.09333978\n",
      "Iteration 682, loss = 0.09426451\n",
      "Iteration 683, loss = 0.09515180\n",
      "Iteration 684, loss = 0.09421494\n",
      "Iteration 685, loss = 0.09610449\n",
      "Iteration 686, loss = 0.09751344\n",
      "Iteration 687, loss = 0.09593532\n",
      "Iteration 688, loss = 0.09253781\n",
      "Iteration 689, loss = 0.09216660\n",
      "Iteration 690, loss = 0.09344680\n",
      "Iteration 691, loss = 0.09305324\n",
      "Iteration 692, loss = 0.09342039\n",
      "Iteration 693, loss = 0.09515025\n",
      "Iteration 694, loss = 0.09127478\n",
      "Iteration 695, loss = 0.09138442\n",
      "Iteration 696, loss = 0.09057236\n",
      "Iteration 697, loss = 0.09027561\n",
      "Iteration 698, loss = 0.09249462\n",
      "Iteration 699, loss = 0.09485506\n",
      "Iteration 700, loss = 0.09035362\n",
      "Iteration 701, loss = 0.09205573\n",
      "Iteration 702, loss = 0.09001744\n",
      "Iteration 703, loss = 0.09031303\n",
      "Iteration 704, loss = 0.08964596\n",
      "Iteration 705, loss = 0.08991986\n",
      "Iteration 706, loss = 0.09040217\n",
      "Iteration 707, loss = 0.09047748\n",
      "Iteration 708, loss = 0.08816236\n",
      "Iteration 709, loss = 0.09019725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 710, loss = 0.09094841\n",
      "Iteration 711, loss = 0.08975881\n",
      "Iteration 712, loss = 0.08940565\n",
      "Iteration 713, loss = 0.08801039\n",
      "Iteration 714, loss = 0.08952000\n",
      "Iteration 715, loss = 0.09011957\n",
      "Iteration 716, loss = 0.08833148\n",
      "Iteration 717, loss = 0.08761920\n",
      "Iteration 718, loss = 0.08669245\n",
      "Iteration 719, loss = 0.08823939\n",
      "Iteration 720, loss = 0.08784468\n",
      "Iteration 721, loss = 0.08684296\n",
      "Iteration 722, loss = 0.08722399\n",
      "Iteration 723, loss = 0.08799919\n",
      "Iteration 724, loss = 0.08815182\n",
      "Iteration 725, loss = 0.08875358\n",
      "Iteration 726, loss = 0.08854761\n",
      "Iteration 727, loss = 0.08681374\n",
      "Iteration 728, loss = 0.08676245\n",
      "Iteration 729, loss = 0.08788162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 22 of 48...\n",
      "Iteration 1, loss = 0.62596380\n",
      "Iteration 2, loss = 0.52888041\n",
      "Iteration 3, loss = 0.48079696\n",
      "Iteration 4, loss = 0.45401042\n",
      "Iteration 5, loss = 0.43159865\n",
      "Iteration 6, loss = 0.41341549\n",
      "Iteration 7, loss = 0.40330925\n",
      "Iteration 8, loss = 0.39178333\n",
      "Iteration 9, loss = 0.38216765\n",
      "Iteration 10, loss = 0.37482115\n",
      "Iteration 11, loss = 0.36736577\n",
      "Iteration 12, loss = 0.36122977\n",
      "Iteration 13, loss = 0.35369816\n",
      "Iteration 14, loss = 0.34373449\n",
      "Iteration 15, loss = 0.34074021\n",
      "Iteration 16, loss = 0.33384836\n",
      "Iteration 17, loss = 0.33634104\n",
      "Iteration 18, loss = 0.32402654\n",
      "Iteration 19, loss = 0.31796234\n",
      "Iteration 20, loss = 0.31222362\n",
      "Iteration 21, loss = 0.30978568\n",
      "Iteration 22, loss = 0.32333155\n",
      "Iteration 23, loss = 0.29946275\n",
      "Iteration 24, loss = 0.29649567\n",
      "Iteration 25, loss = 0.29694129\n",
      "Iteration 26, loss = 0.28975347\n",
      "Iteration 27, loss = 0.28249126\n",
      "Iteration 28, loss = 0.28232387\n",
      "Iteration 29, loss = 0.27678031\n",
      "Iteration 30, loss = 0.27416848\n",
      "Iteration 31, loss = 0.27016801\n",
      "Iteration 32, loss = 0.26953174\n",
      "Iteration 33, loss = 0.26823482\n",
      "Iteration 34, loss = 0.25758133\n",
      "Iteration 35, loss = 0.25900654\n",
      "Iteration 36, loss = 0.26069566\n",
      "Iteration 37, loss = 0.25423681\n",
      "Iteration 38, loss = 0.24773637\n",
      "Iteration 39, loss = 0.24360441\n",
      "Iteration 40, loss = 0.24784353\n",
      "Iteration 41, loss = 0.24804490\n",
      "Iteration 42, loss = 0.24519358\n",
      "Iteration 43, loss = 0.23476866\n",
      "Iteration 44, loss = 0.24073306\n",
      "Iteration 45, loss = 0.24443386\n",
      "Iteration 46, loss = 0.23189935\n",
      "Iteration 47, loss = 0.22764809\n",
      "Iteration 48, loss = 0.22416474\n",
      "Iteration 49, loss = 0.22054855\n",
      "Iteration 50, loss = 0.21947265\n",
      "Iteration 51, loss = 0.21635846\n",
      "Iteration 52, loss = 0.21118261\n",
      "Iteration 53, loss = 0.21457215\n",
      "Iteration 54, loss = 0.22266575\n",
      "Iteration 55, loss = 0.21958370\n",
      "Iteration 56, loss = 0.20957478\n",
      "Iteration 57, loss = 0.20461711\n",
      "Iteration 58, loss = 0.20391032\n",
      "Iteration 59, loss = 0.19698406\n",
      "Iteration 60, loss = 0.20386731\n",
      "Iteration 61, loss = 0.20081136\n",
      "Iteration 62, loss = 0.20389566\n",
      "Iteration 63, loss = 0.19931878\n",
      "Iteration 64, loss = 0.18781147\n",
      "Iteration 65, loss = 0.18486399\n",
      "Iteration 66, loss = 0.18703146\n",
      "Iteration 67, loss = 0.18865724\n",
      "Iteration 68, loss = 0.18079267\n",
      "Iteration 69, loss = 0.17857112\n",
      "Iteration 70, loss = 0.17374744\n",
      "Iteration 71, loss = 0.17166681\n",
      "Iteration 72, loss = 0.17242977\n",
      "Iteration 73, loss = 0.17422212\n",
      "Iteration 74, loss = 0.16813979\n",
      "Iteration 75, loss = 0.16859448\n",
      "Iteration 76, loss = 0.16697445\n",
      "Iteration 77, loss = 0.16355649\n",
      "Iteration 78, loss = 0.16540490\n",
      "Iteration 79, loss = 0.16193729\n",
      "Iteration 80, loss = 0.16082510\n",
      "Iteration 81, loss = 0.15682332\n",
      "Iteration 82, loss = 0.15513107\n",
      "Iteration 83, loss = 0.15450586\n",
      "Iteration 84, loss = 0.15233774\n",
      "Iteration 85, loss = 0.15441894\n",
      "Iteration 86, loss = 0.15446349\n",
      "Iteration 87, loss = 0.14904465\n",
      "Iteration 88, loss = 0.14568947\n",
      "Iteration 89, loss = 0.14379205\n",
      "Iteration 90, loss = 0.13934655\n",
      "Iteration 91, loss = 0.13931018\n",
      "Iteration 92, loss = 0.13907208\n",
      "Iteration 93, loss = 0.13657521\n",
      "Iteration 94, loss = 0.14159244\n",
      "Iteration 95, loss = 0.13565269\n",
      "Iteration 96, loss = 0.13574408\n",
      "Iteration 97, loss = 0.13512439\n",
      "Iteration 98, loss = 0.13400279\n",
      "Iteration 99, loss = 0.13380970\n",
      "Iteration 100, loss = 0.12958318\n",
      "Iteration 101, loss = 0.12535960\n",
      "Iteration 102, loss = 0.12355583\n",
      "Iteration 103, loss = 0.12219242\n",
      "Iteration 104, loss = 0.12113611\n",
      "Iteration 105, loss = 0.12165830\n",
      "Iteration 106, loss = 0.11761297\n",
      "Iteration 107, loss = 0.11989373\n",
      "Iteration 108, loss = 0.11412306\n",
      "Iteration 109, loss = 0.11669200\n",
      "Iteration 110, loss = 0.11745393\n",
      "Iteration 111, loss = 0.11242459\n",
      "Iteration 112, loss = 0.11985910\n",
      "Iteration 113, loss = 0.11616279\n",
      "Iteration 114, loss = 0.10836669\n",
      "Iteration 115, loss = 0.10729701\n",
      "Iteration 116, loss = 0.10877605\n",
      "Iteration 117, loss = 0.10833851\n",
      "Iteration 118, loss = 0.10292387\n",
      "Iteration 119, loss = 0.10712697\n",
      "Iteration 120, loss = 0.10937197\n",
      "Iteration 121, loss = 0.10253560\n",
      "Iteration 122, loss = 0.09934151\n",
      "Iteration 123, loss = 0.09789228\n",
      "Iteration 124, loss = 0.09807772\n",
      "Iteration 125, loss = 0.10842943\n",
      "Iteration 126, loss = 0.10720246\n",
      "Iteration 127, loss = 0.09850749\n",
      "Iteration 128, loss = 0.09514607\n",
      "Iteration 129, loss = 0.09522715\n",
      "Iteration 130, loss = 0.09312842\n",
      "Iteration 131, loss = 0.09134681\n",
      "Iteration 132, loss = 0.09292389\n",
      "Iteration 133, loss = 0.09166668\n",
      "Iteration 134, loss = 0.08911126\n",
      "Iteration 135, loss = 0.08930804\n",
      "Iteration 136, loss = 0.08725667\n",
      "Iteration 137, loss = 0.08826197\n",
      "Iteration 138, loss = 0.08574659\n",
      "Iteration 139, loss = 0.08331611\n",
      "Iteration 140, loss = 0.08363498\n",
      "Iteration 141, loss = 0.08923369\n",
      "Iteration 142, loss = 0.09384922\n",
      "Iteration 143, loss = 0.09174045\n",
      "Iteration 144, loss = 0.08481925\n",
      "Iteration 145, loss = 0.07732454\n",
      "Iteration 146, loss = 0.07682463\n",
      "Iteration 147, loss = 0.07796173\n",
      "Iteration 148, loss = 0.07772002\n",
      "Iteration 149, loss = 0.07375341\n",
      "Iteration 150, loss = 0.07427526\n",
      "Iteration 151, loss = 0.07590420\n",
      "Iteration 152, loss = 0.07392461\n",
      "Iteration 153, loss = 0.07314406\n",
      "Iteration 154, loss = 0.07362065\n",
      "Iteration 155, loss = 0.06933602\n",
      "Iteration 156, loss = 0.06986894\n",
      "Iteration 157, loss = 0.07054681\n",
      "Iteration 158, loss = 0.07339463\n",
      "Iteration 159, loss = 0.06988906\n",
      "Iteration 160, loss = 0.06551644\n",
      "Iteration 161, loss = 0.06797448\n",
      "Iteration 162, loss = 0.06492946\n",
      "Iteration 163, loss = 0.06630587\n",
      "Iteration 164, loss = 0.06724203\n",
      "Iteration 165, loss = 0.06243023\n",
      "Iteration 166, loss = 0.06185165\n",
      "Iteration 167, loss = 0.06103257\n",
      "Iteration 168, loss = 0.06006358\n",
      "Iteration 169, loss = 0.06095512\n",
      "Iteration 170, loss = 0.06077435\n",
      "Iteration 171, loss = 0.05916633\n",
      "Iteration 172, loss = 0.05842201\n",
      "Iteration 173, loss = 0.06122062\n",
      "Iteration 174, loss = 0.05869576\n",
      "Iteration 175, loss = 0.05851251\n",
      "Iteration 176, loss = 0.05782125\n",
      "Iteration 177, loss = 0.06198287\n",
      "Iteration 178, loss = 0.05793850\n",
      "Iteration 179, loss = 0.05651339\n",
      "Iteration 180, loss = 0.05427170\n",
      "Iteration 181, loss = 0.05280850\n",
      "Iteration 182, loss = 0.05233240\n",
      "Iteration 183, loss = 0.05351230\n",
      "Iteration 184, loss = 0.05157760\n",
      "Iteration 185, loss = 0.05116776\n",
      "Iteration 186, loss = 0.05060872\n",
      "Iteration 187, loss = 0.05098768\n",
      "Iteration 188, loss = 0.05029307\n",
      "Iteration 189, loss = 0.05074188\n",
      "Iteration 190, loss = 0.04918272\n",
      "Iteration 191, loss = 0.05077261\n",
      "Iteration 192, loss = 0.04787035\n",
      "Iteration 193, loss = 0.04708626\n",
      "Iteration 194, loss = 0.04692879\n",
      "Iteration 195, loss = 0.05086502\n",
      "Iteration 196, loss = 0.04825093\n",
      "Iteration 197, loss = 0.04560996\n",
      "Iteration 198, loss = 0.04480386\n",
      "Iteration 199, loss = 0.04544716\n",
      "Iteration 200, loss = 0.04680498\n",
      "Iteration 201, loss = 0.05119212\n",
      "Iteration 202, loss = 0.04977899\n",
      "Iteration 203, loss = 0.05330874\n",
      "Iteration 204, loss = 0.04495349\n",
      "Iteration 205, loss = 0.04405391\n",
      "Iteration 206, loss = 0.04439804\n",
      "Iteration 207, loss = 0.04327604\n",
      "Iteration 208, loss = 0.04184479\n",
      "Iteration 209, loss = 0.04127489\n",
      "Iteration 210, loss = 0.04001191\n",
      "Iteration 211, loss = 0.03946281\n",
      "Iteration 212, loss = 0.03905419\n",
      "Iteration 213, loss = 0.03825297\n",
      "Iteration 214, loss = 0.03892019\n",
      "Iteration 215, loss = 0.03940168\n",
      "Iteration 216, loss = 0.04063764\n",
      "Iteration 217, loss = 0.04195520\n",
      "Iteration 218, loss = 0.04240925\n",
      "Iteration 219, loss = 0.04180597\n",
      "Iteration 220, loss = 0.03628424\n",
      "Iteration 221, loss = 0.03732135\n",
      "Iteration 222, loss = 0.03534911\n",
      "Iteration 223, loss = 0.03453656\n",
      "Iteration 224, loss = 0.03429113\n",
      "Iteration 225, loss = 0.03667652\n",
      "Iteration 226, loss = 0.03947440\n",
      "Iteration 227, loss = 0.04082500\n",
      "Iteration 228, loss = 0.04317792\n",
      "Iteration 229, loss = 0.04783976\n",
      "Iteration 230, loss = 0.05672572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 231, loss = 0.03730846\n",
      "Iteration 232, loss = 0.03458442\n",
      "Iteration 233, loss = 0.03448347\n",
      "Iteration 234, loss = 0.03219000\n",
      "Iteration 235, loss = 0.03374253\n",
      "Iteration 236, loss = 0.03296229\n",
      "Iteration 237, loss = 0.03105946\n",
      "Iteration 238, loss = 0.03243725\n",
      "Iteration 239, loss = 0.03049014\n",
      "Iteration 240, loss = 0.03133869\n",
      "Iteration 241, loss = 0.03308504\n",
      "Iteration 242, loss = 0.03183228\n",
      "Iteration 243, loss = 0.02925455\n",
      "Iteration 244, loss = 0.02964260\n",
      "Iteration 245, loss = 0.02852818\n",
      "Iteration 246, loss = 0.02834181\n",
      "Iteration 247, loss = 0.02862885\n",
      "Iteration 248, loss = 0.02766715\n",
      "Iteration 249, loss = 0.02697057\n",
      "Iteration 250, loss = 0.02656147\n",
      "Iteration 251, loss = 0.02663817\n",
      "Iteration 252, loss = 0.02661846\n",
      "Iteration 253, loss = 0.02703958\n",
      "Iteration 254, loss = 0.02649301\n",
      "Iteration 255, loss = 0.02677020\n",
      "Iteration 256, loss = 0.02564661\n",
      "Iteration 257, loss = 0.02561696\n",
      "Iteration 258, loss = 0.02791176\n",
      "Iteration 259, loss = 0.02635658\n",
      "Iteration 260, loss = 0.02629286\n",
      "Iteration 261, loss = 0.02835957\n",
      "Iteration 262, loss = 0.02724303\n",
      "Iteration 263, loss = 0.02494515\n",
      "Iteration 264, loss = 0.02506487\n",
      "Iteration 265, loss = 0.02443790\n",
      "Iteration 266, loss = 0.02445184\n",
      "Iteration 267, loss = 0.02552858\n",
      "Iteration 268, loss = 0.02546952\n",
      "Iteration 269, loss = 0.02767048\n",
      "Iteration 270, loss = 0.02583184\n",
      "Iteration 271, loss = 0.02724406\n",
      "Iteration 272, loss = 0.02633298\n",
      "Iteration 273, loss = 0.02437689\n",
      "Iteration 274, loss = 0.02341795\n",
      "Iteration 275, loss = 0.02424575\n",
      "Iteration 276, loss = 0.02333847\n",
      "Iteration 277, loss = 0.02310916\n",
      "Iteration 278, loss = 0.02371619\n",
      "Iteration 279, loss = 0.02312136\n",
      "Iteration 280, loss = 0.02216456\n",
      "Iteration 281, loss = 0.02096186\n",
      "Iteration 282, loss = 0.02040316\n",
      "Iteration 283, loss = 0.02176262\n",
      "Iteration 284, loss = 0.02164128\n",
      "Iteration 285, loss = 0.02265691\n",
      "Iteration 286, loss = 0.02115028\n",
      "Iteration 287, loss = 0.02013710\n",
      "Iteration 288, loss = 0.01988196\n",
      "Iteration 289, loss = 0.02197820\n",
      "Iteration 290, loss = 0.02430861\n",
      "Iteration 291, loss = 0.02213998\n",
      "Iteration 292, loss = 0.02022312\n",
      "Iteration 293, loss = 0.01929238\n",
      "Iteration 294, loss = 0.01951812\n",
      "Iteration 295, loss = 0.01983075\n",
      "Iteration 296, loss = 0.01795165\n",
      "Iteration 297, loss = 0.01938803\n",
      "Iteration 298, loss = 0.01854004\n",
      "Iteration 299, loss = 0.01820657\n",
      "Iteration 300, loss = 0.01759719\n",
      "Iteration 301, loss = 0.01811940\n",
      "Iteration 302, loss = 0.01802613\n",
      "Iteration 303, loss = 0.01805099\n",
      "Iteration 304, loss = 0.01750090\n",
      "Iteration 305, loss = 0.01726731\n",
      "Iteration 306, loss = 0.01661810\n",
      "Iteration 307, loss = 0.01695746\n",
      "Iteration 308, loss = 0.01806952\n",
      "Iteration 309, loss = 0.01719452\n",
      "Iteration 310, loss = 0.01623996\n",
      "Iteration 311, loss = 0.01729665\n",
      "Iteration 312, loss = 0.01976058\n",
      "Iteration 313, loss = 0.01623554\n",
      "Iteration 314, loss = 0.01632605\n",
      "Iteration 315, loss = 0.01538010\n",
      "Iteration 316, loss = 0.01493166\n",
      "Iteration 317, loss = 0.01571732\n",
      "Iteration 318, loss = 0.01524023\n",
      "Iteration 319, loss = 0.01491834\n",
      "Iteration 320, loss = 0.01562015\n",
      "Iteration 321, loss = 0.01707926\n",
      "Iteration 322, loss = 0.01612922\n",
      "Iteration 323, loss = 0.01609115\n",
      "Iteration 324, loss = 0.01361994\n",
      "Iteration 325, loss = 0.01452610\n",
      "Iteration 326, loss = 0.01567069\n",
      "Iteration 327, loss = 0.01355857\n",
      "Iteration 328, loss = 0.01423431\n",
      "Iteration 329, loss = 0.01445162\n",
      "Iteration 330, loss = 0.01345784\n",
      "Iteration 331, loss = 0.01447328\n",
      "Iteration 332, loss = 0.01393034\n",
      "Iteration 333, loss = 0.01360313\n",
      "Iteration 334, loss = 0.01476850\n",
      "Iteration 335, loss = 0.01352786\n",
      "Iteration 336, loss = 0.01327957\n",
      "Iteration 337, loss = 0.01287646\n",
      "Iteration 338, loss = 0.01306650\n",
      "Iteration 339, loss = 0.01288081\n",
      "Iteration 340, loss = 0.01204921\n",
      "Iteration 341, loss = 0.01254703\n",
      "Iteration 342, loss = 0.01216860\n",
      "Iteration 343, loss = 0.01266807\n",
      "Iteration 344, loss = 0.01168225\n",
      "Iteration 345, loss = 0.01172324\n",
      "Iteration 346, loss = 0.01176330\n",
      "Iteration 347, loss = 0.01117084\n",
      "Iteration 348, loss = 0.01172231\n",
      "Iteration 349, loss = 0.01319802\n",
      "Iteration 350, loss = 0.01237512\n",
      "Iteration 351, loss = 0.01172951\n",
      "Iteration 352, loss = 0.01175442\n",
      "Iteration 353, loss = 0.01104275\n",
      "Iteration 354, loss = 0.01107494\n",
      "Iteration 355, loss = 0.01331480\n",
      "Iteration 356, loss = 0.01180544\n",
      "Iteration 357, loss = 0.01105834\n",
      "Iteration 358, loss = 0.01047110\n",
      "Iteration 359, loss = 0.01074841\n",
      "Iteration 360, loss = 0.01124748\n",
      "Iteration 361, loss = 0.01042039\n",
      "Iteration 362, loss = 0.01049644\n",
      "Iteration 363, loss = 0.01027816\n",
      "Iteration 364, loss = 0.00995056\n",
      "Iteration 365, loss = 0.01051633\n",
      "Iteration 366, loss = 0.01020311\n",
      "Iteration 367, loss = 0.01058865\n",
      "Iteration 368, loss = 0.00973507\n",
      "Iteration 369, loss = 0.00945858\n",
      "Iteration 370, loss = 0.01010631\n",
      "Iteration 371, loss = 0.00954185\n",
      "Iteration 372, loss = 0.01024083\n",
      "Iteration 373, loss = 0.00945838\n",
      "Iteration 374, loss = 0.00978449\n",
      "Iteration 375, loss = 0.01068935\n",
      "Iteration 376, loss = 0.01000302\n",
      "Iteration 377, loss = 0.00938598\n",
      "Iteration 378, loss = 0.00892139\n",
      "Iteration 379, loss = 0.00895087\n",
      "Iteration 380, loss = 0.00889405\n",
      "Iteration 381, loss = 0.00917805\n",
      "Iteration 382, loss = 0.00975483\n",
      "Iteration 383, loss = 0.00896828\n",
      "Iteration 384, loss = 0.00964710\n",
      "Iteration 385, loss = 0.00948770\n",
      "Iteration 386, loss = 0.00937385\n",
      "Iteration 387, loss = 0.00909678\n",
      "Iteration 388, loss = 0.00906185\n",
      "Iteration 389, loss = 0.00958541\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 23 of 48...\n",
      "Iteration 1, loss = 0.65733541\n",
      "Iteration 2, loss = 0.54185121\n",
      "Iteration 3, loss = 0.49176803\n",
      "Iteration 4, loss = 0.46750223\n",
      "Iteration 5, loss = 0.43973652\n",
      "Iteration 6, loss = 0.42077747\n",
      "Iteration 7, loss = 0.40837696\n",
      "Iteration 8, loss = 0.39714300\n",
      "Iteration 9, loss = 0.38390753\n",
      "Iteration 10, loss = 0.37481823\n",
      "Iteration 11, loss = 0.36641231\n",
      "Iteration 12, loss = 0.36010261\n",
      "Iteration 13, loss = 0.35984718\n",
      "Iteration 14, loss = 0.34570737\n",
      "Iteration 15, loss = 0.34749206\n",
      "Iteration 16, loss = 0.33682456\n",
      "Iteration 17, loss = 0.32858885\n",
      "Iteration 18, loss = 0.32407927\n",
      "Iteration 19, loss = 0.31890490\n",
      "Iteration 20, loss = 0.31197601\n",
      "Iteration 21, loss = 0.31098349\n",
      "Iteration 22, loss = 0.30506916\n",
      "Iteration 23, loss = 0.30717520\n",
      "Iteration 24, loss = 0.31427989\n",
      "Iteration 25, loss = 0.31494608\n",
      "Iteration 26, loss = 0.29435578\n",
      "Iteration 27, loss = 0.29851541\n",
      "Iteration 28, loss = 0.29219479\n",
      "Iteration 29, loss = 0.28635349\n",
      "Iteration 30, loss = 0.27491732\n",
      "Iteration 31, loss = 0.27380934\n",
      "Iteration 32, loss = 0.26880788\n",
      "Iteration 33, loss = 0.26990203\n",
      "Iteration 34, loss = 0.27518681\n",
      "Iteration 35, loss = 0.26035350\n",
      "Iteration 36, loss = 0.26323396\n",
      "Iteration 37, loss = 0.25974470\n",
      "Iteration 38, loss = 0.25092599\n",
      "Iteration 39, loss = 0.25863611\n",
      "Iteration 40, loss = 0.25331714\n",
      "Iteration 41, loss = 0.24405182\n",
      "Iteration 42, loss = 0.24268530\n",
      "Iteration 43, loss = 0.24222055\n",
      "Iteration 44, loss = 0.24268716\n",
      "Iteration 45, loss = 0.23280032\n",
      "Iteration 46, loss = 0.23289885\n",
      "Iteration 47, loss = 0.24316319\n",
      "Iteration 48, loss = 0.23034936\n",
      "Iteration 49, loss = 0.22673906\n",
      "Iteration 50, loss = 0.22692318\n",
      "Iteration 51, loss = 0.22723264\n",
      "Iteration 52, loss = 0.22692399\n",
      "Iteration 53, loss = 0.22868840\n",
      "Iteration 54, loss = 0.22663608\n",
      "Iteration 55, loss = 0.21922049\n",
      "Iteration 56, loss = 0.20870196\n",
      "Iteration 57, loss = 0.21315330\n",
      "Iteration 58, loss = 0.21212913\n",
      "Iteration 59, loss = 0.20385019\n",
      "Iteration 60, loss = 0.20362138\n",
      "Iteration 61, loss = 0.20107078\n",
      "Iteration 62, loss = 0.19636480\n",
      "Iteration 63, loss = 0.19574053\n",
      "Iteration 64, loss = 0.19432656\n",
      "Iteration 65, loss = 0.19418918\n",
      "Iteration 66, loss = 0.19628000\n",
      "Iteration 67, loss = 0.19206405\n",
      "Iteration 68, loss = 0.19040157\n",
      "Iteration 69, loss = 0.19331831\n",
      "Iteration 70, loss = 0.19020691\n",
      "Iteration 71, loss = 0.17821271\n",
      "Iteration 72, loss = 0.18111761\n",
      "Iteration 73, loss = 0.18092188\n",
      "Iteration 74, loss = 0.17505639\n",
      "Iteration 75, loss = 0.17257720\n",
      "Iteration 76, loss = 0.16925550\n",
      "Iteration 77, loss = 0.17077914\n",
      "Iteration 78, loss = 0.16802694\n",
      "Iteration 79, loss = 0.16823514\n",
      "Iteration 80, loss = 0.17009414\n",
      "Iteration 81, loss = 0.16659450\n",
      "Iteration 82, loss = 0.16076829\n",
      "Iteration 83, loss = 0.16426219\n",
      "Iteration 84, loss = 0.16038312\n",
      "Iteration 85, loss = 0.16184718\n",
      "Iteration 86, loss = 0.16409034\n",
      "Iteration 87, loss = 0.15602020\n",
      "Iteration 88, loss = 0.15013061\n",
      "Iteration 89, loss = 0.14793786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 90, loss = 0.14792658\n",
      "Iteration 91, loss = 0.14818800\n",
      "Iteration 92, loss = 0.14261679\n",
      "Iteration 93, loss = 0.14723424\n",
      "Iteration 94, loss = 0.14891758\n",
      "Iteration 95, loss = 0.13923182\n",
      "Iteration 96, loss = 0.14527943\n",
      "Iteration 97, loss = 0.13986206\n",
      "Iteration 98, loss = 0.13400899\n",
      "Iteration 99, loss = 0.13898694\n",
      "Iteration 100, loss = 0.13568430\n",
      "Iteration 101, loss = 0.13058711\n",
      "Iteration 102, loss = 0.13987862\n",
      "Iteration 103, loss = 0.14037170\n",
      "Iteration 104, loss = 0.12627590\n",
      "Iteration 105, loss = 0.12633862\n",
      "Iteration 106, loss = 0.12750307\n",
      "Iteration 107, loss = 0.12562696\n",
      "Iteration 108, loss = 0.12640335\n",
      "Iteration 109, loss = 0.12740696\n",
      "Iteration 110, loss = 0.12454584\n",
      "Iteration 111, loss = 0.11811770\n",
      "Iteration 112, loss = 0.11981406\n",
      "Iteration 113, loss = 0.11414462\n",
      "Iteration 114, loss = 0.11442047\n",
      "Iteration 115, loss = 0.11691955\n",
      "Iteration 116, loss = 0.11079299\n",
      "Iteration 117, loss = 0.11062731\n",
      "Iteration 118, loss = 0.10842908\n",
      "Iteration 119, loss = 0.10991595\n",
      "Iteration 120, loss = 0.10903024\n",
      "Iteration 121, loss = 0.10545083\n",
      "Iteration 122, loss = 0.10469298\n",
      "Iteration 123, loss = 0.10226020\n",
      "Iteration 124, loss = 0.10776304\n",
      "Iteration 125, loss = 0.11084890\n",
      "Iteration 126, loss = 0.10216502\n",
      "Iteration 127, loss = 0.09999793\n",
      "Iteration 128, loss = 0.10051816\n",
      "Iteration 129, loss = 0.10068215\n",
      "Iteration 130, loss = 0.09981299\n",
      "Iteration 131, loss = 0.10272395\n",
      "Iteration 132, loss = 0.09786740\n",
      "Iteration 133, loss = 0.09789091\n",
      "Iteration 134, loss = 0.09309450\n",
      "Iteration 135, loss = 0.09369916\n",
      "Iteration 136, loss = 0.09023073\n",
      "Iteration 137, loss = 0.09200559\n",
      "Iteration 138, loss = 0.08859248\n",
      "Iteration 139, loss = 0.08888638\n",
      "Iteration 140, loss = 0.08992310\n",
      "Iteration 141, loss = 0.08828922\n",
      "Iteration 142, loss = 0.08741773\n",
      "Iteration 143, loss = 0.08841289\n",
      "Iteration 144, loss = 0.08666048\n",
      "Iteration 145, loss = 0.08504226\n",
      "Iteration 146, loss = 0.09076311\n",
      "Iteration 147, loss = 0.08559190\n",
      "Iteration 148, loss = 0.08202302\n",
      "Iteration 149, loss = 0.08024897\n",
      "Iteration 150, loss = 0.08204073\n",
      "Iteration 151, loss = 0.08123733\n",
      "Iteration 152, loss = 0.07875495\n",
      "Iteration 153, loss = 0.07677140\n",
      "Iteration 154, loss = 0.07746484\n",
      "Iteration 155, loss = 0.07610397\n",
      "Iteration 156, loss = 0.08157918\n",
      "Iteration 157, loss = 0.07846665\n",
      "Iteration 158, loss = 0.07556644\n",
      "Iteration 159, loss = 0.07612533\n",
      "Iteration 160, loss = 0.07404493\n",
      "Iteration 161, loss = 0.07401746\n",
      "Iteration 162, loss = 0.07214094\n",
      "Iteration 163, loss = 0.07744969\n",
      "Iteration 164, loss = 0.07422097\n",
      "Iteration 165, loss = 0.07104268\n",
      "Iteration 166, loss = 0.06900849\n",
      "Iteration 167, loss = 0.06731408\n",
      "Iteration 168, loss = 0.06784520\n",
      "Iteration 169, loss = 0.06654294\n",
      "Iteration 170, loss = 0.06498714\n",
      "Iteration 171, loss = 0.06534645\n",
      "Iteration 172, loss = 0.06317018\n",
      "Iteration 173, loss = 0.06376074\n",
      "Iteration 174, loss = 0.06328250\n",
      "Iteration 175, loss = 0.06376098\n",
      "Iteration 176, loss = 0.06308930\n",
      "Iteration 177, loss = 0.06021689\n",
      "Iteration 178, loss = 0.05986913\n",
      "Iteration 179, loss = 0.05978992\n",
      "Iteration 180, loss = 0.06060014\n",
      "Iteration 181, loss = 0.05939296\n",
      "Iteration 182, loss = 0.05804576\n",
      "Iteration 183, loss = 0.06119617\n",
      "Iteration 184, loss = 0.06228098\n",
      "Iteration 185, loss = 0.06479684\n",
      "Iteration 186, loss = 0.05979695\n",
      "Iteration 187, loss = 0.05854593\n",
      "Iteration 188, loss = 0.05497497\n",
      "Iteration 189, loss = 0.05473466\n",
      "Iteration 190, loss = 0.05656333\n",
      "Iteration 191, loss = 0.05881315\n",
      "Iteration 192, loss = 0.05563225\n",
      "Iteration 193, loss = 0.05548509\n",
      "Iteration 194, loss = 0.05295662\n",
      "Iteration 195, loss = 0.05300087\n",
      "Iteration 196, loss = 0.05158392\n",
      "Iteration 197, loss = 0.05298399\n",
      "Iteration 198, loss = 0.05064974\n",
      "Iteration 199, loss = 0.05551658\n",
      "Iteration 200, loss = 0.05360860\n",
      "Iteration 201, loss = 0.05022409\n",
      "Iteration 202, loss = 0.04873059\n",
      "Iteration 203, loss = 0.04936312\n",
      "Iteration 204, loss = 0.04838151\n",
      "Iteration 205, loss = 0.04680985\n",
      "Iteration 206, loss = 0.04684220\n",
      "Iteration 207, loss = 0.04612953\n",
      "Iteration 208, loss = 0.04664225\n",
      "Iteration 209, loss = 0.04744103\n",
      "Iteration 210, loss = 0.05054309\n",
      "Iteration 211, loss = 0.04591324\n",
      "Iteration 212, loss = 0.04564109\n",
      "Iteration 213, loss = 0.04483673\n",
      "Iteration 214, loss = 0.04310387\n",
      "Iteration 215, loss = 0.04409045\n",
      "Iteration 216, loss = 0.04438925\n",
      "Iteration 217, loss = 0.04347430\n",
      "Iteration 218, loss = 0.04909096\n",
      "Iteration 219, loss = 0.04881903\n",
      "Iteration 220, loss = 0.04531850\n",
      "Iteration 221, loss = 0.04152561\n",
      "Iteration 222, loss = 0.04114091\n",
      "Iteration 223, loss = 0.04218753\n",
      "Iteration 224, loss = 0.03984441\n",
      "Iteration 225, loss = 0.04117974\n",
      "Iteration 226, loss = 0.04012216\n",
      "Iteration 227, loss = 0.04355035\n",
      "Iteration 228, loss = 0.04207584\n",
      "Iteration 229, loss = 0.04035653\n",
      "Iteration 230, loss = 0.03824571\n",
      "Iteration 231, loss = 0.04191298\n",
      "Iteration 232, loss = 0.03952520\n",
      "Iteration 233, loss = 0.03763064\n",
      "Iteration 234, loss = 0.03772514\n",
      "Iteration 235, loss = 0.03607487\n",
      "Iteration 236, loss = 0.03702927\n",
      "Iteration 237, loss = 0.03634855\n",
      "Iteration 238, loss = 0.03670276\n",
      "Iteration 239, loss = 0.03719585\n",
      "Iteration 240, loss = 0.03570134\n",
      "Iteration 241, loss = 0.03587483\n",
      "Iteration 242, loss = 0.03467623\n",
      "Iteration 243, loss = 0.03518217\n",
      "Iteration 244, loss = 0.03588419\n",
      "Iteration 245, loss = 0.03546400\n",
      "Iteration 246, loss = 0.03425465\n",
      "Iteration 247, loss = 0.03380736\n",
      "Iteration 248, loss = 0.03275537\n",
      "Iteration 249, loss = 0.03577483\n",
      "Iteration 250, loss = 0.03239038\n",
      "Iteration 251, loss = 0.03771084\n",
      "Iteration 252, loss = 0.05612670\n",
      "Iteration 253, loss = 0.05657038\n",
      "Iteration 254, loss = 0.04801692\n",
      "Iteration 255, loss = 0.04696669\n",
      "Iteration 256, loss = 0.04294382\n",
      "Iteration 257, loss = 0.04068431\n",
      "Iteration 258, loss = 0.03808652\n",
      "Iteration 259, loss = 0.03589397\n",
      "Iteration 260, loss = 0.03612792\n",
      "Iteration 261, loss = 0.03636852\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 24 of 48...\n",
      "Iteration 1, loss = 0.65720811\n",
      "Iteration 2, loss = 0.53664566\n",
      "Iteration 3, loss = 0.48807171\n",
      "Iteration 4, loss = 0.46122354\n",
      "Iteration 5, loss = 0.43548208\n",
      "Iteration 6, loss = 0.42312923\n",
      "Iteration 7, loss = 0.40831829\n",
      "Iteration 8, loss = 0.40422013\n",
      "Iteration 9, loss = 0.38909359\n",
      "Iteration 10, loss = 0.37775493\n",
      "Iteration 11, loss = 0.37254433\n",
      "Iteration 12, loss = 0.36732378\n",
      "Iteration 13, loss = 0.36922573\n",
      "Iteration 14, loss = 0.35268798\n",
      "Iteration 15, loss = 0.34342369\n",
      "Iteration 16, loss = 0.33785344\n",
      "Iteration 17, loss = 0.33259112\n",
      "Iteration 18, loss = 0.33020258\n",
      "Iteration 19, loss = 0.32641209\n",
      "Iteration 20, loss = 0.32774228\n",
      "Iteration 21, loss = 0.31977364\n",
      "Iteration 22, loss = 0.32107272\n",
      "Iteration 23, loss = 0.31068021\n",
      "Iteration 24, loss = 0.30327465\n",
      "Iteration 25, loss = 0.29862666\n",
      "Iteration 26, loss = 0.30638389\n",
      "Iteration 27, loss = 0.30022667\n",
      "Iteration 28, loss = 0.30629222\n",
      "Iteration 29, loss = 0.28730785\n",
      "Iteration 30, loss = 0.28622361\n",
      "Iteration 31, loss = 0.28405704\n",
      "Iteration 32, loss = 0.28684596\n",
      "Iteration 33, loss = 0.28977942\n",
      "Iteration 34, loss = 0.27927586\n",
      "Iteration 35, loss = 0.27119200\n",
      "Iteration 36, loss = 0.27153199\n",
      "Iteration 37, loss = 0.26805209\n",
      "Iteration 38, loss = 0.26795115\n",
      "Iteration 39, loss = 0.25806216\n",
      "Iteration 40, loss = 0.25511135\n",
      "Iteration 41, loss = 0.25449587\n",
      "Iteration 42, loss = 0.24841767\n",
      "Iteration 43, loss = 0.25058225\n",
      "Iteration 44, loss = 0.25222787\n",
      "Iteration 45, loss = 0.24772608\n",
      "Iteration 46, loss = 0.24225922\n",
      "Iteration 47, loss = 0.24206197\n",
      "Iteration 48, loss = 0.23512929\n",
      "Iteration 49, loss = 0.23572852\n",
      "Iteration 50, loss = 0.23482504\n",
      "Iteration 51, loss = 0.23707237\n",
      "Iteration 52, loss = 0.23255919\n",
      "Iteration 53, loss = 0.22612603\n",
      "Iteration 54, loss = 0.22459390\n",
      "Iteration 55, loss = 0.22638147\n",
      "Iteration 56, loss = 0.22644827\n",
      "Iteration 57, loss = 0.22255863\n",
      "Iteration 58, loss = 0.22837595\n",
      "Iteration 59, loss = 0.21501147\n",
      "Iteration 60, loss = 0.21561439\n",
      "Iteration 61, loss = 0.21186293\n",
      "Iteration 62, loss = 0.21259982\n",
      "Iteration 63, loss = 0.20956361\n",
      "Iteration 64, loss = 0.20577528\n",
      "Iteration 65, loss = 0.20167954\n",
      "Iteration 66, loss = 0.20099726\n",
      "Iteration 67, loss = 0.19741280\n",
      "Iteration 68, loss = 0.19971893\n",
      "Iteration 69, loss = 0.19640235\n",
      "Iteration 70, loss = 0.19543601\n",
      "Iteration 71, loss = 0.18930644\n",
      "Iteration 72, loss = 0.19034592\n",
      "Iteration 73, loss = 0.18824027\n",
      "Iteration 74, loss = 0.18561053\n",
      "Iteration 75, loss = 0.18508752\n",
      "Iteration 76, loss = 0.18516641\n",
      "Iteration 77, loss = 0.18276028\n",
      "Iteration 78, loss = 0.18901674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79, loss = 0.18297333\n",
      "Iteration 80, loss = 0.17671342\n",
      "Iteration 81, loss = 0.17498542\n",
      "Iteration 82, loss = 0.17919520\n",
      "Iteration 83, loss = 0.17430261\n",
      "Iteration 84, loss = 0.17206781\n",
      "Iteration 85, loss = 0.17402030\n",
      "Iteration 86, loss = 0.18076729\n",
      "Iteration 87, loss = 0.17194469\n",
      "Iteration 88, loss = 0.16699018\n",
      "Iteration 89, loss = 0.17769531\n",
      "Iteration 90, loss = 0.16658475\n",
      "Iteration 91, loss = 0.16662404\n",
      "Iteration 92, loss = 0.16503867\n",
      "Iteration 93, loss = 0.16285088\n",
      "Iteration 94, loss = 0.15668877\n",
      "Iteration 95, loss = 0.15667788\n",
      "Iteration 96, loss = 0.15557282\n",
      "Iteration 97, loss = 0.16064129\n",
      "Iteration 98, loss = 0.16539408\n",
      "Iteration 99, loss = 0.16661553\n",
      "Iteration 100, loss = 0.15049166\n",
      "Iteration 101, loss = 0.14331235\n",
      "Iteration 102, loss = 0.15020197\n",
      "Iteration 103, loss = 0.14136260\n",
      "Iteration 104, loss = 0.14342324\n",
      "Iteration 105, loss = 0.14291195\n",
      "Iteration 106, loss = 0.14032620\n",
      "Iteration 107, loss = 0.13600451\n",
      "Iteration 108, loss = 0.13631637\n",
      "Iteration 109, loss = 0.13620097\n",
      "Iteration 110, loss = 0.13673103\n",
      "Iteration 111, loss = 0.13430036\n",
      "Iteration 112, loss = 0.13383134\n",
      "Iteration 113, loss = 0.13110707\n",
      "Iteration 114, loss = 0.12910018\n",
      "Iteration 115, loss = 0.13182325\n",
      "Iteration 116, loss = 0.12985558\n",
      "Iteration 117, loss = 0.12654837\n",
      "Iteration 118, loss = 0.12375287\n",
      "Iteration 119, loss = 0.12731674\n",
      "Iteration 120, loss = 0.12581423\n",
      "Iteration 121, loss = 0.12877427\n",
      "Iteration 122, loss = 0.12460357\n",
      "Iteration 123, loss = 0.12693155\n",
      "Iteration 124, loss = 0.12495987\n",
      "Iteration 125, loss = 0.12456899\n",
      "Iteration 126, loss = 0.12621479\n",
      "Iteration 127, loss = 0.12710150\n",
      "Iteration 128, loss = 0.13213165\n",
      "Iteration 129, loss = 0.13276270\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 25 of 48...\n",
      "Iteration 1, loss = 0.76510223\n",
      "Iteration 2, loss = 0.73992532\n",
      "Iteration 3, loss = 0.72551342\n",
      "Iteration 4, loss = 0.71552431\n",
      "Iteration 5, loss = 0.70829837\n",
      "Iteration 6, loss = 0.70314925\n",
      "Iteration 7, loss = 0.69925746\n",
      "Iteration 8, loss = 0.69660238\n",
      "Iteration 9, loss = 0.69448822\n",
      "Iteration 10, loss = 0.69293955\n",
      "Iteration 11, loss = 0.69203038\n",
      "Iteration 12, loss = 0.69109322\n",
      "Iteration 13, loss = 0.69041871\n",
      "Iteration 14, loss = 0.68904669\n",
      "Iteration 15, loss = 0.68727170\n",
      "Iteration 16, loss = 0.68437196\n",
      "Iteration 17, loss = 0.67982180\n",
      "Iteration 18, loss = 0.67524398\n",
      "Iteration 19, loss = 0.66998984\n",
      "Iteration 20, loss = 0.66481863\n",
      "Iteration 21, loss = 0.65966387\n",
      "Iteration 22, loss = 0.65395271\n",
      "Iteration 23, loss = 0.64838660\n",
      "Iteration 24, loss = 0.64200885\n",
      "Iteration 25, loss = 0.63459910\n",
      "Iteration 26, loss = 0.62603832\n",
      "Iteration 27, loss = 0.61650786\n",
      "Iteration 28, loss = 0.60658112\n",
      "Iteration 29, loss = 0.59612099\n",
      "Iteration 30, loss = 0.58533926\n",
      "Iteration 31, loss = 0.57474288\n",
      "Iteration 32, loss = 0.56359344\n",
      "Iteration 33, loss = 0.55325623\n",
      "Iteration 34, loss = 0.54335985\n",
      "Iteration 35, loss = 0.53337877\n",
      "Iteration 36, loss = 0.52272228\n",
      "Iteration 37, loss = 0.51323011\n",
      "Iteration 38, loss = 0.50483737\n",
      "Iteration 39, loss = 0.49606892\n",
      "Iteration 40, loss = 0.48779513\n",
      "Iteration 41, loss = 0.48063550\n",
      "Iteration 42, loss = 0.47274531\n",
      "Iteration 43, loss = 0.46598582\n",
      "Iteration 44, loss = 0.45922190\n",
      "Iteration 45, loss = 0.45234275\n",
      "Iteration 46, loss = 0.44623759\n",
      "Iteration 47, loss = 0.44056449\n",
      "Iteration 48, loss = 0.43627674\n",
      "Iteration 49, loss = 0.43018872\n",
      "Iteration 50, loss = 0.42483700\n",
      "Iteration 51, loss = 0.42096641\n",
      "Iteration 52, loss = 0.41841685\n",
      "Iteration 53, loss = 0.41158361\n",
      "Iteration 54, loss = 0.40867844\n",
      "Iteration 55, loss = 0.40522226\n",
      "Iteration 56, loss = 0.40123474\n",
      "Iteration 57, loss = 0.39704902\n",
      "Iteration 58, loss = 0.39432504\n",
      "Iteration 59, loss = 0.39052035\n",
      "Iteration 60, loss = 0.38815449\n",
      "Iteration 61, loss = 0.38448926\n",
      "Iteration 62, loss = 0.38144686\n",
      "Iteration 63, loss = 0.38016000\n",
      "Iteration 64, loss = 0.37609833\n",
      "Iteration 65, loss = 0.37382796\n",
      "Iteration 66, loss = 0.37212915\n",
      "Iteration 67, loss = 0.36889755\n",
      "Iteration 68, loss = 0.36666011\n",
      "Iteration 69, loss = 0.36604146\n",
      "Iteration 70, loss = 0.36257461\n",
      "Iteration 71, loss = 0.35987464\n",
      "Iteration 72, loss = 0.35765542\n",
      "Iteration 73, loss = 0.35583309\n",
      "Iteration 74, loss = 0.35385086\n",
      "Iteration 75, loss = 0.35217683\n",
      "Iteration 76, loss = 0.35000367\n",
      "Iteration 77, loss = 0.34749881\n",
      "Iteration 78, loss = 0.34592084\n",
      "Iteration 79, loss = 0.34395077\n",
      "Iteration 80, loss = 0.34323127\n",
      "Iteration 81, loss = 0.34051898\n",
      "Iteration 82, loss = 0.33849190\n",
      "Iteration 83, loss = 0.33735329\n",
      "Iteration 84, loss = 0.33488958\n",
      "Iteration 85, loss = 0.33364048\n",
      "Iteration 86, loss = 0.33388097\n",
      "Iteration 87, loss = 0.33018891\n",
      "Iteration 88, loss = 0.32863179\n",
      "Iteration 89, loss = 0.32737336\n",
      "Iteration 90, loss = 0.32624668\n",
      "Iteration 91, loss = 0.32411235\n",
      "Iteration 92, loss = 0.32287470\n",
      "Iteration 93, loss = 0.32127798\n",
      "Iteration 94, loss = 0.32019317\n",
      "Iteration 95, loss = 0.31981349\n",
      "Iteration 96, loss = 0.31723474\n",
      "Iteration 97, loss = 0.31553105\n",
      "Iteration 98, loss = 0.31441522\n",
      "Iteration 99, loss = 0.31348010\n",
      "Iteration 100, loss = 0.31158280\n",
      "Iteration 101, loss = 0.31095730\n",
      "Iteration 102, loss = 0.31110800\n",
      "Iteration 103, loss = 0.30823739\n",
      "Iteration 104, loss = 0.30664023\n",
      "Iteration 105, loss = 0.30566308\n",
      "Iteration 106, loss = 0.30568452\n",
      "Iteration 107, loss = 0.30350231\n",
      "Iteration 108, loss = 0.30176942\n",
      "Iteration 109, loss = 0.30129031\n",
      "Iteration 110, loss = 0.30151422\n",
      "Iteration 111, loss = 0.30519991\n",
      "Iteration 112, loss = 0.30110646\n",
      "Iteration 113, loss = 0.29815893\n",
      "Iteration 114, loss = 0.29446168\n",
      "Iteration 115, loss = 0.29381735\n",
      "Iteration 116, loss = 0.29301619\n",
      "Iteration 117, loss = 0.29172512\n",
      "Iteration 118, loss = 0.29151849\n",
      "Iteration 119, loss = 0.29201606\n",
      "Iteration 120, loss = 0.28855475\n",
      "Iteration 121, loss = 0.28685658\n",
      "Iteration 122, loss = 0.28646042\n",
      "Iteration 123, loss = 0.28502807\n",
      "Iteration 124, loss = 0.28341652\n",
      "Iteration 125, loss = 0.28263996\n",
      "Iteration 126, loss = 0.28233838\n",
      "Iteration 127, loss = 0.28028580\n",
      "Iteration 128, loss = 0.27909944\n",
      "Iteration 129, loss = 0.27874867\n",
      "Iteration 130, loss = 0.27761099\n",
      "Iteration 131, loss = 0.27686158\n",
      "Iteration 132, loss = 0.27537499\n",
      "Iteration 133, loss = 0.27462693\n",
      "Iteration 134, loss = 0.27269507\n",
      "Iteration 135, loss = 0.27140887\n",
      "Iteration 136, loss = 0.27144356\n",
      "Iteration 137, loss = 0.26998412\n",
      "Iteration 138, loss = 0.26877358\n",
      "Iteration 139, loss = 0.26747542\n",
      "Iteration 140, loss = 0.26732339\n",
      "Iteration 141, loss = 0.26645116\n",
      "Iteration 142, loss = 0.26491252\n",
      "Iteration 143, loss = 0.26352648\n",
      "Iteration 144, loss = 0.26307489\n",
      "Iteration 145, loss = 0.26235755\n",
      "Iteration 146, loss = 0.26077378\n",
      "Iteration 147, loss = 0.26164591\n",
      "Iteration 148, loss = 0.25846103\n",
      "Iteration 149, loss = 0.25994551\n",
      "Iteration 150, loss = 0.26030766\n",
      "Iteration 151, loss = 0.25821623\n",
      "Iteration 152, loss = 0.25668207\n",
      "Iteration 153, loss = 0.25513288\n",
      "Iteration 154, loss = 0.25549814\n",
      "Iteration 155, loss = 0.25318252\n",
      "Iteration 156, loss = 0.25260902\n",
      "Iteration 157, loss = 0.25176371\n",
      "Iteration 158, loss = 0.25153228\n",
      "Iteration 159, loss = 0.25022500\n",
      "Iteration 160, loss = 0.24972151\n",
      "Iteration 161, loss = 0.24757129\n",
      "Iteration 162, loss = 0.24824098\n",
      "Iteration 163, loss = 0.24533688\n",
      "Iteration 164, loss = 0.24581678\n",
      "Iteration 165, loss = 0.24527674\n",
      "Iteration 166, loss = 0.24396962\n",
      "Iteration 167, loss = 0.24273399\n",
      "Iteration 168, loss = 0.24253496\n",
      "Iteration 169, loss = 0.24246413\n",
      "Iteration 170, loss = 0.24066109\n",
      "Iteration 171, loss = 0.24004714\n",
      "Iteration 172, loss = 0.23941359\n",
      "Iteration 173, loss = 0.23774431\n",
      "Iteration 174, loss = 0.23723199\n",
      "Iteration 175, loss = 0.23598079\n",
      "Iteration 176, loss = 0.23489299\n",
      "Iteration 177, loss = 0.23468351\n",
      "Iteration 178, loss = 0.23457707\n",
      "Iteration 179, loss = 0.23274342\n",
      "Iteration 180, loss = 0.23237115\n",
      "Iteration 181, loss = 0.23107530\n",
      "Iteration 182, loss = 0.23058692\n",
      "Iteration 183, loss = 0.22956999\n",
      "Iteration 184, loss = 0.23036019\n",
      "Iteration 185, loss = 0.22945451\n",
      "Iteration 186, loss = 0.22695458\n",
      "Iteration 187, loss = 0.22631626\n",
      "Iteration 188, loss = 0.22749706\n",
      "Iteration 189, loss = 0.22469692\n",
      "Iteration 190, loss = 0.22392030\n",
      "Iteration 191, loss = 0.22349554\n",
      "Iteration 192, loss = 0.22214443\n",
      "Iteration 193, loss = 0.22227234\n",
      "Iteration 194, loss = 0.22156475\n",
      "Iteration 195, loss = 0.22082705\n",
      "Iteration 196, loss = 0.21962408\n",
      "Iteration 197, loss = 0.21862185\n",
      "Iteration 198, loss = 0.21779154\n",
      "Iteration 199, loss = 0.21727203\n",
      "Iteration 200, loss = 0.21688206\n",
      "Iteration 201, loss = 0.21568967\n",
      "Iteration 202, loss = 0.21531085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 203, loss = 0.21495941\n",
      "Iteration 204, loss = 0.21409392\n",
      "Iteration 205, loss = 0.21327334\n",
      "Iteration 206, loss = 0.21299573\n",
      "Iteration 207, loss = 0.21226650\n",
      "Iteration 208, loss = 0.21129692\n",
      "Iteration 209, loss = 0.21065370\n",
      "Iteration 210, loss = 0.20937557\n",
      "Iteration 211, loss = 0.20940187\n",
      "Iteration 212, loss = 0.20752750\n",
      "Iteration 213, loss = 0.20745301\n",
      "Iteration 214, loss = 0.20688026\n",
      "Iteration 215, loss = 0.20602708\n",
      "Iteration 216, loss = 0.20662539\n",
      "Iteration 217, loss = 0.20603235\n",
      "Iteration 218, loss = 0.20353997\n",
      "Iteration 219, loss = 0.20277507\n",
      "Iteration 220, loss = 0.20197703\n",
      "Iteration 221, loss = 0.20198844\n",
      "Iteration 222, loss = 0.20170447\n",
      "Iteration 223, loss = 0.20168379\n",
      "Iteration 224, loss = 0.19928240\n",
      "Iteration 225, loss = 0.19766558\n",
      "Iteration 226, loss = 0.19788219\n",
      "Iteration 227, loss = 0.19660379\n",
      "Iteration 228, loss = 0.19651800\n",
      "Iteration 229, loss = 0.19522077\n",
      "Iteration 230, loss = 0.19457935\n",
      "Iteration 231, loss = 0.19238814\n",
      "Iteration 232, loss = 0.19259881\n",
      "Iteration 233, loss = 0.19309801\n",
      "Iteration 234, loss = 0.19150453\n",
      "Iteration 235, loss = 0.18961720\n",
      "Iteration 236, loss = 0.18894702\n",
      "Iteration 237, loss = 0.18843839\n",
      "Iteration 238, loss = 0.18786414\n",
      "Iteration 239, loss = 0.18707432\n",
      "Iteration 240, loss = 0.18582585\n",
      "Iteration 241, loss = 0.18649583\n",
      "Iteration 242, loss = 0.18484932\n",
      "Iteration 243, loss = 0.18418111\n",
      "Iteration 244, loss = 0.18260915\n",
      "Iteration 245, loss = 0.18276166\n",
      "Iteration 246, loss = 0.18195964\n",
      "Iteration 247, loss = 0.18139391\n",
      "Iteration 248, loss = 0.18058973\n",
      "Iteration 249, loss = 0.18006218\n",
      "Iteration 250, loss = 0.17965687\n",
      "Iteration 251, loss = 0.17782052\n",
      "Iteration 252, loss = 0.17733177\n",
      "Iteration 253, loss = 0.17650798\n",
      "Iteration 254, loss = 0.17591409\n",
      "Iteration 255, loss = 0.17561763\n",
      "Iteration 256, loss = 0.17508748\n",
      "Iteration 257, loss = 0.17469452\n",
      "Iteration 258, loss = 0.17373071\n",
      "Iteration 259, loss = 0.17247656\n",
      "Iteration 260, loss = 0.17200971\n",
      "Iteration 261, loss = 0.17112135\n",
      "Iteration 262, loss = 0.17041533\n",
      "Iteration 263, loss = 0.17102077\n",
      "Iteration 264, loss = 0.17008964\n",
      "Iteration 265, loss = 0.16909392\n",
      "Iteration 266, loss = 0.16826332\n",
      "Iteration 267, loss = 0.16728971\n",
      "Iteration 268, loss = 0.16841765\n",
      "Iteration 269, loss = 0.16888988\n",
      "Iteration 270, loss = 0.16662705\n",
      "Iteration 271, loss = 0.16713234\n",
      "Iteration 272, loss = 0.16455429\n",
      "Iteration 273, loss = 0.16418821\n",
      "Iteration 274, loss = 0.16423336\n",
      "Iteration 275, loss = 0.16363717\n",
      "Iteration 276, loss = 0.16491466\n",
      "Iteration 277, loss = 0.16549800\n",
      "Iteration 278, loss = 0.16276272\n",
      "Iteration 279, loss = 0.16095493\n",
      "Iteration 280, loss = 0.16139090\n",
      "Iteration 281, loss = 0.16027844\n",
      "Iteration 282, loss = 0.15789264\n",
      "Iteration 283, loss = 0.15785700\n",
      "Iteration 284, loss = 0.15717930\n",
      "Iteration 285, loss = 0.15654689\n",
      "Iteration 286, loss = 0.15660143\n",
      "Iteration 287, loss = 0.15600046\n",
      "Iteration 288, loss = 0.15533352\n",
      "Iteration 289, loss = 0.15487397\n",
      "Iteration 290, loss = 0.15451654\n",
      "Iteration 291, loss = 0.15476210\n",
      "Iteration 292, loss = 0.15315547\n",
      "Iteration 293, loss = 0.15232916\n",
      "Iteration 294, loss = 0.15162180\n",
      "Iteration 295, loss = 0.15089750\n",
      "Iteration 296, loss = 0.14985393\n",
      "Iteration 297, loss = 0.15045273\n",
      "Iteration 298, loss = 0.14987685\n",
      "Iteration 299, loss = 0.14911339\n",
      "Iteration 300, loss = 0.14816373\n",
      "Iteration 301, loss = 0.14789394\n",
      "Iteration 302, loss = 0.14721691\n",
      "Iteration 303, loss = 0.14752250\n",
      "Iteration 304, loss = 0.14708020\n",
      "Iteration 305, loss = 0.14780506\n",
      "Iteration 306, loss = 0.14716344\n",
      "Iteration 307, loss = 0.14530516\n",
      "Iteration 308, loss = 0.14683958\n",
      "Iteration 309, loss = 0.14514982\n",
      "Iteration 310, loss = 0.14339359\n",
      "Iteration 311, loss = 0.14267856\n",
      "Iteration 312, loss = 0.14215734\n",
      "Iteration 313, loss = 0.14187058\n",
      "Iteration 314, loss = 0.14142068\n",
      "Iteration 315, loss = 0.14151206\n",
      "Iteration 316, loss = 0.14087147\n",
      "Iteration 317, loss = 0.14009185\n",
      "Iteration 318, loss = 0.13956192\n",
      "Iteration 319, loss = 0.13928939\n",
      "Iteration 320, loss = 0.13829378\n",
      "Iteration 321, loss = 0.13780351\n",
      "Iteration 322, loss = 0.13703409\n",
      "Iteration 323, loss = 0.13723251\n",
      "Iteration 324, loss = 0.13678467\n",
      "Iteration 325, loss = 0.13646589\n",
      "Iteration 326, loss = 0.13562434\n",
      "Iteration 327, loss = 0.13596749\n",
      "Iteration 328, loss = 0.13445009\n",
      "Iteration 329, loss = 0.13411216\n",
      "Iteration 330, loss = 0.13365971\n",
      "Iteration 331, loss = 0.13376508\n",
      "Iteration 332, loss = 0.13226303\n",
      "Iteration 333, loss = 0.13173576\n",
      "Iteration 334, loss = 0.13163741\n",
      "Iteration 335, loss = 0.13128187\n",
      "Iteration 336, loss = 0.13060533\n",
      "Iteration 337, loss = 0.13001294\n",
      "Iteration 338, loss = 0.12945171\n",
      "Iteration 339, loss = 0.12906425\n",
      "Iteration 340, loss = 0.12873002\n",
      "Iteration 341, loss = 0.12790197\n",
      "Iteration 342, loss = 0.12732185\n",
      "Iteration 343, loss = 0.12596460\n",
      "Iteration 344, loss = 0.12600112\n",
      "Iteration 345, loss = 0.12571937\n",
      "Iteration 346, loss = 0.12408259\n",
      "Iteration 347, loss = 0.12397862\n",
      "Iteration 348, loss = 0.12398656\n",
      "Iteration 349, loss = 0.12461515\n",
      "Iteration 350, loss = 0.12247585\n",
      "Iteration 351, loss = 0.12212857\n",
      "Iteration 352, loss = 0.12109315\n",
      "Iteration 353, loss = 0.12057288\n",
      "Iteration 354, loss = 0.11998081\n",
      "Iteration 355, loss = 0.11969798\n",
      "Iteration 356, loss = 0.12007027\n",
      "Iteration 357, loss = 0.11841637\n",
      "Iteration 358, loss = 0.11804395\n",
      "Iteration 359, loss = 0.11722625\n",
      "Iteration 360, loss = 0.11753848\n",
      "Iteration 361, loss = 0.11675388\n",
      "Iteration 362, loss = 0.11800338\n",
      "Iteration 363, loss = 0.11626239\n",
      "Iteration 364, loss = 0.11555410\n",
      "Iteration 365, loss = 0.11523385\n",
      "Iteration 366, loss = 0.11613072\n",
      "Iteration 367, loss = 0.11578095\n",
      "Iteration 368, loss = 0.11440868\n",
      "Iteration 369, loss = 0.11382102\n",
      "Iteration 370, loss = 0.11276591\n",
      "Iteration 371, loss = 0.11251329\n",
      "Iteration 372, loss = 0.11210950\n",
      "Iteration 373, loss = 0.11291920\n",
      "Iteration 374, loss = 0.11231648\n",
      "Iteration 375, loss = 0.11097716\n",
      "Iteration 376, loss = 0.11096069\n",
      "Iteration 377, loss = 0.11010093\n",
      "Iteration 378, loss = 0.11071942\n",
      "Iteration 379, loss = 0.11046614\n",
      "Iteration 380, loss = 0.11198394\n",
      "Iteration 381, loss = 0.11069949\n",
      "Iteration 382, loss = 0.10879549\n",
      "Iteration 383, loss = 0.10710520\n",
      "Iteration 384, loss = 0.10783926\n",
      "Iteration 385, loss = 0.10639376\n",
      "Iteration 386, loss = 0.10606706\n",
      "Iteration 387, loss = 0.10605849\n",
      "Iteration 388, loss = 0.10559069\n",
      "Iteration 389, loss = 0.10503972\n",
      "Iteration 390, loss = 0.10497434\n",
      "Iteration 391, loss = 0.10413942\n",
      "Iteration 392, loss = 0.10378208\n",
      "Iteration 393, loss = 0.10373475\n",
      "Iteration 394, loss = 0.10319445\n",
      "Iteration 395, loss = 0.10312343\n",
      "Iteration 396, loss = 0.10255215\n",
      "Iteration 397, loss = 0.10222528\n",
      "Iteration 398, loss = 0.10173730\n",
      "Iteration 399, loss = 0.10178554\n",
      "Iteration 400, loss = 0.10274838\n",
      "Iteration 401, loss = 0.10242658\n",
      "Iteration 402, loss = 0.10133929\n",
      "Iteration 403, loss = 0.10099362\n",
      "Iteration 404, loss = 0.10116187\n",
      "Iteration 405, loss = 0.10028952\n",
      "Iteration 406, loss = 0.09948595\n",
      "Iteration 407, loss = 0.09968573\n",
      "Iteration 408, loss = 0.09907528\n",
      "Iteration 409, loss = 0.09863251\n",
      "Iteration 410, loss = 0.09803357\n",
      "Iteration 411, loss = 0.09872214\n",
      "Iteration 412, loss = 0.09709399\n",
      "Iteration 413, loss = 0.09732299\n",
      "Iteration 414, loss = 0.09669800\n",
      "Iteration 415, loss = 0.09705169\n",
      "Iteration 416, loss = 0.09690752\n",
      "Iteration 417, loss = 0.09579110\n",
      "Iteration 418, loss = 0.09602699\n",
      "Iteration 419, loss = 0.09605448\n",
      "Iteration 420, loss = 0.09464524\n",
      "Iteration 421, loss = 0.09554912\n",
      "Iteration 422, loss = 0.09418048\n",
      "Iteration 423, loss = 0.09471889\n",
      "Iteration 424, loss = 0.09367532\n",
      "Iteration 425, loss = 0.09387620\n",
      "Iteration 426, loss = 0.09311058\n",
      "Iteration 427, loss = 0.09285349\n",
      "Iteration 428, loss = 0.09234933\n",
      "Iteration 429, loss = 0.09281809\n",
      "Iteration 430, loss = 0.09262424\n",
      "Iteration 431, loss = 0.09203990\n",
      "Iteration 432, loss = 0.09124483\n",
      "Iteration 433, loss = 0.09082175\n",
      "Iteration 434, loss = 0.09118104\n",
      "Iteration 435, loss = 0.09176187\n",
      "Iteration 436, loss = 0.09058165\n",
      "Iteration 437, loss = 0.09127081\n",
      "Iteration 438, loss = 0.09010154\n",
      "Iteration 439, loss = 0.08998219\n",
      "Iteration 440, loss = 0.08975669\n",
      "Iteration 441, loss = 0.08974897\n",
      "Iteration 442, loss = 0.08901499\n",
      "Iteration 443, loss = 0.08984001\n",
      "Iteration 444, loss = 0.08868054\n",
      "Iteration 445, loss = 0.08878842\n",
      "Iteration 446, loss = 0.08768502\n",
      "Iteration 447, loss = 0.08760512\n",
      "Iteration 448, loss = 0.08815819\n",
      "Iteration 449, loss = 0.08769706\n",
      "Iteration 450, loss = 0.08720231\n",
      "Iteration 451, loss = 0.08776059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 452, loss = 0.08650018\n",
      "Iteration 453, loss = 0.08616926\n",
      "Iteration 454, loss = 0.08584608\n",
      "Iteration 455, loss = 0.08568114\n",
      "Iteration 456, loss = 0.08631614\n",
      "Iteration 457, loss = 0.08562632\n",
      "Iteration 458, loss = 0.08496072\n",
      "Iteration 459, loss = 0.08513658\n",
      "Iteration 460, loss = 0.08452336\n",
      "Iteration 461, loss = 0.08462489\n",
      "Iteration 462, loss = 0.08460021\n",
      "Iteration 463, loss = 0.08370152\n",
      "Iteration 464, loss = 0.08412917\n",
      "Iteration 465, loss = 0.08420122\n",
      "Iteration 466, loss = 0.08322084\n",
      "Iteration 467, loss = 0.08368488\n",
      "Iteration 468, loss = 0.08298961\n",
      "Iteration 469, loss = 0.08290028\n",
      "Iteration 470, loss = 0.08226719\n",
      "Iteration 471, loss = 0.08181538\n",
      "Iteration 472, loss = 0.08157195\n",
      "Iteration 473, loss = 0.08146585\n",
      "Iteration 474, loss = 0.08184430\n",
      "Iteration 475, loss = 0.08127930\n",
      "Iteration 476, loss = 0.08067374\n",
      "Iteration 477, loss = 0.08079524\n",
      "Iteration 478, loss = 0.08121089\n",
      "Iteration 479, loss = 0.08105147\n",
      "Iteration 480, loss = 0.07968312\n",
      "Iteration 481, loss = 0.07921526\n",
      "Iteration 482, loss = 0.07863428\n",
      "Iteration 483, loss = 0.07753936\n",
      "Iteration 484, loss = 0.07856474\n",
      "Iteration 485, loss = 0.07757630\n",
      "Iteration 486, loss = 0.07602236\n",
      "Iteration 487, loss = 0.07535234\n",
      "Iteration 488, loss = 0.07522728\n",
      "Iteration 489, loss = 0.07441171\n",
      "Iteration 490, loss = 0.07374840\n",
      "Iteration 491, loss = 0.07334239\n",
      "Iteration 492, loss = 0.07301610\n",
      "Iteration 493, loss = 0.07256158\n",
      "Iteration 494, loss = 0.07208635\n",
      "Iteration 495, loss = 0.07174663\n",
      "Iteration 496, loss = 0.07114176\n",
      "Iteration 497, loss = 0.07052779\n",
      "Iteration 498, loss = 0.07011798\n",
      "Iteration 499, loss = 0.06983544\n",
      "Iteration 500, loss = 0.06925704\n",
      "Iteration 501, loss = 0.06955071\n",
      "Iteration 502, loss = 0.06955762\n",
      "Iteration 503, loss = 0.06899821\n",
      "Iteration 504, loss = 0.06972996\n",
      "Iteration 505, loss = 0.06864899\n",
      "Iteration 506, loss = 0.06739791\n",
      "Iteration 507, loss = 0.06704149\n",
      "Iteration 508, loss = 0.06713501\n",
      "Iteration 509, loss = 0.06719492\n",
      "Iteration 510, loss = 0.06745297\n",
      "Iteration 511, loss = 0.06627893\n",
      "Iteration 512, loss = 0.06604803\n",
      "Iteration 513, loss = 0.06630712\n",
      "Iteration 514, loss = 0.06576529\n",
      "Iteration 515, loss = 0.06552280\n",
      "Iteration 516, loss = 0.06513235\n",
      "Iteration 517, loss = 0.06499593\n",
      "Iteration 518, loss = 0.06525030\n",
      "Iteration 519, loss = 0.06447492\n",
      "Iteration 520, loss = 0.06497990\n",
      "Iteration 521, loss = 0.06481404\n",
      "Iteration 522, loss = 0.06463309\n",
      "Iteration 523, loss = 0.06351229\n",
      "Iteration 524, loss = 0.06334612\n",
      "Iteration 525, loss = 0.06294229\n",
      "Iteration 526, loss = 0.06347092\n",
      "Iteration 527, loss = 0.06331675\n",
      "Iteration 528, loss = 0.06244858\n",
      "Iteration 529, loss = 0.06222787\n",
      "Iteration 530, loss = 0.06226911\n",
      "Iteration 531, loss = 0.06211096\n",
      "Iteration 532, loss = 0.06194798\n",
      "Iteration 533, loss = 0.06190594\n",
      "Iteration 534, loss = 0.06119673\n",
      "Iteration 535, loss = 0.06166882\n",
      "Iteration 536, loss = 0.06121261\n",
      "Iteration 537, loss = 0.06126556\n",
      "Iteration 538, loss = 0.06145772\n",
      "Iteration 539, loss = 0.06059899\n",
      "Iteration 540, loss = 0.06036566\n",
      "Iteration 541, loss = 0.06051845\n",
      "Iteration 542, loss = 0.05964602\n",
      "Iteration 543, loss = 0.06011627\n",
      "Iteration 544, loss = 0.05961206\n",
      "Iteration 545, loss = 0.05936435\n",
      "Iteration 546, loss = 0.05885667\n",
      "Iteration 547, loss = 0.05868953\n",
      "Iteration 548, loss = 0.05865448\n",
      "Iteration 549, loss = 0.05928506\n",
      "Iteration 550, loss = 0.05870264\n",
      "Iteration 551, loss = 0.05833879\n",
      "Iteration 552, loss = 0.05771017\n",
      "Iteration 553, loss = 0.05779888\n",
      "Iteration 554, loss = 0.05823392\n",
      "Iteration 555, loss = 0.06030585\n",
      "Iteration 556, loss = 0.05767168\n",
      "Iteration 557, loss = 0.05675004\n",
      "Iteration 558, loss = 0.05733273\n",
      "Iteration 559, loss = 0.05697150\n",
      "Iteration 560, loss = 0.05623447\n",
      "Iteration 561, loss = 0.05572285\n",
      "Iteration 562, loss = 0.05638872\n",
      "Iteration 563, loss = 0.05599850\n",
      "Iteration 564, loss = 0.05650319\n",
      "Iteration 565, loss = 0.05550693\n",
      "Iteration 566, loss = 0.05500651\n",
      "Iteration 567, loss = 0.05483626\n",
      "Iteration 568, loss = 0.05468420\n",
      "Iteration 569, loss = 0.05436006\n",
      "Iteration 570, loss = 0.05454329\n",
      "Iteration 571, loss = 0.05457733\n",
      "Iteration 572, loss = 0.05519622\n",
      "Iteration 573, loss = 0.05468320\n",
      "Iteration 574, loss = 0.05473977\n",
      "Iteration 575, loss = 0.05411624\n",
      "Iteration 576, loss = 0.05365475\n",
      "Iteration 577, loss = 0.05361011\n",
      "Iteration 578, loss = 0.05352947\n",
      "Iteration 579, loss = 0.05327122\n",
      "Iteration 580, loss = 0.05281918\n",
      "Iteration 581, loss = 0.05291511\n",
      "Iteration 582, loss = 0.05292149\n",
      "Iteration 583, loss = 0.05190311\n",
      "Iteration 584, loss = 0.05106982\n",
      "Iteration 585, loss = 0.05103172\n",
      "Iteration 586, loss = 0.05168268\n",
      "Iteration 587, loss = 0.05041663\n",
      "Iteration 588, loss = 0.04998164\n",
      "Iteration 589, loss = 0.04975679\n",
      "Iteration 590, loss = 0.04937054\n",
      "Iteration 591, loss = 0.04956700\n",
      "Iteration 592, loss = 0.04903079\n",
      "Iteration 593, loss = 0.04892362\n",
      "Iteration 594, loss = 0.04850723\n",
      "Iteration 595, loss = 0.04831476\n",
      "Iteration 596, loss = 0.04822331\n",
      "Iteration 597, loss = 0.04769944\n",
      "Iteration 598, loss = 0.04776849\n",
      "Iteration 599, loss = 0.04754454\n",
      "Iteration 600, loss = 0.04686959\n",
      "Iteration 601, loss = 0.04652984\n",
      "Iteration 602, loss = 0.04676911\n",
      "Iteration 603, loss = 0.04631872\n",
      "Iteration 604, loss = 0.04575762\n",
      "Iteration 605, loss = 0.04555330\n",
      "Iteration 606, loss = 0.04585252\n",
      "Iteration 607, loss = 0.04490328\n",
      "Iteration 608, loss = 0.04493849\n",
      "Iteration 609, loss = 0.04404248\n",
      "Iteration 610, loss = 0.04376817\n",
      "Iteration 611, loss = 0.04314254\n",
      "Iteration 612, loss = 0.04326351\n",
      "Iteration 613, loss = 0.04281772\n",
      "Iteration 614, loss = 0.04253017\n",
      "Iteration 615, loss = 0.04251128\n",
      "Iteration 616, loss = 0.04165291\n",
      "Iteration 617, loss = 0.04162622\n",
      "Iteration 618, loss = 0.04190590\n",
      "Iteration 619, loss = 0.05423628\n",
      "Iteration 620, loss = 0.05222443\n",
      "Iteration 621, loss = 0.04290814\n",
      "Iteration 622, loss = 0.04144721\n",
      "Iteration 623, loss = 0.04026170\n",
      "Iteration 624, loss = 0.04014514\n",
      "Iteration 625, loss = 0.04041546\n",
      "Iteration 626, loss = 0.04023569\n",
      "Iteration 627, loss = 0.03982452\n",
      "Iteration 628, loss = 0.03949667\n",
      "Iteration 629, loss = 0.03921594\n",
      "Iteration 630, loss = 0.03890155\n",
      "Iteration 631, loss = 0.03932802\n",
      "Iteration 632, loss = 0.03906165\n",
      "Iteration 633, loss = 0.03908372\n",
      "Iteration 634, loss = 0.03819822\n",
      "Iteration 635, loss = 0.03753271\n",
      "Iteration 636, loss = 0.03746838\n",
      "Iteration 637, loss = 0.03743083\n",
      "Iteration 638, loss = 0.03789159\n",
      "Iteration 639, loss = 0.03693707\n",
      "Iteration 640, loss = 0.03645268\n",
      "Iteration 641, loss = 0.03665661\n",
      "Iteration 642, loss = 0.03624764\n",
      "Iteration 643, loss = 0.03646364\n",
      "Iteration 644, loss = 0.03601677\n",
      "Iteration 645, loss = 0.03627133\n",
      "Iteration 646, loss = 0.03565618\n",
      "Iteration 647, loss = 0.03541817\n",
      "Iteration 648, loss = 0.03515174\n",
      "Iteration 649, loss = 0.03575591\n",
      "Iteration 650, loss = 0.03557427\n",
      "Iteration 651, loss = 0.03526182\n",
      "Iteration 652, loss = 0.03469677\n",
      "Iteration 653, loss = 0.03480382\n",
      "Iteration 654, loss = 0.03423527\n",
      "Iteration 655, loss = 0.03478603\n",
      "Iteration 656, loss = 0.03440415\n",
      "Iteration 657, loss = 0.03395408\n",
      "Iteration 658, loss = 0.03423769\n",
      "Iteration 659, loss = 0.03427854\n",
      "Iteration 660, loss = 0.03368632\n",
      "Iteration 661, loss = 0.03375205\n",
      "Iteration 662, loss = 0.03352245\n",
      "Iteration 663, loss = 0.03364006\n",
      "Iteration 664, loss = 0.03346417\n",
      "Iteration 665, loss = 0.03315942\n",
      "Iteration 666, loss = 0.03329003\n",
      "Iteration 667, loss = 0.03310294\n",
      "Iteration 668, loss = 0.03281788\n",
      "Iteration 669, loss = 0.03281832\n",
      "Iteration 670, loss = 0.03333026\n",
      "Iteration 671, loss = 0.03276382\n",
      "Iteration 672, loss = 0.03254398\n",
      "Iteration 673, loss = 0.03248554\n",
      "Iteration 674, loss = 0.03260840\n",
      "Iteration 675, loss = 0.03265188\n",
      "Iteration 676, loss = 0.03221560\n",
      "Iteration 677, loss = 0.03234657\n",
      "Iteration 678, loss = 0.03204633\n",
      "Iteration 679, loss = 0.03214691\n",
      "Iteration 680, loss = 0.03244909\n",
      "Iteration 681, loss = 0.03207474\n",
      "Iteration 682, loss = 0.03172753\n",
      "Iteration 683, loss = 0.03152754\n",
      "Iteration 684, loss = 0.03111056\n",
      "Iteration 685, loss = 0.03108930\n",
      "Iteration 686, loss = 0.03053427\n",
      "Iteration 687, loss = 0.03057372\n",
      "Iteration 688, loss = 0.03056332\n",
      "Iteration 689, loss = 0.03016687\n",
      "Iteration 690, loss = 0.03033081\n",
      "Iteration 691, loss = 0.02980102\n",
      "Iteration 692, loss = 0.02945649\n",
      "Iteration 693, loss = 0.02884779\n",
      "Iteration 694, loss = 0.02922542\n",
      "Iteration 695, loss = 0.02867522\n",
      "Iteration 696, loss = 0.02878889\n",
      "Iteration 697, loss = 0.02891986\n",
      "Iteration 698, loss = 0.02883111\n",
      "Iteration 699, loss = 0.02851150\n",
      "Iteration 700, loss = 0.02834819\n",
      "Iteration 701, loss = 0.02822770\n",
      "Iteration 702, loss = 0.02831197\n",
      "Iteration 703, loss = 0.02807253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 704, loss = 0.02803886\n",
      "Iteration 705, loss = 0.02812401\n",
      "Iteration 706, loss = 0.02803826\n",
      "Iteration 707, loss = 0.02773309\n",
      "Iteration 708, loss = 0.02747526\n",
      "Iteration 709, loss = 0.02755233\n",
      "Iteration 710, loss = 0.02745592\n",
      "Iteration 711, loss = 0.02735928\n",
      "Iteration 712, loss = 0.02745132\n",
      "Iteration 713, loss = 0.02723251\n",
      "Iteration 714, loss = 0.02777102\n",
      "Iteration 715, loss = 0.02718114\n",
      "Iteration 716, loss = 0.02697599\n",
      "Iteration 717, loss = 0.02701686\n",
      "Iteration 718, loss = 0.02692856\n",
      "Iteration 719, loss = 0.02684340\n",
      "Iteration 720, loss = 0.02672540\n",
      "Iteration 721, loss = 0.02644094\n",
      "Iteration 722, loss = 0.02640824\n",
      "Iteration 723, loss = 0.02617253\n",
      "Iteration 724, loss = 0.02670351\n",
      "Iteration 725, loss = 0.02620386\n",
      "Iteration 726, loss = 0.02633613\n",
      "Iteration 727, loss = 0.02618834\n",
      "Iteration 728, loss = 0.02595252\n",
      "Iteration 729, loss = 0.02568325\n",
      "Iteration 730, loss = 0.02568365\n",
      "Iteration 731, loss = 0.02563468\n",
      "Iteration 732, loss = 0.02560557\n",
      "Iteration 733, loss = 0.02557533\n",
      "Iteration 734, loss = 0.02573414\n",
      "Iteration 735, loss = 0.02548706\n",
      "Iteration 736, loss = 0.02520389\n",
      "Iteration 737, loss = 0.02505909\n",
      "Iteration 738, loss = 0.02492910\n",
      "Iteration 739, loss = 0.02510939\n",
      "Iteration 740, loss = 0.02503393\n",
      "Iteration 741, loss = 0.02479276\n",
      "Iteration 742, loss = 0.02481415\n",
      "Iteration 743, loss = 0.02492305\n",
      "Iteration 744, loss = 0.02454886\n",
      "Iteration 745, loss = 0.02432228\n",
      "Iteration 746, loss = 0.02450612\n",
      "Iteration 747, loss = 0.02416356\n",
      "Iteration 748, loss = 0.02410768\n",
      "Iteration 749, loss = 0.02409520\n",
      "Iteration 750, loss = 0.02404075\n",
      "Iteration 751, loss = 0.02371987\n",
      "Iteration 752, loss = 0.02379853\n",
      "Iteration 753, loss = 0.02366960\n",
      "Iteration 754, loss = 0.02339133\n",
      "Iteration 755, loss = 0.02346564\n",
      "Iteration 756, loss = 0.02362798\n",
      "Iteration 757, loss = 0.02344138\n",
      "Iteration 758, loss = 0.02344702\n",
      "Iteration 759, loss = 0.02344311\n",
      "Iteration 760, loss = 0.02302054\n",
      "Iteration 761, loss = 0.02292600\n",
      "Iteration 762, loss = 0.02296679\n",
      "Iteration 763, loss = 0.02271136\n",
      "Iteration 764, loss = 0.02254858\n",
      "Iteration 765, loss = 0.02173752\n",
      "Iteration 766, loss = 0.02144190\n",
      "Iteration 767, loss = 0.02131280\n",
      "Iteration 768, loss = 0.02138777\n",
      "Iteration 769, loss = 0.02129921\n",
      "Iteration 770, loss = 0.02159553\n",
      "Iteration 771, loss = 0.02122774\n",
      "Iteration 772, loss = 0.02134427\n",
      "Iteration 773, loss = 0.02105984\n",
      "Iteration 774, loss = 0.02100699\n",
      "Iteration 775, loss = 0.02100616\n",
      "Iteration 776, loss = 0.02111442\n",
      "Iteration 777, loss = 0.02097387\n",
      "Iteration 778, loss = 0.02082234\n",
      "Iteration 779, loss = 0.02078723\n",
      "Iteration 780, loss = 0.02083597\n",
      "Iteration 781, loss = 0.02125257\n",
      "Iteration 782, loss = 0.02141159\n",
      "Iteration 783, loss = 0.02097951\n",
      "Iteration 784, loss = 0.02072289\n",
      "Iteration 785, loss = 0.02140488\n",
      "Iteration 786, loss = 0.02154207\n",
      "Iteration 787, loss = 0.02062764\n",
      "Iteration 788, loss = 0.02062212\n",
      "Iteration 789, loss = 0.02031593\n",
      "Iteration 790, loss = 0.02065948\n",
      "Iteration 791, loss = 0.02049395\n",
      "Iteration 792, loss = 0.02071006\n",
      "Iteration 793, loss = 0.02054814\n",
      "Iteration 794, loss = 0.02028544\n",
      "Iteration 795, loss = 0.02012951\n",
      "Iteration 796, loss = 0.02049736\n",
      "Iteration 797, loss = 0.02004524\n",
      "Iteration 798, loss = 0.02006379\n",
      "Iteration 799, loss = 0.02009178\n",
      "Iteration 800, loss = 0.01990526\n",
      "Iteration 801, loss = 0.02001961\n",
      "Iteration 802, loss = 0.02025403\n",
      "Iteration 803, loss = 0.01980478\n",
      "Iteration 804, loss = 0.02008982\n",
      "Iteration 805, loss = 0.01989338\n",
      "Iteration 806, loss = 0.01985917\n",
      "Iteration 807, loss = 0.01975453\n",
      "Iteration 808, loss = 0.02000830\n",
      "Iteration 809, loss = 0.01983282\n",
      "Iteration 810, loss = 0.01962191\n",
      "Iteration 811, loss = 0.01958534\n",
      "Iteration 812, loss = 0.01965997\n",
      "Iteration 813, loss = 0.01961041\n",
      "Iteration 814, loss = 0.01953704\n",
      "Iteration 815, loss = 0.01952024\n",
      "Iteration 816, loss = 0.01939901\n",
      "Iteration 817, loss = 0.01941769\n",
      "Iteration 818, loss = 0.01927877\n",
      "Iteration 819, loss = 0.01940347\n",
      "Iteration 820, loss = 0.01940135\n",
      "Iteration 821, loss = 0.01929382\n",
      "Iteration 822, loss = 0.01919626\n",
      "Iteration 823, loss = 0.01920576\n",
      "Iteration 824, loss = 0.01934855\n",
      "Iteration 825, loss = 0.01933991\n",
      "Iteration 826, loss = 0.01918177\n",
      "Iteration 827, loss = 0.01920312\n",
      "Iteration 828, loss = 0.01914453\n",
      "Iteration 829, loss = 0.01914395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 26 of 48...\n",
      "Iteration 1, loss = 0.72516277\n",
      "Iteration 2, loss = 0.70600987\n",
      "Iteration 3, loss = 0.69761189\n",
      "Iteration 4, loss = 0.69310997\n",
      "Iteration 5, loss = 0.68943223\n",
      "Iteration 6, loss = 0.68659020\n",
      "Iteration 7, loss = 0.68322573\n",
      "Iteration 8, loss = 0.67864126\n",
      "Iteration 9, loss = 0.67348816\n",
      "Iteration 10, loss = 0.66737999\n",
      "Iteration 11, loss = 0.66088209\n",
      "Iteration 12, loss = 0.65385387\n",
      "Iteration 13, loss = 0.64605940\n",
      "Iteration 14, loss = 0.63808972\n",
      "Iteration 15, loss = 0.62968388\n",
      "Iteration 16, loss = 0.62049689\n",
      "Iteration 17, loss = 0.61122305\n",
      "Iteration 18, loss = 0.60131222\n",
      "Iteration 19, loss = 0.59140773\n",
      "Iteration 20, loss = 0.58104803\n",
      "Iteration 21, loss = 0.57013126\n",
      "Iteration 22, loss = 0.55961606\n",
      "Iteration 23, loss = 0.54928279\n",
      "Iteration 24, loss = 0.53991316\n",
      "Iteration 25, loss = 0.52996265\n",
      "Iteration 26, loss = 0.52006725\n",
      "Iteration 27, loss = 0.50930882\n",
      "Iteration 28, loss = 0.50169414\n",
      "Iteration 29, loss = 0.49285910\n",
      "Iteration 30, loss = 0.48493798\n",
      "Iteration 31, loss = 0.47719467\n",
      "Iteration 32, loss = 0.46992560\n",
      "Iteration 33, loss = 0.46282525\n",
      "Iteration 34, loss = 0.45687945\n",
      "Iteration 35, loss = 0.45072562\n",
      "Iteration 36, loss = 0.44522509\n",
      "Iteration 37, loss = 0.44011505\n",
      "Iteration 38, loss = 0.43457197\n",
      "Iteration 39, loss = 0.43001937\n",
      "Iteration 40, loss = 0.42497049\n",
      "Iteration 41, loss = 0.42123296\n",
      "Iteration 42, loss = 0.41654599\n",
      "Iteration 43, loss = 0.41283894\n",
      "Iteration 44, loss = 0.40904895\n",
      "Iteration 45, loss = 0.40619384\n",
      "Iteration 46, loss = 0.40221118\n",
      "Iteration 47, loss = 0.39960851\n",
      "Iteration 48, loss = 0.39583386\n",
      "Iteration 49, loss = 0.39509728\n",
      "Iteration 50, loss = 0.39218402\n",
      "Iteration 51, loss = 0.38740785\n",
      "Iteration 52, loss = 0.38393885\n",
      "Iteration 53, loss = 0.38485174\n",
      "Iteration 54, loss = 0.38287095\n",
      "Iteration 55, loss = 0.37907213\n",
      "Iteration 56, loss = 0.37651492\n",
      "Iteration 57, loss = 0.37381975\n",
      "Iteration 58, loss = 0.37023453\n",
      "Iteration 59, loss = 0.37090567\n",
      "Iteration 60, loss = 0.36933369\n",
      "Iteration 61, loss = 0.36719233\n",
      "Iteration 62, loss = 0.36532507\n",
      "Iteration 63, loss = 0.36155999\n",
      "Iteration 64, loss = 0.35977819\n",
      "Iteration 65, loss = 0.35748584\n",
      "Iteration 66, loss = 0.35589825\n",
      "Iteration 67, loss = 0.35487166\n",
      "Iteration 68, loss = 0.35287371\n",
      "Iteration 69, loss = 0.35045871\n",
      "Iteration 70, loss = 0.34940500\n",
      "Iteration 71, loss = 0.34717264\n",
      "Iteration 72, loss = 0.34569121\n",
      "Iteration 73, loss = 0.34485161\n",
      "Iteration 74, loss = 0.34295824\n",
      "Iteration 75, loss = 0.34145186\n",
      "Iteration 76, loss = 0.33940877\n",
      "Iteration 77, loss = 0.34035636\n",
      "Iteration 78, loss = 0.33923860\n",
      "Iteration 79, loss = 0.33615117\n",
      "Iteration 80, loss = 0.33473043\n",
      "Iteration 81, loss = 0.33315588\n",
      "Iteration 82, loss = 0.33261365\n",
      "Iteration 83, loss = 0.33078093\n",
      "Iteration 84, loss = 0.33039114\n",
      "Iteration 85, loss = 0.32801915\n",
      "Iteration 86, loss = 0.32709127\n",
      "Iteration 87, loss = 0.32606343\n",
      "Iteration 88, loss = 0.32819986\n",
      "Iteration 89, loss = 0.32450333\n",
      "Iteration 90, loss = 0.32212450\n",
      "Iteration 91, loss = 0.32261547\n",
      "Iteration 92, loss = 0.31998227\n",
      "Iteration 93, loss = 0.31827828\n",
      "Iteration 94, loss = 0.31863756\n",
      "Iteration 95, loss = 0.31560319\n",
      "Iteration 96, loss = 0.31615683\n",
      "Iteration 97, loss = 0.31644453\n",
      "Iteration 98, loss = 0.31597558\n",
      "Iteration 99, loss = 0.31406442\n",
      "Iteration 100, loss = 0.31182308\n",
      "Iteration 101, loss = 0.30973868\n",
      "Iteration 102, loss = 0.30840262\n",
      "Iteration 103, loss = 0.30798971\n",
      "Iteration 104, loss = 0.30654013\n",
      "Iteration 105, loss = 0.30691118\n",
      "Iteration 106, loss = 0.30427549\n",
      "Iteration 107, loss = 0.30412639\n",
      "Iteration 108, loss = 0.30347941\n",
      "Iteration 109, loss = 0.30131362\n",
      "Iteration 110, loss = 0.30073796\n",
      "Iteration 111, loss = 0.30096789\n",
      "Iteration 112, loss = 0.30123776\n",
      "Iteration 113, loss = 0.29822249\n",
      "Iteration 114, loss = 0.29662171\n",
      "Iteration 115, loss = 0.29628433\n",
      "Iteration 116, loss = 0.29824570\n",
      "Iteration 117, loss = 0.29588860\n",
      "Iteration 118, loss = 0.29473060\n",
      "Iteration 119, loss = 0.29434003\n",
      "Iteration 120, loss = 0.29222515\n",
      "Iteration 121, loss = 0.29118137\n",
      "Iteration 122, loss = 0.29113800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 123, loss = 0.29019523\n",
      "Iteration 124, loss = 0.28802929\n",
      "Iteration 125, loss = 0.28774466\n",
      "Iteration 126, loss = 0.28642309\n",
      "Iteration 127, loss = 0.28757611\n",
      "Iteration 128, loss = 0.28627200\n",
      "Iteration 129, loss = 0.28604062\n",
      "Iteration 130, loss = 0.28588185\n",
      "Iteration 131, loss = 0.28217169\n",
      "Iteration 132, loss = 0.28426793\n",
      "Iteration 133, loss = 0.28146971\n",
      "Iteration 134, loss = 0.28133001\n",
      "Iteration 135, loss = 0.28047840\n",
      "Iteration 136, loss = 0.27809408\n",
      "Iteration 137, loss = 0.27891585\n",
      "Iteration 138, loss = 0.27701616\n",
      "Iteration 139, loss = 0.27857544\n",
      "Iteration 140, loss = 0.27684687\n",
      "Iteration 141, loss = 0.27544607\n",
      "Iteration 142, loss = 0.27423569\n",
      "Iteration 143, loss = 0.27343531\n",
      "Iteration 144, loss = 0.27206869\n",
      "Iteration 145, loss = 0.27285401\n",
      "Iteration 146, loss = 0.27131742\n",
      "Iteration 147, loss = 0.27031972\n",
      "Iteration 148, loss = 0.26900057\n",
      "Iteration 149, loss = 0.26808103\n",
      "Iteration 150, loss = 0.26747168\n",
      "Iteration 151, loss = 0.26666288\n",
      "Iteration 152, loss = 0.26686650\n",
      "Iteration 153, loss = 0.26537199\n",
      "Iteration 154, loss = 0.26474975\n",
      "Iteration 155, loss = 0.26421467\n",
      "Iteration 156, loss = 0.26307664\n",
      "Iteration 157, loss = 0.26364816\n",
      "Iteration 158, loss = 0.26564552\n",
      "Iteration 159, loss = 0.26222756\n",
      "Iteration 160, loss = 0.25972778\n",
      "Iteration 161, loss = 0.25812072\n",
      "Iteration 162, loss = 0.25844678\n",
      "Iteration 163, loss = 0.25763736\n",
      "Iteration 164, loss = 0.25587304\n",
      "Iteration 165, loss = 0.25587779\n",
      "Iteration 166, loss = 0.25344956\n",
      "Iteration 167, loss = 0.25322031\n",
      "Iteration 168, loss = 0.25335479\n",
      "Iteration 169, loss = 0.25133424\n",
      "Iteration 170, loss = 0.25027862\n",
      "Iteration 171, loss = 0.24878884\n",
      "Iteration 172, loss = 0.24813188\n",
      "Iteration 173, loss = 0.24838832\n",
      "Iteration 174, loss = 0.24893395\n",
      "Iteration 175, loss = 0.24633967\n",
      "Iteration 176, loss = 0.24654276\n",
      "Iteration 177, loss = 0.24492950\n",
      "Iteration 178, loss = 0.24283031\n",
      "Iteration 179, loss = 0.24299158\n",
      "Iteration 180, loss = 0.24124467\n",
      "Iteration 181, loss = 0.24065992\n",
      "Iteration 182, loss = 0.24203761\n",
      "Iteration 183, loss = 0.24114078\n",
      "Iteration 184, loss = 0.24128194\n",
      "Iteration 185, loss = 0.23806752\n",
      "Iteration 186, loss = 0.24056972\n",
      "Iteration 187, loss = 0.23946617\n",
      "Iteration 188, loss = 0.23623320\n",
      "Iteration 189, loss = 0.23702053\n",
      "Iteration 190, loss = 0.23621020\n",
      "Iteration 191, loss = 0.23310742\n",
      "Iteration 192, loss = 0.23208770\n",
      "Iteration 193, loss = 0.23130550\n",
      "Iteration 194, loss = 0.23098865\n",
      "Iteration 195, loss = 0.23042114\n",
      "Iteration 196, loss = 0.22949314\n",
      "Iteration 197, loss = 0.22877735\n",
      "Iteration 198, loss = 0.22794961\n",
      "Iteration 199, loss = 0.22746291\n",
      "Iteration 200, loss = 0.22733999\n",
      "Iteration 201, loss = 0.22551187\n",
      "Iteration 202, loss = 0.22457202\n",
      "Iteration 203, loss = 0.22352863\n",
      "Iteration 204, loss = 0.22302629\n",
      "Iteration 205, loss = 0.22197373\n",
      "Iteration 206, loss = 0.22195302\n",
      "Iteration 207, loss = 0.22253310\n",
      "Iteration 208, loss = 0.22073335\n",
      "Iteration 209, loss = 0.22012044\n",
      "Iteration 210, loss = 0.21877381\n",
      "Iteration 211, loss = 0.21853096\n",
      "Iteration 212, loss = 0.21804144\n",
      "Iteration 213, loss = 0.21664281\n",
      "Iteration 214, loss = 0.21603330\n",
      "Iteration 215, loss = 0.21590294\n",
      "Iteration 216, loss = 0.21507580\n",
      "Iteration 217, loss = 0.21352576\n",
      "Iteration 218, loss = 0.21318706\n",
      "Iteration 219, loss = 0.21447909\n",
      "Iteration 220, loss = 0.21156427\n",
      "Iteration 221, loss = 0.21249211\n",
      "Iteration 222, loss = 0.20958511\n",
      "Iteration 223, loss = 0.21005438\n",
      "Iteration 224, loss = 0.20993537\n",
      "Iteration 225, loss = 0.20819575\n",
      "Iteration 226, loss = 0.20782453\n",
      "Iteration 227, loss = 0.20646664\n",
      "Iteration 228, loss = 0.20528720\n",
      "Iteration 229, loss = 0.20523464\n",
      "Iteration 230, loss = 0.20483169\n",
      "Iteration 231, loss = 0.20405923\n",
      "Iteration 232, loss = 0.20368784\n",
      "Iteration 233, loss = 0.20239489\n",
      "Iteration 234, loss = 0.20177273\n",
      "Iteration 235, loss = 0.20177385\n",
      "Iteration 236, loss = 0.20009700\n",
      "Iteration 237, loss = 0.20372073\n",
      "Iteration 238, loss = 0.20051734\n",
      "Iteration 239, loss = 0.19904928\n",
      "Iteration 240, loss = 0.19854088\n",
      "Iteration 241, loss = 0.19838321\n",
      "Iteration 242, loss = 0.19689141\n",
      "Iteration 243, loss = 0.19530592\n",
      "Iteration 244, loss = 0.19509187\n",
      "Iteration 245, loss = 0.19466870\n",
      "Iteration 246, loss = 0.19482835\n",
      "Iteration 247, loss = 0.19335469\n",
      "Iteration 248, loss = 0.19274051\n",
      "Iteration 249, loss = 0.19556515\n",
      "Iteration 250, loss = 0.19177085\n",
      "Iteration 251, loss = 0.19045066\n",
      "Iteration 252, loss = 0.18866257\n",
      "Iteration 253, loss = 0.18856178\n",
      "Iteration 254, loss = 0.18838661\n",
      "Iteration 255, loss = 0.18691847\n",
      "Iteration 256, loss = 0.18602731\n",
      "Iteration 257, loss = 0.18623295\n",
      "Iteration 258, loss = 0.18557449\n",
      "Iteration 259, loss = 0.18420666\n",
      "Iteration 260, loss = 0.18288251\n",
      "Iteration 261, loss = 0.18256202\n",
      "Iteration 262, loss = 0.18318427\n",
      "Iteration 263, loss = 0.18315059\n",
      "Iteration 264, loss = 0.18321377\n",
      "Iteration 265, loss = 0.18020254\n",
      "Iteration 266, loss = 0.17923962\n",
      "Iteration 267, loss = 0.17829947\n",
      "Iteration 268, loss = 0.17939927\n",
      "Iteration 269, loss = 0.17799137\n",
      "Iteration 270, loss = 0.17759933\n",
      "Iteration 271, loss = 0.17572072\n",
      "Iteration 272, loss = 0.17468200\n",
      "Iteration 273, loss = 0.17598402\n",
      "Iteration 274, loss = 0.17411473\n",
      "Iteration 275, loss = 0.17423887\n",
      "Iteration 276, loss = 0.17269552\n",
      "Iteration 277, loss = 0.17200007\n",
      "Iteration 278, loss = 0.17092513\n",
      "Iteration 279, loss = 0.17017582\n",
      "Iteration 280, loss = 0.17041980\n",
      "Iteration 281, loss = 0.17077915\n",
      "Iteration 282, loss = 0.16938917\n",
      "Iteration 283, loss = 0.16992117\n",
      "Iteration 284, loss = 0.16828389\n",
      "Iteration 285, loss = 0.16790919\n",
      "Iteration 286, loss = 0.16659641\n",
      "Iteration 287, loss = 0.16661888\n",
      "Iteration 288, loss = 0.16702712\n",
      "Iteration 289, loss = 0.16573047\n",
      "Iteration 290, loss = 0.16447933\n",
      "Iteration 291, loss = 0.16405478\n",
      "Iteration 292, loss = 0.16287989\n",
      "Iteration 293, loss = 0.16286681\n",
      "Iteration 294, loss = 0.16201234\n",
      "Iteration 295, loss = 0.16227332\n",
      "Iteration 296, loss = 0.16078747\n",
      "Iteration 297, loss = 0.16140021\n",
      "Iteration 298, loss = 0.16084572\n",
      "Iteration 299, loss = 0.15934736\n",
      "Iteration 300, loss = 0.15956565\n",
      "Iteration 301, loss = 0.15819208\n",
      "Iteration 302, loss = 0.16030259\n",
      "Iteration 303, loss = 0.15948196\n",
      "Iteration 304, loss = 0.15657115\n",
      "Iteration 305, loss = 0.15608194\n",
      "Iteration 306, loss = 0.15647302\n",
      "Iteration 307, loss = 0.15610488\n",
      "Iteration 308, loss = 0.15475675\n",
      "Iteration 309, loss = 0.15405737\n",
      "Iteration 310, loss = 0.15333608\n",
      "Iteration 311, loss = 0.15249864\n",
      "Iteration 312, loss = 0.15407709\n",
      "Iteration 313, loss = 0.15297935\n",
      "Iteration 314, loss = 0.15193883\n",
      "Iteration 315, loss = 0.15214220\n",
      "Iteration 316, loss = 0.15019847\n",
      "Iteration 317, loss = 0.14964876\n",
      "Iteration 318, loss = 0.14910971\n",
      "Iteration 319, loss = 0.14938617\n",
      "Iteration 320, loss = 0.14840785\n",
      "Iteration 321, loss = 0.14718430\n",
      "Iteration 322, loss = 0.14792686\n",
      "Iteration 323, loss = 0.14877294\n",
      "Iteration 324, loss = 0.14582505\n",
      "Iteration 325, loss = 0.14547732\n",
      "Iteration 326, loss = 0.14540476\n",
      "Iteration 327, loss = 0.14586619\n",
      "Iteration 328, loss = 0.14407020\n",
      "Iteration 329, loss = 0.14365866\n",
      "Iteration 330, loss = 0.14230620\n",
      "Iteration 331, loss = 0.14242041\n",
      "Iteration 332, loss = 0.14205751\n",
      "Iteration 333, loss = 0.14110920\n",
      "Iteration 334, loss = 0.14081193\n",
      "Iteration 335, loss = 0.14154323\n",
      "Iteration 336, loss = 0.14162448\n",
      "Iteration 337, loss = 0.13987867\n",
      "Iteration 338, loss = 0.13979729\n",
      "Iteration 339, loss = 0.13937539\n",
      "Iteration 340, loss = 0.13804294\n",
      "Iteration 341, loss = 0.13851122\n",
      "Iteration 342, loss = 0.13814090\n",
      "Iteration 343, loss = 0.13677732\n",
      "Iteration 344, loss = 0.13606873\n",
      "Iteration 345, loss = 0.13574879\n",
      "Iteration 346, loss = 0.13535246\n",
      "Iteration 347, loss = 0.13485837\n",
      "Iteration 348, loss = 0.13404558\n",
      "Iteration 349, loss = 0.13398464\n",
      "Iteration 350, loss = 0.13263277\n",
      "Iteration 351, loss = 0.13218066\n",
      "Iteration 352, loss = 0.13230539\n",
      "Iteration 353, loss = 0.13161623\n",
      "Iteration 354, loss = 0.13092491\n",
      "Iteration 355, loss = 0.13006724\n",
      "Iteration 356, loss = 0.13086838\n",
      "Iteration 357, loss = 0.13047338\n",
      "Iteration 358, loss = 0.12950113\n",
      "Iteration 359, loss = 0.13113284\n",
      "Iteration 360, loss = 0.13064556\n",
      "Iteration 361, loss = 0.13064968\n",
      "Iteration 362, loss = 0.12807796\n",
      "Iteration 363, loss = 0.12951058\n",
      "Iteration 364, loss = 0.12850601\n",
      "Iteration 365, loss = 0.12710974\n",
      "Iteration 366, loss = 0.12598783\n",
      "Iteration 367, loss = 0.12454588\n",
      "Iteration 368, loss = 0.12407571\n",
      "Iteration 369, loss = 0.12371975\n",
      "Iteration 370, loss = 0.12379306\n",
      "Iteration 371, loss = 0.12224934\n",
      "Iteration 372, loss = 0.12432648\n",
      "Iteration 373, loss = 0.12228706\n",
      "Iteration 374, loss = 0.12364201\n",
      "Iteration 375, loss = 0.12170723\n",
      "Iteration 376, loss = 0.12118427\n",
      "Iteration 377, loss = 0.11995958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 378, loss = 0.11933712\n",
      "Iteration 379, loss = 0.11869606\n",
      "Iteration 380, loss = 0.11792065\n",
      "Iteration 381, loss = 0.11764078\n",
      "Iteration 382, loss = 0.11844506\n",
      "Iteration 383, loss = 0.11681181\n",
      "Iteration 384, loss = 0.11727477\n",
      "Iteration 385, loss = 0.11550795\n",
      "Iteration 386, loss = 0.11433537\n",
      "Iteration 387, loss = 0.11520324\n",
      "Iteration 388, loss = 0.11566318\n",
      "Iteration 389, loss = 0.11441975\n",
      "Iteration 390, loss = 0.11104686\n",
      "Iteration 391, loss = 0.11082183\n",
      "Iteration 392, loss = 0.10982722\n",
      "Iteration 393, loss = 0.10916486\n",
      "Iteration 394, loss = 0.10848018\n",
      "Iteration 395, loss = 0.10789817\n",
      "Iteration 396, loss = 0.10808714\n",
      "Iteration 397, loss = 0.10780312\n",
      "Iteration 398, loss = 0.10742840\n",
      "Iteration 399, loss = 0.10710594\n",
      "Iteration 400, loss = 0.10620911\n",
      "Iteration 401, loss = 0.10568767\n",
      "Iteration 402, loss = 0.10671787\n",
      "Iteration 403, loss = 0.10651545\n",
      "Iteration 404, loss = 0.10487803\n",
      "Iteration 405, loss = 0.10417047\n",
      "Iteration 406, loss = 0.10384923\n",
      "Iteration 407, loss = 0.10308634\n",
      "Iteration 408, loss = 0.10328640\n",
      "Iteration 409, loss = 0.10331912\n",
      "Iteration 410, loss = 0.10172748\n",
      "Iteration 411, loss = 0.10132261\n",
      "Iteration 412, loss = 0.10089263\n",
      "Iteration 413, loss = 0.10215092\n",
      "Iteration 414, loss = 0.10148049\n",
      "Iteration 415, loss = 0.10034768\n",
      "Iteration 416, loss = 0.10028668\n",
      "Iteration 417, loss = 0.10053474\n",
      "Iteration 418, loss = 0.09945317\n",
      "Iteration 419, loss = 0.09845017\n",
      "Iteration 420, loss = 0.09861904\n",
      "Iteration 421, loss = 0.09902435\n",
      "Iteration 422, loss = 0.09758637\n",
      "Iteration 423, loss = 0.09750746\n",
      "Iteration 424, loss = 0.09630941\n",
      "Iteration 425, loss = 0.09704950\n",
      "Iteration 426, loss = 0.09611291\n",
      "Iteration 427, loss = 0.09595780\n",
      "Iteration 428, loss = 0.09568913\n",
      "Iteration 429, loss = 0.09567863\n",
      "Iteration 430, loss = 0.09536750\n",
      "Iteration 431, loss = 0.09485947\n",
      "Iteration 432, loss = 0.09501813\n",
      "Iteration 433, loss = 0.09349959\n",
      "Iteration 434, loss = 0.09384983\n",
      "Iteration 435, loss = 0.09269477\n",
      "Iteration 436, loss = 0.09237705\n",
      "Iteration 437, loss = 0.09293305\n",
      "Iteration 438, loss = 0.09192008\n",
      "Iteration 439, loss = 0.09188555\n",
      "Iteration 440, loss = 0.09203063\n",
      "Iteration 441, loss = 0.09093664\n",
      "Iteration 442, loss = 0.09110169\n",
      "Iteration 443, loss = 0.09016410\n",
      "Iteration 444, loss = 0.09061090\n",
      "Iteration 445, loss = 0.09062584\n",
      "Iteration 446, loss = 0.09010016\n",
      "Iteration 447, loss = 0.09033714\n",
      "Iteration 448, loss = 0.08953269\n",
      "Iteration 449, loss = 0.08912425\n",
      "Iteration 450, loss = 0.08809927\n",
      "Iteration 451, loss = 0.08840126\n",
      "Iteration 452, loss = 0.08804017\n",
      "Iteration 453, loss = 0.08894047\n",
      "Iteration 454, loss = 0.08736515\n",
      "Iteration 455, loss = 0.08689056\n",
      "Iteration 456, loss = 0.08607656\n",
      "Iteration 457, loss = 0.08570811\n",
      "Iteration 458, loss = 0.08568158\n",
      "Iteration 459, loss = 0.08586243\n",
      "Iteration 460, loss = 0.08860089\n",
      "Iteration 461, loss = 0.08585822\n",
      "Iteration 462, loss = 0.08474477\n",
      "Iteration 463, loss = 0.08462918\n",
      "Iteration 464, loss = 0.08443592\n",
      "Iteration 465, loss = 0.08529448\n",
      "Iteration 466, loss = 0.08444222\n",
      "Iteration 467, loss = 0.08360482\n",
      "Iteration 468, loss = 0.08380562\n",
      "Iteration 469, loss = 0.08245071\n",
      "Iteration 470, loss = 0.08284567\n",
      "Iteration 471, loss = 0.08383581\n",
      "Iteration 472, loss = 0.08141641\n",
      "Iteration 473, loss = 0.08146244\n",
      "Iteration 474, loss = 0.08126200\n",
      "Iteration 475, loss = 0.08166590\n",
      "Iteration 476, loss = 0.08112659\n",
      "Iteration 477, loss = 0.07978628\n",
      "Iteration 478, loss = 0.07944642\n",
      "Iteration 479, loss = 0.07923110\n",
      "Iteration 480, loss = 0.07843301\n",
      "Iteration 481, loss = 0.07811739\n",
      "Iteration 482, loss = 0.07800783\n",
      "Iteration 483, loss = 0.07822313\n",
      "Iteration 484, loss = 0.07861950\n",
      "Iteration 485, loss = 0.07693640\n",
      "Iteration 486, loss = 0.07651883\n",
      "Iteration 487, loss = 0.07609195\n",
      "Iteration 488, loss = 0.07556354\n",
      "Iteration 489, loss = 0.07535353\n",
      "Iteration 490, loss = 0.07539176\n",
      "Iteration 491, loss = 0.07501822\n",
      "Iteration 492, loss = 0.07426712\n",
      "Iteration 493, loss = 0.07368642\n",
      "Iteration 494, loss = 0.07365388\n",
      "Iteration 495, loss = 0.07385456\n",
      "Iteration 496, loss = 0.07453515\n",
      "Iteration 497, loss = 0.07349379\n",
      "Iteration 498, loss = 0.07249413\n",
      "Iteration 499, loss = 0.07182454\n",
      "Iteration 500, loss = 0.07406062\n",
      "Iteration 501, loss = 0.07940575\n",
      "Iteration 502, loss = 0.07881634\n",
      "Iteration 503, loss = 0.07516035\n",
      "Iteration 504, loss = 0.07502453\n",
      "Iteration 505, loss = 0.07184139\n",
      "Iteration 506, loss = 0.07130774\n",
      "Iteration 507, loss = 0.07041829\n",
      "Iteration 508, loss = 0.06864845\n",
      "Iteration 509, loss = 0.06808349\n",
      "Iteration 510, loss = 0.06744865\n",
      "Iteration 511, loss = 0.06677038\n",
      "Iteration 512, loss = 0.06681663\n",
      "Iteration 513, loss = 0.06729437\n",
      "Iteration 514, loss = 0.06739934\n",
      "Iteration 515, loss = 0.06618142\n",
      "Iteration 516, loss = 0.06549750\n",
      "Iteration 517, loss = 0.06498590\n",
      "Iteration 518, loss = 0.06513036\n",
      "Iteration 519, loss = 0.06536211\n",
      "Iteration 520, loss = 0.06580692\n",
      "Iteration 521, loss = 0.06420964\n",
      "Iteration 522, loss = 0.06407808\n",
      "Iteration 523, loss = 0.06343237\n",
      "Iteration 524, loss = 0.06309837\n",
      "Iteration 525, loss = 0.06273762\n",
      "Iteration 526, loss = 0.06313953\n",
      "Iteration 527, loss = 0.06247963\n",
      "Iteration 528, loss = 0.06361098\n",
      "Iteration 529, loss = 0.06199944\n",
      "Iteration 530, loss = 0.06147327\n",
      "Iteration 531, loss = 0.06124469\n",
      "Iteration 532, loss = 0.06111611\n",
      "Iteration 533, loss = 0.06098910\n",
      "Iteration 534, loss = 0.06100790\n",
      "Iteration 535, loss = 0.06072539\n",
      "Iteration 536, loss = 0.06000820\n",
      "Iteration 537, loss = 0.05974915\n",
      "Iteration 538, loss = 0.05937357\n",
      "Iteration 539, loss = 0.05919205\n",
      "Iteration 540, loss = 0.05894192\n",
      "Iteration 541, loss = 0.05876001\n",
      "Iteration 542, loss = 0.05885982\n",
      "Iteration 543, loss = 0.05844194\n",
      "Iteration 544, loss = 0.05821226\n",
      "Iteration 545, loss = 0.05865664\n",
      "Iteration 546, loss = 0.05867799\n",
      "Iteration 547, loss = 0.05812154\n",
      "Iteration 548, loss = 0.05858930\n",
      "Iteration 549, loss = 0.05723694\n",
      "Iteration 550, loss = 0.05663893\n",
      "Iteration 551, loss = 0.05663285\n",
      "Iteration 552, loss = 0.05584307\n",
      "Iteration 553, loss = 0.05572635\n",
      "Iteration 554, loss = 0.05507930\n",
      "Iteration 555, loss = 0.05510027\n",
      "Iteration 556, loss = 0.05589343\n",
      "Iteration 557, loss = 0.05603640\n",
      "Iteration 558, loss = 0.05498288\n",
      "Iteration 559, loss = 0.05431023\n",
      "Iteration 560, loss = 0.05350470\n",
      "Iteration 561, loss = 0.05405655\n",
      "Iteration 562, loss = 0.05331983\n",
      "Iteration 563, loss = 0.05333037\n",
      "Iteration 564, loss = 0.05337151\n",
      "Iteration 565, loss = 0.05438874\n",
      "Iteration 566, loss = 0.05516139\n",
      "Iteration 567, loss = 0.05291657\n",
      "Iteration 568, loss = 0.05190937\n",
      "Iteration 569, loss = 0.05207363\n",
      "Iteration 570, loss = 0.05171372\n",
      "Iteration 571, loss = 0.05262228\n",
      "Iteration 572, loss = 0.05156158\n",
      "Iteration 573, loss = 0.05095293\n",
      "Iteration 574, loss = 0.05101019\n",
      "Iteration 575, loss = 0.05097451\n",
      "Iteration 576, loss = 0.05098834\n",
      "Iteration 577, loss = 0.05059392\n",
      "Iteration 578, loss = 0.05147770\n",
      "Iteration 579, loss = 0.05290762\n",
      "Iteration 580, loss = 0.05093622\n",
      "Iteration 581, loss = 0.05061596\n",
      "Iteration 582, loss = 0.04989543\n",
      "Iteration 583, loss = 0.05002496\n",
      "Iteration 584, loss = 0.04966181\n",
      "Iteration 585, loss = 0.04943922\n",
      "Iteration 586, loss = 0.04902606\n",
      "Iteration 587, loss = 0.04870035\n",
      "Iteration 588, loss = 0.04890243\n",
      "Iteration 589, loss = 0.04913098\n",
      "Iteration 590, loss = 0.04884193\n",
      "Iteration 591, loss = 0.04890958\n",
      "Iteration 592, loss = 0.04811012\n",
      "Iteration 593, loss = 0.04843497\n",
      "Iteration 594, loss = 0.04794460\n",
      "Iteration 595, loss = 0.04757429\n",
      "Iteration 596, loss = 0.04782034\n",
      "Iteration 597, loss = 0.04802263\n",
      "Iteration 598, loss = 0.04911949\n",
      "Iteration 599, loss = 0.04990602\n",
      "Iteration 600, loss = 0.05044904\n",
      "Iteration 601, loss = 0.04774963\n",
      "Iteration 602, loss = 0.04704340\n",
      "Iteration 603, loss = 0.04652359\n",
      "Iteration 604, loss = 0.04653972\n",
      "Iteration 605, loss = 0.04613387\n",
      "Iteration 606, loss = 0.04610457\n",
      "Iteration 607, loss = 0.04612040\n",
      "Iteration 608, loss = 0.04591620\n",
      "Iteration 609, loss = 0.04582897\n",
      "Iteration 610, loss = 0.04635312\n",
      "Iteration 611, loss = 0.04565751\n",
      "Iteration 612, loss = 0.04531905\n",
      "Iteration 613, loss = 0.04497615\n",
      "Iteration 614, loss = 0.04521777\n",
      "Iteration 615, loss = 0.04483260\n",
      "Iteration 616, loss = 0.04431734\n",
      "Iteration 617, loss = 0.04504956\n",
      "Iteration 618, loss = 0.04526275\n",
      "Iteration 619, loss = 0.04398960\n",
      "Iteration 620, loss = 0.04419010\n",
      "Iteration 621, loss = 0.04465345\n",
      "Iteration 622, loss = 0.04394590\n",
      "Iteration 623, loss = 0.04249946\n",
      "Iteration 624, loss = 0.04185324\n",
      "Iteration 625, loss = 0.04180969\n",
      "Iteration 626, loss = 0.04136826\n",
      "Iteration 627, loss = 0.04146083\n",
      "Iteration 628, loss = 0.04068688\n",
      "Iteration 629, loss = 0.04037412\n",
      "Iteration 630, loss = 0.03990172\n",
      "Iteration 631, loss = 0.04022415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 632, loss = 0.03981662\n",
      "Iteration 633, loss = 0.03942121\n",
      "Iteration 634, loss = 0.03868487\n",
      "Iteration 635, loss = 0.03955138\n",
      "Iteration 636, loss = 0.03850793\n",
      "Iteration 637, loss = 0.03810697\n",
      "Iteration 638, loss = 0.03794216\n",
      "Iteration 639, loss = 0.03775536\n",
      "Iteration 640, loss = 0.03753156\n",
      "Iteration 641, loss = 0.03808487\n",
      "Iteration 642, loss = 0.03743936\n",
      "Iteration 643, loss = 0.03681179\n",
      "Iteration 644, loss = 0.03636852\n",
      "Iteration 645, loss = 0.03642794\n",
      "Iteration 646, loss = 0.03647690\n",
      "Iteration 647, loss = 0.03618964\n",
      "Iteration 648, loss = 0.03616065\n",
      "Iteration 649, loss = 0.03617158\n",
      "Iteration 650, loss = 0.03566089\n",
      "Iteration 651, loss = 0.03611606\n",
      "Iteration 652, loss = 0.03564841\n",
      "Iteration 653, loss = 0.03588896\n",
      "Iteration 654, loss = 0.03514910\n",
      "Iteration 655, loss = 0.03498318\n",
      "Iteration 656, loss = 0.03463327\n",
      "Iteration 657, loss = 0.03469274\n",
      "Iteration 658, loss = 0.03476442\n",
      "Iteration 659, loss = 0.03454049\n",
      "Iteration 660, loss = 0.03426315\n",
      "Iteration 661, loss = 0.03458829\n",
      "Iteration 662, loss = 0.03461304\n",
      "Iteration 663, loss = 0.03445799\n",
      "Iteration 664, loss = 0.03467437\n",
      "Iteration 665, loss = 0.03439941\n",
      "Iteration 666, loss = 0.03384495\n",
      "Iteration 667, loss = 0.03367426\n",
      "Iteration 668, loss = 0.03351873\n",
      "Iteration 669, loss = 0.03436434\n",
      "Iteration 670, loss = 0.03343377\n",
      "Iteration 671, loss = 0.03392047\n",
      "Iteration 672, loss = 0.03323180\n",
      "Iteration 673, loss = 0.03331810\n",
      "Iteration 674, loss = 0.03342802\n",
      "Iteration 675, loss = 0.03281521\n",
      "Iteration 676, loss = 0.03251752\n",
      "Iteration 677, loss = 0.03247905\n",
      "Iteration 678, loss = 0.03242494\n",
      "Iteration 679, loss = 0.03225074\n",
      "Iteration 680, loss = 0.03213832\n",
      "Iteration 681, loss = 0.03242618\n",
      "Iteration 682, loss = 0.03189951\n",
      "Iteration 683, loss = 0.03166477\n",
      "Iteration 684, loss = 0.03179673\n",
      "Iteration 685, loss = 0.03167572\n",
      "Iteration 686, loss = 0.03117368\n",
      "Iteration 687, loss = 0.03089059\n",
      "Iteration 688, loss = 0.03043865\n",
      "Iteration 689, loss = 0.03089177\n",
      "Iteration 690, loss = 0.03032320\n",
      "Iteration 691, loss = 0.02973333\n",
      "Iteration 692, loss = 0.02959498\n",
      "Iteration 693, loss = 0.03099610\n",
      "Iteration 694, loss = 0.03145507\n",
      "Iteration 695, loss = 0.02977935\n",
      "Iteration 696, loss = 0.02914728\n",
      "Iteration 697, loss = 0.02885994\n",
      "Iteration 698, loss = 0.02877320\n",
      "Iteration 699, loss = 0.02804558\n",
      "Iteration 700, loss = 0.02775321\n",
      "Iteration 701, loss = 0.02684172\n",
      "Iteration 702, loss = 0.02617265\n",
      "Iteration 703, loss = 0.02569665\n",
      "Iteration 704, loss = 0.02586721\n",
      "Iteration 705, loss = 0.02578017\n",
      "Iteration 706, loss = 0.02544656\n",
      "Iteration 707, loss = 0.02495456\n",
      "Iteration 708, loss = 0.02490394\n",
      "Iteration 709, loss = 0.02482089\n",
      "Iteration 710, loss = 0.02460036\n",
      "Iteration 711, loss = 0.02455399\n",
      "Iteration 712, loss = 0.02433833\n",
      "Iteration 713, loss = 0.02422579\n",
      "Iteration 714, loss = 0.02418492\n",
      "Iteration 715, loss = 0.02415663\n",
      "Iteration 716, loss = 0.02407971\n",
      "Iteration 717, loss = 0.02442378\n",
      "Iteration 718, loss = 0.02413457\n",
      "Iteration 719, loss = 0.02391903\n",
      "Iteration 720, loss = 0.02396216\n",
      "Iteration 721, loss = 0.02421897\n",
      "Iteration 722, loss = 0.02351808\n",
      "Iteration 723, loss = 0.02294376\n",
      "Iteration 724, loss = 0.02307568\n",
      "Iteration 725, loss = 0.02314207\n",
      "Iteration 726, loss = 0.02269371\n",
      "Iteration 727, loss = 0.02274036\n",
      "Iteration 728, loss = 0.02326374\n",
      "Iteration 729, loss = 0.02318121\n",
      "Iteration 730, loss = 0.02279369\n",
      "Iteration 731, loss = 0.02263963\n",
      "Iteration 732, loss = 0.02286224\n",
      "Iteration 733, loss = 0.02220031\n",
      "Iteration 734, loss = 0.02338021\n",
      "Iteration 735, loss = 0.02328047\n",
      "Iteration 736, loss = 0.02370264\n",
      "Iteration 737, loss = 0.02225433\n",
      "Iteration 738, loss = 0.02164914\n",
      "Iteration 739, loss = 0.02159326\n",
      "Iteration 740, loss = 0.02147497\n",
      "Iteration 741, loss = 0.02103980\n",
      "Iteration 742, loss = 0.02091727\n",
      "Iteration 743, loss = 0.02083098\n",
      "Iteration 744, loss = 0.02146318\n",
      "Iteration 745, loss = 0.02181982\n",
      "Iteration 746, loss = 0.02287166\n",
      "Iteration 747, loss = 0.02233097\n",
      "Iteration 748, loss = 0.02080135\n",
      "Iteration 749, loss = 0.02098647\n",
      "Iteration 750, loss = 0.02123196\n",
      "Iteration 751, loss = 0.02163343\n",
      "Iteration 752, loss = 0.02220932\n",
      "Iteration 753, loss = 0.02453790\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 27 of 48...\n",
      "Iteration 1, loss = 0.68819177\n",
      "Iteration 2, loss = 0.68375738\n",
      "Iteration 3, loss = 0.67933927\n",
      "Iteration 4, loss = 0.67354528\n",
      "Iteration 5, loss = 0.66666069\n",
      "Iteration 6, loss = 0.65860097\n",
      "Iteration 7, loss = 0.65013356\n",
      "Iteration 8, loss = 0.64046419\n",
      "Iteration 9, loss = 0.63048603\n",
      "Iteration 10, loss = 0.61988435\n",
      "Iteration 11, loss = 0.60868857\n",
      "Iteration 12, loss = 0.59776415\n",
      "Iteration 13, loss = 0.58579813\n",
      "Iteration 14, loss = 0.57508536\n",
      "Iteration 15, loss = 0.56296093\n",
      "Iteration 16, loss = 0.55183603\n",
      "Iteration 17, loss = 0.54108789\n",
      "Iteration 18, loss = 0.53012624\n",
      "Iteration 19, loss = 0.52235229\n",
      "Iteration 20, loss = 0.51168648\n",
      "Iteration 21, loss = 0.50193794\n",
      "Iteration 22, loss = 0.49487477\n",
      "Iteration 23, loss = 0.48718558\n",
      "Iteration 24, loss = 0.47812969\n",
      "Iteration 25, loss = 0.47020516\n",
      "Iteration 26, loss = 0.46410939\n",
      "Iteration 27, loss = 0.45723245\n",
      "Iteration 28, loss = 0.45127107\n",
      "Iteration 29, loss = 0.44572003\n",
      "Iteration 30, loss = 0.44092871\n",
      "Iteration 31, loss = 0.43680677\n",
      "Iteration 32, loss = 0.43207405\n",
      "Iteration 33, loss = 0.42887028\n",
      "Iteration 34, loss = 0.42820953\n",
      "Iteration 35, loss = 0.41993603\n",
      "Iteration 36, loss = 0.41536145\n",
      "Iteration 37, loss = 0.41190357\n",
      "Iteration 38, loss = 0.40853309\n",
      "Iteration 39, loss = 0.40596627\n",
      "Iteration 40, loss = 0.40212475\n",
      "Iteration 41, loss = 0.39989363\n",
      "Iteration 42, loss = 0.39665279\n",
      "Iteration 43, loss = 0.39565087\n",
      "Iteration 44, loss = 0.39166720\n",
      "Iteration 45, loss = 0.38931194\n",
      "Iteration 46, loss = 0.38656771\n",
      "Iteration 47, loss = 0.38525007\n",
      "Iteration 48, loss = 0.38289453\n",
      "Iteration 49, loss = 0.38018482\n",
      "Iteration 50, loss = 0.37839365\n",
      "Iteration 51, loss = 0.37717238\n",
      "Iteration 52, loss = 0.37442999\n",
      "Iteration 53, loss = 0.37263854\n",
      "Iteration 54, loss = 0.37223329\n",
      "Iteration 55, loss = 0.36871397\n",
      "Iteration 56, loss = 0.36675270\n",
      "Iteration 57, loss = 0.36479725\n",
      "Iteration 58, loss = 0.36499376\n",
      "Iteration 59, loss = 0.36640820\n",
      "Iteration 60, loss = 0.36407963\n",
      "Iteration 61, loss = 0.36040347\n",
      "Iteration 62, loss = 0.35699775\n",
      "Iteration 63, loss = 0.35543825\n",
      "Iteration 64, loss = 0.35442926\n",
      "Iteration 65, loss = 0.35520561\n",
      "Iteration 66, loss = 0.35267504\n",
      "Iteration 67, loss = 0.35061825\n",
      "Iteration 68, loss = 0.34909060\n",
      "Iteration 69, loss = 0.34851791\n",
      "Iteration 70, loss = 0.34520613\n",
      "Iteration 71, loss = 0.34380195\n",
      "Iteration 72, loss = 0.34410080\n",
      "Iteration 73, loss = 0.34333219\n",
      "Iteration 74, loss = 0.34427212\n",
      "Iteration 75, loss = 0.34186396\n",
      "Iteration 76, loss = 0.33839630\n",
      "Iteration 77, loss = 0.33637186\n",
      "Iteration 78, loss = 0.33553858\n",
      "Iteration 79, loss = 0.33375380\n",
      "Iteration 80, loss = 0.33406980\n",
      "Iteration 81, loss = 0.33441707\n",
      "Iteration 82, loss = 0.33372619\n",
      "Iteration 83, loss = 0.33109794\n",
      "Iteration 84, loss = 0.32938475\n",
      "Iteration 85, loss = 0.32880179\n",
      "Iteration 86, loss = 0.32651353\n",
      "Iteration 87, loss = 0.32512644\n",
      "Iteration 88, loss = 0.32588958\n",
      "Iteration 89, loss = 0.32318827\n",
      "Iteration 90, loss = 0.32246837\n",
      "Iteration 91, loss = 0.32146957\n",
      "Iteration 92, loss = 0.32000396\n",
      "Iteration 93, loss = 0.32191231\n",
      "Iteration 94, loss = 0.31915942\n",
      "Iteration 95, loss = 0.31741533\n",
      "Iteration 96, loss = 0.31755754\n",
      "Iteration 97, loss = 0.31607364\n",
      "Iteration 98, loss = 0.31464847\n",
      "Iteration 99, loss = 0.31551301\n",
      "Iteration 100, loss = 0.31327138\n",
      "Iteration 101, loss = 0.31448190\n",
      "Iteration 102, loss = 0.31180224\n",
      "Iteration 103, loss = 0.31112111\n",
      "Iteration 104, loss = 0.31133050\n",
      "Iteration 105, loss = 0.30964547\n",
      "Iteration 106, loss = 0.30806026\n",
      "Iteration 107, loss = 0.30700017\n",
      "Iteration 108, loss = 0.30509276\n",
      "Iteration 109, loss = 0.30388117\n",
      "Iteration 110, loss = 0.30612534\n",
      "Iteration 111, loss = 0.30808962\n",
      "Iteration 112, loss = 0.30694056\n",
      "Iteration 113, loss = 0.30503968\n",
      "Iteration 114, loss = 0.30304644\n",
      "Iteration 115, loss = 0.30065123\n",
      "Iteration 116, loss = 0.29928942\n",
      "Iteration 117, loss = 0.29841738\n",
      "Iteration 118, loss = 0.29923924\n",
      "Iteration 119, loss = 0.29662696\n",
      "Iteration 120, loss = 0.29689069\n",
      "Iteration 121, loss = 0.29699085\n",
      "Iteration 122, loss = 0.29673692\n",
      "Iteration 123, loss = 0.29393032\n",
      "Iteration 124, loss = 0.29330332\n",
      "Iteration 125, loss = 0.29281914\n",
      "Iteration 126, loss = 0.29225320\n",
      "Iteration 127, loss = 0.29082172\n",
      "Iteration 128, loss = 0.29123435\n",
      "Iteration 129, loss = 0.28960264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 130, loss = 0.28923832\n",
      "Iteration 131, loss = 0.28897489\n",
      "Iteration 132, loss = 0.28819980\n",
      "Iteration 133, loss = 0.28657885\n",
      "Iteration 134, loss = 0.28763614\n",
      "Iteration 135, loss = 0.28826356\n",
      "Iteration 136, loss = 0.28509835\n",
      "Iteration 137, loss = 0.28467936\n",
      "Iteration 138, loss = 0.28336478\n",
      "Iteration 139, loss = 0.28299905\n",
      "Iteration 140, loss = 0.28191076\n",
      "Iteration 141, loss = 0.28342944\n",
      "Iteration 142, loss = 0.28444038\n",
      "Iteration 143, loss = 0.28544968\n",
      "Iteration 144, loss = 0.28210366\n",
      "Iteration 145, loss = 0.28071815\n",
      "Iteration 146, loss = 0.27993531\n",
      "Iteration 147, loss = 0.27948023\n",
      "Iteration 148, loss = 0.27905570\n",
      "Iteration 149, loss = 0.27777609\n",
      "Iteration 150, loss = 0.27632403\n",
      "Iteration 151, loss = 0.27573889\n",
      "Iteration 152, loss = 0.27790362\n",
      "Iteration 153, loss = 0.27422739\n",
      "Iteration 154, loss = 0.27312321\n",
      "Iteration 155, loss = 0.27709963\n",
      "Iteration 156, loss = 0.27430696\n",
      "Iteration 157, loss = 0.27216529\n",
      "Iteration 158, loss = 0.27269489\n",
      "Iteration 159, loss = 0.27276060\n",
      "Iteration 160, loss = 0.27237639\n",
      "Iteration 161, loss = 0.26935017\n",
      "Iteration 162, loss = 0.26922394\n",
      "Iteration 163, loss = 0.26749112\n",
      "Iteration 164, loss = 0.26876234\n",
      "Iteration 165, loss = 0.26636992\n",
      "Iteration 166, loss = 0.26535878\n",
      "Iteration 167, loss = 0.26503445\n",
      "Iteration 168, loss = 0.26471617\n",
      "Iteration 169, loss = 0.26597173\n",
      "Iteration 170, loss = 0.26449590\n",
      "Iteration 171, loss = 0.26343247\n",
      "Iteration 172, loss = 0.26900469\n",
      "Iteration 173, loss = 0.26575849\n",
      "Iteration 174, loss = 0.26149268\n",
      "Iteration 175, loss = 0.26126744\n",
      "Iteration 176, loss = 0.25888619\n",
      "Iteration 177, loss = 0.25819270\n",
      "Iteration 178, loss = 0.25823288\n",
      "Iteration 179, loss = 0.25738234\n",
      "Iteration 180, loss = 0.25642595\n",
      "Iteration 181, loss = 0.25836576\n",
      "Iteration 182, loss = 0.25682976\n",
      "Iteration 183, loss = 0.25641424\n",
      "Iteration 184, loss = 0.25520959\n",
      "Iteration 185, loss = 0.25579711\n",
      "Iteration 186, loss = 0.25323395\n",
      "Iteration 187, loss = 0.25314264\n",
      "Iteration 188, loss = 0.25379469\n",
      "Iteration 189, loss = 0.25143728\n",
      "Iteration 190, loss = 0.25009760\n",
      "Iteration 191, loss = 0.25049121\n",
      "Iteration 192, loss = 0.24921474\n",
      "Iteration 193, loss = 0.24781313\n",
      "Iteration 194, loss = 0.24819228\n",
      "Iteration 195, loss = 0.24771681\n",
      "Iteration 196, loss = 0.24864163\n",
      "Iteration 197, loss = 0.24645113\n",
      "Iteration 198, loss = 0.24640520\n",
      "Iteration 199, loss = 0.24559859\n",
      "Iteration 200, loss = 0.24437671\n",
      "Iteration 201, loss = 0.24349513\n",
      "Iteration 202, loss = 0.24342029\n",
      "Iteration 203, loss = 0.24275392\n",
      "Iteration 204, loss = 0.24213498\n",
      "Iteration 205, loss = 0.24169334\n",
      "Iteration 206, loss = 0.24097321\n",
      "Iteration 207, loss = 0.23994694\n",
      "Iteration 208, loss = 0.23967322\n",
      "Iteration 209, loss = 0.23973488\n",
      "Iteration 210, loss = 0.23817316\n",
      "Iteration 211, loss = 0.23854424\n",
      "Iteration 212, loss = 0.23637217\n",
      "Iteration 213, loss = 0.23661935\n",
      "Iteration 214, loss = 0.23617515\n",
      "Iteration 215, loss = 0.23672041\n",
      "Iteration 216, loss = 0.23524558\n",
      "Iteration 217, loss = 0.23527892\n",
      "Iteration 218, loss = 0.23400858\n",
      "Iteration 219, loss = 0.23173657\n",
      "Iteration 220, loss = 0.23174949\n",
      "Iteration 221, loss = 0.23366593\n",
      "Iteration 222, loss = 0.23246684\n",
      "Iteration 223, loss = 0.22983166\n",
      "Iteration 224, loss = 0.22955765\n",
      "Iteration 225, loss = 0.22987855\n",
      "Iteration 226, loss = 0.23071733\n",
      "Iteration 227, loss = 0.22963632\n",
      "Iteration 228, loss = 0.22776225\n",
      "Iteration 229, loss = 0.22715421\n",
      "Iteration 230, loss = 0.22600485\n",
      "Iteration 231, loss = 0.22801667\n",
      "Iteration 232, loss = 0.22647853\n",
      "Iteration 233, loss = 0.22628864\n",
      "Iteration 234, loss = 0.22602853\n",
      "Iteration 235, loss = 0.22342278\n",
      "Iteration 236, loss = 0.22277327\n",
      "Iteration 237, loss = 0.22379827\n",
      "Iteration 238, loss = 0.22260066\n",
      "Iteration 239, loss = 0.22139927\n",
      "Iteration 240, loss = 0.22298003\n",
      "Iteration 241, loss = 0.22231836\n",
      "Iteration 242, loss = 0.22276223\n",
      "Iteration 243, loss = 0.22031651\n",
      "Iteration 244, loss = 0.21906598\n",
      "Iteration 245, loss = 0.21978231\n",
      "Iteration 246, loss = 0.21873195\n",
      "Iteration 247, loss = 0.22042013\n",
      "Iteration 248, loss = 0.21585436\n",
      "Iteration 249, loss = 0.21529570\n",
      "Iteration 250, loss = 0.21737216\n",
      "Iteration 251, loss = 0.21446924\n",
      "Iteration 252, loss = 0.21374967\n",
      "Iteration 253, loss = 0.21356509\n",
      "Iteration 254, loss = 0.21260828\n",
      "Iteration 255, loss = 0.21234666\n",
      "Iteration 256, loss = 0.21107507\n",
      "Iteration 257, loss = 0.20989502\n",
      "Iteration 258, loss = 0.21134521\n",
      "Iteration 259, loss = 0.21334813\n",
      "Iteration 260, loss = 0.20875346\n",
      "Iteration 261, loss = 0.21180577\n",
      "Iteration 262, loss = 0.21039000\n",
      "Iteration 263, loss = 0.20753447\n",
      "Iteration 264, loss = 0.20751524\n",
      "Iteration 265, loss = 0.20647043\n",
      "Iteration 266, loss = 0.20595533\n",
      "Iteration 267, loss = 0.20463137\n",
      "Iteration 268, loss = 0.20531087\n",
      "Iteration 269, loss = 0.20537648\n",
      "Iteration 270, loss = 0.20351044\n",
      "Iteration 271, loss = 0.20281807\n",
      "Iteration 272, loss = 0.20281890\n",
      "Iteration 273, loss = 0.20451414\n",
      "Iteration 274, loss = 0.20236156\n",
      "Iteration 275, loss = 0.20083107\n",
      "Iteration 276, loss = 0.20297089\n",
      "Iteration 277, loss = 0.20199189\n",
      "Iteration 278, loss = 0.20023215\n",
      "Iteration 279, loss = 0.19885417\n",
      "Iteration 280, loss = 0.19849868\n",
      "Iteration 281, loss = 0.19840358\n",
      "Iteration 282, loss = 0.19742794\n",
      "Iteration 283, loss = 0.19874958\n",
      "Iteration 284, loss = 0.19746790\n",
      "Iteration 285, loss = 0.19531849\n",
      "Iteration 286, loss = 0.19498521\n",
      "Iteration 287, loss = 0.19508007\n",
      "Iteration 288, loss = 0.19445355\n",
      "Iteration 289, loss = 0.19273665\n",
      "Iteration 290, loss = 0.19303067\n",
      "Iteration 291, loss = 0.19228583\n",
      "Iteration 292, loss = 0.19212366\n",
      "Iteration 293, loss = 0.19165593\n",
      "Iteration 294, loss = 0.19133720\n",
      "Iteration 295, loss = 0.19104104\n",
      "Iteration 296, loss = 0.18921384\n",
      "Iteration 297, loss = 0.18870793\n",
      "Iteration 298, loss = 0.18781821\n",
      "Iteration 299, loss = 0.18772495\n",
      "Iteration 300, loss = 0.18904135\n",
      "Iteration 301, loss = 0.18889654\n",
      "Iteration 302, loss = 0.18755277\n",
      "Iteration 303, loss = 0.18516554\n",
      "Iteration 304, loss = 0.18586835\n",
      "Iteration 305, loss = 0.18329714\n",
      "Iteration 306, loss = 0.18517006\n",
      "Iteration 307, loss = 0.18545982\n",
      "Iteration 308, loss = 0.18546443\n",
      "Iteration 309, loss = 0.18457875\n",
      "Iteration 310, loss = 0.18439008\n",
      "Iteration 311, loss = 0.18227979\n",
      "Iteration 312, loss = 0.18048688\n",
      "Iteration 313, loss = 0.17961839\n",
      "Iteration 314, loss = 0.18199629\n",
      "Iteration 315, loss = 0.17880197\n",
      "Iteration 316, loss = 0.17808323\n",
      "Iteration 317, loss = 0.17879593\n",
      "Iteration 318, loss = 0.17830618\n",
      "Iteration 319, loss = 0.17709288\n",
      "Iteration 320, loss = 0.17651395\n",
      "Iteration 321, loss = 0.17644744\n",
      "Iteration 322, loss = 0.17504146\n",
      "Iteration 323, loss = 0.17481801\n",
      "Iteration 324, loss = 0.17531843\n",
      "Iteration 325, loss = 0.17502290\n",
      "Iteration 326, loss = 0.17402213\n",
      "Iteration 327, loss = 0.17263416\n",
      "Iteration 328, loss = 0.17203370\n",
      "Iteration 329, loss = 0.17175711\n",
      "Iteration 330, loss = 0.17105622\n",
      "Iteration 331, loss = 0.17060516\n",
      "Iteration 332, loss = 0.17024638\n",
      "Iteration 333, loss = 0.17056532\n",
      "Iteration 334, loss = 0.17440125\n",
      "Iteration 335, loss = 0.17034999\n",
      "Iteration 336, loss = 0.16932829\n",
      "Iteration 337, loss = 0.16789629\n",
      "Iteration 338, loss = 0.16848669\n",
      "Iteration 339, loss = 0.16652188\n",
      "Iteration 340, loss = 0.16611652\n",
      "Iteration 341, loss = 0.16618896\n",
      "Iteration 342, loss = 0.16606907\n",
      "Iteration 343, loss = 0.16443119\n",
      "Iteration 344, loss = 0.16360213\n",
      "Iteration 345, loss = 0.16287852\n",
      "Iteration 346, loss = 0.16489259\n",
      "Iteration 347, loss = 0.16589481\n",
      "Iteration 348, loss = 0.16807678\n",
      "Iteration 349, loss = 0.16767646\n",
      "Iteration 350, loss = 0.16329067\n",
      "Iteration 351, loss = 0.16190966\n",
      "Iteration 352, loss = 0.16125274\n",
      "Iteration 353, loss = 0.16048838\n",
      "Iteration 354, loss = 0.15808310\n",
      "Iteration 355, loss = 0.15791488\n",
      "Iteration 356, loss = 0.15702934\n",
      "Iteration 357, loss = 0.15778117\n",
      "Iteration 358, loss = 0.15631423\n",
      "Iteration 359, loss = 0.15492328\n",
      "Iteration 360, loss = 0.15537552\n",
      "Iteration 361, loss = 0.15470055\n",
      "Iteration 362, loss = 0.15358414\n",
      "Iteration 363, loss = 0.15288670\n",
      "Iteration 364, loss = 0.15361980\n",
      "Iteration 365, loss = 0.15243772\n",
      "Iteration 366, loss = 0.15345326\n",
      "Iteration 367, loss = 0.15210883\n",
      "Iteration 368, loss = 0.15152153\n",
      "Iteration 369, loss = 0.15190442\n",
      "Iteration 370, loss = 0.15006492\n",
      "Iteration 371, loss = 0.15267678\n",
      "Iteration 372, loss = 0.14885521\n",
      "Iteration 373, loss = 0.14954841\n",
      "Iteration 374, loss = 0.14909366\n",
      "Iteration 375, loss = 0.14760113\n",
      "Iteration 376, loss = 0.14679247\n",
      "Iteration 377, loss = 0.14755594\n",
      "Iteration 378, loss = 0.14864119\n",
      "Iteration 379, loss = 0.14641062\n",
      "Iteration 380, loss = 0.14639707\n",
      "Iteration 381, loss = 0.14346979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 382, loss = 0.14814858\n",
      "Iteration 383, loss = 0.14550149\n",
      "Iteration 384, loss = 0.14517173\n",
      "Iteration 385, loss = 0.14168596\n",
      "Iteration 386, loss = 0.14109873\n",
      "Iteration 387, loss = 0.14064630\n",
      "Iteration 388, loss = 0.14216501\n",
      "Iteration 389, loss = 0.14144391\n",
      "Iteration 390, loss = 0.13933937\n",
      "Iteration 391, loss = 0.13868217\n",
      "Iteration 392, loss = 0.13835161\n",
      "Iteration 393, loss = 0.13838835\n",
      "Iteration 394, loss = 0.13721457\n",
      "Iteration 395, loss = 0.13726749\n",
      "Iteration 396, loss = 0.13636811\n",
      "Iteration 397, loss = 0.13564764\n",
      "Iteration 398, loss = 0.13494786\n",
      "Iteration 399, loss = 0.13464577\n",
      "Iteration 400, loss = 0.13383188\n",
      "Iteration 401, loss = 0.13419171\n",
      "Iteration 402, loss = 0.13497423\n",
      "Iteration 403, loss = 0.13296702\n",
      "Iteration 404, loss = 0.13295421\n",
      "Iteration 405, loss = 0.13240387\n",
      "Iteration 406, loss = 0.13251360\n",
      "Iteration 407, loss = 0.13195847\n",
      "Iteration 408, loss = 0.13100692\n",
      "Iteration 409, loss = 0.12995379\n",
      "Iteration 410, loss = 0.12932231\n",
      "Iteration 411, loss = 0.13012735\n",
      "Iteration 412, loss = 0.12856349\n",
      "Iteration 413, loss = 0.12903468\n",
      "Iteration 414, loss = 0.12828545\n",
      "Iteration 415, loss = 0.12832574\n",
      "Iteration 416, loss = 0.12772246\n",
      "Iteration 417, loss = 0.12712630\n",
      "Iteration 418, loss = 0.12687981\n",
      "Iteration 419, loss = 0.12777147\n",
      "Iteration 420, loss = 0.12545201\n",
      "Iteration 421, loss = 0.12646513\n",
      "Iteration 422, loss = 0.12508703\n",
      "Iteration 423, loss = 0.12469225\n",
      "Iteration 424, loss = 0.12662229\n",
      "Iteration 425, loss = 0.12494226\n",
      "Iteration 426, loss = 0.12674346\n",
      "Iteration 427, loss = 0.12343961\n",
      "Iteration 428, loss = 0.12189751\n",
      "Iteration 429, loss = 0.12108694\n",
      "Iteration 430, loss = 0.12280998\n",
      "Iteration 431, loss = 0.12078554\n",
      "Iteration 432, loss = 0.12140060\n",
      "Iteration 433, loss = 0.12236297\n",
      "Iteration 434, loss = 0.12149954\n",
      "Iteration 435, loss = 0.11914141\n",
      "Iteration 436, loss = 0.11856342\n",
      "Iteration 437, loss = 0.11768461\n",
      "Iteration 438, loss = 0.11778933\n",
      "Iteration 439, loss = 0.11656385\n",
      "Iteration 440, loss = 0.11672101\n",
      "Iteration 441, loss = 0.11780095\n",
      "Iteration 442, loss = 0.11575652\n",
      "Iteration 443, loss = 0.11544349\n",
      "Iteration 444, loss = 0.11538195\n",
      "Iteration 445, loss = 0.11459947\n",
      "Iteration 446, loss = 0.11491620\n",
      "Iteration 447, loss = 0.11369187\n",
      "Iteration 448, loss = 0.11391573\n",
      "Iteration 449, loss = 0.11457932\n",
      "Iteration 450, loss = 0.11608964\n",
      "Iteration 451, loss = 0.11904710\n",
      "Iteration 452, loss = 0.11299533\n",
      "Iteration 453, loss = 0.11420715\n",
      "Iteration 454, loss = 0.11175179\n",
      "Iteration 455, loss = 0.11304707\n",
      "Iteration 456, loss = 0.11024228\n",
      "Iteration 457, loss = 0.10932188\n",
      "Iteration 458, loss = 0.10862791\n",
      "Iteration 459, loss = 0.10959085\n",
      "Iteration 460, loss = 0.11071256\n",
      "Iteration 461, loss = 0.10984033\n",
      "Iteration 462, loss = 0.10938606\n",
      "Iteration 463, loss = 0.10983416\n",
      "Iteration 464, loss = 0.10864100\n",
      "Iteration 465, loss = 0.10764045\n",
      "Iteration 466, loss = 0.10552196\n",
      "Iteration 467, loss = 0.10443257\n",
      "Iteration 468, loss = 0.10555169\n",
      "Iteration 469, loss = 0.10406605\n",
      "Iteration 470, loss = 0.10444837\n",
      "Iteration 471, loss = 0.10374827\n",
      "Iteration 472, loss = 0.10254805\n",
      "Iteration 473, loss = 0.10285522\n",
      "Iteration 474, loss = 0.10212572\n",
      "Iteration 475, loss = 0.10118570\n",
      "Iteration 476, loss = 0.10235886\n",
      "Iteration 477, loss = 0.10250094\n",
      "Iteration 478, loss = 0.10327515\n",
      "Iteration 479, loss = 0.10145429\n",
      "Iteration 480, loss = 0.10053081\n",
      "Iteration 481, loss = 0.10267933\n",
      "Iteration 482, loss = 0.09941127\n",
      "Iteration 483, loss = 0.09916389\n",
      "Iteration 484, loss = 0.09954601\n",
      "Iteration 485, loss = 0.09841571\n",
      "Iteration 486, loss = 0.09793192\n",
      "Iteration 487, loss = 0.09762974\n",
      "Iteration 488, loss = 0.09721226\n",
      "Iteration 489, loss = 0.09796290\n",
      "Iteration 490, loss = 0.09879299\n",
      "Iteration 491, loss = 0.09877608\n",
      "Iteration 492, loss = 0.09733125\n",
      "Iteration 493, loss = 0.09719892\n",
      "Iteration 494, loss = 0.09661567\n",
      "Iteration 495, loss = 0.09441246\n",
      "Iteration 496, loss = 0.09321551\n",
      "Iteration 497, loss = 0.09340788\n",
      "Iteration 498, loss = 0.09284235\n",
      "Iteration 499, loss = 0.09237220\n",
      "Iteration 500, loss = 0.09354322\n",
      "Iteration 501, loss = 0.09209507\n",
      "Iteration 502, loss = 0.09223943\n",
      "Iteration 503, loss = 0.09052871\n",
      "Iteration 504, loss = 0.08989370\n",
      "Iteration 505, loss = 0.08941946\n",
      "Iteration 506, loss = 0.09109545\n",
      "Iteration 507, loss = 0.08944517\n",
      "Iteration 508, loss = 0.08800841\n",
      "Iteration 509, loss = 0.08772649\n",
      "Iteration 510, loss = 0.08797796\n",
      "Iteration 511, loss = 0.08719516\n",
      "Iteration 512, loss = 0.08690200\n",
      "Iteration 513, loss = 0.08831944\n",
      "Iteration 514, loss = 0.08608278\n",
      "Iteration 515, loss = 0.08570983\n",
      "Iteration 516, loss = 0.08432962\n",
      "Iteration 517, loss = 0.08491718\n",
      "Iteration 518, loss = 0.08443267\n",
      "Iteration 519, loss = 0.08430613\n",
      "Iteration 520, loss = 0.08392891\n",
      "Iteration 521, loss = 0.08517280\n",
      "Iteration 522, loss = 0.08572509\n",
      "Iteration 523, loss = 0.08411585\n",
      "Iteration 524, loss = 0.08290818\n",
      "Iteration 525, loss = 0.08345025\n",
      "Iteration 526, loss = 0.08317508\n",
      "Iteration 527, loss = 0.08259174\n",
      "Iteration 528, loss = 0.08140723\n",
      "Iteration 529, loss = 0.08122076\n",
      "Iteration 530, loss = 0.08110390\n",
      "Iteration 531, loss = 0.08052611\n",
      "Iteration 532, loss = 0.08062178\n",
      "Iteration 533, loss = 0.08030138\n",
      "Iteration 534, loss = 0.08037599\n",
      "Iteration 535, loss = 0.07901944\n",
      "Iteration 536, loss = 0.07906160\n",
      "Iteration 537, loss = 0.07884429\n",
      "Iteration 538, loss = 0.07854692\n",
      "Iteration 539, loss = 0.07836754\n",
      "Iteration 540, loss = 0.07780576\n",
      "Iteration 541, loss = 0.07773789\n",
      "Iteration 542, loss = 0.07759897\n",
      "Iteration 543, loss = 0.07811061\n",
      "Iteration 544, loss = 0.07712352\n",
      "Iteration 545, loss = 0.07722960\n",
      "Iteration 546, loss = 0.07691030\n",
      "Iteration 547, loss = 0.07661224\n",
      "Iteration 548, loss = 0.07589709\n",
      "Iteration 549, loss = 0.07573674\n",
      "Iteration 550, loss = 0.07716433\n",
      "Iteration 551, loss = 0.07557853\n",
      "Iteration 552, loss = 0.07565433\n",
      "Iteration 553, loss = 0.07602296\n",
      "Iteration 554, loss = 0.07657396\n",
      "Iteration 555, loss = 0.07441320\n",
      "Iteration 556, loss = 0.07416829\n",
      "Iteration 557, loss = 0.07431383\n",
      "Iteration 558, loss = 0.07382495\n",
      "Iteration 559, loss = 0.07354230\n",
      "Iteration 560, loss = 0.07284517\n",
      "Iteration 561, loss = 0.07305615\n",
      "Iteration 562, loss = 0.07366449\n",
      "Iteration 563, loss = 0.07302623\n",
      "Iteration 564, loss = 0.07358329\n",
      "Iteration 565, loss = 0.07246826\n",
      "Iteration 566, loss = 0.07210934\n",
      "Iteration 567, loss = 0.07228069\n",
      "Iteration 568, loss = 0.07193926\n",
      "Iteration 569, loss = 0.07228586\n",
      "Iteration 570, loss = 0.07188264\n",
      "Iteration 571, loss = 0.07336148\n",
      "Iteration 572, loss = 0.07200793\n",
      "Iteration 573, loss = 0.07307263\n",
      "Iteration 574, loss = 0.07113119\n",
      "Iteration 575, loss = 0.07159488\n",
      "Iteration 576, loss = 0.07257539\n",
      "Iteration 577, loss = 0.07242827\n",
      "Iteration 578, loss = 0.07048578\n",
      "Iteration 579, loss = 0.07108243\n",
      "Iteration 580, loss = 0.07011856\n",
      "Iteration 581, loss = 0.07461822\n",
      "Iteration 582, loss = 0.08317895\n",
      "Iteration 583, loss = 0.07581180\n",
      "Iteration 584, loss = 0.07600959\n",
      "Iteration 585, loss = 0.07594987\n",
      "Iteration 586, loss = 0.07196278\n",
      "Iteration 587, loss = 0.07064469\n",
      "Iteration 588, loss = 0.07080721\n",
      "Iteration 589, loss = 0.06838254\n",
      "Iteration 590, loss = 0.07142432\n",
      "Iteration 591, loss = 0.07163700\n",
      "Iteration 592, loss = 0.07028805\n",
      "Iteration 593, loss = 0.06836104\n",
      "Iteration 594, loss = 0.06752347\n",
      "Iteration 595, loss = 0.06771952\n",
      "Iteration 596, loss = 0.06763372\n",
      "Iteration 597, loss = 0.06661213\n",
      "Iteration 598, loss = 0.06694274\n",
      "Iteration 599, loss = 0.06654636\n",
      "Iteration 600, loss = 0.06578386\n",
      "Iteration 601, loss = 0.06788136\n",
      "Iteration 602, loss = 0.06701359\n",
      "Iteration 603, loss = 0.06687203\n",
      "Iteration 604, loss = 0.06557581\n",
      "Iteration 605, loss = 0.06582598\n",
      "Iteration 606, loss = 0.06550186\n",
      "Iteration 607, loss = 0.06480332\n",
      "Iteration 608, loss = 0.06463269\n",
      "Iteration 609, loss = 0.06498646\n",
      "Iteration 610, loss = 0.06475481\n",
      "Iteration 611, loss = 0.06426005\n",
      "Iteration 612, loss = 0.06399608\n",
      "Iteration 613, loss = 0.06721450\n",
      "Iteration 614, loss = 0.06631400\n",
      "Iteration 615, loss = 0.06378307\n",
      "Iteration 616, loss = 0.06406900\n",
      "Iteration 617, loss = 0.06352245\n",
      "Iteration 618, loss = 0.06439845\n",
      "Iteration 619, loss = 0.06296876\n",
      "Iteration 620, loss = 0.06295159\n",
      "Iteration 621, loss = 0.06303333\n",
      "Iteration 622, loss = 0.06307856\n",
      "Iteration 623, loss = 0.06261892\n",
      "Iteration 624, loss = 0.06189837\n",
      "Iteration 625, loss = 0.06213245\n",
      "Iteration 626, loss = 0.06111561\n",
      "Iteration 627, loss = 0.06081401\n",
      "Iteration 628, loss = 0.05954924\n",
      "Iteration 629, loss = 0.05928922\n",
      "Iteration 630, loss = 0.05923538\n",
      "Iteration 631, loss = 0.05885636\n",
      "Iteration 632, loss = 0.05856220\n",
      "Iteration 633, loss = 0.05774878\n",
      "Iteration 634, loss = 0.05763242\n",
      "Iteration 635, loss = 0.05714012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 636, loss = 0.05800163\n",
      "Iteration 637, loss = 0.05734265\n",
      "Iteration 638, loss = 0.05793762\n",
      "Iteration 639, loss = 0.06039113\n",
      "Iteration 640, loss = 0.05822038\n",
      "Iteration 641, loss = 0.05655752\n",
      "Iteration 642, loss = 0.05650513\n",
      "Iteration 643, loss = 0.05614835\n",
      "Iteration 644, loss = 0.05580619\n",
      "Iteration 645, loss = 0.06005414\n",
      "Iteration 646, loss = 0.05876417\n",
      "Iteration 647, loss = 0.05718450\n",
      "Iteration 648, loss = 0.05894021\n",
      "Iteration 649, loss = 0.05832804\n",
      "Iteration 650, loss = 0.05869553\n",
      "Iteration 651, loss = 0.05774865\n",
      "Iteration 652, loss = 0.05777456\n",
      "Iteration 653, loss = 0.05695232\n",
      "Iteration 654, loss = 0.05549941\n",
      "Iteration 655, loss = 0.05473524\n",
      "Iteration 656, loss = 0.05463090\n",
      "Iteration 657, loss = 0.05446881\n",
      "Iteration 658, loss = 0.05406616\n",
      "Iteration 659, loss = 0.05432530\n",
      "Iteration 660, loss = 0.05567011\n",
      "Iteration 661, loss = 0.05459621\n",
      "Iteration 662, loss = 0.05398438\n",
      "Iteration 663, loss = 0.05487655\n",
      "Iteration 664, loss = 0.05466595\n",
      "Iteration 665, loss = 0.05670468\n",
      "Iteration 666, loss = 0.05434526\n",
      "Iteration 667, loss = 0.05326695\n",
      "Iteration 668, loss = 0.05383589\n",
      "Iteration 669, loss = 0.05405671\n",
      "Iteration 670, loss = 0.05439110\n",
      "Iteration 671, loss = 0.05355593\n",
      "Iteration 672, loss = 0.05339822\n",
      "Iteration 673, loss = 0.05354584\n",
      "Iteration 674, loss = 0.05227517\n",
      "Iteration 675, loss = 0.05263560\n",
      "Iteration 676, loss = 0.05302494\n",
      "Iteration 677, loss = 0.05268939\n",
      "Iteration 678, loss = 0.05235393\n",
      "Iteration 679, loss = 0.05229199\n",
      "Iteration 680, loss = 0.05236736\n",
      "Iteration 681, loss = 0.05178686\n",
      "Iteration 682, loss = 0.05181702\n",
      "Iteration 683, loss = 0.05135503\n",
      "Iteration 684, loss = 0.05145733\n",
      "Iteration 685, loss = 0.05146623\n",
      "Iteration 686, loss = 0.05113384\n",
      "Iteration 687, loss = 0.05130927\n",
      "Iteration 688, loss = 0.05101088\n",
      "Iteration 689, loss = 0.05213581\n",
      "Iteration 690, loss = 0.05099030\n",
      "Iteration 691, loss = 0.05186396\n",
      "Iteration 692, loss = 0.05298981\n",
      "Iteration 693, loss = 0.05206357\n",
      "Iteration 694, loss = 0.05090507\n",
      "Iteration 695, loss = 0.05030548\n",
      "Iteration 696, loss = 0.05010428\n",
      "Iteration 697, loss = 0.04996325\n",
      "Iteration 698, loss = 0.05004132\n",
      "Iteration 699, loss = 0.05110198\n",
      "Iteration 700, loss = 0.05160971\n",
      "Iteration 701, loss = 0.05092808\n",
      "Iteration 702, loss = 0.05050490\n",
      "Iteration 703, loss = 0.05013447\n",
      "Iteration 704, loss = 0.04987141\n",
      "Iteration 705, loss = 0.04972862\n",
      "Iteration 706, loss = 0.04958459\n",
      "Iteration 707, loss = 0.04959927\n",
      "Iteration 708, loss = 0.04945411\n",
      "Iteration 709, loss = 0.04883484\n",
      "Iteration 710, loss = 0.04892603\n",
      "Iteration 711, loss = 0.04943698\n",
      "Iteration 712, loss = 0.04965430\n",
      "Iteration 713, loss = 0.04906855\n",
      "Iteration 714, loss = 0.04922816\n",
      "Iteration 715, loss = 0.04888136\n",
      "Iteration 716, loss = 0.04904793\n",
      "Iteration 717, loss = 0.04834956\n",
      "Iteration 718, loss = 0.04843515\n",
      "Iteration 719, loss = 0.05227668\n",
      "Iteration 720, loss = 0.05088453\n",
      "Iteration 721, loss = 0.05032510\n",
      "Iteration 722, loss = 0.05091285\n",
      "Iteration 723, loss = 0.05467862\n",
      "Iteration 724, loss = 0.04948212\n",
      "Iteration 725, loss = 0.04829040\n",
      "Iteration 726, loss = 0.04772061\n",
      "Iteration 727, loss = 0.04759616\n",
      "Iteration 728, loss = 0.04739812\n",
      "Iteration 729, loss = 0.04730698\n",
      "Iteration 730, loss = 0.04727655\n",
      "Iteration 731, loss = 0.04831919\n",
      "Iteration 732, loss = 0.04905185\n",
      "Iteration 733, loss = 0.04692492\n",
      "Iteration 734, loss = 0.04718802\n",
      "Iteration 735, loss = 0.04800006\n",
      "Iteration 736, loss = 0.04830499\n",
      "Iteration 737, loss = 0.04690505\n",
      "Iteration 738, loss = 0.04747555\n",
      "Iteration 739, loss = 0.04710423\n",
      "Iteration 740, loss = 0.04687212\n",
      "Iteration 741, loss = 0.04704038\n",
      "Iteration 742, loss = 0.04656078\n",
      "Iteration 743, loss = 0.04605321\n",
      "Iteration 744, loss = 0.04592032\n",
      "Iteration 745, loss = 0.04517435\n",
      "Iteration 746, loss = 0.04561551\n",
      "Iteration 747, loss = 0.04507910\n",
      "Iteration 748, loss = 0.04480500\n",
      "Iteration 749, loss = 0.04468933\n",
      "Iteration 750, loss = 0.04433771\n",
      "Iteration 751, loss = 0.04337282\n",
      "Iteration 752, loss = 0.04312477\n",
      "Iteration 753, loss = 0.04232073\n",
      "Iteration 754, loss = 0.04207808\n",
      "Iteration 755, loss = 0.04181178\n",
      "Iteration 756, loss = 0.04147340\n",
      "Iteration 757, loss = 0.04198804\n",
      "Iteration 758, loss = 0.04132124\n",
      "Iteration 759, loss = 0.04115392\n",
      "Iteration 760, loss = 0.04119623\n",
      "Iteration 761, loss = 0.04150752\n",
      "Iteration 762, loss = 0.04130367\n",
      "Iteration 763, loss = 0.04120128\n",
      "Iteration 764, loss = 0.04078597\n",
      "Iteration 765, loss = 0.04098992\n",
      "Iteration 766, loss = 0.04071454\n",
      "Iteration 767, loss = 0.04057804\n",
      "Iteration 768, loss = 0.04055681\n",
      "Iteration 769, loss = 0.04050435\n",
      "Iteration 770, loss = 0.04009571\n",
      "Iteration 771, loss = 0.04028438\n",
      "Iteration 772, loss = 0.04031768\n",
      "Iteration 773, loss = 0.04001976\n",
      "Iteration 774, loss = 0.04064869\n",
      "Iteration 775, loss = 0.04003559\n",
      "Iteration 776, loss = 0.03974505\n",
      "Iteration 777, loss = 0.03979942\n",
      "Iteration 778, loss = 0.03964205\n",
      "Iteration 779, loss = 0.03963690\n",
      "Iteration 780, loss = 0.03924082\n",
      "Iteration 781, loss = 0.03926166\n",
      "Iteration 782, loss = 0.03921292\n",
      "Iteration 783, loss = 0.03930942\n",
      "Iteration 784, loss = 0.03992776\n",
      "Iteration 785, loss = 0.04016903\n",
      "Iteration 786, loss = 0.04012765\n",
      "Iteration 787, loss = 0.03939243\n",
      "Iteration 788, loss = 0.03913590\n",
      "Iteration 789, loss = 0.04006781\n",
      "Iteration 790, loss = 0.04318444\n",
      "Iteration 791, loss = 0.03994163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 28 of 48...\n",
      "Iteration 1, loss = 0.72012537\n",
      "Iteration 2, loss = 0.67834078\n",
      "Iteration 3, loss = 0.66408143\n",
      "Iteration 4, loss = 0.64069672\n",
      "Iteration 5, loss = 0.60881872\n",
      "Iteration 6, loss = 0.57344269\n",
      "Iteration 7, loss = 0.53763111\n",
      "Iteration 8, loss = 0.50822006\n",
      "Iteration 9, loss = 0.48435122\n",
      "Iteration 10, loss = 0.46311778\n",
      "Iteration 11, loss = 0.44057410\n",
      "Iteration 12, loss = 0.42643459\n",
      "Iteration 13, loss = 0.41395353\n",
      "Iteration 14, loss = 0.40378266\n",
      "Iteration 15, loss = 0.39066670\n",
      "Iteration 16, loss = 0.38171999\n",
      "Iteration 17, loss = 0.37122876\n",
      "Iteration 18, loss = 0.36293190\n",
      "Iteration 19, loss = 0.35795000\n",
      "Iteration 20, loss = 0.35307832\n",
      "Iteration 21, loss = 0.34882006\n",
      "Iteration 22, loss = 0.34069202\n",
      "Iteration 23, loss = 0.33627567\n",
      "Iteration 24, loss = 0.34353098\n",
      "Iteration 25, loss = 0.32822040\n",
      "Iteration 26, loss = 0.32428633\n",
      "Iteration 27, loss = 0.31766623\n",
      "Iteration 28, loss = 0.32237214\n",
      "Iteration 29, loss = 0.30972607\n",
      "Iteration 30, loss = 0.30430735\n",
      "Iteration 31, loss = 0.30054902\n",
      "Iteration 32, loss = 0.29519598\n",
      "Iteration 33, loss = 0.29220428\n",
      "Iteration 34, loss = 0.29028149\n",
      "Iteration 35, loss = 0.29037761\n",
      "Iteration 36, loss = 0.28352114\n",
      "Iteration 37, loss = 0.28151886\n",
      "Iteration 38, loss = 0.27891913\n",
      "Iteration 39, loss = 0.27027544\n",
      "Iteration 40, loss = 0.26668846\n",
      "Iteration 41, loss = 0.26779340\n",
      "Iteration 42, loss = 0.26406468\n",
      "Iteration 43, loss = 0.26336134\n",
      "Iteration 44, loss = 0.25649305\n",
      "Iteration 45, loss = 0.25407479\n",
      "Iteration 46, loss = 0.25287109\n",
      "Iteration 47, loss = 0.24706757\n",
      "Iteration 48, loss = 0.24766784\n",
      "Iteration 49, loss = 0.24067028\n",
      "Iteration 50, loss = 0.23517360\n",
      "Iteration 51, loss = 0.23216861\n",
      "Iteration 52, loss = 0.22924079\n",
      "Iteration 53, loss = 0.22775697\n",
      "Iteration 54, loss = 0.22754585\n",
      "Iteration 55, loss = 0.22201428\n",
      "Iteration 56, loss = 0.22013323\n",
      "Iteration 57, loss = 0.22431336\n",
      "Iteration 58, loss = 0.21760113\n",
      "Iteration 59, loss = 0.21170319\n",
      "Iteration 60, loss = 0.20861160\n",
      "Iteration 61, loss = 0.21109169\n",
      "Iteration 62, loss = 0.21331335\n",
      "Iteration 63, loss = 0.21061061\n",
      "Iteration 64, loss = 0.20207476\n",
      "Iteration 65, loss = 0.19763933\n",
      "Iteration 66, loss = 0.19754692\n",
      "Iteration 67, loss = 0.19474926\n",
      "Iteration 68, loss = 0.19021248\n",
      "Iteration 69, loss = 0.18861947\n",
      "Iteration 70, loss = 0.18726615\n",
      "Iteration 71, loss = 0.18517376\n",
      "Iteration 72, loss = 0.18473124\n",
      "Iteration 73, loss = 0.18153893\n",
      "Iteration 74, loss = 0.17668637\n",
      "Iteration 75, loss = 0.17516263\n",
      "Iteration 76, loss = 0.17438030\n",
      "Iteration 77, loss = 0.17363010\n",
      "Iteration 78, loss = 0.17002565\n",
      "Iteration 79, loss = 0.16651927\n",
      "Iteration 80, loss = 0.16427766\n",
      "Iteration 81, loss = 0.16366210\n",
      "Iteration 82, loss = 0.15979318\n",
      "Iteration 83, loss = 0.15809460\n",
      "Iteration 84, loss = 0.15699562\n",
      "Iteration 85, loss = 0.15478446\n",
      "Iteration 86, loss = 0.15166986\n",
      "Iteration 87, loss = 0.15261425\n",
      "Iteration 88, loss = 0.14932231\n",
      "Iteration 89, loss = 0.14616151\n",
      "Iteration 90, loss = 0.14736276\n",
      "Iteration 91, loss = 0.14569532\n",
      "Iteration 92, loss = 0.14343908\n",
      "Iteration 93, loss = 0.14222212\n",
      "Iteration 94, loss = 0.13808340\n",
      "Iteration 95, loss = 0.13845019\n",
      "Iteration 96, loss = 0.13888138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 97, loss = 0.13502374\n",
      "Iteration 98, loss = 0.13404920\n",
      "Iteration 99, loss = 0.12955356\n",
      "Iteration 100, loss = 0.12967254\n",
      "Iteration 101, loss = 0.12756477\n",
      "Iteration 102, loss = 0.12505479\n",
      "Iteration 103, loss = 0.12347761\n",
      "Iteration 104, loss = 0.12425583\n",
      "Iteration 105, loss = 0.12185900\n",
      "Iteration 106, loss = 0.11953846\n",
      "Iteration 107, loss = 0.11902255\n",
      "Iteration 108, loss = 0.12165334\n",
      "Iteration 109, loss = 0.11471700\n",
      "Iteration 110, loss = 0.11521435\n",
      "Iteration 111, loss = 0.12485072\n",
      "Iteration 112, loss = 0.11233135\n",
      "Iteration 113, loss = 0.10691053\n",
      "Iteration 114, loss = 0.11445005\n",
      "Iteration 115, loss = 0.11011290\n",
      "Iteration 116, loss = 0.10760652\n",
      "Iteration 117, loss = 0.10458090\n",
      "Iteration 118, loss = 0.10570465\n",
      "Iteration 119, loss = 0.10147056\n",
      "Iteration 120, loss = 0.10071997\n",
      "Iteration 121, loss = 0.09806722\n",
      "Iteration 122, loss = 0.09830970\n",
      "Iteration 123, loss = 0.09731402\n",
      "Iteration 124, loss = 0.09511535\n",
      "Iteration 125, loss = 0.09112590\n",
      "Iteration 126, loss = 0.09804545\n",
      "Iteration 127, loss = 0.09583123\n",
      "Iteration 128, loss = 0.09224748\n",
      "Iteration 129, loss = 0.09232405\n",
      "Iteration 130, loss = 0.08644406\n",
      "Iteration 131, loss = 0.08732696\n",
      "Iteration 132, loss = 0.08619568\n",
      "Iteration 133, loss = 0.08897819\n",
      "Iteration 134, loss = 0.08522084\n",
      "Iteration 135, loss = 0.08142658\n",
      "Iteration 136, loss = 0.08644776\n",
      "Iteration 137, loss = 0.08030884\n",
      "Iteration 138, loss = 0.08173024\n",
      "Iteration 139, loss = 0.08075633\n",
      "Iteration 140, loss = 0.08140704\n",
      "Iteration 141, loss = 0.07489869\n",
      "Iteration 142, loss = 0.07661319\n",
      "Iteration 143, loss = 0.07870478\n",
      "Iteration 144, loss = 0.07499920\n",
      "Iteration 145, loss = 0.07166105\n",
      "Iteration 146, loss = 0.07107478\n",
      "Iteration 147, loss = 0.07366177\n",
      "Iteration 148, loss = 0.06690060\n",
      "Iteration 149, loss = 0.06500861\n",
      "Iteration 150, loss = 0.06517868\n",
      "Iteration 151, loss = 0.06587951\n",
      "Iteration 152, loss = 0.07137723\n",
      "Iteration 153, loss = 0.07015634\n",
      "Iteration 154, loss = 0.06762217\n",
      "Iteration 155, loss = 0.06668935\n",
      "Iteration 156, loss = 0.06881058\n",
      "Iteration 157, loss = 0.06989672\n",
      "Iteration 158, loss = 0.06414242\n",
      "Iteration 159, loss = 0.06174528\n",
      "Iteration 160, loss = 0.05777639\n",
      "Iteration 161, loss = 0.05784764\n",
      "Iteration 162, loss = 0.05536903\n",
      "Iteration 163, loss = 0.05429289\n",
      "Iteration 164, loss = 0.05613464\n",
      "Iteration 165, loss = 0.05515334\n",
      "Iteration 166, loss = 0.05418885\n",
      "Iteration 167, loss = 0.05334217\n",
      "Iteration 168, loss = 0.05247067\n",
      "Iteration 169, loss = 0.05437443\n",
      "Iteration 170, loss = 0.05562565\n",
      "Iteration 171, loss = 0.05457344\n",
      "Iteration 172, loss = 0.05017578\n",
      "Iteration 173, loss = 0.05262016\n",
      "Iteration 174, loss = 0.05631946\n",
      "Iteration 175, loss = 0.04846972\n",
      "Iteration 176, loss = 0.04671488\n",
      "Iteration 177, loss = 0.04864821\n",
      "Iteration 178, loss = 0.05566238\n",
      "Iteration 179, loss = 0.05565216\n",
      "Iteration 180, loss = 0.05502985\n",
      "Iteration 181, loss = 0.04634924\n",
      "Iteration 182, loss = 0.04393446\n",
      "Iteration 183, loss = 0.04205787\n",
      "Iteration 184, loss = 0.04244390\n",
      "Iteration 185, loss = 0.04269236\n",
      "Iteration 186, loss = 0.04285553\n",
      "Iteration 187, loss = 0.04239696\n",
      "Iteration 188, loss = 0.04056054\n",
      "Iteration 189, loss = 0.04235701\n",
      "Iteration 190, loss = 0.04174814\n",
      "Iteration 191, loss = 0.04129161\n",
      "Iteration 192, loss = 0.03972598\n",
      "Iteration 193, loss = 0.03890716\n",
      "Iteration 194, loss = 0.03956416\n",
      "Iteration 195, loss = 0.03765530\n",
      "Iteration 196, loss = 0.03750952\n",
      "Iteration 197, loss = 0.03613742\n",
      "Iteration 198, loss = 0.03821798\n",
      "Iteration 199, loss = 0.03703294\n",
      "Iteration 200, loss = 0.03544754\n",
      "Iteration 201, loss = 0.03935189\n",
      "Iteration 202, loss = 0.03420200\n",
      "Iteration 203, loss = 0.03438598\n",
      "Iteration 204, loss = 0.03430393\n",
      "Iteration 205, loss = 0.03767644\n",
      "Iteration 206, loss = 0.04161162\n",
      "Iteration 207, loss = 0.03509926\n",
      "Iteration 208, loss = 0.03082548\n",
      "Iteration 209, loss = 0.03135069\n",
      "Iteration 210, loss = 0.03112639\n",
      "Iteration 211, loss = 0.03040494\n",
      "Iteration 212, loss = 0.02984176\n",
      "Iteration 213, loss = 0.02835847\n",
      "Iteration 214, loss = 0.02995857\n",
      "Iteration 215, loss = 0.02963282\n",
      "Iteration 216, loss = 0.02843841\n",
      "Iteration 217, loss = 0.02656395\n",
      "Iteration 218, loss = 0.02784343\n",
      "Iteration 219, loss = 0.02739731\n",
      "Iteration 220, loss = 0.02706008\n",
      "Iteration 221, loss = 0.02755589\n",
      "Iteration 222, loss = 0.02647852\n",
      "Iteration 223, loss = 0.02720630\n",
      "Iteration 224, loss = 0.02643111\n",
      "Iteration 225, loss = 0.02729170\n",
      "Iteration 226, loss = 0.02750423\n",
      "Iteration 227, loss = 0.03054708\n",
      "Iteration 228, loss = 0.02719481\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 29 of 48...\n",
      "Iteration 1, loss = 0.71281399\n",
      "Iteration 2, loss = 0.63000419\n",
      "Iteration 3, loss = 0.58889921\n",
      "Iteration 4, loss = 0.55415443\n",
      "Iteration 5, loss = 0.52240630\n",
      "Iteration 6, loss = 0.49596655\n",
      "Iteration 7, loss = 0.47326076\n",
      "Iteration 8, loss = 0.45489902\n",
      "Iteration 9, loss = 0.43857723\n",
      "Iteration 10, loss = 0.42428901\n",
      "Iteration 11, loss = 0.41354751\n",
      "Iteration 12, loss = 0.40476854\n",
      "Iteration 13, loss = 0.39354703\n",
      "Iteration 14, loss = 0.38534864\n",
      "Iteration 15, loss = 0.37520623\n",
      "Iteration 16, loss = 0.36872616\n",
      "Iteration 17, loss = 0.36136054\n",
      "Iteration 18, loss = 0.35470078\n",
      "Iteration 19, loss = 0.35138339\n",
      "Iteration 20, loss = 0.34800202\n",
      "Iteration 21, loss = 0.34337908\n",
      "Iteration 22, loss = 0.33590507\n",
      "Iteration 23, loss = 0.33281893\n",
      "Iteration 24, loss = 0.32540722\n",
      "Iteration 25, loss = 0.32616400\n",
      "Iteration 26, loss = 0.31689475\n",
      "Iteration 27, loss = 0.31277664\n",
      "Iteration 28, loss = 0.31463350\n",
      "Iteration 29, loss = 0.30940737\n",
      "Iteration 30, loss = 0.30585317\n",
      "Iteration 31, loss = 0.29667447\n",
      "Iteration 32, loss = 0.29404847\n",
      "Iteration 33, loss = 0.29108020\n",
      "Iteration 34, loss = 0.28794958\n",
      "Iteration 35, loss = 0.28641404\n",
      "Iteration 36, loss = 0.27915504\n",
      "Iteration 37, loss = 0.27785100\n",
      "Iteration 38, loss = 0.27796594\n",
      "Iteration 39, loss = 0.26819294\n",
      "Iteration 40, loss = 0.26676518\n",
      "Iteration 41, loss = 0.26587111\n",
      "Iteration 42, loss = 0.26251705\n",
      "Iteration 43, loss = 0.26077600\n",
      "Iteration 44, loss = 0.25499458\n",
      "Iteration 45, loss = 0.25394273\n",
      "Iteration 46, loss = 0.25260689\n",
      "Iteration 47, loss = 0.24714678\n",
      "Iteration 48, loss = 0.24142784\n",
      "Iteration 49, loss = 0.24088010\n",
      "Iteration 50, loss = 0.24017786\n",
      "Iteration 51, loss = 0.24691158\n",
      "Iteration 52, loss = 0.23705690\n",
      "Iteration 53, loss = 0.24394974\n",
      "Iteration 54, loss = 0.22780470\n",
      "Iteration 55, loss = 0.22935757\n",
      "Iteration 56, loss = 0.22566186\n",
      "Iteration 57, loss = 0.22073016\n",
      "Iteration 58, loss = 0.22122811\n",
      "Iteration 59, loss = 0.21792859\n",
      "Iteration 60, loss = 0.21426315\n",
      "Iteration 61, loss = 0.21508298\n",
      "Iteration 62, loss = 0.21366457\n",
      "Iteration 63, loss = 0.20984933\n",
      "Iteration 64, loss = 0.21865889\n",
      "Iteration 65, loss = 0.20542431\n",
      "Iteration 66, loss = 0.20148463\n",
      "Iteration 67, loss = 0.19770037\n",
      "Iteration 68, loss = 0.19463793\n",
      "Iteration 69, loss = 0.19498602\n",
      "Iteration 70, loss = 0.19376569\n",
      "Iteration 71, loss = 0.18942365\n",
      "Iteration 72, loss = 0.18684780\n",
      "Iteration 73, loss = 0.18656716\n",
      "Iteration 74, loss = 0.18919713\n",
      "Iteration 75, loss = 0.18116502\n",
      "Iteration 76, loss = 0.17792106\n",
      "Iteration 77, loss = 0.17675260\n",
      "Iteration 78, loss = 0.17976784\n",
      "Iteration 79, loss = 0.17745818\n",
      "Iteration 80, loss = 0.17320091\n",
      "Iteration 81, loss = 0.17213098\n",
      "Iteration 82, loss = 0.16978914\n",
      "Iteration 83, loss = 0.16378124\n",
      "Iteration 84, loss = 0.16643452\n",
      "Iteration 85, loss = 0.17117991\n",
      "Iteration 86, loss = 0.16018919\n",
      "Iteration 87, loss = 0.16183659\n",
      "Iteration 88, loss = 0.15863959\n",
      "Iteration 89, loss = 0.15169652\n",
      "Iteration 90, loss = 0.15291471\n",
      "Iteration 91, loss = 0.15302845\n",
      "Iteration 92, loss = 0.15175763\n",
      "Iteration 93, loss = 0.14860613\n",
      "Iteration 94, loss = 0.14444610\n",
      "Iteration 95, loss = 0.14627621\n",
      "Iteration 96, loss = 0.14220740\n",
      "Iteration 97, loss = 0.14547224\n",
      "Iteration 98, loss = 0.14116459\n",
      "Iteration 99, loss = 0.13704224\n",
      "Iteration 100, loss = 0.13882416\n",
      "Iteration 101, loss = 0.13719898\n",
      "Iteration 102, loss = 0.13375510\n",
      "Iteration 103, loss = 0.14092150\n",
      "Iteration 104, loss = 0.13482252\n",
      "Iteration 105, loss = 0.12750600\n",
      "Iteration 106, loss = 0.12561501\n",
      "Iteration 107, loss = 0.12624511\n",
      "Iteration 108, loss = 0.12486415\n",
      "Iteration 109, loss = 0.12302672\n",
      "Iteration 110, loss = 0.12375203\n",
      "Iteration 111, loss = 0.12669112\n",
      "Iteration 112, loss = 0.11841983\n",
      "Iteration 113, loss = 0.11545193\n",
      "Iteration 114, loss = 0.11468791\n",
      "Iteration 115, loss = 0.11222649\n",
      "Iteration 116, loss = 0.11025462\n",
      "Iteration 117, loss = 0.11357209\n",
      "Iteration 118, loss = 0.11793998\n",
      "Iteration 119, loss = 0.11329083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 120, loss = 0.10718074\n",
      "Iteration 121, loss = 0.10557237\n",
      "Iteration 122, loss = 0.10455593\n",
      "Iteration 123, loss = 0.10803678\n",
      "Iteration 124, loss = 0.10712052\n",
      "Iteration 125, loss = 0.10257078\n",
      "Iteration 126, loss = 0.10088963\n",
      "Iteration 127, loss = 0.09871926\n",
      "Iteration 128, loss = 0.09565123\n",
      "Iteration 129, loss = 0.09390955\n",
      "Iteration 130, loss = 0.09333094\n",
      "Iteration 131, loss = 0.09397752\n",
      "Iteration 132, loss = 0.10048685\n",
      "Iteration 133, loss = 0.10927364\n",
      "Iteration 134, loss = 0.10217840\n",
      "Iteration 135, loss = 0.09419960\n",
      "Iteration 136, loss = 0.08767363\n",
      "Iteration 137, loss = 0.08769240\n",
      "Iteration 138, loss = 0.08873816\n",
      "Iteration 139, loss = 0.08500465\n",
      "Iteration 140, loss = 0.08668022\n",
      "Iteration 141, loss = 0.09054489\n",
      "Iteration 142, loss = 0.08595528\n",
      "Iteration 143, loss = 0.08698728\n",
      "Iteration 144, loss = 0.08421160\n",
      "Iteration 145, loss = 0.08007789\n",
      "Iteration 146, loss = 0.07863081\n",
      "Iteration 147, loss = 0.08089027\n",
      "Iteration 148, loss = 0.07883897\n",
      "Iteration 149, loss = 0.07319930\n",
      "Iteration 150, loss = 0.07548404\n",
      "Iteration 151, loss = 0.07340693\n",
      "Iteration 152, loss = 0.07006700\n",
      "Iteration 153, loss = 0.07121942\n",
      "Iteration 154, loss = 0.07295707\n",
      "Iteration 155, loss = 0.06742878\n",
      "Iteration 156, loss = 0.07043769\n",
      "Iteration 157, loss = 0.07024008\n",
      "Iteration 158, loss = 0.07065240\n",
      "Iteration 159, loss = 0.06597517\n",
      "Iteration 160, loss = 0.06342064\n",
      "Iteration 161, loss = 0.06238757\n",
      "Iteration 162, loss = 0.06132229\n",
      "Iteration 163, loss = 0.06622600\n",
      "Iteration 164, loss = 0.07124281\n",
      "Iteration 165, loss = 0.06912548\n",
      "Iteration 166, loss = 0.06819862\n",
      "Iteration 167, loss = 0.06306380\n",
      "Iteration 168, loss = 0.06695572\n",
      "Iteration 169, loss = 0.07503949\n",
      "Iteration 170, loss = 0.06815961\n",
      "Iteration 171, loss = 0.06910360\n",
      "Iteration 172, loss = 0.07105408\n",
      "Iteration 173, loss = 0.06770112\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 30 of 48...\n",
      "Iteration 1, loss = 0.72578117\n",
      "Iteration 2, loss = 0.64994565\n",
      "Iteration 3, loss = 0.60783132\n",
      "Iteration 4, loss = 0.56915640\n",
      "Iteration 5, loss = 0.53497712\n",
      "Iteration 6, loss = 0.50819277\n",
      "Iteration 7, loss = 0.48464494\n",
      "Iteration 8, loss = 0.46425390\n",
      "Iteration 9, loss = 0.44465859\n",
      "Iteration 10, loss = 0.43408196\n",
      "Iteration 11, loss = 0.42162518\n",
      "Iteration 12, loss = 0.40894435\n",
      "Iteration 13, loss = 0.40485392\n",
      "Iteration 14, loss = 0.39031350\n",
      "Iteration 15, loss = 0.37951509\n",
      "Iteration 16, loss = 0.38007666\n",
      "Iteration 17, loss = 0.37032546\n",
      "Iteration 18, loss = 0.36032579\n",
      "Iteration 19, loss = 0.35486207\n",
      "Iteration 20, loss = 0.35088293\n",
      "Iteration 21, loss = 0.34306684\n",
      "Iteration 22, loss = 0.33783463\n",
      "Iteration 23, loss = 0.34103541\n",
      "Iteration 24, loss = 0.32980213\n",
      "Iteration 25, loss = 0.32707258\n",
      "Iteration 26, loss = 0.32040649\n",
      "Iteration 27, loss = 0.32019701\n",
      "Iteration 28, loss = 0.31665750\n",
      "Iteration 29, loss = 0.31475791\n",
      "Iteration 30, loss = 0.30800942\n",
      "Iteration 31, loss = 0.30090590\n",
      "Iteration 32, loss = 0.29682989\n",
      "Iteration 33, loss = 0.29379085\n",
      "Iteration 34, loss = 0.29436942\n",
      "Iteration 35, loss = 0.30196061\n",
      "Iteration 36, loss = 0.28546049\n",
      "Iteration 37, loss = 0.28144531\n",
      "Iteration 38, loss = 0.28077972\n",
      "Iteration 39, loss = 0.28533976\n",
      "Iteration 40, loss = 0.27519642\n",
      "Iteration 41, loss = 0.27020922\n",
      "Iteration 42, loss = 0.26725257\n",
      "Iteration 43, loss = 0.26572542\n",
      "Iteration 44, loss = 0.26043947\n",
      "Iteration 45, loss = 0.25642078\n",
      "Iteration 46, loss = 0.25742851\n",
      "Iteration 47, loss = 0.25417877\n",
      "Iteration 48, loss = 0.25223241\n",
      "Iteration 49, loss = 0.26029750\n",
      "Iteration 50, loss = 0.24513156\n",
      "Iteration 51, loss = 0.24925287\n",
      "Iteration 52, loss = 0.24800148\n",
      "Iteration 53, loss = 0.23562293\n",
      "Iteration 54, loss = 0.23023030\n",
      "Iteration 55, loss = 0.22627337\n",
      "Iteration 56, loss = 0.23005388\n",
      "Iteration 57, loss = 0.22845036\n",
      "Iteration 58, loss = 0.22344476\n",
      "Iteration 59, loss = 0.22874830\n",
      "Iteration 60, loss = 0.22795780\n",
      "Iteration 61, loss = 0.22019053\n",
      "Iteration 62, loss = 0.21331863\n",
      "Iteration 63, loss = 0.21423045\n",
      "Iteration 64, loss = 0.20798978\n",
      "Iteration 65, loss = 0.20375721\n",
      "Iteration 66, loss = 0.20317741\n",
      "Iteration 67, loss = 0.21162571\n",
      "Iteration 68, loss = 0.20586093\n",
      "Iteration 69, loss = 0.19757641\n",
      "Iteration 70, loss = 0.19051547\n",
      "Iteration 71, loss = 0.19043440\n",
      "Iteration 72, loss = 0.18437489\n",
      "Iteration 73, loss = 0.18296091\n",
      "Iteration 74, loss = 0.18169667\n",
      "Iteration 75, loss = 0.17896088\n",
      "Iteration 76, loss = 0.18406799\n",
      "Iteration 77, loss = 0.18233026\n",
      "Iteration 78, loss = 0.18019607\n",
      "Iteration 79, loss = 0.17604639\n",
      "Iteration 80, loss = 0.16870186\n",
      "Iteration 81, loss = 0.17520139\n",
      "Iteration 82, loss = 0.17374246\n",
      "Iteration 83, loss = 0.16519187\n",
      "Iteration 84, loss = 0.16174877\n",
      "Iteration 85, loss = 0.16462072\n",
      "Iteration 86, loss = 0.16257219\n",
      "Iteration 87, loss = 0.15841348\n",
      "Iteration 88, loss = 0.15407936\n",
      "Iteration 89, loss = 0.15024124\n",
      "Iteration 90, loss = 0.15134546\n",
      "Iteration 91, loss = 0.14799896\n",
      "Iteration 92, loss = 0.14704710\n",
      "Iteration 93, loss = 0.14301555\n",
      "Iteration 94, loss = 0.14260200\n",
      "Iteration 95, loss = 0.14706700\n",
      "Iteration 96, loss = 0.14310519\n",
      "Iteration 97, loss = 0.14132754\n",
      "Iteration 98, loss = 0.13917107\n",
      "Iteration 99, loss = 0.13673771\n",
      "Iteration 100, loss = 0.13265614\n",
      "Iteration 101, loss = 0.13461344\n",
      "Iteration 102, loss = 0.13211206\n",
      "Iteration 103, loss = 0.12788558\n",
      "Iteration 104, loss = 0.14010891\n",
      "Iteration 105, loss = 0.14701887\n",
      "Iteration 106, loss = 0.12357049\n",
      "Iteration 107, loss = 0.12078719\n",
      "Iteration 108, loss = 0.12345658\n",
      "Iteration 109, loss = 0.12281200\n",
      "Iteration 110, loss = 0.11863740\n",
      "Iteration 111, loss = 0.11607000\n",
      "Iteration 112, loss = 0.11480717\n",
      "Iteration 113, loss = 0.11491568\n",
      "Iteration 114, loss = 0.11276839\n",
      "Iteration 115, loss = 0.11512763\n",
      "Iteration 116, loss = 0.10902239\n",
      "Iteration 117, loss = 0.10756147\n",
      "Iteration 118, loss = 0.10964564\n",
      "Iteration 119, loss = 0.10586325\n",
      "Iteration 120, loss = 0.10352288\n",
      "Iteration 121, loss = 0.10350019\n",
      "Iteration 122, loss = 0.10536323\n",
      "Iteration 123, loss = 0.09881447\n",
      "Iteration 124, loss = 0.09972601\n",
      "Iteration 125, loss = 0.10174986\n",
      "Iteration 126, loss = 0.09808579\n",
      "Iteration 127, loss = 0.09551436\n",
      "Iteration 128, loss = 0.09137810\n",
      "Iteration 129, loss = 0.09036217\n",
      "Iteration 130, loss = 0.09398604\n",
      "Iteration 131, loss = 0.09802997\n",
      "Iteration 132, loss = 0.09112896\n",
      "Iteration 133, loss = 0.08880900\n",
      "Iteration 134, loss = 0.09295212\n",
      "Iteration 135, loss = 0.08764347\n",
      "Iteration 136, loss = 0.08668893\n",
      "Iteration 137, loss = 0.08297009\n",
      "Iteration 138, loss = 0.08210603\n",
      "Iteration 139, loss = 0.08224295\n",
      "Iteration 140, loss = 0.08170566\n",
      "Iteration 141, loss = 0.08045727\n",
      "Iteration 142, loss = 0.07809552\n",
      "Iteration 143, loss = 0.08089374\n",
      "Iteration 144, loss = 0.07903847\n",
      "Iteration 145, loss = 0.07809537\n",
      "Iteration 146, loss = 0.07748333\n",
      "Iteration 147, loss = 0.07487429\n",
      "Iteration 148, loss = 0.07352052\n",
      "Iteration 149, loss = 0.07633578\n",
      "Iteration 150, loss = 0.07254103\n",
      "Iteration 151, loss = 0.07237815\n",
      "Iteration 152, loss = 0.07186849\n",
      "Iteration 153, loss = 0.06786347\n",
      "Iteration 154, loss = 0.07063109\n",
      "Iteration 155, loss = 0.06640479\n",
      "Iteration 156, loss = 0.06719263\n",
      "Iteration 157, loss = 0.06871305\n",
      "Iteration 158, loss = 0.06534569\n",
      "Iteration 159, loss = 0.06145557\n",
      "Iteration 160, loss = 0.06678999\n",
      "Iteration 161, loss = 0.06479636\n",
      "Iteration 162, loss = 0.06233665\n",
      "Iteration 163, loss = 0.06072263\n",
      "Iteration 164, loss = 0.06061758\n",
      "Iteration 165, loss = 0.05982714\n",
      "Iteration 166, loss = 0.05725527\n",
      "Iteration 167, loss = 0.06184308\n",
      "Iteration 168, loss = 0.05663315\n",
      "Iteration 169, loss = 0.05522322\n",
      "Iteration 170, loss = 0.05760761\n",
      "Iteration 171, loss = 0.05645588\n",
      "Iteration 172, loss = 0.06269239\n",
      "Iteration 173, loss = 0.05832202\n",
      "Iteration 174, loss = 0.05628728\n",
      "Iteration 175, loss = 0.05399116\n",
      "Iteration 176, loss = 0.05635184\n",
      "Iteration 177, loss = 0.05179717\n",
      "Iteration 178, loss = 0.05349527\n",
      "Iteration 179, loss = 0.05547336\n",
      "Iteration 180, loss = 0.05866998\n",
      "Iteration 181, loss = 0.05336374\n",
      "Iteration 182, loss = 0.05115145\n",
      "Iteration 183, loss = 0.05066283\n",
      "Iteration 184, loss = 0.05425950\n",
      "Iteration 185, loss = 0.05024389\n",
      "Iteration 186, loss = 0.04831831\n",
      "Iteration 187, loss = 0.04784779\n",
      "Iteration 188, loss = 0.04504348\n",
      "Iteration 189, loss = 0.04492078\n",
      "Iteration 190, loss = 0.05162235\n",
      "Iteration 191, loss = 0.04662821\n",
      "Iteration 192, loss = 0.04333150\n",
      "Iteration 193, loss = 0.04416135\n",
      "Iteration 194, loss = 0.04623220\n",
      "Iteration 195, loss = 0.04811135\n",
      "Iteration 196, loss = 0.04384217\n",
      "Iteration 197, loss = 0.04382274\n",
      "Iteration 198, loss = 0.03988656\n",
      "Iteration 199, loss = 0.04069338\n",
      "Iteration 200, loss = 0.04035621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 201, loss = 0.04030444\n",
      "Iteration 202, loss = 0.03923581\n",
      "Iteration 203, loss = 0.04079267\n",
      "Iteration 204, loss = 0.04131239\n",
      "Iteration 205, loss = 0.05256045\n",
      "Iteration 206, loss = 0.04150128\n",
      "Iteration 207, loss = 0.03773217\n",
      "Iteration 208, loss = 0.03647869\n",
      "Iteration 209, loss = 0.03640216\n",
      "Iteration 210, loss = 0.03950860\n",
      "Iteration 211, loss = 0.04313941\n",
      "Iteration 212, loss = 0.04379566\n",
      "Iteration 213, loss = 0.04022334\n",
      "Iteration 214, loss = 0.03539839\n",
      "Iteration 215, loss = 0.03692728\n",
      "Iteration 216, loss = 0.03936586\n",
      "Iteration 217, loss = 0.03840625\n",
      "Iteration 218, loss = 0.03812775\n",
      "Iteration 219, loss = 0.03612833\n",
      "Iteration 220, loss = 0.03427589\n",
      "Iteration 221, loss = 0.03443577\n",
      "Iteration 222, loss = 0.04063978\n",
      "Iteration 223, loss = 0.04917922\n",
      "Iteration 224, loss = 0.05164859\n",
      "Iteration 225, loss = 0.07212358\n",
      "Iteration 226, loss = 0.06409201\n",
      "Iteration 227, loss = 0.04411800\n",
      "Iteration 228, loss = 0.03868299\n",
      "Iteration 229, loss = 0.03449699\n",
      "Iteration 230, loss = 0.03601657\n",
      "Iteration 231, loss = 0.03271367\n",
      "Iteration 232, loss = 0.03247077\n",
      "Iteration 233, loss = 0.03088420\n",
      "Iteration 234, loss = 0.03082380\n",
      "Iteration 235, loss = 0.03136571\n",
      "Iteration 236, loss = 0.03314662\n",
      "Iteration 237, loss = 0.03273854\n",
      "Iteration 238, loss = 0.03352407\n",
      "Iteration 239, loss = 0.03037085\n",
      "Iteration 240, loss = 0.03085748\n",
      "Iteration 241, loss = 0.02952581\n",
      "Iteration 242, loss = 0.02641996\n",
      "Iteration 243, loss = 0.02804742\n",
      "Iteration 244, loss = 0.02667798\n",
      "Iteration 245, loss = 0.02898345\n",
      "Iteration 246, loss = 0.02688821\n",
      "Iteration 247, loss = 0.02753394\n",
      "Iteration 248, loss = 0.03084046\n",
      "Iteration 249, loss = 0.03113614\n",
      "Iteration 250, loss = 0.02878369\n",
      "Iteration 251, loss = 0.02787313\n",
      "Iteration 252, loss = 0.02786902\n",
      "Iteration 253, loss = 0.02650350\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 31 of 48...\n",
      "Iteration 1, loss = 0.68759740\n",
      "Iteration 2, loss = 0.67913045\n",
      "Iteration 3, loss = 0.66993504\n",
      "Iteration 4, loss = 0.65768851\n",
      "Iteration 5, loss = 0.64287882\n",
      "Iteration 6, loss = 0.62626457\n",
      "Iteration 7, loss = 0.60897387\n",
      "Iteration 8, loss = 0.59097873\n",
      "Iteration 9, loss = 0.57346608\n",
      "Iteration 10, loss = 0.55575579\n",
      "Iteration 11, loss = 0.53827069\n",
      "Iteration 12, loss = 0.52191417\n",
      "Iteration 13, loss = 0.50754189\n",
      "Iteration 14, loss = 0.49384377\n",
      "Iteration 15, loss = 0.48052609\n",
      "Iteration 16, loss = 0.46890520\n",
      "Iteration 17, loss = 0.45865366\n",
      "Iteration 18, loss = 0.44896556\n",
      "Iteration 19, loss = 0.44089584\n",
      "Iteration 20, loss = 0.43255670\n",
      "Iteration 21, loss = 0.42542497\n",
      "Iteration 22, loss = 0.41724783\n",
      "Iteration 23, loss = 0.41272410\n",
      "Iteration 24, loss = 0.40726974\n",
      "Iteration 25, loss = 0.40232474\n",
      "Iteration 26, loss = 0.39635635\n",
      "Iteration 27, loss = 0.39110514\n",
      "Iteration 28, loss = 0.38705026\n",
      "Iteration 29, loss = 0.38326826\n",
      "Iteration 30, loss = 0.38063692\n",
      "Iteration 31, loss = 0.37615580\n",
      "Iteration 32, loss = 0.37189210\n",
      "Iteration 33, loss = 0.36914097\n",
      "Iteration 34, loss = 0.36920431\n",
      "Iteration 35, loss = 0.36521989\n",
      "Iteration 36, loss = 0.36344246\n",
      "Iteration 37, loss = 0.36113192\n",
      "Iteration 38, loss = 0.35648432\n",
      "Iteration 39, loss = 0.35491646\n",
      "Iteration 40, loss = 0.35014575\n",
      "Iteration 41, loss = 0.34717997\n",
      "Iteration 42, loss = 0.34522497\n",
      "Iteration 43, loss = 0.34223438\n",
      "Iteration 44, loss = 0.34221008\n",
      "Iteration 45, loss = 0.33927612\n",
      "Iteration 46, loss = 0.33618605\n",
      "Iteration 47, loss = 0.33468894\n",
      "Iteration 48, loss = 0.33211765\n",
      "Iteration 49, loss = 0.32982483\n",
      "Iteration 50, loss = 0.32756523\n",
      "Iteration 51, loss = 0.32511944\n",
      "Iteration 52, loss = 0.32427573\n",
      "Iteration 53, loss = 0.32324427\n",
      "Iteration 54, loss = 0.32191790\n",
      "Iteration 55, loss = 0.31944803\n",
      "Iteration 56, loss = 0.31762781\n",
      "Iteration 57, loss = 0.31604912\n",
      "Iteration 58, loss = 0.31313947\n",
      "Iteration 59, loss = 0.31163701\n",
      "Iteration 60, loss = 0.31112876\n",
      "Iteration 61, loss = 0.30933449\n",
      "Iteration 62, loss = 0.30858104\n",
      "Iteration 63, loss = 0.30694272\n",
      "Iteration 64, loss = 0.31315498\n",
      "Iteration 65, loss = 0.30661024\n",
      "Iteration 66, loss = 0.30455722\n",
      "Iteration 67, loss = 0.30093036\n",
      "Iteration 68, loss = 0.29910041\n",
      "Iteration 69, loss = 0.29822782\n",
      "Iteration 70, loss = 0.29828255\n",
      "Iteration 71, loss = 0.29574109\n",
      "Iteration 72, loss = 0.29972939\n",
      "Iteration 73, loss = 0.29697464\n",
      "Iteration 74, loss = 0.29391050\n",
      "Iteration 75, loss = 0.29234939\n",
      "Iteration 76, loss = 0.29625259\n",
      "Iteration 77, loss = 0.29036308\n",
      "Iteration 78, loss = 0.29060483\n",
      "Iteration 79, loss = 0.28749040\n",
      "Iteration 80, loss = 0.28441623\n",
      "Iteration 81, loss = 0.28374961\n",
      "Iteration 82, loss = 0.28188451\n",
      "Iteration 83, loss = 0.28131070\n",
      "Iteration 84, loss = 0.27988270\n",
      "Iteration 85, loss = 0.27813279\n",
      "Iteration 86, loss = 0.28040634\n",
      "Iteration 87, loss = 0.27734803\n",
      "Iteration 88, loss = 0.27594085\n",
      "Iteration 89, loss = 0.27381231\n",
      "Iteration 90, loss = 0.27277041\n",
      "Iteration 91, loss = 0.27168429\n",
      "Iteration 92, loss = 0.27683928\n",
      "Iteration 93, loss = 0.27227614\n",
      "Iteration 94, loss = 0.27050276\n",
      "Iteration 95, loss = 0.26808741\n",
      "Iteration 96, loss = 0.27262393\n",
      "Iteration 97, loss = 0.26856916\n",
      "Iteration 98, loss = 0.26583478\n",
      "Iteration 99, loss = 0.26488496\n",
      "Iteration 100, loss = 0.26438266\n",
      "Iteration 101, loss = 0.26140975\n",
      "Iteration 102, loss = 0.26245757\n",
      "Iteration 103, loss = 0.25964004\n",
      "Iteration 104, loss = 0.25945001\n",
      "Iteration 105, loss = 0.25858476\n",
      "Iteration 106, loss = 0.25826506\n",
      "Iteration 107, loss = 0.25579602\n",
      "Iteration 108, loss = 0.25436673\n",
      "Iteration 109, loss = 0.25501859\n",
      "Iteration 110, loss = 0.25322531\n",
      "Iteration 111, loss = 0.25326838\n",
      "Iteration 112, loss = 0.25082028\n",
      "Iteration 113, loss = 0.25098943\n",
      "Iteration 114, loss = 0.24929743\n",
      "Iteration 115, loss = 0.24800772\n",
      "Iteration 116, loss = 0.24772624\n",
      "Iteration 117, loss = 0.24794713\n",
      "Iteration 118, loss = 0.24521204\n",
      "Iteration 119, loss = 0.24702867\n",
      "Iteration 120, loss = 0.24454829\n",
      "Iteration 121, loss = 0.24330892\n",
      "Iteration 122, loss = 0.24667235\n",
      "Iteration 123, loss = 0.24340788\n",
      "Iteration 124, loss = 0.24362569\n",
      "Iteration 125, loss = 0.24112217\n",
      "Iteration 126, loss = 0.23870024\n",
      "Iteration 127, loss = 0.23811368\n",
      "Iteration 128, loss = 0.23573978\n",
      "Iteration 129, loss = 0.23518440\n",
      "Iteration 130, loss = 0.23739083\n",
      "Iteration 131, loss = 0.23931633\n",
      "Iteration 132, loss = 0.23224260\n",
      "Iteration 133, loss = 0.23193684\n",
      "Iteration 134, loss = 0.22973181\n",
      "Iteration 135, loss = 0.22930201\n",
      "Iteration 136, loss = 0.22873811\n",
      "Iteration 137, loss = 0.22707083\n",
      "Iteration 138, loss = 0.22490707\n",
      "Iteration 139, loss = 0.22480161\n",
      "Iteration 140, loss = 0.22530850\n",
      "Iteration 141, loss = 0.22239320\n",
      "Iteration 142, loss = 0.22128824\n",
      "Iteration 143, loss = 0.22153816\n",
      "Iteration 144, loss = 0.21885179\n",
      "Iteration 145, loss = 0.22254518\n",
      "Iteration 146, loss = 0.22264997\n",
      "Iteration 147, loss = 0.21701249\n",
      "Iteration 148, loss = 0.21541875\n",
      "Iteration 149, loss = 0.21489776\n",
      "Iteration 150, loss = 0.21650453\n",
      "Iteration 151, loss = 0.21317310\n",
      "Iteration 152, loss = 0.21355986\n",
      "Iteration 153, loss = 0.21257267\n",
      "Iteration 154, loss = 0.21090113\n",
      "Iteration 155, loss = 0.20894841\n",
      "Iteration 156, loss = 0.20913054\n",
      "Iteration 157, loss = 0.20846824\n",
      "Iteration 158, loss = 0.20758559\n",
      "Iteration 159, loss = 0.20735506\n",
      "Iteration 160, loss = 0.20656882\n",
      "Iteration 161, loss = 0.20592335\n",
      "Iteration 162, loss = 0.20241471\n",
      "Iteration 163, loss = 0.20095650\n",
      "Iteration 164, loss = 0.20320099\n",
      "Iteration 165, loss = 0.20124988\n",
      "Iteration 166, loss = 0.19930565\n",
      "Iteration 167, loss = 0.19897815\n",
      "Iteration 168, loss = 0.19711356\n",
      "Iteration 169, loss = 0.19842642\n",
      "Iteration 170, loss = 0.19818342\n",
      "Iteration 171, loss = 0.19443322\n",
      "Iteration 172, loss = 0.19356547\n",
      "Iteration 173, loss = 0.19306867\n",
      "Iteration 174, loss = 0.19163645\n",
      "Iteration 175, loss = 0.18995006\n",
      "Iteration 176, loss = 0.18962465\n",
      "Iteration 177, loss = 0.19152026\n",
      "Iteration 178, loss = 0.19017646\n",
      "Iteration 179, loss = 0.18695655\n",
      "Iteration 180, loss = 0.18735718\n",
      "Iteration 181, loss = 0.18696421\n",
      "Iteration 182, loss = 0.18677358\n",
      "Iteration 183, loss = 0.18430498\n",
      "Iteration 184, loss = 0.18422389\n",
      "Iteration 185, loss = 0.18427205\n",
      "Iteration 186, loss = 0.18059063\n",
      "Iteration 187, loss = 0.17992030\n",
      "Iteration 188, loss = 0.17868309\n",
      "Iteration 189, loss = 0.17876406\n",
      "Iteration 190, loss = 0.17797913\n",
      "Iteration 191, loss = 0.17710199\n",
      "Iteration 192, loss = 0.17537972\n",
      "Iteration 193, loss = 0.17644172\n",
      "Iteration 194, loss = 0.17571175\n",
      "Iteration 195, loss = 0.17658136\n",
      "Iteration 196, loss = 0.17303844\n",
      "Iteration 197, loss = 0.17243710\n",
      "Iteration 198, loss = 0.17261964\n",
      "Iteration 199, loss = 0.17189394\n",
      "Iteration 200, loss = 0.17256594\n",
      "Iteration 201, loss = 0.16746820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 202, loss = 0.16920162\n",
      "Iteration 203, loss = 0.16979790\n",
      "Iteration 204, loss = 0.16747750\n",
      "Iteration 205, loss = 0.16481377\n",
      "Iteration 206, loss = 0.16495106\n",
      "Iteration 207, loss = 0.16300850\n",
      "Iteration 208, loss = 0.16206008\n",
      "Iteration 209, loss = 0.16229354\n",
      "Iteration 210, loss = 0.16044210\n",
      "Iteration 211, loss = 0.16149604\n",
      "Iteration 212, loss = 0.16068527\n",
      "Iteration 213, loss = 0.15938542\n",
      "Iteration 214, loss = 0.15898987\n",
      "Iteration 215, loss = 0.15742679\n",
      "Iteration 216, loss = 0.15530942\n",
      "Iteration 217, loss = 0.15643288\n",
      "Iteration 218, loss = 0.15619236\n",
      "Iteration 219, loss = 0.15857230\n",
      "Iteration 220, loss = 0.15795534\n",
      "Iteration 221, loss = 0.15445006\n",
      "Iteration 222, loss = 0.15246983\n",
      "Iteration 223, loss = 0.14957179\n",
      "Iteration 224, loss = 0.15051434\n",
      "Iteration 225, loss = 0.15080894\n",
      "Iteration 226, loss = 0.14974256\n",
      "Iteration 227, loss = 0.15121240\n",
      "Iteration 228, loss = 0.15062345\n",
      "Iteration 229, loss = 0.14710367\n",
      "Iteration 230, loss = 0.15038506\n",
      "Iteration 231, loss = 0.14563143\n",
      "Iteration 232, loss = 0.14477790\n",
      "Iteration 233, loss = 0.14535069\n",
      "Iteration 234, loss = 0.14494407\n",
      "Iteration 235, loss = 0.14378685\n",
      "Iteration 236, loss = 0.14170915\n",
      "Iteration 237, loss = 0.14007583\n",
      "Iteration 238, loss = 0.13908592\n",
      "Iteration 239, loss = 0.13992727\n",
      "Iteration 240, loss = 0.13795275\n",
      "Iteration 241, loss = 0.13711445\n",
      "Iteration 242, loss = 0.13683634\n",
      "Iteration 243, loss = 0.13670370\n",
      "Iteration 244, loss = 0.13594434\n",
      "Iteration 245, loss = 0.13531741\n",
      "Iteration 246, loss = 0.13395320\n",
      "Iteration 247, loss = 0.13319579\n",
      "Iteration 248, loss = 0.13331538\n",
      "Iteration 249, loss = 0.13361240\n",
      "Iteration 250, loss = 0.13118699\n",
      "Iteration 251, loss = 0.12957957\n",
      "Iteration 252, loss = 0.12926545\n",
      "Iteration 253, loss = 0.12954500\n",
      "Iteration 254, loss = 0.12845809\n",
      "Iteration 255, loss = 0.12994513\n",
      "Iteration 256, loss = 0.12735884\n",
      "Iteration 257, loss = 0.12765269\n",
      "Iteration 258, loss = 0.12820898\n",
      "Iteration 259, loss = 0.12650673\n",
      "Iteration 260, loss = 0.12674531\n",
      "Iteration 261, loss = 0.12327414\n",
      "Iteration 262, loss = 0.12220125\n",
      "Iteration 263, loss = 0.12326229\n",
      "Iteration 264, loss = 0.11984714\n",
      "Iteration 265, loss = 0.11909062\n",
      "Iteration 266, loss = 0.11916256\n",
      "Iteration 267, loss = 0.11802385\n",
      "Iteration 268, loss = 0.11783223\n",
      "Iteration 269, loss = 0.11811588\n",
      "Iteration 270, loss = 0.11582415\n",
      "Iteration 271, loss = 0.11641927\n",
      "Iteration 272, loss = 0.11443339\n",
      "Iteration 273, loss = 0.11413490\n",
      "Iteration 274, loss = 0.11518988\n",
      "Iteration 275, loss = 0.11402250\n",
      "Iteration 276, loss = 0.11374326\n",
      "Iteration 277, loss = 0.11124115\n",
      "Iteration 278, loss = 0.10940206\n",
      "Iteration 279, loss = 0.10829580\n",
      "Iteration 280, loss = 0.10763770\n",
      "Iteration 281, loss = 0.10595497\n",
      "Iteration 282, loss = 0.10471857\n",
      "Iteration 283, loss = 0.10413615\n",
      "Iteration 284, loss = 0.10598536\n",
      "Iteration 285, loss = 0.10356647\n",
      "Iteration 286, loss = 0.10490753\n",
      "Iteration 287, loss = 0.10567912\n",
      "Iteration 288, loss = 0.10000818\n",
      "Iteration 289, loss = 0.09972545\n",
      "Iteration 290, loss = 0.10255078\n",
      "Iteration 291, loss = 0.10156355\n",
      "Iteration 292, loss = 0.09884904\n",
      "Iteration 293, loss = 0.09825510\n",
      "Iteration 294, loss = 0.09574939\n",
      "Iteration 295, loss = 0.09518936\n",
      "Iteration 296, loss = 0.09524885\n",
      "Iteration 297, loss = 0.09309878\n",
      "Iteration 298, loss = 0.09228961\n",
      "Iteration 299, loss = 0.09205693\n",
      "Iteration 300, loss = 0.09291351\n",
      "Iteration 301, loss = 0.09095938\n",
      "Iteration 302, loss = 0.09454313\n",
      "Iteration 303, loss = 0.09088420\n",
      "Iteration 304, loss = 0.09034001\n",
      "Iteration 305, loss = 0.08854218\n",
      "Iteration 306, loss = 0.08730527\n",
      "Iteration 307, loss = 0.08789810\n",
      "Iteration 308, loss = 0.08610445\n",
      "Iteration 309, loss = 0.08595016\n",
      "Iteration 310, loss = 0.08693678\n",
      "Iteration 311, loss = 0.08409353\n",
      "Iteration 312, loss = 0.08381695\n",
      "Iteration 313, loss = 0.08536811\n",
      "Iteration 314, loss = 0.08385789\n",
      "Iteration 315, loss = 0.08440162\n",
      "Iteration 316, loss = 0.08331946\n",
      "Iteration 317, loss = 0.08308517\n",
      "Iteration 318, loss = 0.08112020\n",
      "Iteration 319, loss = 0.08069237\n",
      "Iteration 320, loss = 0.08143288\n",
      "Iteration 321, loss = 0.07848550\n",
      "Iteration 322, loss = 0.07853654\n",
      "Iteration 323, loss = 0.07687295\n",
      "Iteration 324, loss = 0.07765582\n",
      "Iteration 325, loss = 0.07760615\n",
      "Iteration 326, loss = 0.07889735\n",
      "Iteration 327, loss = 0.07540477\n",
      "Iteration 328, loss = 0.07422769\n",
      "Iteration 329, loss = 0.07335668\n",
      "Iteration 330, loss = 0.07442542\n",
      "Iteration 331, loss = 0.07549411\n",
      "Iteration 332, loss = 0.07377581\n",
      "Iteration 333, loss = 0.07296581\n",
      "Iteration 334, loss = 0.07438903\n",
      "Iteration 335, loss = 0.07186041\n",
      "Iteration 336, loss = 0.07244979\n",
      "Iteration 337, loss = 0.07249039\n",
      "Iteration 338, loss = 0.06957756\n",
      "Iteration 339, loss = 0.06819844\n",
      "Iteration 340, loss = 0.06919111\n",
      "Iteration 341, loss = 0.06777091\n",
      "Iteration 342, loss = 0.06798899\n",
      "Iteration 343, loss = 0.07187049\n",
      "Iteration 344, loss = 0.06730986\n",
      "Iteration 345, loss = 0.06783082\n",
      "Iteration 346, loss = 0.06654254\n",
      "Iteration 347, loss = 0.06550289\n",
      "Iteration 348, loss = 0.06499935\n",
      "Iteration 349, loss = 0.06404632\n",
      "Iteration 350, loss = 0.06395869\n",
      "Iteration 351, loss = 0.06542845\n",
      "Iteration 352, loss = 0.06576912\n",
      "Iteration 353, loss = 0.06488536\n",
      "Iteration 354, loss = 0.06711646\n",
      "Iteration 355, loss = 0.06338111\n",
      "Iteration 356, loss = 0.06515436\n",
      "Iteration 357, loss = 0.06290431\n",
      "Iteration 358, loss = 0.06117677\n",
      "Iteration 359, loss = 0.06063690\n",
      "Iteration 360, loss = 0.06061703\n",
      "Iteration 361, loss = 0.06133615\n",
      "Iteration 362, loss = 0.06293789\n",
      "Iteration 363, loss = 0.06039994\n",
      "Iteration 364, loss = 0.06195249\n",
      "Iteration 365, loss = 0.05891023\n",
      "Iteration 366, loss = 0.05811526\n",
      "Iteration 367, loss = 0.05693111\n",
      "Iteration 368, loss = 0.05771908\n",
      "Iteration 369, loss = 0.05778215\n",
      "Iteration 370, loss = 0.05618073\n",
      "Iteration 371, loss = 0.05643599\n",
      "Iteration 372, loss = 0.05661673\n",
      "Iteration 373, loss = 0.05557422\n",
      "Iteration 374, loss = 0.05588956\n",
      "Iteration 375, loss = 0.05640944\n",
      "Iteration 376, loss = 0.05441149\n",
      "Iteration 377, loss = 0.05452524\n",
      "Iteration 378, loss = 0.05392031\n",
      "Iteration 379, loss = 0.05380799\n",
      "Iteration 380, loss = 0.05297934\n",
      "Iteration 381, loss = 0.05290158\n",
      "Iteration 382, loss = 0.05228530\n",
      "Iteration 383, loss = 0.05411306\n",
      "Iteration 384, loss = 0.05753859\n",
      "Iteration 385, loss = 0.05181604\n",
      "Iteration 386, loss = 0.05166006\n",
      "Iteration 387, loss = 0.05138998\n",
      "Iteration 388, loss = 0.05139434\n",
      "Iteration 389, loss = 0.05032441\n",
      "Iteration 390, loss = 0.04954143\n",
      "Iteration 391, loss = 0.04917596\n",
      "Iteration 392, loss = 0.05181863\n",
      "Iteration 393, loss = 0.05055277\n",
      "Iteration 394, loss = 0.04920732\n",
      "Iteration 395, loss = 0.04896570\n",
      "Iteration 396, loss = 0.04829404\n",
      "Iteration 397, loss = 0.04710808\n",
      "Iteration 398, loss = 0.04809972\n",
      "Iteration 399, loss = 0.04749327\n",
      "Iteration 400, loss = 0.04743819\n",
      "Iteration 401, loss = 0.04857772\n",
      "Iteration 402, loss = 0.04845490\n",
      "Iteration 403, loss = 0.04594012\n",
      "Iteration 404, loss = 0.04614653\n",
      "Iteration 405, loss = 0.04586269\n",
      "Iteration 406, loss = 0.04764618\n",
      "Iteration 407, loss = 0.04674023\n",
      "Iteration 408, loss = 0.04446537\n",
      "Iteration 409, loss = 0.04679280\n",
      "Iteration 410, loss = 0.04521738\n",
      "Iteration 411, loss = 0.04347376\n",
      "Iteration 412, loss = 0.04434791\n",
      "Iteration 413, loss = 0.04373438\n",
      "Iteration 414, loss = 0.04334432\n",
      "Iteration 415, loss = 0.04264091\n",
      "Iteration 416, loss = 0.04252152\n",
      "Iteration 417, loss = 0.04389666\n",
      "Iteration 418, loss = 0.04209764\n",
      "Iteration 419, loss = 0.04206238\n",
      "Iteration 420, loss = 0.04117717\n",
      "Iteration 421, loss = 0.04136790\n",
      "Iteration 422, loss = 0.04082356\n",
      "Iteration 423, loss = 0.04093131\n",
      "Iteration 424, loss = 0.04103014\n",
      "Iteration 425, loss = 0.03986264\n",
      "Iteration 426, loss = 0.04126286\n",
      "Iteration 427, loss = 0.04125206\n",
      "Iteration 428, loss = 0.04047766\n",
      "Iteration 429, loss = 0.04045687\n",
      "Iteration 430, loss = 0.03961019\n",
      "Iteration 431, loss = 0.03849608\n",
      "Iteration 432, loss = 0.03875671\n",
      "Iteration 433, loss = 0.03938878\n",
      "Iteration 434, loss = 0.03806896\n",
      "Iteration 435, loss = 0.03831740\n",
      "Iteration 436, loss = 0.03724468\n",
      "Iteration 437, loss = 0.03765517\n",
      "Iteration 438, loss = 0.03716853\n",
      "Iteration 439, loss = 0.03763874\n",
      "Iteration 440, loss = 0.03883434\n",
      "Iteration 441, loss = 0.03728854\n",
      "Iteration 442, loss = 0.03993996\n",
      "Iteration 443, loss = 0.03840856\n",
      "Iteration 444, loss = 0.03785564\n",
      "Iteration 445, loss = 0.03657250\n",
      "Iteration 446, loss = 0.03616548\n",
      "Iteration 447, loss = 0.03603049\n",
      "Iteration 448, loss = 0.03592770\n",
      "Iteration 449, loss = 0.03620154\n",
      "Iteration 450, loss = 0.03854468\n",
      "Iteration 451, loss = 0.03623907\n",
      "Iteration 452, loss = 0.03530681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 453, loss = 0.03446708\n",
      "Iteration 454, loss = 0.03335376\n",
      "Iteration 455, loss = 0.03368761\n",
      "Iteration 456, loss = 0.03321278\n",
      "Iteration 457, loss = 0.03517556\n",
      "Iteration 458, loss = 0.03301772\n",
      "Iteration 459, loss = 0.03302652\n",
      "Iteration 460, loss = 0.03248489\n",
      "Iteration 461, loss = 0.03352595\n",
      "Iteration 462, loss = 0.03339594\n",
      "Iteration 463, loss = 0.03239123\n",
      "Iteration 464, loss = 0.03224938\n",
      "Iteration 465, loss = 0.03173906\n",
      "Iteration 466, loss = 0.03265080\n",
      "Iteration 467, loss = 0.03149480\n",
      "Iteration 468, loss = 0.03186990\n",
      "Iteration 469, loss = 0.03073067\n",
      "Iteration 470, loss = 0.03106429\n",
      "Iteration 471, loss = 0.03156118\n",
      "Iteration 472, loss = 0.03115023\n",
      "Iteration 473, loss = 0.03101287\n",
      "Iteration 474, loss = 0.03196825\n",
      "Iteration 475, loss = 0.03287906\n",
      "Iteration 476, loss = 0.03161332\n",
      "Iteration 477, loss = 0.03184454\n",
      "Iteration 478, loss = 0.03035089\n",
      "Iteration 479, loss = 0.02977328\n",
      "Iteration 480, loss = 0.02964745\n",
      "Iteration 481, loss = 0.02930348\n",
      "Iteration 482, loss = 0.02894761\n",
      "Iteration 483, loss = 0.02961296\n",
      "Iteration 484, loss = 0.02933677\n",
      "Iteration 485, loss = 0.03015414\n",
      "Iteration 486, loss = 0.02960538\n",
      "Iteration 487, loss = 0.02875546\n",
      "Iteration 488, loss = 0.02950144\n",
      "Iteration 489, loss = 0.03136942\n",
      "Iteration 490, loss = 0.02944127\n",
      "Iteration 491, loss = 0.03003116\n",
      "Iteration 492, loss = 0.02786161\n",
      "Iteration 493, loss = 0.02798308\n",
      "Iteration 494, loss = 0.03660933\n",
      "Iteration 495, loss = 0.04914093\n",
      "Iteration 496, loss = 0.04973794\n",
      "Iteration 497, loss = 0.04521249\n",
      "Iteration 498, loss = 0.04109657\n",
      "Iteration 499, loss = 0.03313181\n",
      "Iteration 500, loss = 0.03165303\n",
      "Iteration 501, loss = 0.03124053\n",
      "Iteration 502, loss = 0.03083098\n",
      "Iteration 503, loss = 0.02755775\n",
      "Iteration 504, loss = 0.02807533\n",
      "Iteration 505, loss = 0.02694856\n",
      "Iteration 506, loss = 0.02618536\n",
      "Iteration 507, loss = 0.02535597\n",
      "Iteration 508, loss = 0.02566954\n",
      "Iteration 509, loss = 0.02599160\n",
      "Iteration 510, loss = 0.02604459\n",
      "Iteration 511, loss = 0.02575295\n",
      "Iteration 512, loss = 0.02564561\n",
      "Iteration 513, loss = 0.02499240\n",
      "Iteration 514, loss = 0.02479182\n",
      "Iteration 515, loss = 0.02538758\n",
      "Iteration 516, loss = 0.02492867\n",
      "Iteration 517, loss = 0.02459977\n",
      "Iteration 518, loss = 0.02439280\n",
      "Iteration 519, loss = 0.02415777\n",
      "Iteration 520, loss = 0.02422396\n",
      "Iteration 521, loss = 0.02427247\n",
      "Iteration 522, loss = 0.02427520\n",
      "Iteration 523, loss = 0.02498947\n",
      "Iteration 524, loss = 0.02595630\n",
      "Iteration 525, loss = 0.02494444\n",
      "Iteration 526, loss = 0.02382193\n",
      "Iteration 527, loss = 0.02418187\n",
      "Iteration 528, loss = 0.02409517\n",
      "Iteration 529, loss = 0.02386642\n",
      "Iteration 530, loss = 0.02340562\n",
      "Iteration 531, loss = 0.02368831\n",
      "Iteration 532, loss = 0.02314890\n",
      "Iteration 533, loss = 0.02309101\n",
      "Iteration 534, loss = 0.02302979\n",
      "Iteration 535, loss = 0.02309613\n",
      "Iteration 536, loss = 0.02277644\n",
      "Iteration 537, loss = 0.02279964\n",
      "Iteration 538, loss = 0.02263043\n",
      "Iteration 539, loss = 0.02278135\n",
      "Iteration 540, loss = 0.02322205\n",
      "Iteration 541, loss = 0.02397371\n",
      "Iteration 542, loss = 0.02445996\n",
      "Iteration 543, loss = 0.02284060\n",
      "Iteration 544, loss = 0.02225838\n",
      "Iteration 545, loss = 0.02258753\n",
      "Iteration 546, loss = 0.02246117\n",
      "Iteration 547, loss = 0.02195287\n",
      "Iteration 548, loss = 0.02346501\n",
      "Iteration 549, loss = 0.02243668\n",
      "Iteration 550, loss = 0.02192088\n",
      "Iteration 551, loss = 0.02150358\n",
      "Iteration 552, loss = 0.02146196\n",
      "Iteration 553, loss = 0.02229493\n",
      "Iteration 554, loss = 0.02152765\n",
      "Iteration 555, loss = 0.02130622\n",
      "Iteration 556, loss = 0.02095128\n",
      "Iteration 557, loss = 0.02085234\n",
      "Iteration 558, loss = 0.02085354\n",
      "Iteration 559, loss = 0.02078863\n",
      "Iteration 560, loss = 0.02072607\n",
      "Iteration 561, loss = 0.02056477\n",
      "Iteration 562, loss = 0.02104650\n",
      "Iteration 563, loss = 0.02064997\n",
      "Iteration 564, loss = 0.02077715\n",
      "Iteration 565, loss = 0.02084037\n",
      "Iteration 566, loss = 0.02045870\n",
      "Iteration 567, loss = 0.02003243\n",
      "Iteration 568, loss = 0.02019180\n",
      "Iteration 569, loss = 0.01962945\n",
      "Iteration 570, loss = 0.02035436\n",
      "Iteration 571, loss = 0.01982042\n",
      "Iteration 572, loss = 0.01962956\n",
      "Iteration 573, loss = 0.01936621\n",
      "Iteration 574, loss = 0.01934785\n",
      "Iteration 575, loss = 0.01957960\n",
      "Iteration 576, loss = 0.01944370\n",
      "Iteration 577, loss = 0.01941001\n",
      "Iteration 578, loss = 0.01988277\n",
      "Iteration 579, loss = 0.01949347\n",
      "Iteration 580, loss = 0.02027179\n",
      "Iteration 581, loss = 0.02004196\n",
      "Iteration 582, loss = 0.02015236\n",
      "Iteration 583, loss = 0.01942923\n",
      "Iteration 584, loss = 0.01836571\n",
      "Iteration 585, loss = 0.01870011\n",
      "Iteration 586, loss = 0.01931348\n",
      "Iteration 587, loss = 0.01824715\n",
      "Iteration 588, loss = 0.01836672\n",
      "Iteration 589, loss = 0.01825126\n",
      "Iteration 590, loss = 0.01830238\n",
      "Iteration 591, loss = 0.01784044\n",
      "Iteration 592, loss = 0.01754717\n",
      "Iteration 593, loss = 0.01730657\n",
      "Iteration 594, loss = 0.01721556\n",
      "Iteration 595, loss = 0.01716200\n",
      "Iteration 596, loss = 0.01657257\n",
      "Iteration 597, loss = 0.01664907\n",
      "Iteration 598, loss = 0.01697337\n",
      "Iteration 599, loss = 0.01648990\n",
      "Iteration 600, loss = 0.01703275\n",
      "Iteration 601, loss = 0.01613177\n",
      "Iteration 602, loss = 0.01600136\n",
      "Iteration 603, loss = 0.01642847\n",
      "Iteration 604, loss = 0.01633417\n",
      "Iteration 605, loss = 0.01672590\n",
      "Iteration 606, loss = 0.01613549\n",
      "Iteration 607, loss = 0.01571055\n",
      "Iteration 608, loss = 0.01506386\n",
      "Iteration 609, loss = 0.01450682\n",
      "Iteration 610, loss = 0.01392881\n",
      "Iteration 611, loss = 0.01429940\n",
      "Iteration 612, loss = 0.01452471\n",
      "Iteration 613, loss = 0.01431909\n",
      "Iteration 614, loss = 0.01428334\n",
      "Iteration 615, loss = 0.01419591\n",
      "Iteration 616, loss = 0.01335487\n",
      "Iteration 617, loss = 0.01317767\n",
      "Iteration 618, loss = 0.01301761\n",
      "Iteration 619, loss = 0.01378448\n",
      "Iteration 620, loss = 0.01299975\n",
      "Iteration 621, loss = 0.01340753\n",
      "Iteration 622, loss = 0.01328028\n",
      "Iteration 623, loss = 0.01279656\n",
      "Iteration 624, loss = 0.01309593\n",
      "Iteration 625, loss = 0.01275633\n",
      "Iteration 626, loss = 0.01309106\n",
      "Iteration 627, loss = 0.01405464\n",
      "Iteration 628, loss = 0.01299858\n",
      "Iteration 629, loss = 0.01273935\n",
      "Iteration 630, loss = 0.01297860\n",
      "Iteration 631, loss = 0.01268025\n",
      "Iteration 632, loss = 0.01231416\n",
      "Iteration 633, loss = 0.01251247\n",
      "Iteration 634, loss = 0.01251153\n",
      "Iteration 635, loss = 0.01217413\n",
      "Iteration 636, loss = 0.01266812\n",
      "Iteration 637, loss = 0.01255126\n",
      "Iteration 638, loss = 0.01187588\n",
      "Iteration 639, loss = 0.01213861\n",
      "Iteration 640, loss = 0.01189227\n",
      "Iteration 641, loss = 0.01205256\n",
      "Iteration 642, loss = 0.01180947\n",
      "Iteration 643, loss = 0.01162827\n",
      "Iteration 644, loss = 0.01201166\n",
      "Iteration 645, loss = 0.01175373\n",
      "Iteration 646, loss = 0.01172731\n",
      "Iteration 647, loss = 0.01268139\n",
      "Iteration 648, loss = 0.01367529\n",
      "Iteration 649, loss = 0.01306930\n",
      "Iteration 650, loss = 0.01350215\n",
      "Iteration 651, loss = 0.01288248\n",
      "Iteration 652, loss = 0.01493353\n",
      "Iteration 653, loss = 0.01234435\n",
      "Iteration 654, loss = 0.02457229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 32 of 48...\n",
      "Iteration 1, loss = 0.68800693\n",
      "Iteration 2, loss = 0.68320871\n",
      "Iteration 3, loss = 0.67713821\n",
      "Iteration 4, loss = 0.66783160\n",
      "Iteration 5, loss = 0.65598371\n",
      "Iteration 6, loss = 0.64186467\n",
      "Iteration 7, loss = 0.62581002\n",
      "Iteration 8, loss = 0.60880677\n",
      "Iteration 9, loss = 0.58969220\n",
      "Iteration 10, loss = 0.57037877\n",
      "Iteration 11, loss = 0.55110470\n",
      "Iteration 12, loss = 0.53260171\n",
      "Iteration 13, loss = 0.51783809\n",
      "Iteration 14, loss = 0.50084112\n",
      "Iteration 15, loss = 0.48835225\n",
      "Iteration 16, loss = 0.47488265\n",
      "Iteration 17, loss = 0.46205562\n",
      "Iteration 18, loss = 0.45255111\n",
      "Iteration 19, loss = 0.44342733\n",
      "Iteration 20, loss = 0.43554076\n",
      "Iteration 21, loss = 0.43291802\n",
      "Iteration 22, loss = 0.42122122\n",
      "Iteration 23, loss = 0.41510451\n",
      "Iteration 24, loss = 0.40949526\n",
      "Iteration 25, loss = 0.40600292\n",
      "Iteration 26, loss = 0.40138478\n",
      "Iteration 27, loss = 0.39623083\n",
      "Iteration 28, loss = 0.39165088\n",
      "Iteration 29, loss = 0.38778121\n",
      "Iteration 30, loss = 0.38386232\n",
      "Iteration 31, loss = 0.38013345\n",
      "Iteration 32, loss = 0.37783149\n",
      "Iteration 33, loss = 0.37442611\n",
      "Iteration 34, loss = 0.37041213\n",
      "Iteration 35, loss = 0.36712672\n",
      "Iteration 36, loss = 0.36437246\n",
      "Iteration 37, loss = 0.36149810\n",
      "Iteration 38, loss = 0.36028261\n",
      "Iteration 39, loss = 0.35888358\n",
      "Iteration 40, loss = 0.35569365\n",
      "Iteration 41, loss = 0.35084106\n",
      "Iteration 42, loss = 0.34838354\n",
      "Iteration 43, loss = 0.34662445\n",
      "Iteration 44, loss = 0.34511531\n",
      "Iteration 45, loss = 0.34474426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.34119731\n",
      "Iteration 47, loss = 0.33914412\n",
      "Iteration 48, loss = 0.34073795\n",
      "Iteration 49, loss = 0.33505993\n",
      "Iteration 50, loss = 0.33275152\n",
      "Iteration 51, loss = 0.32960380\n",
      "Iteration 52, loss = 0.32838920\n",
      "Iteration 53, loss = 0.32663779\n",
      "Iteration 54, loss = 0.32416090\n",
      "Iteration 55, loss = 0.32430910\n",
      "Iteration 56, loss = 0.32087783\n",
      "Iteration 57, loss = 0.32272264\n",
      "Iteration 58, loss = 0.31854755\n",
      "Iteration 59, loss = 0.31781458\n",
      "Iteration 60, loss = 0.31556985\n",
      "Iteration 61, loss = 0.31747061\n",
      "Iteration 62, loss = 0.31457236\n",
      "Iteration 63, loss = 0.31044769\n",
      "Iteration 64, loss = 0.30890159\n",
      "Iteration 65, loss = 0.30875180\n",
      "Iteration 66, loss = 0.30717956\n",
      "Iteration 67, loss = 0.30839150\n",
      "Iteration 68, loss = 0.30731358\n",
      "Iteration 69, loss = 0.30275070\n",
      "Iteration 70, loss = 0.29969599\n",
      "Iteration 71, loss = 0.30044348\n",
      "Iteration 72, loss = 0.29798229\n",
      "Iteration 73, loss = 0.29780566\n",
      "Iteration 74, loss = 0.29520012\n",
      "Iteration 75, loss = 0.29433702\n",
      "Iteration 76, loss = 0.29548320\n",
      "Iteration 77, loss = 0.29424345\n",
      "Iteration 78, loss = 0.29004572\n",
      "Iteration 79, loss = 0.29114552\n",
      "Iteration 80, loss = 0.29161231\n",
      "Iteration 81, loss = 0.29066384\n",
      "Iteration 82, loss = 0.28977355\n",
      "Iteration 83, loss = 0.28548321\n",
      "Iteration 84, loss = 0.28593014\n",
      "Iteration 85, loss = 0.28441603\n",
      "Iteration 86, loss = 0.28297581\n",
      "Iteration 87, loss = 0.28305183\n",
      "Iteration 88, loss = 0.28590389\n",
      "Iteration 89, loss = 0.28816058\n",
      "Iteration 90, loss = 0.28247010\n",
      "Iteration 91, loss = 0.27774529\n",
      "Iteration 92, loss = 0.27830958\n",
      "Iteration 93, loss = 0.27660097\n",
      "Iteration 94, loss = 0.27639728\n",
      "Iteration 95, loss = 0.27672609\n",
      "Iteration 96, loss = 0.27382350\n",
      "Iteration 97, loss = 0.27494825\n",
      "Iteration 98, loss = 0.27128396\n",
      "Iteration 99, loss = 0.27181745\n",
      "Iteration 100, loss = 0.26934245\n",
      "Iteration 101, loss = 0.26949230\n",
      "Iteration 102, loss = 0.27056959\n",
      "Iteration 103, loss = 0.26778514\n",
      "Iteration 104, loss = 0.26724799\n",
      "Iteration 105, loss = 0.26680328\n",
      "Iteration 106, loss = 0.26424897\n",
      "Iteration 107, loss = 0.26335842\n",
      "Iteration 108, loss = 0.26261569\n",
      "Iteration 109, loss = 0.26300123\n",
      "Iteration 110, loss = 0.26522131\n",
      "Iteration 111, loss = 0.26005043\n",
      "Iteration 112, loss = 0.25937287\n",
      "Iteration 113, loss = 0.25777185\n",
      "Iteration 114, loss = 0.25921011\n",
      "Iteration 115, loss = 0.25644363\n",
      "Iteration 116, loss = 0.25625770\n",
      "Iteration 117, loss = 0.25366121\n",
      "Iteration 118, loss = 0.25287956\n",
      "Iteration 119, loss = 0.25184534\n",
      "Iteration 120, loss = 0.25695508\n",
      "Iteration 121, loss = 0.25678190\n",
      "Iteration 122, loss = 0.25305570\n",
      "Iteration 123, loss = 0.24978937\n",
      "Iteration 124, loss = 0.25190055\n",
      "Iteration 125, loss = 0.25196356\n",
      "Iteration 126, loss = 0.24748259\n",
      "Iteration 127, loss = 0.24672698\n",
      "Iteration 128, loss = 0.24478290\n",
      "Iteration 129, loss = 0.24583026\n",
      "Iteration 130, loss = 0.24389205\n",
      "Iteration 131, loss = 0.24266720\n",
      "Iteration 132, loss = 0.24256471\n",
      "Iteration 133, loss = 0.24108991\n",
      "Iteration 134, loss = 0.24408763\n",
      "Iteration 135, loss = 0.23929295\n",
      "Iteration 136, loss = 0.23925171\n",
      "Iteration 137, loss = 0.23927316\n",
      "Iteration 138, loss = 0.23852305\n",
      "Iteration 139, loss = 0.23872781\n",
      "Iteration 140, loss = 0.24029333\n",
      "Iteration 141, loss = 0.23489215\n",
      "Iteration 142, loss = 0.23778323\n",
      "Iteration 143, loss = 0.23743990\n",
      "Iteration 144, loss = 0.23473977\n",
      "Iteration 145, loss = 0.23448044\n",
      "Iteration 146, loss = 0.23536371\n",
      "Iteration 147, loss = 0.23335379\n",
      "Iteration 148, loss = 0.23091598\n",
      "Iteration 149, loss = 0.23112660\n",
      "Iteration 150, loss = 0.22974488\n",
      "Iteration 151, loss = 0.22871040\n",
      "Iteration 152, loss = 0.22747038\n",
      "Iteration 153, loss = 0.22762594\n",
      "Iteration 154, loss = 0.22788815\n",
      "Iteration 155, loss = 0.23074327\n",
      "Iteration 156, loss = 0.22917001\n",
      "Iteration 157, loss = 0.22628049\n",
      "Iteration 158, loss = 0.22441177\n",
      "Iteration 159, loss = 0.22707317\n",
      "Iteration 160, loss = 0.22802626\n",
      "Iteration 161, loss = 0.22200582\n",
      "Iteration 162, loss = 0.22137365\n",
      "Iteration 163, loss = 0.22247433\n",
      "Iteration 164, loss = 0.22084844\n",
      "Iteration 165, loss = 0.22185025\n",
      "Iteration 166, loss = 0.22368011\n",
      "Iteration 167, loss = 0.22113062\n",
      "Iteration 168, loss = 0.21629144\n",
      "Iteration 169, loss = 0.21642868\n",
      "Iteration 170, loss = 0.21474651\n",
      "Iteration 171, loss = 0.21414734\n",
      "Iteration 172, loss = 0.21351750\n",
      "Iteration 173, loss = 0.21323250\n",
      "Iteration 174, loss = 0.21136363\n",
      "Iteration 175, loss = 0.21392054\n",
      "Iteration 176, loss = 0.21103359\n",
      "Iteration 177, loss = 0.21032182\n",
      "Iteration 178, loss = 0.21001569\n",
      "Iteration 179, loss = 0.21250472\n",
      "Iteration 180, loss = 0.20973745\n",
      "Iteration 181, loss = 0.20939352\n",
      "Iteration 182, loss = 0.20861644\n",
      "Iteration 183, loss = 0.20834469\n",
      "Iteration 184, loss = 0.20647147\n",
      "Iteration 185, loss = 0.20721521\n",
      "Iteration 186, loss = 0.20705466\n",
      "Iteration 187, loss = 0.20446741\n",
      "Iteration 188, loss = 0.20327618\n",
      "Iteration 189, loss = 0.20579983\n",
      "Iteration 190, loss = 0.20677317\n",
      "Iteration 191, loss = 0.20677412\n",
      "Iteration 192, loss = 0.20449646\n",
      "Iteration 193, loss = 0.20063932\n",
      "Iteration 194, loss = 0.20028076\n",
      "Iteration 195, loss = 0.20264344\n",
      "Iteration 196, loss = 0.19953789\n",
      "Iteration 197, loss = 0.19947138\n",
      "Iteration 198, loss = 0.19824799\n",
      "Iteration 199, loss = 0.19677841\n",
      "Iteration 200, loss = 0.19610679\n",
      "Iteration 201, loss = 0.19619601\n",
      "Iteration 202, loss = 0.19627864\n",
      "Iteration 203, loss = 0.19473374\n",
      "Iteration 204, loss = 0.19354218\n",
      "Iteration 205, loss = 0.19335941\n",
      "Iteration 206, loss = 0.19390193\n",
      "Iteration 207, loss = 0.19186549\n",
      "Iteration 208, loss = 0.19016201\n",
      "Iteration 209, loss = 0.19209237\n",
      "Iteration 210, loss = 0.19033515\n",
      "Iteration 211, loss = 0.19037476\n",
      "Iteration 212, loss = 0.18897656\n",
      "Iteration 213, loss = 0.18860379\n",
      "Iteration 214, loss = 0.18626311\n",
      "Iteration 215, loss = 0.18661118\n",
      "Iteration 216, loss = 0.18493957\n",
      "Iteration 217, loss = 0.18366658\n",
      "Iteration 218, loss = 0.18336003\n",
      "Iteration 219, loss = 0.18563288\n",
      "Iteration 220, loss = 0.18631826\n",
      "Iteration 221, loss = 0.18243417\n",
      "Iteration 222, loss = 0.18026721\n",
      "Iteration 223, loss = 0.18061398\n",
      "Iteration 224, loss = 0.18036193\n",
      "Iteration 225, loss = 0.17833555\n",
      "Iteration 226, loss = 0.17808105\n",
      "Iteration 227, loss = 0.17927372\n",
      "Iteration 228, loss = 0.17839710\n",
      "Iteration 229, loss = 0.17666553\n",
      "Iteration 230, loss = 0.17457107\n",
      "Iteration 231, loss = 0.17572707\n",
      "Iteration 232, loss = 0.17432498\n",
      "Iteration 233, loss = 0.17326830\n",
      "Iteration 234, loss = 0.17255738\n",
      "Iteration 235, loss = 0.17352776\n",
      "Iteration 236, loss = 0.17070718\n",
      "Iteration 237, loss = 0.16956394\n",
      "Iteration 238, loss = 0.16901871\n",
      "Iteration 239, loss = 0.16826533\n",
      "Iteration 240, loss = 0.16999167\n",
      "Iteration 241, loss = 0.16969468\n",
      "Iteration 242, loss = 0.16736249\n",
      "Iteration 243, loss = 0.16556027\n",
      "Iteration 244, loss = 0.16526866\n",
      "Iteration 245, loss = 0.16578237\n",
      "Iteration 246, loss = 0.16435950\n",
      "Iteration 247, loss = 0.16353530\n",
      "Iteration 248, loss = 0.16650032\n",
      "Iteration 249, loss = 0.16290250\n",
      "Iteration 250, loss = 0.16114979\n",
      "Iteration 251, loss = 0.16196010\n",
      "Iteration 252, loss = 0.16027080\n",
      "Iteration 253, loss = 0.16158354\n",
      "Iteration 254, loss = 0.15929831\n",
      "Iteration 255, loss = 0.15656507\n",
      "Iteration 256, loss = 0.15550298\n",
      "Iteration 257, loss = 0.15602597\n",
      "Iteration 258, loss = 0.15517906\n",
      "Iteration 259, loss = 0.15443783\n",
      "Iteration 260, loss = 0.15340952\n",
      "Iteration 261, loss = 0.15520407\n",
      "Iteration 262, loss = 0.15414356\n",
      "Iteration 263, loss = 0.15217404\n",
      "Iteration 264, loss = 0.15275657\n",
      "Iteration 265, loss = 0.15188764\n",
      "Iteration 266, loss = 0.14864918\n",
      "Iteration 267, loss = 0.14881855\n",
      "Iteration 268, loss = 0.14834625\n",
      "Iteration 269, loss = 0.14741556\n",
      "Iteration 270, loss = 0.14846711\n",
      "Iteration 271, loss = 0.14603807\n",
      "Iteration 272, loss = 0.14550330\n",
      "Iteration 273, loss = 0.14302155\n",
      "Iteration 274, loss = 0.14292568\n",
      "Iteration 275, loss = 0.14318643\n",
      "Iteration 276, loss = 0.14299910\n",
      "Iteration 277, loss = 0.14090988\n",
      "Iteration 278, loss = 0.14293379\n",
      "Iteration 279, loss = 0.14340284\n",
      "Iteration 280, loss = 0.14189667\n",
      "Iteration 281, loss = 0.14111135\n",
      "Iteration 282, loss = 0.14258927\n",
      "Iteration 283, loss = 0.14595049\n",
      "Iteration 284, loss = 0.14030217\n",
      "Iteration 285, loss = 0.14064048\n",
      "Iteration 286, loss = 0.13708372\n",
      "Iteration 287, loss = 0.13603635\n",
      "Iteration 288, loss = 0.13613608\n",
      "Iteration 289, loss = 0.13234758\n",
      "Iteration 290, loss = 0.13524414\n",
      "Iteration 291, loss = 0.13159417\n",
      "Iteration 292, loss = 0.13118242\n",
      "Iteration 293, loss = 0.13205512\n",
      "Iteration 294, loss = 0.12915564\n",
      "Iteration 295, loss = 0.13078791\n",
      "Iteration 296, loss = 0.12749370\n",
      "Iteration 297, loss = 0.12725583\n",
      "Iteration 298, loss = 0.12725740\n",
      "Iteration 299, loss = 0.12503637\n",
      "Iteration 300, loss = 0.12615572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 301, loss = 0.12333805\n",
      "Iteration 302, loss = 0.12438286\n",
      "Iteration 303, loss = 0.12215921\n",
      "Iteration 304, loss = 0.12113556\n",
      "Iteration 305, loss = 0.12172587\n",
      "Iteration 306, loss = 0.12419535\n",
      "Iteration 307, loss = 0.11920917\n",
      "Iteration 308, loss = 0.11971267\n",
      "Iteration 309, loss = 0.11979135\n",
      "Iteration 310, loss = 0.12051211\n",
      "Iteration 311, loss = 0.11672066\n",
      "Iteration 312, loss = 0.11508651\n",
      "Iteration 313, loss = 0.11426348\n",
      "Iteration 314, loss = 0.11433411\n",
      "Iteration 315, loss = 0.11461802\n",
      "Iteration 316, loss = 0.11336192\n",
      "Iteration 317, loss = 0.11241155\n",
      "Iteration 318, loss = 0.11115735\n",
      "Iteration 319, loss = 0.11151468\n",
      "Iteration 320, loss = 0.11015563\n",
      "Iteration 321, loss = 0.10925820\n",
      "Iteration 322, loss = 0.11125890\n",
      "Iteration 323, loss = 0.10810852\n",
      "Iteration 324, loss = 0.10750248\n",
      "Iteration 325, loss = 0.10719199\n",
      "Iteration 326, loss = 0.10946466\n",
      "Iteration 327, loss = 0.10587952\n",
      "Iteration 328, loss = 0.10503311\n",
      "Iteration 329, loss = 0.10450354\n",
      "Iteration 330, loss = 0.10392014\n",
      "Iteration 331, loss = 0.10310729\n",
      "Iteration 332, loss = 0.10357579\n",
      "Iteration 333, loss = 0.10222223\n",
      "Iteration 334, loss = 0.10089476\n",
      "Iteration 335, loss = 0.10174843\n",
      "Iteration 336, loss = 0.10076729\n",
      "Iteration 337, loss = 0.10023466\n",
      "Iteration 338, loss = 0.09796818\n",
      "Iteration 339, loss = 0.09660944\n",
      "Iteration 340, loss = 0.09642500\n",
      "Iteration 341, loss = 0.09647640\n",
      "Iteration 342, loss = 0.09514488\n",
      "Iteration 343, loss = 0.09651907\n",
      "Iteration 344, loss = 0.09594158\n",
      "Iteration 345, loss = 0.09390704\n",
      "Iteration 346, loss = 0.09381632\n",
      "Iteration 347, loss = 0.09182167\n",
      "Iteration 348, loss = 0.09338831\n",
      "Iteration 349, loss = 0.09663284\n",
      "Iteration 350, loss = 0.09229351\n",
      "Iteration 351, loss = 0.09090708\n",
      "Iteration 352, loss = 0.09015122\n",
      "Iteration 353, loss = 0.09050531\n",
      "Iteration 354, loss = 0.08700812\n",
      "Iteration 355, loss = 0.08583619\n",
      "Iteration 356, loss = 0.08594711\n",
      "Iteration 357, loss = 0.08504657\n",
      "Iteration 358, loss = 0.08458176\n",
      "Iteration 359, loss = 0.08456688\n",
      "Iteration 360, loss = 0.08632619\n",
      "Iteration 361, loss = 0.08478428\n",
      "Iteration 362, loss = 0.08393553\n",
      "Iteration 363, loss = 0.08647635\n",
      "Iteration 364, loss = 0.08484336\n",
      "Iteration 365, loss = 0.08301789\n",
      "Iteration 366, loss = 0.08159065\n",
      "Iteration 367, loss = 0.08197701\n",
      "Iteration 368, loss = 0.08040444\n",
      "Iteration 369, loss = 0.07895615\n",
      "Iteration 370, loss = 0.07951135\n",
      "Iteration 371, loss = 0.07891220\n",
      "Iteration 372, loss = 0.07846077\n",
      "Iteration 373, loss = 0.07933884\n",
      "Iteration 374, loss = 0.07720077\n",
      "Iteration 375, loss = 0.07678583\n",
      "Iteration 376, loss = 0.07736032\n",
      "Iteration 377, loss = 0.07591824\n",
      "Iteration 378, loss = 0.07474890\n",
      "Iteration 379, loss = 0.07549167\n",
      "Iteration 380, loss = 0.07524190\n",
      "Iteration 381, loss = 0.07519122\n",
      "Iteration 382, loss = 0.07851440\n",
      "Iteration 383, loss = 0.07766984\n",
      "Iteration 384, loss = 0.07517358\n",
      "Iteration 385, loss = 0.07362990\n",
      "Iteration 386, loss = 0.07302643\n",
      "Iteration 387, loss = 0.07403569\n",
      "Iteration 388, loss = 0.07224875\n",
      "Iteration 389, loss = 0.07055072\n",
      "Iteration 390, loss = 0.07089765\n",
      "Iteration 391, loss = 0.07106493\n",
      "Iteration 392, loss = 0.07043867\n",
      "Iteration 393, loss = 0.07068791\n",
      "Iteration 394, loss = 0.07048025\n",
      "Iteration 395, loss = 0.06848753\n",
      "Iteration 396, loss = 0.06777636\n",
      "Iteration 397, loss = 0.06969409\n",
      "Iteration 398, loss = 0.06659333\n",
      "Iteration 399, loss = 0.06772760\n",
      "Iteration 400, loss = 0.06658628\n",
      "Iteration 401, loss = 0.06597305\n",
      "Iteration 402, loss = 0.06580350\n",
      "Iteration 403, loss = 0.06487879\n",
      "Iteration 404, loss = 0.06558617\n",
      "Iteration 405, loss = 0.06442084\n",
      "Iteration 406, loss = 0.06532615\n",
      "Iteration 407, loss = 0.06481023\n",
      "Iteration 408, loss = 0.06370324\n",
      "Iteration 409, loss = 0.06474399\n",
      "Iteration 410, loss = 0.06439472\n",
      "Iteration 411, loss = 0.06365646\n",
      "Iteration 412, loss = 0.06407608\n",
      "Iteration 413, loss = 0.06288890\n",
      "Iteration 414, loss = 0.06247760\n",
      "Iteration 415, loss = 0.06092449\n",
      "Iteration 416, loss = 0.06052700\n",
      "Iteration 417, loss = 0.05950008\n",
      "Iteration 418, loss = 0.06040214\n",
      "Iteration 419, loss = 0.05863187\n",
      "Iteration 420, loss = 0.05855736\n",
      "Iteration 421, loss = 0.05829940\n",
      "Iteration 422, loss = 0.05719470\n",
      "Iteration 423, loss = 0.05749814\n",
      "Iteration 424, loss = 0.05680307\n",
      "Iteration 425, loss = 0.05607815\n",
      "Iteration 426, loss = 0.05597084\n",
      "Iteration 427, loss = 0.05492629\n",
      "Iteration 428, loss = 0.05588979\n",
      "Iteration 429, loss = 0.05633293\n",
      "Iteration 430, loss = 0.05634198\n",
      "Iteration 431, loss = 0.05423720\n",
      "Iteration 432, loss = 0.05513063\n",
      "Iteration 433, loss = 0.05342895\n",
      "Iteration 434, loss = 0.05276019\n",
      "Iteration 435, loss = 0.05191968\n",
      "Iteration 436, loss = 0.06059624\n",
      "Iteration 437, loss = 0.05262191\n",
      "Iteration 438, loss = 0.05361605\n",
      "Iteration 439, loss = 0.05226435\n",
      "Iteration 440, loss = 0.05152781\n",
      "Iteration 441, loss = 0.05097845\n",
      "Iteration 442, loss = 0.04977946\n",
      "Iteration 443, loss = 0.05161487\n",
      "Iteration 444, loss = 0.05084903\n",
      "Iteration 445, loss = 0.04603783\n",
      "Iteration 446, loss = 0.04783930\n",
      "Iteration 447, loss = 0.04520066\n",
      "Iteration 448, loss = 0.04502694\n",
      "Iteration 449, loss = 0.04435232\n",
      "Iteration 450, loss = 0.04595619\n",
      "Iteration 451, loss = 0.04792897\n",
      "Iteration 452, loss = 0.04432281\n",
      "Iteration 453, loss = 0.04454384\n",
      "Iteration 454, loss = 0.04234505\n",
      "Iteration 455, loss = 0.04346748\n",
      "Iteration 456, loss = 0.04242033\n",
      "Iteration 457, loss = 0.04288486\n",
      "Iteration 458, loss = 0.04223701\n",
      "Iteration 459, loss = 0.04131644\n",
      "Iteration 460, loss = 0.04074434\n",
      "Iteration 461, loss = 0.04152917\n",
      "Iteration 462, loss = 0.04004189\n",
      "Iteration 463, loss = 0.03950965\n",
      "Iteration 464, loss = 0.04215095\n",
      "Iteration 465, loss = 0.04033315\n",
      "Iteration 466, loss = 0.03996057\n",
      "Iteration 467, loss = 0.03972312\n",
      "Iteration 468, loss = 0.03875373\n",
      "Iteration 469, loss = 0.03859319\n",
      "Iteration 470, loss = 0.04209433\n",
      "Iteration 471, loss = 0.04081786\n",
      "Iteration 472, loss = 0.03739867\n",
      "Iteration 473, loss = 0.03999559\n",
      "Iteration 474, loss = 0.04117661\n",
      "Iteration 475, loss = 0.04073846\n",
      "Iteration 476, loss = 0.03751830\n",
      "Iteration 477, loss = 0.03705598\n",
      "Iteration 478, loss = 0.03725666\n",
      "Iteration 479, loss = 0.03604982\n",
      "Iteration 480, loss = 0.03706209\n",
      "Iteration 481, loss = 0.03923727\n",
      "Iteration 482, loss = 0.03635970\n",
      "Iteration 483, loss = 0.03580662\n",
      "Iteration 484, loss = 0.03626706\n",
      "Iteration 485, loss = 0.03733922\n",
      "Iteration 486, loss = 0.03606328\n",
      "Iteration 487, loss = 0.03510072\n",
      "Iteration 488, loss = 0.03514460\n",
      "Iteration 489, loss = 0.03465159\n",
      "Iteration 490, loss = 0.03459247\n",
      "Iteration 491, loss = 0.03401948\n",
      "Iteration 492, loss = 0.03365873\n",
      "Iteration 493, loss = 0.03363642\n",
      "Iteration 494, loss = 0.03369677\n",
      "Iteration 495, loss = 0.03387109\n",
      "Iteration 496, loss = 0.03394099\n",
      "Iteration 497, loss = 0.03298062\n",
      "Iteration 498, loss = 0.03273697\n",
      "Iteration 499, loss = 0.03423828\n",
      "Iteration 500, loss = 0.03299319\n",
      "Iteration 501, loss = 0.03207855\n",
      "Iteration 502, loss = 0.03303139\n",
      "Iteration 503, loss = 0.03144047\n",
      "Iteration 504, loss = 0.03103154\n",
      "Iteration 505, loss = 0.03120369\n",
      "Iteration 506, loss = 0.03094157\n",
      "Iteration 507, loss = 0.03058136\n",
      "Iteration 508, loss = 0.03036496\n",
      "Iteration 509, loss = 0.03223102\n",
      "Iteration 510, loss = 0.03521003\n",
      "Iteration 511, loss = 0.03809088\n",
      "Iteration 512, loss = 0.03460858\n",
      "Iteration 513, loss = 0.03327111\n",
      "Iteration 514, loss = 0.03281192\n",
      "Iteration 515, loss = 0.03013612\n",
      "Iteration 516, loss = 0.02906820\n",
      "Iteration 517, loss = 0.02988204\n",
      "Iteration 518, loss = 0.02922416\n",
      "Iteration 519, loss = 0.02988395\n",
      "Iteration 520, loss = 0.02875007\n",
      "Iteration 521, loss = 0.02856151\n",
      "Iteration 522, loss = 0.02783791\n",
      "Iteration 523, loss = 0.02754240\n",
      "Iteration 524, loss = 0.02881931\n",
      "Iteration 525, loss = 0.02946206\n",
      "Iteration 526, loss = 0.02940812\n",
      "Iteration 527, loss = 0.02833759\n",
      "Iteration 528, loss = 0.02699061\n",
      "Iteration 529, loss = 0.02790777\n",
      "Iteration 530, loss = 0.02762299\n",
      "Iteration 531, loss = 0.02728588\n",
      "Iteration 532, loss = 0.02893528\n",
      "Iteration 533, loss = 0.02828259\n",
      "Iteration 534, loss = 0.02870153\n",
      "Iteration 535, loss = 0.02624052\n",
      "Iteration 536, loss = 0.02594480\n",
      "Iteration 537, loss = 0.02555784\n",
      "Iteration 538, loss = 0.02578695\n",
      "Iteration 539, loss = 0.02568191\n",
      "Iteration 540, loss = 0.02686309\n",
      "Iteration 541, loss = 0.02514534\n",
      "Iteration 542, loss = 0.02528663\n",
      "Iteration 543, loss = 0.02493923\n",
      "Iteration 544, loss = 0.02563616\n",
      "Iteration 545, loss = 0.02506162\n",
      "Iteration 546, loss = 0.02495788\n",
      "Iteration 547, loss = 0.02496355\n",
      "Iteration 548, loss = 0.02445997\n",
      "Iteration 549, loss = 0.02563441\n",
      "Iteration 550, loss = 0.02596547\n",
      "Iteration 551, loss = 0.02534640\n",
      "Iteration 552, loss = 0.02609612\n",
      "Iteration 553, loss = 0.02647072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 554, loss = 0.02512260\n",
      "Iteration 555, loss = 0.02387432\n",
      "Iteration 556, loss = 0.02335062\n",
      "Iteration 557, loss = 0.02371629\n",
      "Iteration 558, loss = 0.02439043\n",
      "Iteration 559, loss = 0.02456964\n",
      "Iteration 560, loss = 0.02268343\n",
      "Iteration 561, loss = 0.02369670\n",
      "Iteration 562, loss = 0.02240499\n",
      "Iteration 563, loss = 0.02387484\n",
      "Iteration 564, loss = 0.02319169\n",
      "Iteration 565, loss = 0.02274883\n",
      "Iteration 566, loss = 0.02275233\n",
      "Iteration 567, loss = 0.02341605\n",
      "Iteration 568, loss = 0.02356921\n",
      "Iteration 569, loss = 0.02405193\n",
      "Iteration 570, loss = 0.02655094\n",
      "Iteration 571, loss = 0.02447190\n",
      "Iteration 572, loss = 0.02245480\n",
      "Iteration 573, loss = 0.02208101\n",
      "Iteration 574, loss = 0.02153477\n",
      "Iteration 575, loss = 0.02122709\n",
      "Iteration 576, loss = 0.02199467\n",
      "Iteration 577, loss = 0.02127642\n",
      "Iteration 578, loss = 0.02056971\n",
      "Iteration 579, loss = 0.02096529\n",
      "Iteration 580, loss = 0.02058030\n",
      "Iteration 581, loss = 0.02032505\n",
      "Iteration 582, loss = 0.02095244\n",
      "Iteration 583, loss = 0.02007896\n",
      "Iteration 584, loss = 0.02054839\n",
      "Iteration 585, loss = 0.02062308\n",
      "Iteration 586, loss = 0.02043687\n",
      "Iteration 587, loss = 0.02035595\n",
      "Iteration 588, loss = 0.02062522\n",
      "Iteration 589, loss = 0.02027250\n",
      "Iteration 590, loss = 0.02062994\n",
      "Iteration 591, loss = 0.02030848\n",
      "Iteration 592, loss = 0.02162160\n",
      "Iteration 593, loss = 0.01966308\n",
      "Iteration 594, loss = 0.02026940\n",
      "Iteration 595, loss = 0.01921095\n",
      "Iteration 596, loss = 0.01897345\n",
      "Iteration 597, loss = 0.01968594\n",
      "Iteration 598, loss = 0.01921476\n",
      "Iteration 599, loss = 0.01875109\n",
      "Iteration 600, loss = 0.01934332\n",
      "Iteration 601, loss = 0.01896856\n",
      "Iteration 602, loss = 0.01915324\n",
      "Iteration 603, loss = 0.02013028\n",
      "Iteration 604, loss = 0.01834653\n",
      "Iteration 605, loss = 0.01843770\n",
      "Iteration 606, loss = 0.01756904\n",
      "Iteration 607, loss = 0.01747004\n",
      "Iteration 608, loss = 0.01809308\n",
      "Iteration 609, loss = 0.01760818\n",
      "Iteration 610, loss = 0.01772706\n",
      "Iteration 611, loss = 0.01727463\n",
      "Iteration 612, loss = 0.01745323\n",
      "Iteration 613, loss = 0.01732152\n",
      "Iteration 614, loss = 0.01729643\n",
      "Iteration 615, loss = 0.01908701\n",
      "Iteration 616, loss = 0.01730793\n",
      "Iteration 617, loss = 0.01750950\n",
      "Iteration 618, loss = 0.01872018\n",
      "Iteration 619, loss = 0.01694281\n",
      "Iteration 620, loss = 0.01702707\n",
      "Iteration 621, loss = 0.01808270\n",
      "Iteration 622, loss = 0.01779192\n",
      "Iteration 623, loss = 0.01788866\n",
      "Iteration 624, loss = 0.01887815\n",
      "Iteration 625, loss = 0.01813887\n",
      "Iteration 626, loss = 0.01855296\n",
      "Iteration 627, loss = 0.01848192\n",
      "Iteration 628, loss = 0.01661454\n",
      "Iteration 629, loss = 0.01636975\n",
      "Iteration 630, loss = 0.01630588\n",
      "Iteration 631, loss = 0.01641252\n",
      "Iteration 632, loss = 0.01593438\n",
      "Iteration 633, loss = 0.01575433\n",
      "Iteration 634, loss = 0.01632915\n",
      "Iteration 635, loss = 0.01669293\n",
      "Iteration 636, loss = 0.01616922\n",
      "Iteration 637, loss = 0.01572620\n",
      "Iteration 638, loss = 0.01577172\n",
      "Iteration 639, loss = 0.01590491\n",
      "Iteration 640, loss = 0.01553722\n",
      "Iteration 641, loss = 0.01530753\n",
      "Iteration 642, loss = 0.01540190\n",
      "Iteration 643, loss = 0.01487608\n",
      "Iteration 644, loss = 0.01478672\n",
      "Iteration 645, loss = 0.01461789\n",
      "Iteration 646, loss = 0.01572735\n",
      "Iteration 647, loss = 0.01560533\n",
      "Iteration 648, loss = 0.01538767\n",
      "Iteration 649, loss = 0.01434915\n",
      "Iteration 650, loss = 0.01465543\n",
      "Iteration 651, loss = 0.01547029\n",
      "Iteration 652, loss = 0.01480923\n",
      "Iteration 653, loss = 0.01485712\n",
      "Iteration 654, loss = 0.01501022\n",
      "Iteration 655, loss = 0.01401253\n",
      "Iteration 656, loss = 0.01393709\n",
      "Iteration 657, loss = 0.01473457\n",
      "Iteration 658, loss = 0.01406568\n",
      "Iteration 659, loss = 0.01433873\n",
      "Iteration 660, loss = 0.01402879\n",
      "Iteration 661, loss = 0.01525252\n",
      "Iteration 662, loss = 0.01446760\n",
      "Iteration 663, loss = 0.01526212\n",
      "Iteration 664, loss = 0.01601409\n",
      "Iteration 665, loss = 0.01499214\n",
      "Iteration 666, loss = 0.01380072\n",
      "Iteration 667, loss = 0.01466345\n",
      "Iteration 668, loss = 0.01350033\n",
      "Iteration 669, loss = 0.01396202\n",
      "Iteration 670, loss = 0.01388532\n",
      "Iteration 671, loss = 0.01348646\n",
      "Iteration 672, loss = 0.01424608\n",
      "Iteration 673, loss = 0.01295297\n",
      "Iteration 674, loss = 0.01323080\n",
      "Iteration 675, loss = 0.01301786\n",
      "Iteration 676, loss = 0.01302171\n",
      "Iteration 677, loss = 0.01368223\n",
      "Iteration 678, loss = 0.01426634\n",
      "Iteration 679, loss = 0.01460984\n",
      "Iteration 680, loss = 0.01287044\n",
      "Iteration 681, loss = 0.01270865\n",
      "Iteration 682, loss = 0.01276448\n",
      "Iteration 683, loss = 0.01211119\n",
      "Iteration 684, loss = 0.01227821\n",
      "Iteration 685, loss = 0.01302626\n",
      "Iteration 686, loss = 0.01298814\n",
      "Iteration 687, loss = 0.01294553\n",
      "Iteration 688, loss = 0.01287376\n",
      "Iteration 689, loss = 0.01210570\n",
      "Iteration 690, loss = 0.01212927\n",
      "Iteration 691, loss = 0.01201967\n",
      "Iteration 692, loss = 0.01238156\n",
      "Iteration 693, loss = 0.01262210\n",
      "Iteration 694, loss = 0.01218591\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 33 of 48...\n",
      "Iteration 1, loss = 0.69506097\n",
      "Iteration 2, loss = 0.68583019\n",
      "Iteration 3, loss = 0.68302306\n",
      "Iteration 4, loss = 0.67625903\n",
      "Iteration 5, loss = 0.66797287\n",
      "Iteration 6, loss = 0.65720737\n",
      "Iteration 7, loss = 0.64457249\n",
      "Iteration 8, loss = 0.62999745\n",
      "Iteration 9, loss = 0.61442767\n",
      "Iteration 10, loss = 0.59692734\n",
      "Iteration 11, loss = 0.57887484\n",
      "Iteration 12, loss = 0.56085801\n",
      "Iteration 13, loss = 0.54386997\n",
      "Iteration 14, loss = 0.52637143\n",
      "Iteration 15, loss = 0.51057101\n",
      "Iteration 16, loss = 0.49760912\n",
      "Iteration 17, loss = 0.48417101\n",
      "Iteration 18, loss = 0.47227759\n",
      "Iteration 19, loss = 0.46212070\n",
      "Iteration 20, loss = 0.45280559\n",
      "Iteration 21, loss = 0.44556999\n",
      "Iteration 22, loss = 0.43664873\n",
      "Iteration 23, loss = 0.43116962\n",
      "Iteration 24, loss = 0.42360962\n",
      "Iteration 25, loss = 0.41771840\n",
      "Iteration 26, loss = 0.41335848\n",
      "Iteration 27, loss = 0.40774398\n",
      "Iteration 28, loss = 0.40540575\n",
      "Iteration 29, loss = 0.40312747\n",
      "Iteration 30, loss = 0.39900916\n",
      "Iteration 31, loss = 0.39639505\n",
      "Iteration 32, loss = 0.38964370\n",
      "Iteration 33, loss = 0.38752684\n",
      "Iteration 34, loss = 0.38578828\n",
      "Iteration 35, loss = 0.38247320\n",
      "Iteration 36, loss = 0.37756283\n",
      "Iteration 37, loss = 0.37439592\n",
      "Iteration 38, loss = 0.37468202\n",
      "Iteration 39, loss = 0.36995048\n",
      "Iteration 40, loss = 0.36905297\n",
      "Iteration 41, loss = 0.36548680\n",
      "Iteration 42, loss = 0.36358753\n",
      "Iteration 43, loss = 0.36404090\n",
      "Iteration 44, loss = 0.36214725\n",
      "Iteration 45, loss = 0.35853003\n",
      "Iteration 46, loss = 0.35605034\n",
      "Iteration 47, loss = 0.35300926\n",
      "Iteration 48, loss = 0.35066943\n",
      "Iteration 49, loss = 0.34924608\n",
      "Iteration 50, loss = 0.34776202\n",
      "Iteration 51, loss = 0.34624226\n",
      "Iteration 52, loss = 0.34358826\n",
      "Iteration 53, loss = 0.34229890\n",
      "Iteration 54, loss = 0.34902345\n",
      "Iteration 55, loss = 0.34459595\n",
      "Iteration 56, loss = 0.34002842\n",
      "Iteration 57, loss = 0.34149932\n",
      "Iteration 58, loss = 0.33562776\n",
      "Iteration 59, loss = 0.33411860\n",
      "Iteration 60, loss = 0.33161550\n",
      "Iteration 61, loss = 0.33061289\n",
      "Iteration 62, loss = 0.32983078\n",
      "Iteration 63, loss = 0.32752217\n",
      "Iteration 64, loss = 0.32747226\n",
      "Iteration 65, loss = 0.32552208\n",
      "Iteration 66, loss = 0.32804834\n",
      "Iteration 67, loss = 0.32609966\n",
      "Iteration 68, loss = 0.32213932\n",
      "Iteration 69, loss = 0.32352323\n",
      "Iteration 70, loss = 0.32030308\n",
      "Iteration 71, loss = 0.31784641\n",
      "Iteration 72, loss = 0.31641437\n",
      "Iteration 73, loss = 0.31508215\n",
      "Iteration 74, loss = 0.31424077\n",
      "Iteration 75, loss = 0.31325397\n",
      "Iteration 76, loss = 0.31698061\n",
      "Iteration 77, loss = 0.31544706\n",
      "Iteration 78, loss = 0.31110331\n",
      "Iteration 79, loss = 0.31037642\n",
      "Iteration 80, loss = 0.31034689\n",
      "Iteration 81, loss = 0.30882085\n",
      "Iteration 82, loss = 0.30732124\n",
      "Iteration 83, loss = 0.30625879\n",
      "Iteration 84, loss = 0.30515234\n",
      "Iteration 85, loss = 0.30238941\n",
      "Iteration 86, loss = 0.30318459\n",
      "Iteration 87, loss = 0.30092263\n",
      "Iteration 88, loss = 0.30027149\n",
      "Iteration 89, loss = 0.29960866\n",
      "Iteration 90, loss = 0.29846704\n",
      "Iteration 91, loss = 0.29790204\n",
      "Iteration 92, loss = 0.29645793\n",
      "Iteration 93, loss = 0.29525394\n",
      "Iteration 94, loss = 0.29483306\n",
      "Iteration 95, loss = 0.29335274\n",
      "Iteration 96, loss = 0.29802582\n",
      "Iteration 97, loss = 0.29647959\n",
      "Iteration 98, loss = 0.29211383\n",
      "Iteration 99, loss = 0.29004803\n",
      "Iteration 100, loss = 0.29055165\n",
      "Iteration 101, loss = 0.28906166\n",
      "Iteration 102, loss = 0.28748417\n",
      "Iteration 103, loss = 0.28882695\n",
      "Iteration 104, loss = 0.28669508\n",
      "Iteration 105, loss = 0.28490565\n",
      "Iteration 106, loss = 0.28556856\n",
      "Iteration 107, loss = 0.28816836\n",
      "Iteration 108, loss = 0.28699373\n",
      "Iteration 109, loss = 0.28320781\n",
      "Iteration 110, loss = 0.28023579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 111, loss = 0.27977406\n",
      "Iteration 112, loss = 0.27768999\n",
      "Iteration 113, loss = 0.27704376\n",
      "Iteration 114, loss = 0.27677397\n",
      "Iteration 115, loss = 0.27637929\n",
      "Iteration 116, loss = 0.27409282\n",
      "Iteration 117, loss = 0.27749194\n",
      "Iteration 118, loss = 0.27617692\n",
      "Iteration 119, loss = 0.27334880\n",
      "Iteration 120, loss = 0.27730127\n",
      "Iteration 121, loss = 0.27215039\n",
      "Iteration 122, loss = 0.27136853\n",
      "Iteration 123, loss = 0.26878847\n",
      "Iteration 124, loss = 0.26953968\n",
      "Iteration 125, loss = 0.26892797\n",
      "Iteration 126, loss = 0.26701710\n",
      "Iteration 127, loss = 0.27006505\n",
      "Iteration 128, loss = 0.26327053\n",
      "Iteration 129, loss = 0.26351153\n",
      "Iteration 130, loss = 0.26275529\n",
      "Iteration 131, loss = 0.26163803\n",
      "Iteration 132, loss = 0.25975867\n",
      "Iteration 133, loss = 0.26109651\n",
      "Iteration 134, loss = 0.26050860\n",
      "Iteration 135, loss = 0.25858153\n",
      "Iteration 136, loss = 0.26136808\n",
      "Iteration 137, loss = 0.25650111\n",
      "Iteration 138, loss = 0.25520019\n",
      "Iteration 139, loss = 0.25549409\n",
      "Iteration 140, loss = 0.25402388\n",
      "Iteration 141, loss = 0.25194512\n",
      "Iteration 142, loss = 0.25325923\n",
      "Iteration 143, loss = 0.25212027\n",
      "Iteration 144, loss = 0.24947654\n",
      "Iteration 145, loss = 0.25026894\n",
      "Iteration 146, loss = 0.24796594\n",
      "Iteration 147, loss = 0.24777996\n",
      "Iteration 148, loss = 0.24644272\n",
      "Iteration 149, loss = 0.24713979\n",
      "Iteration 150, loss = 0.24560808\n",
      "Iteration 151, loss = 0.24741397\n",
      "Iteration 152, loss = 0.24871870\n",
      "Iteration 153, loss = 0.24493019\n",
      "Iteration 154, loss = 0.24335297\n",
      "Iteration 155, loss = 0.24030742\n",
      "Iteration 156, loss = 0.24135194\n",
      "Iteration 157, loss = 0.23845203\n",
      "Iteration 158, loss = 0.23928733\n",
      "Iteration 159, loss = 0.23817148\n",
      "Iteration 160, loss = 0.23651150\n",
      "Iteration 161, loss = 0.23574179\n",
      "Iteration 162, loss = 0.23400643\n",
      "Iteration 163, loss = 0.23488607\n",
      "Iteration 164, loss = 0.23431386\n",
      "Iteration 165, loss = 0.24157766\n",
      "Iteration 166, loss = 0.23262468\n",
      "Iteration 167, loss = 0.23536566\n",
      "Iteration 168, loss = 0.23064068\n",
      "Iteration 169, loss = 0.22974068\n",
      "Iteration 170, loss = 0.23062657\n",
      "Iteration 171, loss = 0.22802063\n",
      "Iteration 172, loss = 0.22814170\n",
      "Iteration 173, loss = 0.22633430\n",
      "Iteration 174, loss = 0.22448356\n",
      "Iteration 175, loss = 0.22355482\n",
      "Iteration 176, loss = 0.22215460\n",
      "Iteration 177, loss = 0.22328779\n",
      "Iteration 178, loss = 0.22294588\n",
      "Iteration 179, loss = 0.22441684\n",
      "Iteration 180, loss = 0.23000019\n",
      "Iteration 181, loss = 0.22156712\n",
      "Iteration 182, loss = 0.22059464\n",
      "Iteration 183, loss = 0.22336905\n",
      "Iteration 184, loss = 0.22080368\n",
      "Iteration 185, loss = 0.21690484\n",
      "Iteration 186, loss = 0.21516203\n",
      "Iteration 187, loss = 0.21414961\n",
      "Iteration 188, loss = 0.21209562\n",
      "Iteration 189, loss = 0.21270482\n",
      "Iteration 190, loss = 0.21469507\n",
      "Iteration 191, loss = 0.21083069\n",
      "Iteration 192, loss = 0.20941566\n",
      "Iteration 193, loss = 0.21015146\n",
      "Iteration 194, loss = 0.20809241\n",
      "Iteration 195, loss = 0.20961341\n",
      "Iteration 196, loss = 0.20674638\n",
      "Iteration 197, loss = 0.20684781\n",
      "Iteration 198, loss = 0.20612423\n",
      "Iteration 199, loss = 0.20426199\n",
      "Iteration 200, loss = 0.20495702\n",
      "Iteration 201, loss = 0.20316430\n",
      "Iteration 202, loss = 0.20258912\n",
      "Iteration 203, loss = 0.20385094\n",
      "Iteration 204, loss = 0.19909934\n",
      "Iteration 205, loss = 0.19864104\n",
      "Iteration 206, loss = 0.19958943\n",
      "Iteration 207, loss = 0.19723388\n",
      "Iteration 208, loss = 0.19830358\n",
      "Iteration 209, loss = 0.19565295\n",
      "Iteration 210, loss = 0.19310047\n",
      "Iteration 211, loss = 0.19515487\n",
      "Iteration 212, loss = 0.19711927\n",
      "Iteration 213, loss = 0.19411245\n",
      "Iteration 214, loss = 0.19115360\n",
      "Iteration 215, loss = 0.19130001\n",
      "Iteration 216, loss = 0.19078713\n",
      "Iteration 217, loss = 0.18990319\n",
      "Iteration 218, loss = 0.18764888\n",
      "Iteration 219, loss = 0.18605208\n",
      "Iteration 220, loss = 0.18654745\n",
      "Iteration 221, loss = 0.18553020\n",
      "Iteration 222, loss = 0.18404672\n",
      "Iteration 223, loss = 0.18441513\n",
      "Iteration 224, loss = 0.18619079\n",
      "Iteration 225, loss = 0.18139796\n",
      "Iteration 226, loss = 0.18064031\n",
      "Iteration 227, loss = 0.18025179\n",
      "Iteration 228, loss = 0.17939624\n",
      "Iteration 229, loss = 0.17861933\n",
      "Iteration 230, loss = 0.17938698\n",
      "Iteration 231, loss = 0.17954594\n",
      "Iteration 232, loss = 0.17932474\n",
      "Iteration 233, loss = 0.17632295\n",
      "Iteration 234, loss = 0.17320100\n",
      "Iteration 235, loss = 0.17314246\n",
      "Iteration 236, loss = 0.17347243\n",
      "Iteration 237, loss = 0.17092161\n",
      "Iteration 238, loss = 0.16880951\n",
      "Iteration 239, loss = 0.16798578\n",
      "Iteration 240, loss = 0.16960085\n",
      "Iteration 241, loss = 0.17038805\n",
      "Iteration 242, loss = 0.17170018\n",
      "Iteration 243, loss = 0.16919167\n",
      "Iteration 244, loss = 0.17000227\n",
      "Iteration 245, loss = 0.16997783\n",
      "Iteration 246, loss = 0.16825219\n",
      "Iteration 247, loss = 0.16180157\n",
      "Iteration 248, loss = 0.16483413\n",
      "Iteration 249, loss = 0.16357096\n",
      "Iteration 250, loss = 0.16046258\n",
      "Iteration 251, loss = 0.15883940\n",
      "Iteration 252, loss = 0.15740951\n",
      "Iteration 253, loss = 0.15909088\n",
      "Iteration 254, loss = 0.15836642\n",
      "Iteration 255, loss = 0.15663190\n",
      "Iteration 256, loss = 0.15577453\n",
      "Iteration 257, loss = 0.15252865\n",
      "Iteration 258, loss = 0.15223588\n",
      "Iteration 259, loss = 0.15196096\n",
      "Iteration 260, loss = 0.15223243\n",
      "Iteration 261, loss = 0.15363981\n",
      "Iteration 262, loss = 0.14976442\n",
      "Iteration 263, loss = 0.14859990\n",
      "Iteration 264, loss = 0.14916731\n",
      "Iteration 265, loss = 0.14727693\n",
      "Iteration 266, loss = 0.14857654\n",
      "Iteration 267, loss = 0.14609183\n",
      "Iteration 268, loss = 0.14502147\n",
      "Iteration 269, loss = 0.14453241\n",
      "Iteration 270, loss = 0.14607109\n",
      "Iteration 271, loss = 0.14442112\n",
      "Iteration 272, loss = 0.14459351\n",
      "Iteration 273, loss = 0.14361223\n",
      "Iteration 274, loss = 0.14038170\n",
      "Iteration 275, loss = 0.14056893\n",
      "Iteration 276, loss = 0.14048239\n",
      "Iteration 277, loss = 0.13942263\n",
      "Iteration 278, loss = 0.13816265\n",
      "Iteration 279, loss = 0.13987611\n",
      "Iteration 280, loss = 0.13676294\n",
      "Iteration 281, loss = 0.13604540\n",
      "Iteration 282, loss = 0.13669846\n",
      "Iteration 283, loss = 0.13728641\n",
      "Iteration 284, loss = 0.13456163\n",
      "Iteration 285, loss = 0.13652318\n",
      "Iteration 286, loss = 0.13478341\n",
      "Iteration 287, loss = 0.13217971\n",
      "Iteration 288, loss = 0.13622694\n",
      "Iteration 289, loss = 0.13381952\n",
      "Iteration 290, loss = 0.13040888\n",
      "Iteration 291, loss = 0.13043572\n",
      "Iteration 292, loss = 0.13156053\n",
      "Iteration 293, loss = 0.13050914\n",
      "Iteration 294, loss = 0.12597090\n",
      "Iteration 295, loss = 0.13114825\n",
      "Iteration 296, loss = 0.13259727\n",
      "Iteration 297, loss = 0.13330993\n",
      "Iteration 298, loss = 0.13058118\n",
      "Iteration 299, loss = 0.12923639\n",
      "Iteration 300, loss = 0.13026389\n",
      "Iteration 301, loss = 0.12498722\n",
      "Iteration 302, loss = 0.12652890\n",
      "Iteration 303, loss = 0.12266630\n",
      "Iteration 304, loss = 0.12288842\n",
      "Iteration 305, loss = 0.12662917\n",
      "Iteration 306, loss = 0.12455856\n",
      "Iteration 307, loss = 0.12224163\n",
      "Iteration 308, loss = 0.12031914\n",
      "Iteration 309, loss = 0.12114026\n",
      "Iteration 310, loss = 0.11864000\n",
      "Iteration 311, loss = 0.11796347\n",
      "Iteration 312, loss = 0.11758245\n",
      "Iteration 313, loss = 0.11738453\n",
      "Iteration 314, loss = 0.11629026\n",
      "Iteration 315, loss = 0.11615655\n",
      "Iteration 316, loss = 0.11685693\n",
      "Iteration 317, loss = 0.11856759\n",
      "Iteration 318, loss = 0.11920453\n",
      "Iteration 319, loss = 0.11495991\n",
      "Iteration 320, loss = 0.11356822\n",
      "Iteration 321, loss = 0.11309474\n",
      "Iteration 322, loss = 0.11369511\n",
      "Iteration 323, loss = 0.11204737\n",
      "Iteration 324, loss = 0.11385682\n",
      "Iteration 325, loss = 0.11283461\n",
      "Iteration 326, loss = 0.11345017\n",
      "Iteration 327, loss = 0.10929558\n",
      "Iteration 328, loss = 0.10972076\n",
      "Iteration 329, loss = 0.11093601\n",
      "Iteration 330, loss = 0.10916548\n",
      "Iteration 331, loss = 0.10754457\n",
      "Iteration 332, loss = 0.10899054\n",
      "Iteration 333, loss = 0.11078317\n",
      "Iteration 334, loss = 0.11220204\n",
      "Iteration 335, loss = 0.10666684\n",
      "Iteration 336, loss = 0.10631894\n",
      "Iteration 337, loss = 0.10640580\n",
      "Iteration 338, loss = 0.10458085\n",
      "Iteration 339, loss = 0.10555170\n",
      "Iteration 340, loss = 0.10514115\n",
      "Iteration 341, loss = 0.10692554\n",
      "Iteration 342, loss = 0.10391457\n",
      "Iteration 343, loss = 0.10586998\n",
      "Iteration 344, loss = 0.10467475\n",
      "Iteration 345, loss = 0.10208961\n",
      "Iteration 346, loss = 0.10286952\n",
      "Iteration 347, loss = 0.10184101\n",
      "Iteration 348, loss = 0.10305385\n",
      "Iteration 349, loss = 0.10242472\n",
      "Iteration 350, loss = 0.10064534\n",
      "Iteration 351, loss = 0.09885360\n",
      "Iteration 352, loss = 0.09848446\n",
      "Iteration 353, loss = 0.09742440\n",
      "Iteration 354, loss = 0.09940034\n",
      "Iteration 355, loss = 0.09859947\n",
      "Iteration 356, loss = 0.09949987\n",
      "Iteration 357, loss = 0.09826200\n",
      "Iteration 358, loss = 0.09788765\n",
      "Iteration 359, loss = 0.09662290\n",
      "Iteration 360, loss = 0.09607099\n",
      "Iteration 361, loss = 0.09696944\n",
      "Iteration 362, loss = 0.09595105\n",
      "Iteration 363, loss = 0.09598176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 364, loss = 0.09502366\n",
      "Iteration 365, loss = 0.09518900\n",
      "Iteration 366, loss = 0.09482494\n",
      "Iteration 367, loss = 0.09268017\n",
      "Iteration 368, loss = 0.09257428\n",
      "Iteration 369, loss = 0.09307903\n",
      "Iteration 370, loss = 0.09242654\n",
      "Iteration 371, loss = 0.09267330\n",
      "Iteration 372, loss = 0.09194512\n",
      "Iteration 373, loss = 0.09119607\n",
      "Iteration 374, loss = 0.09065619\n",
      "Iteration 375, loss = 0.08973689\n",
      "Iteration 376, loss = 0.09381072\n",
      "Iteration 377, loss = 0.09100657\n",
      "Iteration 378, loss = 0.08990106\n",
      "Iteration 379, loss = 0.09004468\n",
      "Iteration 380, loss = 0.08896308\n",
      "Iteration 381, loss = 0.08763990\n",
      "Iteration 382, loss = 0.08767506\n",
      "Iteration 383, loss = 0.08748617\n",
      "Iteration 384, loss = 0.08718508\n",
      "Iteration 385, loss = 0.08710219\n",
      "Iteration 386, loss = 0.08738330\n",
      "Iteration 387, loss = 0.08850663\n",
      "Iteration 388, loss = 0.08646333\n",
      "Iteration 389, loss = 0.08523867\n",
      "Iteration 390, loss = 0.08586203\n",
      "Iteration 391, loss = 0.08380329\n",
      "Iteration 392, loss = 0.08455345\n",
      "Iteration 393, loss = 0.08358271\n",
      "Iteration 394, loss = 0.08340867\n",
      "Iteration 395, loss = 0.08412058\n",
      "Iteration 396, loss = 0.08330483\n",
      "Iteration 397, loss = 0.08223636\n",
      "Iteration 398, loss = 0.08214326\n",
      "Iteration 399, loss = 0.08266886\n",
      "Iteration 400, loss = 0.08189058\n",
      "Iteration 401, loss = 0.08144032\n",
      "Iteration 402, loss = 0.08101774\n",
      "Iteration 403, loss = 0.08099076\n",
      "Iteration 404, loss = 0.08062336\n",
      "Iteration 405, loss = 0.07983079\n",
      "Iteration 406, loss = 0.08311049\n",
      "Iteration 407, loss = 0.08499959\n",
      "Iteration 408, loss = 0.08702099\n",
      "Iteration 409, loss = 0.10415284\n",
      "Iteration 410, loss = 0.09908492\n",
      "Iteration 411, loss = 0.10164581\n",
      "Iteration 412, loss = 0.09181145\n",
      "Iteration 413, loss = 0.08337326\n",
      "Iteration 414, loss = 0.08223624\n",
      "Iteration 415, loss = 0.07987669\n",
      "Iteration 416, loss = 0.08185975\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 34 of 48...\n",
      "Iteration 1, loss = 0.63451348\n",
      "Iteration 2, loss = 0.55643783\n",
      "Iteration 3, loss = 0.50524346\n",
      "Iteration 4, loss = 0.47279992\n",
      "Iteration 5, loss = 0.44803905\n",
      "Iteration 6, loss = 0.42266568\n",
      "Iteration 7, loss = 0.40867487\n",
      "Iteration 8, loss = 0.40910835\n",
      "Iteration 9, loss = 0.41671934\n",
      "Iteration 10, loss = 0.39078932\n",
      "Iteration 11, loss = 0.37017922\n",
      "Iteration 12, loss = 0.36139981\n",
      "Iteration 13, loss = 0.36347223\n",
      "Iteration 14, loss = 0.35126332\n",
      "Iteration 15, loss = 0.34157167\n",
      "Iteration 16, loss = 0.33334201\n",
      "Iteration 17, loss = 0.32880607\n",
      "Iteration 18, loss = 0.32115643\n",
      "Iteration 19, loss = 0.31799855\n",
      "Iteration 20, loss = 0.31085679\n",
      "Iteration 21, loss = 0.30634580\n",
      "Iteration 22, loss = 0.30783229\n",
      "Iteration 23, loss = 0.30356124\n",
      "Iteration 24, loss = 0.29937638\n",
      "Iteration 25, loss = 0.28710678\n",
      "Iteration 26, loss = 0.28793544\n",
      "Iteration 27, loss = 0.29324372\n",
      "Iteration 28, loss = 0.27379970\n",
      "Iteration 29, loss = 0.27238417\n",
      "Iteration 30, loss = 0.26289028\n",
      "Iteration 31, loss = 0.26760842\n",
      "Iteration 32, loss = 0.25631236\n",
      "Iteration 33, loss = 0.25626485\n",
      "Iteration 34, loss = 0.24947064\n",
      "Iteration 35, loss = 0.25964416\n",
      "Iteration 36, loss = 0.25493555\n",
      "Iteration 37, loss = 0.24492240\n",
      "Iteration 38, loss = 0.23663287\n",
      "Iteration 39, loss = 0.22985218\n",
      "Iteration 40, loss = 0.23253726\n",
      "Iteration 41, loss = 0.23096517\n",
      "Iteration 42, loss = 0.22112763\n",
      "Iteration 43, loss = 0.21718409\n",
      "Iteration 44, loss = 0.21295111\n",
      "Iteration 45, loss = 0.22020693\n",
      "Iteration 46, loss = 0.21744858\n",
      "Iteration 47, loss = 0.21595207\n",
      "Iteration 48, loss = 0.21326668\n",
      "Iteration 49, loss = 0.19849286\n",
      "Iteration 50, loss = 0.19487488\n",
      "Iteration 51, loss = 0.20067997\n",
      "Iteration 52, loss = 0.19156596\n",
      "Iteration 53, loss = 0.18646633\n",
      "Iteration 54, loss = 0.18791816\n",
      "Iteration 55, loss = 0.18670985\n",
      "Iteration 56, loss = 0.18512127\n",
      "Iteration 57, loss = 0.18292615\n",
      "Iteration 58, loss = 0.17959140\n",
      "Iteration 59, loss = 0.18068484\n",
      "Iteration 60, loss = 0.17757753\n",
      "Iteration 61, loss = 0.16649006\n",
      "Iteration 62, loss = 0.16709803\n",
      "Iteration 63, loss = 0.16268579\n",
      "Iteration 64, loss = 0.16810900\n",
      "Iteration 65, loss = 0.15826905\n",
      "Iteration 66, loss = 0.15672472\n",
      "Iteration 67, loss = 0.15652329\n",
      "Iteration 68, loss = 0.14957597\n",
      "Iteration 69, loss = 0.14641673\n",
      "Iteration 70, loss = 0.14194883\n",
      "Iteration 71, loss = 0.13783305\n",
      "Iteration 72, loss = 0.14236611\n",
      "Iteration 73, loss = 0.13779451\n",
      "Iteration 74, loss = 0.13774627\n",
      "Iteration 75, loss = 0.14150411\n",
      "Iteration 76, loss = 0.13036930\n",
      "Iteration 77, loss = 0.12709312\n",
      "Iteration 78, loss = 0.12156209\n",
      "Iteration 79, loss = 0.12465417\n",
      "Iteration 80, loss = 0.12963375\n",
      "Iteration 81, loss = 0.12036399\n",
      "Iteration 82, loss = 0.12151248\n",
      "Iteration 83, loss = 0.12265847\n",
      "Iteration 84, loss = 0.11347135\n",
      "Iteration 85, loss = 0.11370924\n",
      "Iteration 86, loss = 0.11406058\n",
      "Iteration 87, loss = 0.11339032\n",
      "Iteration 88, loss = 0.10516934\n",
      "Iteration 89, loss = 0.10216303\n",
      "Iteration 90, loss = 0.10202489\n",
      "Iteration 91, loss = 0.10523050\n",
      "Iteration 92, loss = 0.11239205\n",
      "Iteration 93, loss = 0.12701361\n",
      "Iteration 94, loss = 0.13168053\n",
      "Iteration 95, loss = 0.11090095\n",
      "Iteration 96, loss = 0.11360327\n",
      "Iteration 97, loss = 0.10614028\n",
      "Iteration 98, loss = 0.09703624\n",
      "Iteration 99, loss = 0.09214570\n",
      "Iteration 100, loss = 0.08903034\n",
      "Iteration 101, loss = 0.08954769\n",
      "Iteration 102, loss = 0.08820244\n",
      "Iteration 103, loss = 0.08994004\n",
      "Iteration 104, loss = 0.08988100\n",
      "Iteration 105, loss = 0.08766275\n",
      "Iteration 106, loss = 0.08104814\n",
      "Iteration 107, loss = 0.08171242\n",
      "Iteration 108, loss = 0.07712338\n",
      "Iteration 109, loss = 0.07701246\n",
      "Iteration 110, loss = 0.07516716\n",
      "Iteration 111, loss = 0.07913358\n",
      "Iteration 112, loss = 0.08709195\n",
      "Iteration 113, loss = 0.07755262\n",
      "Iteration 114, loss = 0.07479652\n",
      "Iteration 115, loss = 0.07526603\n",
      "Iteration 116, loss = 0.06866816\n",
      "Iteration 117, loss = 0.07216268\n",
      "Iteration 118, loss = 0.07197393\n",
      "Iteration 119, loss = 0.06784869\n",
      "Iteration 120, loss = 0.07009568\n",
      "Iteration 121, loss = 0.06769907\n",
      "Iteration 122, loss = 0.06191253\n",
      "Iteration 123, loss = 0.06444033\n",
      "Iteration 124, loss = 0.06324911\n",
      "Iteration 125, loss = 0.06247262\n",
      "Iteration 126, loss = 0.06104524\n",
      "Iteration 127, loss = 0.06562029\n",
      "Iteration 128, loss = 0.06267783\n",
      "Iteration 129, loss = 0.06137075\n",
      "Iteration 130, loss = 0.05817643\n",
      "Iteration 131, loss = 0.05460833\n",
      "Iteration 132, loss = 0.05587177\n",
      "Iteration 133, loss = 0.05695847\n",
      "Iteration 134, loss = 0.05382283\n",
      "Iteration 135, loss = 0.05213130\n",
      "Iteration 136, loss = 0.05335114\n",
      "Iteration 137, loss = 0.04949223\n",
      "Iteration 138, loss = 0.05323289\n",
      "Iteration 139, loss = 0.05043022\n",
      "Iteration 140, loss = 0.04868228\n",
      "Iteration 141, loss = 0.04460901\n",
      "Iteration 142, loss = 0.04638369\n",
      "Iteration 143, loss = 0.04579990\n",
      "Iteration 144, loss = 0.04417491\n",
      "Iteration 145, loss = 0.05248277\n",
      "Iteration 146, loss = 0.05436802\n",
      "Iteration 147, loss = 0.04826636\n",
      "Iteration 148, loss = 0.04964244\n",
      "Iteration 149, loss = 0.05611200\n",
      "Iteration 150, loss = 0.04583074\n",
      "Iteration 151, loss = 0.04308056\n",
      "Iteration 152, loss = 0.04082002\n",
      "Iteration 153, loss = 0.04489008\n",
      "Iteration 154, loss = 0.04877274\n",
      "Iteration 155, loss = 0.04680497\n",
      "Iteration 156, loss = 0.04355236\n",
      "Iteration 157, loss = 0.04126105\n",
      "Iteration 158, loss = 0.03687435\n",
      "Iteration 159, loss = 0.03608639\n",
      "Iteration 160, loss = 0.04125691\n",
      "Iteration 161, loss = 0.03600362\n",
      "Iteration 162, loss = 0.03785849\n",
      "Iteration 163, loss = 0.03614426\n",
      "Iteration 164, loss = 0.03733888\n",
      "Iteration 165, loss = 0.03775875\n",
      "Iteration 166, loss = 0.03370648\n",
      "Iteration 167, loss = 0.04043742\n",
      "Iteration 168, loss = 0.04213987\n",
      "Iteration 169, loss = 0.03289145\n",
      "Iteration 170, loss = 0.03203485\n",
      "Iteration 171, loss = 0.03202766\n",
      "Iteration 172, loss = 0.03318117\n",
      "Iteration 173, loss = 0.03254543\n",
      "Iteration 174, loss = 0.02872269\n",
      "Iteration 175, loss = 0.03009464\n",
      "Iteration 176, loss = 0.03015305\n",
      "Iteration 177, loss = 0.02781749\n",
      "Iteration 178, loss = 0.02704091\n",
      "Iteration 179, loss = 0.03279067\n",
      "Iteration 180, loss = 0.03075473\n",
      "Iteration 181, loss = 0.02996617\n",
      "Iteration 182, loss = 0.02713538\n",
      "Iteration 183, loss = 0.02685971\n",
      "Iteration 184, loss = 0.02891567\n",
      "Iteration 185, loss = 0.02812506\n",
      "Iteration 186, loss = 0.02546905\n",
      "Iteration 187, loss = 0.03073489\n",
      "Iteration 188, loss = 0.02972329\n",
      "Iteration 189, loss = 0.03075815\n",
      "Iteration 190, loss = 0.02952422\n",
      "Iteration 191, loss = 0.02989476\n",
      "Iteration 192, loss = 0.02605810\n",
      "Iteration 193, loss = 0.02360611\n",
      "Iteration 194, loss = 0.02486319\n",
      "Iteration 195, loss = 0.02550254\n",
      "Iteration 196, loss = 0.02708714\n",
      "Iteration 197, loss = 0.02520281\n",
      "Iteration 198, loss = 0.03146519\n",
      "Iteration 199, loss = 0.02191140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200, loss = 0.02442068\n",
      "Iteration 201, loss = 0.02530445\n",
      "Iteration 202, loss = 0.02180538\n",
      "Iteration 203, loss = 0.01802092\n",
      "Iteration 204, loss = 0.02100176\n",
      "Iteration 205, loss = 0.02017542\n",
      "Iteration 206, loss = 0.01727567\n",
      "Iteration 207, loss = 0.01942167\n",
      "Iteration 208, loss = 0.02518052\n",
      "Iteration 209, loss = 0.02174978\n",
      "Iteration 210, loss = 0.01954497\n",
      "Iteration 211, loss = 0.01689038\n",
      "Iteration 212, loss = 0.01680777\n",
      "Iteration 213, loss = 0.01602127\n",
      "Iteration 214, loss = 0.01577990\n",
      "Iteration 215, loss = 0.01608935\n",
      "Iteration 216, loss = 0.01772044\n",
      "Iteration 217, loss = 0.02061523\n",
      "Iteration 218, loss = 0.02140021\n",
      "Iteration 219, loss = 0.02253912\n",
      "Iteration 220, loss = 0.02089679\n",
      "Iteration 221, loss = 0.03145871\n",
      "Iteration 222, loss = 0.02964363\n",
      "Iteration 223, loss = 0.04909668\n",
      "Iteration 224, loss = 0.10930147\n",
      "Iteration 225, loss = 0.08944004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 35 of 48...\n",
      "Iteration 1, loss = 0.64931717\n",
      "Iteration 2, loss = 0.57035519\n",
      "Iteration 3, loss = 0.51881625\n",
      "Iteration 4, loss = 0.48030090\n",
      "Iteration 5, loss = 0.44934912\n",
      "Iteration 6, loss = 0.42662447\n",
      "Iteration 7, loss = 0.40617226\n",
      "Iteration 8, loss = 0.39282504\n",
      "Iteration 9, loss = 0.39068050\n",
      "Iteration 10, loss = 0.37336813\n",
      "Iteration 11, loss = 0.36224122\n",
      "Iteration 12, loss = 0.35822580\n",
      "Iteration 13, loss = 0.34981808\n",
      "Iteration 14, loss = 0.33881767\n",
      "Iteration 15, loss = 0.33361637\n",
      "Iteration 16, loss = 0.32840097\n",
      "Iteration 17, loss = 0.33751310\n",
      "Iteration 18, loss = 0.32297280\n",
      "Iteration 19, loss = 0.31275250\n",
      "Iteration 20, loss = 0.30362348\n",
      "Iteration 21, loss = 0.30881774\n",
      "Iteration 22, loss = 0.30267539\n",
      "Iteration 23, loss = 0.29942594\n",
      "Iteration 24, loss = 0.28422999\n",
      "Iteration 25, loss = 0.28563729\n",
      "Iteration 26, loss = 0.27772478\n",
      "Iteration 27, loss = 0.28006394\n",
      "Iteration 28, loss = 0.27312838\n",
      "Iteration 29, loss = 0.26529181\n",
      "Iteration 30, loss = 0.26659362\n",
      "Iteration 31, loss = 0.26825391\n",
      "Iteration 32, loss = 0.26317477\n",
      "Iteration 33, loss = 0.25049757\n",
      "Iteration 34, loss = 0.24765165\n",
      "Iteration 35, loss = 0.25187847\n",
      "Iteration 36, loss = 0.25004197\n",
      "Iteration 37, loss = 0.23769114\n",
      "Iteration 38, loss = 0.23248100\n",
      "Iteration 39, loss = 0.22793938\n",
      "Iteration 40, loss = 0.23188502\n",
      "Iteration 41, loss = 0.22816665\n",
      "Iteration 42, loss = 0.21810345\n",
      "Iteration 43, loss = 0.21290607\n",
      "Iteration 44, loss = 0.20932160\n",
      "Iteration 45, loss = 0.21143749\n",
      "Iteration 46, loss = 0.20782326\n",
      "Iteration 47, loss = 0.20349336\n",
      "Iteration 48, loss = 0.20346843\n",
      "Iteration 49, loss = 0.20083560\n",
      "Iteration 50, loss = 0.20164013\n",
      "Iteration 51, loss = 0.19095297\n",
      "Iteration 52, loss = 0.18676943\n",
      "Iteration 53, loss = 0.18871857\n",
      "Iteration 54, loss = 0.18427602\n",
      "Iteration 55, loss = 0.18137503\n",
      "Iteration 56, loss = 0.17525004\n",
      "Iteration 57, loss = 0.17418612\n",
      "Iteration 58, loss = 0.17708274\n",
      "Iteration 59, loss = 0.17316662\n",
      "Iteration 60, loss = 0.16283648\n",
      "Iteration 61, loss = 0.16413135\n",
      "Iteration 62, loss = 0.16373800\n",
      "Iteration 63, loss = 0.15287845\n",
      "Iteration 64, loss = 0.14866539\n",
      "Iteration 65, loss = 0.14810199\n",
      "Iteration 66, loss = 0.14499673\n",
      "Iteration 67, loss = 0.15161998\n",
      "Iteration 68, loss = 0.14576233\n",
      "Iteration 69, loss = 0.14317015\n",
      "Iteration 70, loss = 0.14945329\n",
      "Iteration 71, loss = 0.15371756\n",
      "Iteration 72, loss = 0.15593159\n",
      "Iteration 73, loss = 0.13407509\n",
      "Iteration 74, loss = 0.12893947\n",
      "Iteration 75, loss = 0.13777485\n",
      "Iteration 76, loss = 0.12565003\n",
      "Iteration 77, loss = 0.12161368\n",
      "Iteration 78, loss = 0.12638891\n",
      "Iteration 79, loss = 0.12375593\n",
      "Iteration 80, loss = 0.11507328\n",
      "Iteration 81, loss = 0.11170065\n",
      "Iteration 82, loss = 0.11836419\n",
      "Iteration 83, loss = 0.11461615\n",
      "Iteration 84, loss = 0.10584290\n",
      "Iteration 85, loss = 0.10386995\n",
      "Iteration 86, loss = 0.10738183\n",
      "Iteration 87, loss = 0.10479571\n",
      "Iteration 88, loss = 0.10571959\n",
      "Iteration 89, loss = 0.10533882\n",
      "Iteration 90, loss = 0.10693492\n",
      "Iteration 91, loss = 0.10375675\n",
      "Iteration 92, loss = 0.10068238\n",
      "Iteration 93, loss = 0.10969347\n",
      "Iteration 94, loss = 0.10219232\n",
      "Iteration 95, loss = 0.09601620\n",
      "Iteration 96, loss = 0.09524535\n",
      "Iteration 97, loss = 0.11672431\n",
      "Iteration 98, loss = 0.11264178\n",
      "Iteration 99, loss = 0.10490006\n",
      "Iteration 100, loss = 0.08494904\n",
      "Iteration 101, loss = 0.08646844\n",
      "Iteration 102, loss = 0.08127489\n",
      "Iteration 103, loss = 0.08434841\n",
      "Iteration 104, loss = 0.07871781\n",
      "Iteration 105, loss = 0.08124555\n",
      "Iteration 106, loss = 0.08650859\n",
      "Iteration 107, loss = 0.07570615\n",
      "Iteration 108, loss = 0.07392441\n",
      "Iteration 109, loss = 0.07085128\n",
      "Iteration 110, loss = 0.06827744\n",
      "Iteration 111, loss = 0.07037885\n",
      "Iteration 112, loss = 0.07383416\n",
      "Iteration 113, loss = 0.07408253\n",
      "Iteration 114, loss = 0.06773061\n",
      "Iteration 115, loss = 0.06592254\n",
      "Iteration 116, loss = 0.06685538\n",
      "Iteration 117, loss = 0.06358275\n",
      "Iteration 118, loss = 0.06450772\n",
      "Iteration 119, loss = 0.06906889\n",
      "Iteration 120, loss = 0.06819634\n",
      "Iteration 121, loss = 0.06610611\n",
      "Iteration 122, loss = 0.05743771\n",
      "Iteration 123, loss = 0.05850500\n",
      "Iteration 124, loss = 0.05718475\n",
      "Iteration 125, loss = 0.06432341\n",
      "Iteration 126, loss = 0.06166771\n",
      "Iteration 127, loss = 0.05628880\n",
      "Iteration 128, loss = 0.05352811\n",
      "Iteration 129, loss = 0.06615056\n",
      "Iteration 130, loss = 0.08490026\n",
      "Iteration 131, loss = 0.05957947\n",
      "Iteration 132, loss = 0.06041484\n",
      "Iteration 133, loss = 0.05473697\n",
      "Iteration 134, loss = 0.04885215\n",
      "Iteration 135, loss = 0.05293030\n",
      "Iteration 136, loss = 0.05816227\n",
      "Iteration 137, loss = 0.05020553\n",
      "Iteration 138, loss = 0.05047775\n",
      "Iteration 139, loss = 0.05129093\n",
      "Iteration 140, loss = 0.05254452\n",
      "Iteration 141, loss = 0.04739858\n",
      "Iteration 142, loss = 0.04788583\n",
      "Iteration 143, loss = 0.04486750\n",
      "Iteration 144, loss = 0.04200559\n",
      "Iteration 145, loss = 0.04071218\n",
      "Iteration 146, loss = 0.05038259\n",
      "Iteration 147, loss = 0.04440136\n",
      "Iteration 148, loss = 0.04339145\n",
      "Iteration 149, loss = 0.04027344\n",
      "Iteration 150, loss = 0.04456007\n",
      "Iteration 151, loss = 0.04350148\n",
      "Iteration 152, loss = 0.05433506\n",
      "Iteration 153, loss = 0.07222374\n",
      "Iteration 154, loss = 0.06737154\n",
      "Iteration 155, loss = 0.04243035\n",
      "Iteration 156, loss = 0.04339755\n",
      "Iteration 157, loss = 0.04552775\n",
      "Iteration 158, loss = 0.07065299\n",
      "Iteration 159, loss = 0.07048608\n",
      "Iteration 160, loss = 0.07084114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 36 of 48...\n",
      "Iteration 1, loss = 0.67194750\n",
      "Iteration 2, loss = 0.59925119\n",
      "Iteration 3, loss = 0.54832888\n",
      "Iteration 4, loss = 0.50751737\n",
      "Iteration 5, loss = 0.47159806\n",
      "Iteration 6, loss = 0.44850728\n",
      "Iteration 7, loss = 0.42630962\n",
      "Iteration 8, loss = 0.40756136\n",
      "Iteration 9, loss = 0.39190932\n",
      "Iteration 10, loss = 0.38131219\n",
      "Iteration 11, loss = 0.37138850\n",
      "Iteration 12, loss = 0.36530962\n",
      "Iteration 13, loss = 0.35635659\n",
      "Iteration 14, loss = 0.34677430\n",
      "Iteration 15, loss = 0.34163596\n",
      "Iteration 16, loss = 0.33628265\n",
      "Iteration 17, loss = 0.32807749\n",
      "Iteration 18, loss = 0.31790051\n",
      "Iteration 19, loss = 0.31605127\n",
      "Iteration 20, loss = 0.30727524\n",
      "Iteration 21, loss = 0.30222200\n",
      "Iteration 22, loss = 0.29761861\n",
      "Iteration 23, loss = 0.29341557\n",
      "Iteration 24, loss = 0.29094042\n",
      "Iteration 25, loss = 0.28357229\n",
      "Iteration 26, loss = 0.27859076\n",
      "Iteration 27, loss = 0.27917277\n",
      "Iteration 28, loss = 0.27658185\n",
      "Iteration 29, loss = 0.26519145\n",
      "Iteration 30, loss = 0.25884981\n",
      "Iteration 31, loss = 0.25335661\n",
      "Iteration 32, loss = 0.24856105\n",
      "Iteration 33, loss = 0.25134018\n",
      "Iteration 34, loss = 0.24461558\n",
      "Iteration 35, loss = 0.25087348\n",
      "Iteration 36, loss = 0.24039016\n",
      "Iteration 37, loss = 0.23065915\n",
      "Iteration 38, loss = 0.22406901\n",
      "Iteration 39, loss = 0.22725668\n",
      "Iteration 40, loss = 0.21730709\n",
      "Iteration 41, loss = 0.21351002\n",
      "Iteration 42, loss = 0.21308643\n",
      "Iteration 43, loss = 0.21089472\n",
      "Iteration 44, loss = 0.21730298\n",
      "Iteration 45, loss = 0.20659308\n",
      "Iteration 46, loss = 0.20094217\n",
      "Iteration 47, loss = 0.19806098\n",
      "Iteration 48, loss = 0.19660363\n",
      "Iteration 49, loss = 0.20127315\n",
      "Iteration 50, loss = 0.18578718\n",
      "Iteration 51, loss = 0.18297756\n",
      "Iteration 52, loss = 0.19161042\n",
      "Iteration 53, loss = 0.18445298\n",
      "Iteration 54, loss = 0.18010792\n",
      "Iteration 55, loss = 0.17650878\n",
      "Iteration 56, loss = 0.17248806\n",
      "Iteration 57, loss = 0.16430953\n",
      "Iteration 58, loss = 0.16845670\n",
      "Iteration 59, loss = 0.16372864\n",
      "Iteration 60, loss = 0.15720180\n",
      "Iteration 61, loss = 0.15551865\n",
      "Iteration 62, loss = 0.15346719\n",
      "Iteration 63, loss = 0.15438605\n",
      "Iteration 64, loss = 0.15179257\n",
      "Iteration 65, loss = 0.14799088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66, loss = 0.14707626\n",
      "Iteration 67, loss = 0.14466368\n",
      "Iteration 68, loss = 0.14162706\n",
      "Iteration 69, loss = 0.13632256\n",
      "Iteration 70, loss = 0.13979398\n",
      "Iteration 71, loss = 0.13473765\n",
      "Iteration 72, loss = 0.13265047\n",
      "Iteration 73, loss = 0.12524338\n",
      "Iteration 74, loss = 0.12877571\n",
      "Iteration 75, loss = 0.12346978\n",
      "Iteration 76, loss = 0.12476517\n",
      "Iteration 77, loss = 0.12096806\n",
      "Iteration 78, loss = 0.12051672\n",
      "Iteration 79, loss = 0.11833584\n",
      "Iteration 80, loss = 0.11893856\n",
      "Iteration 81, loss = 0.11328193\n",
      "Iteration 82, loss = 0.10928194\n",
      "Iteration 83, loss = 0.10872367\n",
      "Iteration 84, loss = 0.10599202\n",
      "Iteration 85, loss = 0.10472178\n",
      "Iteration 86, loss = 0.10242451\n",
      "Iteration 87, loss = 0.10238648\n",
      "Iteration 88, loss = 0.10270196\n",
      "Iteration 89, loss = 0.10702448\n",
      "Iteration 90, loss = 0.10325118\n",
      "Iteration 91, loss = 0.09417885\n",
      "Iteration 92, loss = 0.09400352\n",
      "Iteration 93, loss = 0.09376707\n",
      "Iteration 94, loss = 0.09516991\n",
      "Iteration 95, loss = 0.08983617\n",
      "Iteration 96, loss = 0.09494745\n",
      "Iteration 97, loss = 0.09277848\n",
      "Iteration 98, loss = 0.08656750\n",
      "Iteration 99, loss = 0.08524356\n",
      "Iteration 100, loss = 0.09296360\n",
      "Iteration 101, loss = 0.08554157\n",
      "Iteration 102, loss = 0.08403265\n",
      "Iteration 103, loss = 0.08295055\n",
      "Iteration 104, loss = 0.08144055\n",
      "Iteration 105, loss = 0.08180458\n",
      "Iteration 106, loss = 0.08008724\n",
      "Iteration 107, loss = 0.07797764\n",
      "Iteration 108, loss = 0.07529340\n",
      "Iteration 109, loss = 0.07131068\n",
      "Iteration 110, loss = 0.07060998\n",
      "Iteration 111, loss = 0.06981165\n",
      "Iteration 112, loss = 0.07728738\n",
      "Iteration 113, loss = 0.08822448\n",
      "Iteration 114, loss = 0.07576593\n",
      "Iteration 115, loss = 0.07306398\n",
      "Iteration 116, loss = 0.07190576\n",
      "Iteration 117, loss = 0.06409316\n",
      "Iteration 118, loss = 0.06291861\n",
      "Iteration 119, loss = 0.06502578\n",
      "Iteration 120, loss = 0.06433615\n",
      "Iteration 121, loss = 0.06146729\n",
      "Iteration 122, loss = 0.06375976\n",
      "Iteration 123, loss = 0.06672246\n",
      "Iteration 124, loss = 0.06447219\n",
      "Iteration 125, loss = 0.08042689\n",
      "Iteration 126, loss = 0.08018370\n",
      "Iteration 127, loss = 0.06040627\n",
      "Iteration 128, loss = 0.05660795\n",
      "Iteration 129, loss = 0.05229385\n",
      "Iteration 130, loss = 0.05425262\n",
      "Iteration 131, loss = 0.05027979\n",
      "Iteration 132, loss = 0.05024065\n",
      "Iteration 133, loss = 0.05202843\n",
      "Iteration 134, loss = 0.05623931\n",
      "Iteration 135, loss = 0.05333930\n",
      "Iteration 136, loss = 0.05520752\n",
      "Iteration 137, loss = 0.05733966\n",
      "Iteration 138, loss = 0.06560410\n",
      "Iteration 139, loss = 0.05734035\n",
      "Iteration 140, loss = 0.04997495\n",
      "Iteration 141, loss = 0.05047203\n",
      "Iteration 142, loss = 0.05709750\n",
      "Iteration 143, loss = 0.05125017\n",
      "Iteration 144, loss = 0.04633282\n",
      "Iteration 145, loss = 0.06176339\n",
      "Iteration 146, loss = 0.07350610\n",
      "Iteration 147, loss = 0.06992798\n",
      "Iteration 148, loss = 0.07232527\n",
      "Iteration 149, loss = 0.08109028\n",
      "Iteration 150, loss = 0.06376952\n",
      "Iteration 151, loss = 0.05245110\n",
      "Iteration 152, loss = 0.04898217\n",
      "Iteration 153, loss = 0.04257687\n",
      "Iteration 154, loss = 0.04167037\n",
      "Iteration 155, loss = 0.04220957\n",
      "Iteration 156, loss = 0.04082263\n",
      "Iteration 157, loss = 0.03983168\n",
      "Iteration 158, loss = 0.04026244\n",
      "Iteration 159, loss = 0.03867261\n",
      "Iteration 160, loss = 0.04238946\n",
      "Iteration 161, loss = 0.03880785\n",
      "Iteration 162, loss = 0.03910294\n",
      "Iteration 163, loss = 0.03944685\n",
      "Iteration 164, loss = 0.03737312\n",
      "Iteration 165, loss = 0.03586502\n",
      "Iteration 166, loss = 0.03747110\n",
      "Iteration 167, loss = 0.03576428\n",
      "Iteration 168, loss = 0.03482577\n",
      "Iteration 169, loss = 0.03725056\n",
      "Iteration 170, loss = 0.03903665\n",
      "Iteration 171, loss = 0.04593555\n",
      "Iteration 172, loss = 0.04006748\n",
      "Iteration 173, loss = 0.03392739\n",
      "Iteration 174, loss = 0.03410774\n",
      "Iteration 175, loss = 0.03393710\n",
      "Iteration 176, loss = 0.04687518\n",
      "Iteration 177, loss = 0.04365744\n",
      "Iteration 178, loss = 0.03826535\n",
      "Iteration 179, loss = 0.03151880\n",
      "Iteration 180, loss = 0.03108811\n",
      "Iteration 181, loss = 0.03278303\n",
      "Iteration 182, loss = 0.03208256\n",
      "Iteration 183, loss = 0.03269768\n",
      "Iteration 184, loss = 0.03165482\n",
      "Iteration 185, loss = 0.03494395\n",
      "Iteration 186, loss = 0.03148295\n",
      "Iteration 187, loss = 0.03290513\n",
      "Iteration 188, loss = 0.03177496\n",
      "Iteration 189, loss = 0.02746924\n",
      "Iteration 190, loss = 0.02830571\n",
      "Iteration 191, loss = 0.02677247\n",
      "Iteration 192, loss = 0.02952164\n",
      "Iteration 193, loss = 0.02794551\n",
      "Iteration 194, loss = 0.02711107\n",
      "Iteration 195, loss = 0.02602297\n",
      "Iteration 196, loss = 0.02618697\n",
      "Iteration 197, loss = 0.02811926\n",
      "Iteration 198, loss = 0.02762829\n",
      "Iteration 199, loss = 0.03062087\n",
      "Iteration 200, loss = 0.02797045\n",
      "Iteration 201, loss = 0.02775260\n",
      "Iteration 202, loss = 0.02696690\n",
      "Iteration 203, loss = 0.02876212\n",
      "Iteration 204, loss = 0.03416899\n",
      "Iteration 205, loss = 0.03811696\n",
      "Iteration 206, loss = 0.03086648\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 37 of 48...\n",
      "Iteration 1, loss = 0.68655482\n",
      "Iteration 2, loss = 0.67467895\n",
      "Iteration 3, loss = 0.65772455\n",
      "Iteration 4, loss = 0.63221027\n",
      "Iteration 5, loss = 0.60243483\n",
      "Iteration 6, loss = 0.57612519\n",
      "Iteration 7, loss = 0.54980635\n",
      "Iteration 8, loss = 0.52523313\n",
      "Iteration 9, loss = 0.50386112\n",
      "Iteration 10, loss = 0.48840405\n",
      "Iteration 11, loss = 0.46997489\n",
      "Iteration 12, loss = 0.45882893\n",
      "Iteration 13, loss = 0.44457771\n",
      "Iteration 14, loss = 0.43450836\n",
      "Iteration 15, loss = 0.42594484\n",
      "Iteration 16, loss = 0.41601734\n",
      "Iteration 17, loss = 0.40859434\n",
      "Iteration 18, loss = 0.40166692\n",
      "Iteration 19, loss = 0.39437122\n",
      "Iteration 20, loss = 0.38974955\n",
      "Iteration 21, loss = 0.38522103\n",
      "Iteration 22, loss = 0.38075761\n",
      "Iteration 23, loss = 0.37442485\n",
      "Iteration 24, loss = 0.37052349\n",
      "Iteration 25, loss = 0.36619253\n",
      "Iteration 26, loss = 0.36328035\n",
      "Iteration 27, loss = 0.35863416\n",
      "Iteration 28, loss = 0.35694596\n",
      "Iteration 29, loss = 0.35468074\n",
      "Iteration 30, loss = 0.35092297\n",
      "Iteration 31, loss = 0.34565255\n",
      "Iteration 32, loss = 0.34619412\n",
      "Iteration 33, loss = 0.34248508\n",
      "Iteration 34, loss = 0.33797649\n",
      "Iteration 35, loss = 0.33578584\n",
      "Iteration 36, loss = 0.33222991\n",
      "Iteration 37, loss = 0.33136541\n",
      "Iteration 38, loss = 0.33257667\n",
      "Iteration 39, loss = 0.32590998\n",
      "Iteration 40, loss = 0.32371761\n",
      "Iteration 41, loss = 0.32269795\n",
      "Iteration 42, loss = 0.31887659\n",
      "Iteration 43, loss = 0.31678002\n",
      "Iteration 44, loss = 0.31546638\n",
      "Iteration 45, loss = 0.31245798\n",
      "Iteration 46, loss = 0.31172696\n",
      "Iteration 47, loss = 0.30827868\n",
      "Iteration 48, loss = 0.30716264\n",
      "Iteration 49, loss = 0.31527435\n",
      "Iteration 50, loss = 0.30881321\n",
      "Iteration 51, loss = 0.30438308\n",
      "Iteration 52, loss = 0.30160971\n",
      "Iteration 53, loss = 0.30650844\n",
      "Iteration 54, loss = 0.30630188\n",
      "Iteration 55, loss = 0.30022442\n",
      "Iteration 56, loss = 0.29340066\n",
      "Iteration 57, loss = 0.29168540\n",
      "Iteration 58, loss = 0.29185508\n",
      "Iteration 59, loss = 0.29319332\n",
      "Iteration 60, loss = 0.29375626\n",
      "Iteration 61, loss = 0.28884548\n",
      "Iteration 62, loss = 0.28356721\n",
      "Iteration 63, loss = 0.28485611\n",
      "Iteration 64, loss = 0.28704198\n",
      "Iteration 65, loss = 0.28673614\n",
      "Iteration 66, loss = 0.28736389\n",
      "Iteration 67, loss = 0.27671104\n",
      "Iteration 68, loss = 0.27644179\n",
      "Iteration 69, loss = 0.27499545\n",
      "Iteration 70, loss = 0.27458991\n",
      "Iteration 71, loss = 0.27505218\n",
      "Iteration 72, loss = 0.27547350\n",
      "Iteration 73, loss = 0.26943328\n",
      "Iteration 74, loss = 0.27084579\n",
      "Iteration 75, loss = 0.26772251\n",
      "Iteration 76, loss = 0.26840124\n",
      "Iteration 77, loss = 0.26865668\n",
      "Iteration 78, loss = 0.26383019\n",
      "Iteration 79, loss = 0.26141227\n",
      "Iteration 80, loss = 0.26468648\n",
      "Iteration 81, loss = 0.26437929\n",
      "Iteration 82, loss = 0.25995202\n",
      "Iteration 83, loss = 0.25650726\n",
      "Iteration 84, loss = 0.25751443\n",
      "Iteration 85, loss = 0.25755651\n",
      "Iteration 86, loss = 0.25630007\n",
      "Iteration 87, loss = 0.25440079\n",
      "Iteration 88, loss = 0.25562648\n",
      "Iteration 89, loss = 0.25319313\n",
      "Iteration 90, loss = 0.25359702\n",
      "Iteration 91, loss = 0.26118004\n",
      "Iteration 92, loss = 0.25265255\n",
      "Iteration 93, loss = 0.24734366\n",
      "Iteration 94, loss = 0.24407451\n",
      "Iteration 95, loss = 0.24297328\n",
      "Iteration 96, loss = 0.24361051\n",
      "Iteration 97, loss = 0.24550836\n",
      "Iteration 98, loss = 0.24283546\n",
      "Iteration 99, loss = 0.24476363\n",
      "Iteration 100, loss = 0.23940046\n",
      "Iteration 101, loss = 0.23764309\n",
      "Iteration 102, loss = 0.23746941\n",
      "Iteration 103, loss = 0.24141044\n",
      "Iteration 104, loss = 0.23814546\n",
      "Iteration 105, loss = 0.23837669\n",
      "Iteration 106, loss = 0.23405533\n",
      "Iteration 107, loss = 0.23425084\n",
      "Iteration 108, loss = 0.23126592\n",
      "Iteration 109, loss = 0.23271129\n",
      "Iteration 110, loss = 0.22994347\n",
      "Iteration 111, loss = 0.22837265\n",
      "Iteration 112, loss = 0.22787615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 113, loss = 0.23458361\n",
      "Iteration 114, loss = 0.22933043\n",
      "Iteration 115, loss = 0.22364971\n",
      "Iteration 116, loss = 0.22527244\n",
      "Iteration 117, loss = 0.22291790\n",
      "Iteration 118, loss = 0.22291411\n",
      "Iteration 119, loss = 0.22074853\n",
      "Iteration 120, loss = 0.22071326\n",
      "Iteration 121, loss = 0.22022817\n",
      "Iteration 122, loss = 0.21860290\n",
      "Iteration 123, loss = 0.22082076\n",
      "Iteration 124, loss = 0.21888750\n",
      "Iteration 125, loss = 0.21527955\n",
      "Iteration 126, loss = 0.22160355\n",
      "Iteration 127, loss = 0.22313428\n",
      "Iteration 128, loss = 0.21705125\n",
      "Iteration 129, loss = 0.21464812\n",
      "Iteration 130, loss = 0.21236930\n",
      "Iteration 131, loss = 0.20937162\n",
      "Iteration 132, loss = 0.21029536\n",
      "Iteration 133, loss = 0.21008475\n",
      "Iteration 134, loss = 0.20881694\n",
      "Iteration 135, loss = 0.21393952\n",
      "Iteration 136, loss = 0.21455788\n",
      "Iteration 137, loss = 0.20630513\n",
      "Iteration 138, loss = 0.20554302\n",
      "Iteration 139, loss = 0.20645919\n",
      "Iteration 140, loss = 0.20281436\n",
      "Iteration 141, loss = 0.20309418\n",
      "Iteration 142, loss = 0.20130152\n",
      "Iteration 143, loss = 0.19827036\n",
      "Iteration 144, loss = 0.20099155\n",
      "Iteration 145, loss = 0.19887087\n",
      "Iteration 146, loss = 0.19716441\n",
      "Iteration 147, loss = 0.19545409\n",
      "Iteration 148, loss = 0.19811206\n",
      "Iteration 149, loss = 0.19655910\n",
      "Iteration 150, loss = 0.19587275\n",
      "Iteration 151, loss = 0.19648280\n",
      "Iteration 152, loss = 0.19186792\n",
      "Iteration 153, loss = 0.18925431\n",
      "Iteration 154, loss = 0.19038568\n",
      "Iteration 155, loss = 0.19045230\n",
      "Iteration 156, loss = 0.18716673\n",
      "Iteration 157, loss = 0.18624761\n",
      "Iteration 158, loss = 0.18680215\n",
      "Iteration 159, loss = 0.18481030\n",
      "Iteration 160, loss = 0.18533739\n",
      "Iteration 161, loss = 0.18340262\n",
      "Iteration 162, loss = 0.18265991\n",
      "Iteration 163, loss = 0.17944250\n",
      "Iteration 164, loss = 0.18102566\n",
      "Iteration 165, loss = 0.18008090\n",
      "Iteration 166, loss = 0.17871429\n",
      "Iteration 167, loss = 0.18088684\n",
      "Iteration 168, loss = 0.17529520\n",
      "Iteration 169, loss = 0.17533635\n",
      "Iteration 170, loss = 0.17333022\n",
      "Iteration 171, loss = 0.17048017\n",
      "Iteration 172, loss = 0.17257370\n",
      "Iteration 173, loss = 0.17410462\n",
      "Iteration 174, loss = 0.17636042\n",
      "Iteration 175, loss = 0.16860158\n",
      "Iteration 176, loss = 0.17161865\n",
      "Iteration 177, loss = 0.16743338\n",
      "Iteration 178, loss = 0.16541671\n",
      "Iteration 179, loss = 0.16137054\n",
      "Iteration 180, loss = 0.16407739\n",
      "Iteration 181, loss = 0.16160632\n",
      "Iteration 182, loss = 0.16452117\n",
      "Iteration 183, loss = 0.16274050\n",
      "Iteration 184, loss = 0.15976716\n",
      "Iteration 185, loss = 0.15991439\n",
      "Iteration 186, loss = 0.15467148\n",
      "Iteration 187, loss = 0.15531743\n",
      "Iteration 188, loss = 0.15299387\n",
      "Iteration 189, loss = 0.15396030\n",
      "Iteration 190, loss = 0.15311166\n",
      "Iteration 191, loss = 0.15502354\n",
      "Iteration 192, loss = 0.15090005\n",
      "Iteration 193, loss = 0.15156004\n",
      "Iteration 194, loss = 0.15084888\n",
      "Iteration 195, loss = 0.15155319\n",
      "Iteration 196, loss = 0.15309239\n",
      "Iteration 197, loss = 0.14753018\n",
      "Iteration 198, loss = 0.14612673\n",
      "Iteration 199, loss = 0.14707286\n",
      "Iteration 200, loss = 0.14454623\n",
      "Iteration 201, loss = 0.15250403\n",
      "Iteration 202, loss = 0.14719274\n",
      "Iteration 203, loss = 0.14442007\n",
      "Iteration 204, loss = 0.13971404\n",
      "Iteration 205, loss = 0.13905768\n",
      "Iteration 206, loss = 0.13913455\n",
      "Iteration 207, loss = 0.14166967\n",
      "Iteration 208, loss = 0.13933200\n",
      "Iteration 209, loss = 0.13556430\n",
      "Iteration 210, loss = 0.13510765\n",
      "Iteration 211, loss = 0.13490243\n",
      "Iteration 212, loss = 0.13286661\n",
      "Iteration 213, loss = 0.13300388\n",
      "Iteration 214, loss = 0.13071838\n",
      "Iteration 215, loss = 0.13044045\n",
      "Iteration 216, loss = 0.12968150\n",
      "Iteration 217, loss = 0.12992772\n",
      "Iteration 218, loss = 0.12930970\n",
      "Iteration 219, loss = 0.12712082\n",
      "Iteration 220, loss = 0.12679378\n",
      "Iteration 221, loss = 0.12853677\n",
      "Iteration 222, loss = 0.12769055\n",
      "Iteration 223, loss = 0.12331674\n",
      "Iteration 224, loss = 0.12316184\n",
      "Iteration 225, loss = 0.12251409\n",
      "Iteration 226, loss = 0.12588492\n",
      "Iteration 227, loss = 0.12158849\n",
      "Iteration 228, loss = 0.12091715\n",
      "Iteration 229, loss = 0.11861237\n",
      "Iteration 230, loss = 0.11754133\n",
      "Iteration 231, loss = 0.12029121\n",
      "Iteration 232, loss = 0.11961305\n",
      "Iteration 233, loss = 0.11764803\n",
      "Iteration 234, loss = 0.11848343\n",
      "Iteration 235, loss = 0.11677516\n",
      "Iteration 236, loss = 0.11665599\n",
      "Iteration 237, loss = 0.11268269\n",
      "Iteration 238, loss = 0.11317227\n",
      "Iteration 239, loss = 0.11475106\n",
      "Iteration 240, loss = 0.11124734\n",
      "Iteration 241, loss = 0.11280338\n",
      "Iteration 242, loss = 0.10854859\n",
      "Iteration 243, loss = 0.10671349\n",
      "Iteration 244, loss = 0.10561205\n",
      "Iteration 245, loss = 0.10561376\n",
      "Iteration 246, loss = 0.10376469\n",
      "Iteration 247, loss = 0.10299603\n",
      "Iteration 248, loss = 0.10168852\n",
      "Iteration 249, loss = 0.10226312\n",
      "Iteration 250, loss = 0.10179484\n",
      "Iteration 251, loss = 0.10111269\n",
      "Iteration 252, loss = 0.10047110\n",
      "Iteration 253, loss = 0.09783770\n",
      "Iteration 254, loss = 0.09737134\n",
      "Iteration 255, loss = 0.10138376\n",
      "Iteration 256, loss = 0.09685187\n",
      "Iteration 257, loss = 0.09830446\n",
      "Iteration 258, loss = 0.09752552\n",
      "Iteration 259, loss = 0.09587523\n",
      "Iteration 260, loss = 0.09433606\n",
      "Iteration 261, loss = 0.09354379\n",
      "Iteration 262, loss = 0.09102065\n",
      "Iteration 263, loss = 0.09253930\n",
      "Iteration 264, loss = 0.09054996\n",
      "Iteration 265, loss = 0.09147525\n",
      "Iteration 266, loss = 0.08982552\n",
      "Iteration 267, loss = 0.08953695\n",
      "Iteration 268, loss = 0.09027887\n",
      "Iteration 269, loss = 0.08495456\n",
      "Iteration 270, loss = 0.08981329\n",
      "Iteration 271, loss = 0.08661224\n",
      "Iteration 272, loss = 0.08380407\n",
      "Iteration 273, loss = 0.08341362\n",
      "Iteration 274, loss = 0.08641844\n",
      "Iteration 275, loss = 0.08942155\n",
      "Iteration 276, loss = 0.08362158\n",
      "Iteration 277, loss = 0.08048810\n",
      "Iteration 278, loss = 0.08356093\n",
      "Iteration 279, loss = 0.08408097\n",
      "Iteration 280, loss = 0.08379262\n",
      "Iteration 281, loss = 0.07931853\n",
      "Iteration 282, loss = 0.07707735\n",
      "Iteration 283, loss = 0.07871466\n",
      "Iteration 284, loss = 0.08013251\n",
      "Iteration 285, loss = 0.07702390\n",
      "Iteration 286, loss = 0.07515804\n",
      "Iteration 287, loss = 0.07405047\n",
      "Iteration 288, loss = 0.07826358\n",
      "Iteration 289, loss = 0.07898877\n",
      "Iteration 290, loss = 0.07407755\n",
      "Iteration 291, loss = 0.07220945\n",
      "Iteration 292, loss = 0.07133485\n",
      "Iteration 293, loss = 0.06990853\n",
      "Iteration 294, loss = 0.06883067\n",
      "Iteration 295, loss = 0.06955408\n",
      "Iteration 296, loss = 0.07044478\n",
      "Iteration 297, loss = 0.06898604\n",
      "Iteration 298, loss = 0.06963521\n",
      "Iteration 299, loss = 0.06883438\n",
      "Iteration 300, loss = 0.06720322\n",
      "Iteration 301, loss = 0.06567423\n",
      "Iteration 302, loss = 0.06695406\n",
      "Iteration 303, loss = 0.06568066\n",
      "Iteration 304, loss = 0.06326138\n",
      "Iteration 305, loss = 0.06413464\n",
      "Iteration 306, loss = 0.06561208\n",
      "Iteration 307, loss = 0.06346385\n",
      "Iteration 308, loss = 0.06881361\n",
      "Iteration 309, loss = 0.06763666\n",
      "Iteration 310, loss = 0.06253893\n",
      "Iteration 311, loss = 0.06182307\n",
      "Iteration 312, loss = 0.06362131\n",
      "Iteration 313, loss = 0.06144829\n",
      "Iteration 314, loss = 0.06159517\n",
      "Iteration 315, loss = 0.06010044\n",
      "Iteration 316, loss = 0.05989175\n",
      "Iteration 317, loss = 0.06201458\n",
      "Iteration 318, loss = 0.05811715\n",
      "Iteration 319, loss = 0.05717219\n",
      "Iteration 320, loss = 0.05554381\n",
      "Iteration 321, loss = 0.05503767\n",
      "Iteration 322, loss = 0.05583821\n",
      "Iteration 323, loss = 0.05528627\n",
      "Iteration 324, loss = 0.05416923\n",
      "Iteration 325, loss = 0.05312280\n",
      "Iteration 326, loss = 0.05314023\n",
      "Iteration 327, loss = 0.05418314\n",
      "Iteration 328, loss = 0.05301120\n",
      "Iteration 329, loss = 0.05242349\n",
      "Iteration 330, loss = 0.05344111\n",
      "Iteration 331, loss = 0.05263509\n",
      "Iteration 332, loss = 0.05068622\n",
      "Iteration 333, loss = 0.05145224\n",
      "Iteration 334, loss = 0.05164956\n",
      "Iteration 335, loss = 0.05611881\n",
      "Iteration 336, loss = 0.04926143\n",
      "Iteration 337, loss = 0.05024206\n",
      "Iteration 338, loss = 0.04831129\n",
      "Iteration 339, loss = 0.04851775\n",
      "Iteration 340, loss = 0.04828581\n",
      "Iteration 341, loss = 0.04828675\n",
      "Iteration 342, loss = 0.04728184\n",
      "Iteration 343, loss = 0.04632078\n",
      "Iteration 344, loss = 0.04603072\n",
      "Iteration 345, loss = 0.04738317\n",
      "Iteration 346, loss = 0.04642434\n",
      "Iteration 347, loss = 0.05118060\n",
      "Iteration 348, loss = 0.05915267\n",
      "Iteration 349, loss = 0.05779357\n",
      "Iteration 350, loss = 0.04594033\n",
      "Iteration 351, loss = 0.04430408\n",
      "Iteration 352, loss = 0.04316904\n",
      "Iteration 353, loss = 0.04218510\n",
      "Iteration 354, loss = 0.04200149\n",
      "Iteration 355, loss = 0.04053184\n",
      "Iteration 356, loss = 0.04034224\n",
      "Iteration 357, loss = 0.04123308\n",
      "Iteration 358, loss = 0.04078271\n",
      "Iteration 359, loss = 0.04290285\n",
      "Iteration 360, loss = 0.04103029\n",
      "Iteration 361, loss = 0.03939368\n",
      "Iteration 362, loss = 0.04101232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 363, loss = 0.03989759\n",
      "Iteration 364, loss = 0.03945839\n",
      "Iteration 365, loss = 0.03964286\n",
      "Iteration 366, loss = 0.03890704\n",
      "Iteration 367, loss = 0.03917636\n",
      "Iteration 368, loss = 0.03759482\n",
      "Iteration 369, loss = 0.03839219\n",
      "Iteration 370, loss = 0.03659439\n",
      "Iteration 371, loss = 0.03891900\n",
      "Iteration 372, loss = 0.03614806\n",
      "Iteration 373, loss = 0.03623051\n",
      "Iteration 374, loss = 0.03490273\n",
      "Iteration 375, loss = 0.03498784\n",
      "Iteration 376, loss = 0.03660673\n",
      "Iteration 377, loss = 0.03479766\n",
      "Iteration 378, loss = 0.03410043\n",
      "Iteration 379, loss = 0.03373762\n",
      "Iteration 380, loss = 0.03315850\n",
      "Iteration 381, loss = 0.03330629\n",
      "Iteration 382, loss = 0.03274550\n",
      "Iteration 383, loss = 0.03429444\n",
      "Iteration 384, loss = 0.03276942\n",
      "Iteration 385, loss = 0.03260943\n",
      "Iteration 386, loss = 0.03141879\n",
      "Iteration 387, loss = 0.03266752\n",
      "Iteration 388, loss = 0.03462488\n",
      "Iteration 389, loss = 0.03446109\n",
      "Iteration 390, loss = 0.03166886\n",
      "Iteration 391, loss = 0.03031934\n",
      "Iteration 392, loss = 0.03184987\n",
      "Iteration 393, loss = 0.03253252\n",
      "Iteration 394, loss = 0.03125713\n",
      "Iteration 395, loss = 0.03112222\n",
      "Iteration 396, loss = 0.03011749\n",
      "Iteration 397, loss = 0.03049827\n",
      "Iteration 398, loss = 0.02882351\n",
      "Iteration 399, loss = 0.02805125\n",
      "Iteration 400, loss = 0.02866710\n",
      "Iteration 401, loss = 0.02959425\n",
      "Iteration 402, loss = 0.03001021\n",
      "Iteration 403, loss = 0.03088082\n",
      "Iteration 404, loss = 0.02847676\n",
      "Iteration 405, loss = 0.02747747\n",
      "Iteration 406, loss = 0.02672109\n",
      "Iteration 407, loss = 0.02675039\n",
      "Iteration 408, loss = 0.02854627\n",
      "Iteration 409, loss = 0.02868106\n",
      "Iteration 410, loss = 0.02692307\n",
      "Iteration 411, loss = 0.02662166\n",
      "Iteration 412, loss = 0.02633597\n",
      "Iteration 413, loss = 0.02645213\n",
      "Iteration 414, loss = 0.02618813\n",
      "Iteration 415, loss = 0.02729583\n",
      "Iteration 416, loss = 0.02608940\n",
      "Iteration 417, loss = 0.02549619\n",
      "Iteration 418, loss = 0.02492847\n",
      "Iteration 419, loss = 0.02931269\n",
      "Iteration 420, loss = 0.02543904\n",
      "Iteration 421, loss = 0.02586126\n",
      "Iteration 422, loss = 0.02346837\n",
      "Iteration 423, loss = 0.02439729\n",
      "Iteration 424, loss = 0.02457046\n",
      "Iteration 425, loss = 0.02347273\n",
      "Iteration 426, loss = 0.02324038\n",
      "Iteration 427, loss = 0.02332550\n",
      "Iteration 428, loss = 0.02333219\n",
      "Iteration 429, loss = 0.02569482\n",
      "Iteration 430, loss = 0.02175029\n",
      "Iteration 431, loss = 0.02135670\n",
      "Iteration 432, loss = 0.02250373\n",
      "Iteration 433, loss = 0.02066766\n",
      "Iteration 434, loss = 0.02108310\n",
      "Iteration 435, loss = 0.02151054\n",
      "Iteration 436, loss = 0.02240688\n",
      "Iteration 437, loss = 0.02090232\n",
      "Iteration 438, loss = 0.02041250\n",
      "Iteration 439, loss = 0.01980672\n",
      "Iteration 440, loss = 0.02007600\n",
      "Iteration 441, loss = 0.01930522\n",
      "Iteration 442, loss = 0.01870281\n",
      "Iteration 443, loss = 0.02023756\n",
      "Iteration 444, loss = 0.01961641\n",
      "Iteration 445, loss = 0.02012891\n",
      "Iteration 446, loss = 0.01985951\n",
      "Iteration 447, loss = 0.02008922\n",
      "Iteration 448, loss = 0.01862470\n",
      "Iteration 449, loss = 0.02055227\n",
      "Iteration 450, loss = 0.01944657\n",
      "Iteration 451, loss = 0.01919287\n",
      "Iteration 452, loss = 0.01807783\n",
      "Iteration 453, loss = 0.01750526\n",
      "Iteration 454, loss = 0.02220696\n",
      "Iteration 455, loss = 0.02341516\n",
      "Iteration 456, loss = 0.02733826\n",
      "Iteration 457, loss = 0.01979630\n",
      "Iteration 458, loss = 0.03053317\n",
      "Iteration 459, loss = 0.06036317\n",
      "Iteration 460, loss = 0.08098072\n",
      "Iteration 461, loss = 0.07065329\n",
      "Iteration 462, loss = 0.04690383\n",
      "Iteration 463, loss = 0.03166833\n",
      "Iteration 464, loss = 0.02420880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 38 of 48...\n",
      "Iteration 1, loss = 0.70984912\n",
      "Iteration 2, loss = 0.68651202\n",
      "Iteration 3, loss = 0.68333928\n",
      "Iteration 4, loss = 0.67574958\n",
      "Iteration 5, loss = 0.66283185\n",
      "Iteration 6, loss = 0.64512112\n",
      "Iteration 7, loss = 0.62158072\n",
      "Iteration 8, loss = 0.59539985\n",
      "Iteration 9, loss = 0.56914696\n",
      "Iteration 10, loss = 0.54220147\n",
      "Iteration 11, loss = 0.51870504\n",
      "Iteration 12, loss = 0.49900119\n",
      "Iteration 13, loss = 0.48115939\n",
      "Iteration 14, loss = 0.46546639\n",
      "Iteration 15, loss = 0.45100306\n",
      "Iteration 16, loss = 0.43918015\n",
      "Iteration 17, loss = 0.42888025\n",
      "Iteration 18, loss = 0.41990520\n",
      "Iteration 19, loss = 0.41211874\n",
      "Iteration 20, loss = 0.40454135\n",
      "Iteration 21, loss = 0.39888706\n",
      "Iteration 22, loss = 0.39282623\n",
      "Iteration 23, loss = 0.38726405\n",
      "Iteration 24, loss = 0.38098633\n",
      "Iteration 25, loss = 0.37723686\n",
      "Iteration 26, loss = 0.37468932\n",
      "Iteration 27, loss = 0.36959696\n",
      "Iteration 28, loss = 0.37271694\n",
      "Iteration 29, loss = 0.36242732\n",
      "Iteration 30, loss = 0.36122058\n",
      "Iteration 31, loss = 0.35957487\n",
      "Iteration 32, loss = 0.35767583\n",
      "Iteration 33, loss = 0.35322006\n",
      "Iteration 34, loss = 0.34530573\n",
      "Iteration 35, loss = 0.34224708\n",
      "Iteration 36, loss = 0.34041635\n",
      "Iteration 37, loss = 0.33850004\n",
      "Iteration 38, loss = 0.33700383\n",
      "Iteration 39, loss = 0.33262543\n",
      "Iteration 40, loss = 0.33283815\n",
      "Iteration 41, loss = 0.32750303\n",
      "Iteration 42, loss = 0.32802592\n",
      "Iteration 43, loss = 0.32471448\n",
      "Iteration 44, loss = 0.32498122\n",
      "Iteration 45, loss = 0.32079123\n",
      "Iteration 46, loss = 0.31962505\n",
      "Iteration 47, loss = 0.31649753\n",
      "Iteration 48, loss = 0.31545299\n",
      "Iteration 49, loss = 0.31436029\n",
      "Iteration 50, loss = 0.31335405\n",
      "Iteration 51, loss = 0.31015936\n",
      "Iteration 52, loss = 0.30657936\n",
      "Iteration 53, loss = 0.30511934\n",
      "Iteration 54, loss = 0.30245988\n",
      "Iteration 55, loss = 0.30102120\n",
      "Iteration 56, loss = 0.29829569\n",
      "Iteration 57, loss = 0.29847103\n",
      "Iteration 58, loss = 0.29619494\n",
      "Iteration 59, loss = 0.29829288\n",
      "Iteration 60, loss = 0.29883547\n",
      "Iteration 61, loss = 0.29408020\n",
      "Iteration 62, loss = 0.29071472\n",
      "Iteration 63, loss = 0.28880552\n",
      "Iteration 64, loss = 0.28969032\n",
      "Iteration 65, loss = 0.28503240\n",
      "Iteration 66, loss = 0.28264213\n",
      "Iteration 67, loss = 0.28232435\n",
      "Iteration 68, loss = 0.28062670\n",
      "Iteration 69, loss = 0.27746212\n",
      "Iteration 70, loss = 0.27539016\n",
      "Iteration 71, loss = 0.27439110\n",
      "Iteration 72, loss = 0.27331595\n",
      "Iteration 73, loss = 0.27276746\n",
      "Iteration 74, loss = 0.27094576\n",
      "Iteration 75, loss = 0.26756406\n",
      "Iteration 76, loss = 0.26863041\n",
      "Iteration 77, loss = 0.26402688\n",
      "Iteration 78, loss = 0.26422499\n",
      "Iteration 79, loss = 0.26233731\n",
      "Iteration 80, loss = 0.26083812\n",
      "Iteration 81, loss = 0.25939491\n",
      "Iteration 82, loss = 0.26229392\n",
      "Iteration 83, loss = 0.26508112\n",
      "Iteration 84, loss = 0.26064901\n",
      "Iteration 85, loss = 0.25503133\n",
      "Iteration 86, loss = 0.25276274\n",
      "Iteration 87, loss = 0.25152988\n",
      "Iteration 88, loss = 0.25094527\n",
      "Iteration 89, loss = 0.24758881\n",
      "Iteration 90, loss = 0.24623083\n",
      "Iteration 91, loss = 0.24648065\n",
      "Iteration 92, loss = 0.24421882\n",
      "Iteration 93, loss = 0.24519421\n",
      "Iteration 94, loss = 0.24400776\n",
      "Iteration 95, loss = 0.24076604\n",
      "Iteration 96, loss = 0.23833099\n",
      "Iteration 97, loss = 0.23845409\n",
      "Iteration 98, loss = 0.23849133\n",
      "Iteration 99, loss = 0.23781784\n",
      "Iteration 100, loss = 0.24250295\n",
      "Iteration 101, loss = 0.23542391\n",
      "Iteration 102, loss = 0.23237351\n",
      "Iteration 103, loss = 0.22976037\n",
      "Iteration 104, loss = 0.23039918\n",
      "Iteration 105, loss = 0.22972040\n",
      "Iteration 106, loss = 0.23004299\n",
      "Iteration 107, loss = 0.22790894\n",
      "Iteration 108, loss = 0.22785435\n",
      "Iteration 109, loss = 0.24082897\n",
      "Iteration 110, loss = 0.23763079\n",
      "Iteration 111, loss = 0.22398848\n",
      "Iteration 112, loss = 0.21992483\n",
      "Iteration 113, loss = 0.21828152\n",
      "Iteration 114, loss = 0.22026237\n",
      "Iteration 115, loss = 0.22614492\n",
      "Iteration 116, loss = 0.21507267\n",
      "Iteration 117, loss = 0.21152772\n",
      "Iteration 118, loss = 0.21323133\n",
      "Iteration 119, loss = 0.21151601\n",
      "Iteration 120, loss = 0.21690406\n",
      "Iteration 121, loss = 0.21468213\n",
      "Iteration 122, loss = 0.21371694\n",
      "Iteration 123, loss = 0.20563318\n",
      "Iteration 124, loss = 0.20435727\n",
      "Iteration 125, loss = 0.20399810\n",
      "Iteration 126, loss = 0.20370654\n",
      "Iteration 127, loss = 0.20059106\n",
      "Iteration 128, loss = 0.20201632\n",
      "Iteration 129, loss = 0.20158205\n",
      "Iteration 130, loss = 0.19881932\n",
      "Iteration 131, loss = 0.19610037\n",
      "Iteration 132, loss = 0.20457051\n",
      "Iteration 133, loss = 0.19999837\n",
      "Iteration 134, loss = 0.19790381\n",
      "Iteration 135, loss = 0.19501725\n",
      "Iteration 136, loss = 0.19323508\n",
      "Iteration 137, loss = 0.19064748\n",
      "Iteration 138, loss = 0.19173440\n",
      "Iteration 139, loss = 0.18787267\n",
      "Iteration 140, loss = 0.18670598\n",
      "Iteration 141, loss = 0.18480674\n",
      "Iteration 142, loss = 0.18508207\n",
      "Iteration 143, loss = 0.18519319\n",
      "Iteration 144, loss = 0.18260702\n",
      "Iteration 145, loss = 0.18319705\n",
      "Iteration 146, loss = 0.17994394\n",
      "Iteration 147, loss = 0.17809577\n",
      "Iteration 148, loss = 0.17700590\n",
      "Iteration 149, loss = 0.17725667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 150, loss = 0.18060741\n",
      "Iteration 151, loss = 0.17399443\n",
      "Iteration 152, loss = 0.17446115\n",
      "Iteration 153, loss = 0.17414655\n",
      "Iteration 154, loss = 0.17142323\n",
      "Iteration 155, loss = 0.16977368\n",
      "Iteration 156, loss = 0.17064682\n",
      "Iteration 157, loss = 0.16928384\n",
      "Iteration 158, loss = 0.16697307\n",
      "Iteration 159, loss = 0.16614047\n",
      "Iteration 160, loss = 0.16545299\n",
      "Iteration 161, loss = 0.16580938\n",
      "Iteration 162, loss = 0.16362250\n",
      "Iteration 163, loss = 0.16281259\n",
      "Iteration 164, loss = 0.16086019\n",
      "Iteration 165, loss = 0.15874714\n",
      "Iteration 166, loss = 0.15747320\n",
      "Iteration 167, loss = 0.15884105\n",
      "Iteration 168, loss = 0.15568705\n",
      "Iteration 169, loss = 0.15407153\n",
      "Iteration 170, loss = 0.15427522\n",
      "Iteration 171, loss = 0.15357550\n",
      "Iteration 172, loss = 0.15428282\n",
      "Iteration 173, loss = 0.15395232\n",
      "Iteration 174, loss = 0.15184323\n",
      "Iteration 175, loss = 0.15504072\n",
      "Iteration 176, loss = 0.15057583\n",
      "Iteration 177, loss = 0.15193507\n",
      "Iteration 178, loss = 0.14434405\n",
      "Iteration 179, loss = 0.14794176\n",
      "Iteration 180, loss = 0.14520459\n",
      "Iteration 181, loss = 0.14271664\n",
      "Iteration 182, loss = 0.14144393\n",
      "Iteration 183, loss = 0.14015731\n",
      "Iteration 184, loss = 0.14277182\n",
      "Iteration 185, loss = 0.14201903\n",
      "Iteration 186, loss = 0.13765470\n",
      "Iteration 187, loss = 0.13800881\n",
      "Iteration 188, loss = 0.14015314\n",
      "Iteration 189, loss = 0.13691378\n",
      "Iteration 190, loss = 0.13686371\n",
      "Iteration 191, loss = 0.13433586\n",
      "Iteration 192, loss = 0.13033822\n",
      "Iteration 193, loss = 0.13092174\n",
      "Iteration 194, loss = 0.12855489\n",
      "Iteration 195, loss = 0.12751824\n",
      "Iteration 196, loss = 0.12673451\n",
      "Iteration 197, loss = 0.12729022\n",
      "Iteration 198, loss = 0.12989522\n",
      "Iteration 199, loss = 0.12516883\n",
      "Iteration 200, loss = 0.12535411\n",
      "Iteration 201, loss = 0.12370216\n",
      "Iteration 202, loss = 0.12351742\n",
      "Iteration 203, loss = 0.12991623\n",
      "Iteration 204, loss = 0.12176545\n",
      "Iteration 205, loss = 0.11906073\n",
      "Iteration 206, loss = 0.12555008\n",
      "Iteration 207, loss = 0.12062989\n",
      "Iteration 208, loss = 0.11611552\n",
      "Iteration 209, loss = 0.11687153\n",
      "Iteration 210, loss = 0.11494174\n",
      "Iteration 211, loss = 0.11503222\n",
      "Iteration 212, loss = 0.11599857\n",
      "Iteration 213, loss = 0.11377357\n",
      "Iteration 214, loss = 0.11733997\n",
      "Iteration 215, loss = 0.11284781\n",
      "Iteration 216, loss = 0.11624519\n",
      "Iteration 217, loss = 0.11331367\n",
      "Iteration 218, loss = 0.11027993\n",
      "Iteration 219, loss = 0.10727821\n",
      "Iteration 220, loss = 0.11025520\n",
      "Iteration 221, loss = 0.11101072\n",
      "Iteration 222, loss = 0.10537258\n",
      "Iteration 223, loss = 0.10646292\n",
      "Iteration 224, loss = 0.10544628\n",
      "Iteration 225, loss = 0.10363989\n",
      "Iteration 226, loss = 0.10501008\n",
      "Iteration 227, loss = 0.10485771\n",
      "Iteration 228, loss = 0.10018772\n",
      "Iteration 229, loss = 0.10045732\n",
      "Iteration 230, loss = 0.10328482\n",
      "Iteration 231, loss = 0.10151841\n",
      "Iteration 232, loss = 0.10016340\n",
      "Iteration 233, loss = 0.10155780\n",
      "Iteration 234, loss = 0.10449743\n",
      "Iteration 235, loss = 0.10444835\n",
      "Iteration 236, loss = 0.10255412\n",
      "Iteration 237, loss = 0.09988358\n",
      "Iteration 238, loss = 0.10625726\n",
      "Iteration 239, loss = 0.10575425\n",
      "Iteration 240, loss = 0.10423368\n",
      "Iteration 241, loss = 0.09890777\n",
      "Iteration 242, loss = 0.09806912\n",
      "Iteration 243, loss = 0.09762597\n",
      "Iteration 244, loss = 0.09474177\n",
      "Iteration 245, loss = 0.09271349\n",
      "Iteration 246, loss = 0.08916975\n",
      "Iteration 247, loss = 0.08775777\n",
      "Iteration 248, loss = 0.08781349\n",
      "Iteration 249, loss = 0.08706570\n",
      "Iteration 250, loss = 0.08889397\n",
      "Iteration 251, loss = 0.08740195\n",
      "Iteration 252, loss = 0.08892195\n",
      "Iteration 253, loss = 0.08907191\n",
      "Iteration 254, loss = 0.08501199\n",
      "Iteration 255, loss = 0.08806035\n",
      "Iteration 256, loss = 0.09243867\n",
      "Iteration 257, loss = 0.09540769\n",
      "Iteration 258, loss = 0.08973776\n",
      "Iteration 259, loss = 0.08297185\n",
      "Iteration 260, loss = 0.08045565\n",
      "Iteration 261, loss = 0.07943673\n",
      "Iteration 262, loss = 0.07988385\n",
      "Iteration 263, loss = 0.07929574\n",
      "Iteration 264, loss = 0.08015461\n",
      "Iteration 265, loss = 0.07764242\n",
      "Iteration 266, loss = 0.07983476\n",
      "Iteration 267, loss = 0.08026492\n",
      "Iteration 268, loss = 0.07744840\n",
      "Iteration 269, loss = 0.07530794\n",
      "Iteration 270, loss = 0.07530791\n",
      "Iteration 271, loss = 0.07621156\n",
      "Iteration 272, loss = 0.07598998\n",
      "Iteration 273, loss = 0.07718618\n",
      "Iteration 274, loss = 0.08163261\n",
      "Iteration 275, loss = 0.07516414\n",
      "Iteration 276, loss = 0.07155645\n",
      "Iteration 277, loss = 0.07254774\n",
      "Iteration 278, loss = 0.07189107\n",
      "Iteration 279, loss = 0.07155645\n",
      "Iteration 280, loss = 0.07051442\n",
      "Iteration 281, loss = 0.07125269\n",
      "Iteration 282, loss = 0.06938915\n",
      "Iteration 283, loss = 0.06881893\n",
      "Iteration 284, loss = 0.06820496\n",
      "Iteration 285, loss = 0.06832749\n",
      "Iteration 286, loss = 0.07116420\n",
      "Iteration 287, loss = 0.07212154\n",
      "Iteration 288, loss = 0.06743923\n",
      "Iteration 289, loss = 0.06639963\n",
      "Iteration 290, loss = 0.06689681\n",
      "Iteration 291, loss = 0.06565706\n",
      "Iteration 292, loss = 0.06757273\n",
      "Iteration 293, loss = 0.06946135\n",
      "Iteration 294, loss = 0.06762392\n",
      "Iteration 295, loss = 0.06425508\n",
      "Iteration 296, loss = 0.06349457\n",
      "Iteration 297, loss = 0.06318408\n",
      "Iteration 298, loss = 0.06151855\n",
      "Iteration 299, loss = 0.06478523\n",
      "Iteration 300, loss = 0.06136739\n",
      "Iteration 301, loss = 0.06195813\n",
      "Iteration 302, loss = 0.06184387\n",
      "Iteration 303, loss = 0.06310370\n",
      "Iteration 304, loss = 0.06381997\n",
      "Iteration 305, loss = 0.06151651\n",
      "Iteration 306, loss = 0.06025843\n",
      "Iteration 307, loss = 0.05985118\n",
      "Iteration 308, loss = 0.05982990\n",
      "Iteration 309, loss = 0.05688670\n",
      "Iteration 310, loss = 0.05847855\n",
      "Iteration 311, loss = 0.05761396\n",
      "Iteration 312, loss = 0.05369303\n",
      "Iteration 313, loss = 0.05244865\n",
      "Iteration 314, loss = 0.05371032\n",
      "Iteration 315, loss = 0.05215479\n",
      "Iteration 316, loss = 0.05246716\n",
      "Iteration 317, loss = 0.05212169\n",
      "Iteration 318, loss = 0.05126932\n",
      "Iteration 319, loss = 0.04967314\n",
      "Iteration 320, loss = 0.05020041\n",
      "Iteration 321, loss = 0.05122611\n",
      "Iteration 322, loss = 0.05104632\n",
      "Iteration 323, loss = 0.04806567\n",
      "Iteration 324, loss = 0.04964002\n",
      "Iteration 325, loss = 0.04881387\n",
      "Iteration 326, loss = 0.04843305\n",
      "Iteration 327, loss = 0.04664685\n",
      "Iteration 328, loss = 0.04825303\n",
      "Iteration 329, loss = 0.05193150\n",
      "Iteration 330, loss = 0.04783320\n",
      "Iteration 331, loss = 0.04694375\n",
      "Iteration 332, loss = 0.04761602\n",
      "Iteration 333, loss = 0.04609424\n",
      "Iteration 334, loss = 0.04413797\n",
      "Iteration 335, loss = 0.04500044\n",
      "Iteration 336, loss = 0.04580019\n",
      "Iteration 337, loss = 0.04410850\n",
      "Iteration 338, loss = 0.04323405\n",
      "Iteration 339, loss = 0.04406876\n",
      "Iteration 340, loss = 0.04212514\n",
      "Iteration 341, loss = 0.04354938\n",
      "Iteration 342, loss = 0.04309813\n",
      "Iteration 343, loss = 0.04166557\n",
      "Iteration 344, loss = 0.04222215\n",
      "Iteration 345, loss = 0.04107800\n",
      "Iteration 346, loss = 0.04096847\n",
      "Iteration 347, loss = 0.04091375\n",
      "Iteration 348, loss = 0.04060407\n",
      "Iteration 349, loss = 0.04093674\n",
      "Iteration 350, loss = 0.03982083\n",
      "Iteration 351, loss = 0.03987911\n",
      "Iteration 352, loss = 0.04147810\n",
      "Iteration 353, loss = 0.04130582\n",
      "Iteration 354, loss = 0.03871000\n",
      "Iteration 355, loss = 0.03971942\n",
      "Iteration 356, loss = 0.03854032\n",
      "Iteration 357, loss = 0.03953524\n",
      "Iteration 358, loss = 0.03851072\n",
      "Iteration 359, loss = 0.03960201\n",
      "Iteration 360, loss = 0.03755124\n",
      "Iteration 361, loss = 0.03742282\n",
      "Iteration 362, loss = 0.03653614\n",
      "Iteration 363, loss = 0.03875103\n",
      "Iteration 364, loss = 0.03713279\n",
      "Iteration 365, loss = 0.03762451\n",
      "Iteration 366, loss = 0.04002661\n",
      "Iteration 367, loss = 0.03863374\n",
      "Iteration 368, loss = 0.03644093\n",
      "Iteration 369, loss = 0.03831820\n",
      "Iteration 370, loss = 0.03621017\n",
      "Iteration 371, loss = 0.03547658\n",
      "Iteration 372, loss = 0.03663026\n",
      "Iteration 373, loss = 0.03643399\n",
      "Iteration 374, loss = 0.03416529\n",
      "Iteration 375, loss = 0.03352719\n",
      "Iteration 376, loss = 0.03316195\n",
      "Iteration 377, loss = 0.03422897\n",
      "Iteration 378, loss = 0.03863571\n",
      "Iteration 379, loss = 0.03595815\n",
      "Iteration 380, loss = 0.03362340\n",
      "Iteration 381, loss = 0.03353762\n",
      "Iteration 382, loss = 0.03174041\n",
      "Iteration 383, loss = 0.03188114\n",
      "Iteration 384, loss = 0.03244790\n",
      "Iteration 385, loss = 0.03172121\n",
      "Iteration 386, loss = 0.03220717\n",
      "Iteration 387, loss = 0.03216972\n",
      "Iteration 388, loss = 0.03077226\n",
      "Iteration 389, loss = 0.03056547\n",
      "Iteration 390, loss = 0.03075100\n",
      "Iteration 391, loss = 0.03279270\n",
      "Iteration 392, loss = 0.03106145\n",
      "Iteration 393, loss = 0.03139550\n",
      "Iteration 394, loss = 0.03134113\n",
      "Iteration 395, loss = 0.02868172\n",
      "Iteration 396, loss = 0.02868294\n",
      "Iteration 397, loss = 0.02941639\n",
      "Iteration 398, loss = 0.02955648\n",
      "Iteration 399, loss = 0.02950147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400, loss = 0.03077082\n",
      "Iteration 401, loss = 0.02836395\n",
      "Iteration 402, loss = 0.02768968\n",
      "Iteration 403, loss = 0.03006227\n",
      "Iteration 404, loss = 0.02760006\n",
      "Iteration 405, loss = 0.02824667\n",
      "Iteration 406, loss = 0.02718160\n",
      "Iteration 407, loss = 0.02729185\n",
      "Iteration 408, loss = 0.02708606\n",
      "Iteration 409, loss = 0.02628577\n",
      "Iteration 410, loss = 0.02845109\n",
      "Iteration 411, loss = 0.02964489\n",
      "Iteration 412, loss = 0.02790744\n",
      "Iteration 413, loss = 0.02715345\n",
      "Iteration 414, loss = 0.02745007\n",
      "Iteration 415, loss = 0.03378767\n",
      "Iteration 416, loss = 0.02686837\n",
      "Iteration 417, loss = 0.02925585\n",
      "Iteration 418, loss = 0.03770948\n",
      "Iteration 419, loss = 0.03188283\n",
      "Iteration 420, loss = 0.03126940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 39 of 48...\n",
      "Iteration 1, loss = 0.68761359\n",
      "Iteration 2, loss = 0.67760564\n",
      "Iteration 3, loss = 0.66004200\n",
      "Iteration 4, loss = 0.63499905\n",
      "Iteration 5, loss = 0.60808546\n",
      "Iteration 6, loss = 0.57990389\n",
      "Iteration 7, loss = 0.55395262\n",
      "Iteration 8, loss = 0.53155485\n",
      "Iteration 9, loss = 0.51117707\n",
      "Iteration 10, loss = 0.49176185\n",
      "Iteration 11, loss = 0.47588487\n",
      "Iteration 12, loss = 0.46327977\n",
      "Iteration 13, loss = 0.45101363\n",
      "Iteration 14, loss = 0.44233014\n",
      "Iteration 15, loss = 0.43225384\n",
      "Iteration 16, loss = 0.42389363\n",
      "Iteration 17, loss = 0.41587198\n",
      "Iteration 18, loss = 0.40957640\n",
      "Iteration 19, loss = 0.40446051\n",
      "Iteration 20, loss = 0.39885905\n",
      "Iteration 21, loss = 0.39462462\n",
      "Iteration 22, loss = 0.38845939\n",
      "Iteration 23, loss = 0.38396838\n",
      "Iteration 24, loss = 0.38198116\n",
      "Iteration 25, loss = 0.37757500\n",
      "Iteration 26, loss = 0.37425448\n",
      "Iteration 27, loss = 0.36982147\n",
      "Iteration 28, loss = 0.36717198\n",
      "Iteration 29, loss = 0.36542572\n",
      "Iteration 30, loss = 0.36185230\n",
      "Iteration 31, loss = 0.36009651\n",
      "Iteration 32, loss = 0.35710857\n",
      "Iteration 33, loss = 0.35349750\n",
      "Iteration 34, loss = 0.35303608\n",
      "Iteration 35, loss = 0.35108199\n",
      "Iteration 36, loss = 0.34710401\n",
      "Iteration 37, loss = 0.34509585\n",
      "Iteration 38, loss = 0.34289342\n",
      "Iteration 39, loss = 0.34395806\n",
      "Iteration 40, loss = 0.34295199\n",
      "Iteration 41, loss = 0.33982146\n",
      "Iteration 42, loss = 0.33686365\n",
      "Iteration 43, loss = 0.33411082\n",
      "Iteration 44, loss = 0.33201632\n",
      "Iteration 45, loss = 0.33129653\n",
      "Iteration 46, loss = 0.32692763\n",
      "Iteration 47, loss = 0.32713812\n",
      "Iteration 48, loss = 0.32458989\n",
      "Iteration 49, loss = 0.32340538\n",
      "Iteration 50, loss = 0.32457259\n",
      "Iteration 51, loss = 0.32146178\n",
      "Iteration 52, loss = 0.31954032\n",
      "Iteration 53, loss = 0.32212020\n",
      "Iteration 54, loss = 0.32094636\n",
      "Iteration 55, loss = 0.31480573\n",
      "Iteration 56, loss = 0.31349358\n",
      "Iteration 57, loss = 0.31476578\n",
      "Iteration 58, loss = 0.31781870\n",
      "Iteration 59, loss = 0.31450417\n",
      "Iteration 60, loss = 0.31104490\n",
      "Iteration 61, loss = 0.30824400\n",
      "Iteration 62, loss = 0.30754540\n",
      "Iteration 63, loss = 0.30451196\n",
      "Iteration 64, loss = 0.30466238\n",
      "Iteration 65, loss = 0.30949104\n",
      "Iteration 66, loss = 0.30517356\n",
      "Iteration 67, loss = 0.30077815\n",
      "Iteration 68, loss = 0.30038254\n",
      "Iteration 69, loss = 0.29904594\n",
      "Iteration 70, loss = 0.30327280\n",
      "Iteration 71, loss = 0.29953947\n",
      "Iteration 72, loss = 0.29534318\n",
      "Iteration 73, loss = 0.29538996\n",
      "Iteration 74, loss = 0.29475718\n",
      "Iteration 75, loss = 0.29172394\n",
      "Iteration 76, loss = 0.29469854\n",
      "Iteration 77, loss = 0.28889709\n",
      "Iteration 78, loss = 0.29130812\n",
      "Iteration 79, loss = 0.29313565\n",
      "Iteration 80, loss = 0.29172896\n",
      "Iteration 81, loss = 0.28598220\n",
      "Iteration 82, loss = 0.28570025\n",
      "Iteration 83, loss = 0.28509121\n",
      "Iteration 84, loss = 0.28995954\n",
      "Iteration 85, loss = 0.29136447\n",
      "Iteration 86, loss = 0.29210413\n",
      "Iteration 87, loss = 0.28588900\n",
      "Iteration 88, loss = 0.28191666\n",
      "Iteration 89, loss = 0.27828113\n",
      "Iteration 90, loss = 0.27978735\n",
      "Iteration 91, loss = 0.27930676\n",
      "Iteration 92, loss = 0.28023978\n",
      "Iteration 93, loss = 0.27585194\n",
      "Iteration 94, loss = 0.27864810\n",
      "Iteration 95, loss = 0.27398059\n",
      "Iteration 96, loss = 0.27761502\n",
      "Iteration 97, loss = 0.27630402\n",
      "Iteration 98, loss = 0.27798833\n",
      "Iteration 99, loss = 0.28529623\n",
      "Iteration 100, loss = 0.28307485\n",
      "Iteration 101, loss = 0.28000486\n",
      "Iteration 102, loss = 0.27718987\n",
      "Iteration 103, loss = 0.27109080\n",
      "Iteration 104, loss = 0.27457421\n",
      "Iteration 105, loss = 0.27019270\n",
      "Iteration 106, loss = 0.26912810\n",
      "Iteration 107, loss = 0.26703437\n",
      "Iteration 108, loss = 0.26786897\n",
      "Iteration 109, loss = 0.26825279\n",
      "Iteration 110, loss = 0.27201251\n",
      "Iteration 111, loss = 0.26884697\n",
      "Iteration 112, loss = 0.26494857\n",
      "Iteration 113, loss = 0.26295380\n",
      "Iteration 114, loss = 0.26691538\n",
      "Iteration 115, loss = 0.26198809\n",
      "Iteration 116, loss = 0.26069835\n",
      "Iteration 117, loss = 0.26084063\n",
      "Iteration 118, loss = 0.26108222\n",
      "Iteration 119, loss = 0.26731626\n",
      "Iteration 120, loss = 0.25956537\n",
      "Iteration 121, loss = 0.25944111\n",
      "Iteration 122, loss = 0.25758347\n",
      "Iteration 123, loss = 0.25676975\n",
      "Iteration 124, loss = 0.25805793\n",
      "Iteration 125, loss = 0.25462353\n",
      "Iteration 126, loss = 0.25472237\n",
      "Iteration 127, loss = 0.25442390\n",
      "Iteration 128, loss = 0.25424429\n",
      "Iteration 129, loss = 0.25431666\n",
      "Iteration 130, loss = 0.25437727\n",
      "Iteration 131, loss = 0.25666197\n",
      "Iteration 132, loss = 0.25724395\n",
      "Iteration 133, loss = 0.25425613\n",
      "Iteration 134, loss = 0.25279379\n",
      "Iteration 135, loss = 0.25151852\n",
      "Iteration 136, loss = 0.25310893\n",
      "Iteration 137, loss = 0.25169215\n",
      "Iteration 138, loss = 0.25390749\n",
      "Iteration 139, loss = 0.25078426\n",
      "Iteration 140, loss = 0.25017544\n",
      "Iteration 141, loss = 0.24850935\n",
      "Iteration 142, loss = 0.25054060\n",
      "Iteration 143, loss = 0.24968474\n",
      "Iteration 144, loss = 0.25056243\n",
      "Iteration 145, loss = 0.25251721\n",
      "Iteration 146, loss = 0.24935715\n",
      "Iteration 147, loss = 0.25096822\n",
      "Iteration 148, loss = 0.24821675\n",
      "Iteration 149, loss = 0.25845894\n",
      "Iteration 150, loss = 0.24917979\n",
      "Iteration 151, loss = 0.24408218\n",
      "Iteration 152, loss = 0.24285803\n",
      "Iteration 153, loss = 0.24482543\n",
      "Iteration 154, loss = 0.24023074\n",
      "Iteration 155, loss = 0.24002047\n",
      "Iteration 156, loss = 0.24196558\n",
      "Iteration 157, loss = 0.23974222\n",
      "Iteration 158, loss = 0.24441116\n",
      "Iteration 159, loss = 0.24211276\n",
      "Iteration 160, loss = 0.23908880\n",
      "Iteration 161, loss = 0.23810589\n",
      "Iteration 162, loss = 0.23678227\n",
      "Iteration 163, loss = 0.24470914\n",
      "Iteration 164, loss = 0.24638526\n",
      "Iteration 165, loss = 0.24191401\n",
      "Iteration 166, loss = 0.24033813\n",
      "Iteration 167, loss = 0.23558572\n",
      "Iteration 168, loss = 0.23652171\n",
      "Iteration 169, loss = 0.23634334\n",
      "Iteration 170, loss = 0.23623115\n",
      "Iteration 171, loss = 0.23457322\n",
      "Iteration 172, loss = 0.23319796\n",
      "Iteration 173, loss = 0.23253685\n",
      "Iteration 174, loss = 0.23314901\n",
      "Iteration 175, loss = 0.23312297\n",
      "Iteration 176, loss = 0.23236845\n",
      "Iteration 177, loss = 0.22936339\n",
      "Iteration 178, loss = 0.23271873\n",
      "Iteration 179, loss = 0.23271807\n",
      "Iteration 180, loss = 0.23688503\n",
      "Iteration 181, loss = 0.22789555\n",
      "Iteration 182, loss = 0.22667435\n",
      "Iteration 183, loss = 0.23094230\n",
      "Iteration 184, loss = 0.23041296\n",
      "Iteration 185, loss = 0.22890402\n",
      "Iteration 186, loss = 0.22736307\n",
      "Iteration 187, loss = 0.23051363\n",
      "Iteration 188, loss = 0.22310135\n",
      "Iteration 189, loss = 0.22261828\n",
      "Iteration 190, loss = 0.22371572\n",
      "Iteration 191, loss = 0.22375832\n",
      "Iteration 192, loss = 0.23144768\n",
      "Iteration 193, loss = 0.22233599\n",
      "Iteration 194, loss = 0.22119416\n",
      "Iteration 195, loss = 0.22031020\n",
      "Iteration 196, loss = 0.22329869\n",
      "Iteration 197, loss = 0.22373847\n",
      "Iteration 198, loss = 0.22231829\n",
      "Iteration 199, loss = 0.22223169\n",
      "Iteration 200, loss = 0.22110442\n",
      "Iteration 201, loss = 0.22473028\n",
      "Iteration 202, loss = 0.22044735\n",
      "Iteration 203, loss = 0.21208449\n",
      "Iteration 204, loss = 0.21438067\n",
      "Iteration 205, loss = 0.21149595\n",
      "Iteration 206, loss = 0.21309759\n",
      "Iteration 207, loss = 0.21229248\n",
      "Iteration 208, loss = 0.21742542\n",
      "Iteration 209, loss = 0.21970377\n",
      "Iteration 210, loss = 0.21274718\n",
      "Iteration 211, loss = 0.20781381\n",
      "Iteration 212, loss = 0.21243050\n",
      "Iteration 213, loss = 0.20923507\n",
      "Iteration 214, loss = 0.20745159\n",
      "Iteration 215, loss = 0.20818984\n",
      "Iteration 216, loss = 0.21286489\n",
      "Iteration 217, loss = 0.21288973\n",
      "Iteration 218, loss = 0.21000821\n",
      "Iteration 219, loss = 0.20301197\n",
      "Iteration 220, loss = 0.20929178\n",
      "Iteration 221, loss = 0.20752714\n",
      "Iteration 222, loss = 0.20224448\n",
      "Iteration 223, loss = 0.19977517\n",
      "Iteration 224, loss = 0.19950474\n",
      "Iteration 225, loss = 0.19952089\n",
      "Iteration 226, loss = 0.19959015\n",
      "Iteration 227, loss = 0.20420577\n",
      "Iteration 228, loss = 0.19855887\n",
      "Iteration 229, loss = 0.19675782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 230, loss = 0.19695611\n",
      "Iteration 231, loss = 0.19624815\n",
      "Iteration 232, loss = 0.20305137\n",
      "Iteration 233, loss = 0.19603689\n",
      "Iteration 234, loss = 0.19360467\n",
      "Iteration 235, loss = 0.19178268\n",
      "Iteration 236, loss = 0.19026501\n",
      "Iteration 237, loss = 0.19207164\n",
      "Iteration 238, loss = 0.19048532\n",
      "Iteration 239, loss = 0.19296534\n",
      "Iteration 240, loss = 0.18772146\n",
      "Iteration 241, loss = 0.18961492\n",
      "Iteration 242, loss = 0.18946987\n",
      "Iteration 243, loss = 0.18739934\n",
      "Iteration 244, loss = 0.18680843\n",
      "Iteration 245, loss = 0.18355804\n",
      "Iteration 246, loss = 0.18280738\n",
      "Iteration 247, loss = 0.18402553\n",
      "Iteration 248, loss = 0.18264833\n",
      "Iteration 249, loss = 0.18118790\n",
      "Iteration 250, loss = 0.17972676\n",
      "Iteration 251, loss = 0.17976988\n",
      "Iteration 252, loss = 0.17871920\n",
      "Iteration 253, loss = 0.17944258\n",
      "Iteration 254, loss = 0.17864374\n",
      "Iteration 255, loss = 0.17931313\n",
      "Iteration 256, loss = 0.17869710\n",
      "Iteration 257, loss = 0.18378604\n",
      "Iteration 258, loss = 0.18129023\n",
      "Iteration 259, loss = 0.18572582\n",
      "Iteration 260, loss = 0.19679027\n",
      "Iteration 261, loss = 0.18761344\n",
      "Iteration 262, loss = 0.17559430\n",
      "Iteration 263, loss = 0.17658886\n",
      "Iteration 264, loss = 0.17170211\n",
      "Iteration 265, loss = 0.17026812\n",
      "Iteration 266, loss = 0.16983614\n",
      "Iteration 267, loss = 0.17415455\n",
      "Iteration 268, loss = 0.17316605\n",
      "Iteration 269, loss = 0.16784371\n",
      "Iteration 270, loss = 0.16740712\n",
      "Iteration 271, loss = 0.16685553\n",
      "Iteration 272, loss = 0.16624747\n",
      "Iteration 273, loss = 0.16822832\n",
      "Iteration 274, loss = 0.16341446\n",
      "Iteration 275, loss = 0.16330800\n",
      "Iteration 276, loss = 0.16252084\n",
      "Iteration 277, loss = 0.16291257\n",
      "Iteration 278, loss = 0.17561468\n",
      "Iteration 279, loss = 0.16951598\n",
      "Iteration 280, loss = 0.15869983\n",
      "Iteration 281, loss = 0.15721117\n",
      "Iteration 282, loss = 0.15685550\n",
      "Iteration 283, loss = 0.15604308\n",
      "Iteration 284, loss = 0.15544410\n",
      "Iteration 285, loss = 0.15466444\n",
      "Iteration 286, loss = 0.15334359\n",
      "Iteration 287, loss = 0.15271102\n",
      "Iteration 288, loss = 0.15674644\n",
      "Iteration 289, loss = 0.15655682\n",
      "Iteration 290, loss = 0.15066605\n",
      "Iteration 291, loss = 0.15125279\n",
      "Iteration 292, loss = 0.14852523\n",
      "Iteration 293, loss = 0.14861027\n",
      "Iteration 294, loss = 0.14646936\n",
      "Iteration 295, loss = 0.14541613\n",
      "Iteration 296, loss = 0.14478666\n",
      "Iteration 297, loss = 0.14549685\n",
      "Iteration 298, loss = 0.14418426\n",
      "Iteration 299, loss = 0.14610141\n",
      "Iteration 300, loss = 0.14191075\n",
      "Iteration 301, loss = 0.14246087\n",
      "Iteration 302, loss = 0.14318697\n",
      "Iteration 303, loss = 0.14205992\n",
      "Iteration 304, loss = 0.14295076\n",
      "Iteration 305, loss = 0.14638602\n",
      "Iteration 306, loss = 0.14142890\n",
      "Iteration 307, loss = 0.14266738\n",
      "Iteration 308, loss = 0.13880231\n",
      "Iteration 309, loss = 0.14231302\n",
      "Iteration 310, loss = 0.13810247\n",
      "Iteration 311, loss = 0.13636699\n",
      "Iteration 312, loss = 0.13598854\n",
      "Iteration 313, loss = 0.13443604\n",
      "Iteration 314, loss = 0.13447847\n",
      "Iteration 315, loss = 0.13725330\n",
      "Iteration 316, loss = 0.13613185\n",
      "Iteration 317, loss = 0.13393150\n",
      "Iteration 318, loss = 0.13280702\n",
      "Iteration 319, loss = 0.12967629\n",
      "Iteration 320, loss = 0.13082172\n",
      "Iteration 321, loss = 0.13057034\n",
      "Iteration 322, loss = 0.12711263\n",
      "Iteration 323, loss = 0.13043157\n",
      "Iteration 324, loss = 0.12920341\n",
      "Iteration 325, loss = 0.12772875\n",
      "Iteration 326, loss = 0.12404801\n",
      "Iteration 327, loss = 0.12462592\n",
      "Iteration 328, loss = 0.13018257\n",
      "Iteration 329, loss = 0.12887303\n",
      "Iteration 330, loss = 0.13081982\n",
      "Iteration 331, loss = 0.13225382\n",
      "Iteration 332, loss = 0.12992648\n",
      "Iteration 333, loss = 0.12296982\n",
      "Iteration 334, loss = 0.12382208\n",
      "Iteration 335, loss = 0.11994282\n",
      "Iteration 336, loss = 0.11920036\n",
      "Iteration 337, loss = 0.11907143\n",
      "Iteration 338, loss = 0.11762223\n",
      "Iteration 339, loss = 0.11905799\n",
      "Iteration 340, loss = 0.12330148\n",
      "Iteration 341, loss = 0.11836771\n",
      "Iteration 342, loss = 0.12475792\n",
      "Iteration 343, loss = 0.11725676\n",
      "Iteration 344, loss = 0.11482600\n",
      "Iteration 345, loss = 0.11564005\n",
      "Iteration 346, loss = 0.11218473\n",
      "Iteration 347, loss = 0.11258030\n",
      "Iteration 348, loss = 0.11228095\n",
      "Iteration 349, loss = 0.12827888\n",
      "Iteration 350, loss = 0.13807876\n",
      "Iteration 351, loss = 0.12710112\n",
      "Iteration 352, loss = 0.12779434\n",
      "Iteration 353, loss = 0.11456706\n",
      "Iteration 354, loss = 0.11801178\n",
      "Iteration 355, loss = 0.10913953\n",
      "Iteration 356, loss = 0.10843372\n",
      "Iteration 357, loss = 0.10758956\n",
      "Iteration 358, loss = 0.10907268\n",
      "Iteration 359, loss = 0.11490401\n",
      "Iteration 360, loss = 0.11338105\n",
      "Iteration 361, loss = 0.10710936\n",
      "Iteration 362, loss = 0.10608465\n",
      "Iteration 363, loss = 0.10963116\n",
      "Iteration 364, loss = 0.11010064\n",
      "Iteration 365, loss = 0.10307740\n",
      "Iteration 366, loss = 0.10320506\n",
      "Iteration 367, loss = 0.10386068\n",
      "Iteration 368, loss = 0.11142861\n",
      "Iteration 369, loss = 0.11326802\n",
      "Iteration 370, loss = 0.10366242\n",
      "Iteration 371, loss = 0.10195010\n",
      "Iteration 372, loss = 0.10161783\n",
      "Iteration 373, loss = 0.10031016\n",
      "Iteration 374, loss = 0.09961802\n",
      "Iteration 375, loss = 0.09885884\n",
      "Iteration 376, loss = 0.09979183\n",
      "Iteration 377, loss = 0.09994306\n",
      "Iteration 378, loss = 0.09866709\n",
      "Iteration 379, loss = 0.09824997\n",
      "Iteration 380, loss = 0.09709749\n",
      "Iteration 381, loss = 0.09690564\n",
      "Iteration 382, loss = 0.09914218\n",
      "Iteration 383, loss = 0.10231166\n",
      "Iteration 384, loss = 0.10610757\n",
      "Iteration 385, loss = 0.09709524\n",
      "Iteration 386, loss = 0.09617212\n",
      "Iteration 387, loss = 0.09427824\n",
      "Iteration 388, loss = 0.09491218\n",
      "Iteration 389, loss = 0.09383328\n",
      "Iteration 390, loss = 0.09329307\n",
      "Iteration 391, loss = 0.09178964\n",
      "Iteration 392, loss = 0.09287155\n",
      "Iteration 393, loss = 0.09132335\n",
      "Iteration 394, loss = 0.09092937\n",
      "Iteration 395, loss = 0.09498575\n",
      "Iteration 396, loss = 0.09512567\n",
      "Iteration 397, loss = 0.09314857\n",
      "Iteration 398, loss = 0.09343916\n",
      "Iteration 399, loss = 0.09235710\n",
      "Iteration 400, loss = 0.09246928\n",
      "Iteration 401, loss = 0.08951086\n",
      "Iteration 402, loss = 0.09057055\n",
      "Iteration 403, loss = 0.09400576\n",
      "Iteration 404, loss = 0.09702283\n",
      "Iteration 405, loss = 0.08803498\n",
      "Iteration 406, loss = 0.08663208\n",
      "Iteration 407, loss = 0.08723571\n",
      "Iteration 408, loss = 0.08711946\n",
      "Iteration 409, loss = 0.08645796\n",
      "Iteration 410, loss = 0.08516676\n",
      "Iteration 411, loss = 0.08493595\n",
      "Iteration 412, loss = 0.08489773\n",
      "Iteration 413, loss = 0.08385535\n",
      "Iteration 414, loss = 0.08442313\n",
      "Iteration 415, loss = 0.08411029\n",
      "Iteration 416, loss = 0.08819358\n",
      "Iteration 417, loss = 0.08340485\n",
      "Iteration 418, loss = 0.08583180\n",
      "Iteration 419, loss = 0.08550783\n",
      "Iteration 420, loss = 0.08170374\n",
      "Iteration 421, loss = 0.08120543\n",
      "Iteration 422, loss = 0.08159749\n",
      "Iteration 423, loss = 0.08282002\n",
      "Iteration 424, loss = 0.08470097\n",
      "Iteration 425, loss = 0.07960554\n",
      "Iteration 426, loss = 0.08149759\n",
      "Iteration 427, loss = 0.08169720\n",
      "Iteration 428, loss = 0.08050961\n",
      "Iteration 429, loss = 0.07922097\n",
      "Iteration 430, loss = 0.08035340\n",
      "Iteration 431, loss = 0.07864886\n",
      "Iteration 432, loss = 0.07916850\n",
      "Iteration 433, loss = 0.08144653\n",
      "Iteration 434, loss = 0.07798955\n",
      "Iteration 435, loss = 0.07910778\n",
      "Iteration 436, loss = 0.08004894\n",
      "Iteration 437, loss = 0.07739059\n",
      "Iteration 438, loss = 0.07706969\n",
      "Iteration 439, loss = 0.07832981\n",
      "Iteration 440, loss = 0.07716003\n",
      "Iteration 441, loss = 0.07586494\n",
      "Iteration 442, loss = 0.07713521\n",
      "Iteration 443, loss = 0.07471523\n",
      "Iteration 444, loss = 0.07515756\n",
      "Iteration 445, loss = 0.07552808\n",
      "Iteration 446, loss = 0.07482946\n",
      "Iteration 447, loss = 0.07463557\n",
      "Iteration 448, loss = 0.07589560\n",
      "Iteration 449, loss = 0.07382184\n",
      "Iteration 450, loss = 0.07350373\n",
      "Iteration 451, loss = 0.07295795\n",
      "Iteration 452, loss = 0.07269958\n",
      "Iteration 453, loss = 0.07230982\n",
      "Iteration 454, loss = 0.07828621\n",
      "Iteration 455, loss = 0.08808802\n",
      "Iteration 456, loss = 0.08054544\n",
      "Iteration 457, loss = 0.10007463\n",
      "Iteration 458, loss = 0.10774451\n",
      "Iteration 459, loss = 0.10770112\n",
      "Iteration 460, loss = 0.09431119\n",
      "Iteration 461, loss = 0.07835101\n",
      "Iteration 462, loss = 0.08022402\n",
      "Iteration 463, loss = 0.08128244\n",
      "Iteration 464, loss = 0.07909248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 40 of 48...\n",
      "Iteration 1, loss = 0.66988072\n",
      "Iteration 2, loss = 0.56258965\n",
      "Iteration 3, loss = 0.50714123\n",
      "Iteration 4, loss = 0.46504443\n",
      "Iteration 5, loss = 0.43153635\n",
      "Iteration 6, loss = 0.40885959\n",
      "Iteration 7, loss = 0.39739327\n",
      "Iteration 8, loss = 0.38142234\n",
      "Iteration 9, loss = 0.36543296\n",
      "Iteration 10, loss = 0.35350339\n",
      "Iteration 11, loss = 0.34458935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.34280547\n",
      "Iteration 13, loss = 0.32997805\n",
      "Iteration 14, loss = 0.32640799\n",
      "Iteration 15, loss = 0.34781068\n",
      "Iteration 16, loss = 0.32468815\n",
      "Iteration 17, loss = 0.30836245\n",
      "Iteration 18, loss = 0.30321365\n",
      "Iteration 19, loss = 0.29037124\n",
      "Iteration 20, loss = 0.28726224\n",
      "Iteration 21, loss = 0.28231649\n",
      "Iteration 22, loss = 0.27600490\n",
      "Iteration 23, loss = 0.27283751\n",
      "Iteration 24, loss = 0.26860472\n",
      "Iteration 25, loss = 0.27338617\n",
      "Iteration 26, loss = 0.27644879\n",
      "Iteration 27, loss = 0.25341412\n",
      "Iteration 28, loss = 0.25290086\n",
      "Iteration 29, loss = 0.24197041\n",
      "Iteration 30, loss = 0.24073710\n",
      "Iteration 31, loss = 0.26110011\n",
      "Iteration 32, loss = 0.26265882\n",
      "Iteration 33, loss = 0.23282712\n",
      "Iteration 34, loss = 0.21866283\n",
      "Iteration 35, loss = 0.22463537\n",
      "Iteration 36, loss = 0.23204779\n",
      "Iteration 37, loss = 0.21511985\n",
      "Iteration 38, loss = 0.21575002\n",
      "Iteration 39, loss = 0.21078180\n",
      "Iteration 40, loss = 0.20175914\n",
      "Iteration 41, loss = 0.19907078\n",
      "Iteration 42, loss = 0.19414009\n",
      "Iteration 43, loss = 0.19862244\n",
      "Iteration 44, loss = 0.19327252\n",
      "Iteration 45, loss = 0.18701792\n",
      "Iteration 46, loss = 0.18709346\n",
      "Iteration 47, loss = 0.17514090\n",
      "Iteration 48, loss = 0.16919800\n",
      "Iteration 49, loss = 0.17256542\n",
      "Iteration 50, loss = 0.16525579\n",
      "Iteration 51, loss = 0.15949937\n",
      "Iteration 52, loss = 0.16778256\n",
      "Iteration 53, loss = 0.15411204\n",
      "Iteration 54, loss = 0.15432144\n",
      "Iteration 55, loss = 0.15045632\n",
      "Iteration 56, loss = 0.15697666\n",
      "Iteration 57, loss = 0.15319469\n",
      "Iteration 58, loss = 0.14321915\n",
      "Iteration 59, loss = 0.14585218\n",
      "Iteration 60, loss = 0.15511860\n",
      "Iteration 61, loss = 0.14430408\n",
      "Iteration 62, loss = 0.14148068\n",
      "Iteration 63, loss = 0.14304117\n",
      "Iteration 64, loss = 0.12954164\n",
      "Iteration 65, loss = 0.12855739\n",
      "Iteration 66, loss = 0.12941009\n",
      "Iteration 67, loss = 0.13364725\n",
      "Iteration 68, loss = 0.13108062\n",
      "Iteration 69, loss = 0.12535324\n",
      "Iteration 70, loss = 0.11401082\n",
      "Iteration 71, loss = 0.10898340\n",
      "Iteration 72, loss = 0.10939716\n",
      "Iteration 73, loss = 0.10541717\n",
      "Iteration 74, loss = 0.10460703\n",
      "Iteration 75, loss = 0.10247669\n",
      "Iteration 76, loss = 0.09903884\n",
      "Iteration 77, loss = 0.09690502\n",
      "Iteration 78, loss = 0.10078851\n",
      "Iteration 79, loss = 0.10054085\n",
      "Iteration 80, loss = 0.10060837\n",
      "Iteration 81, loss = 0.09888791\n",
      "Iteration 82, loss = 0.09769549\n",
      "Iteration 83, loss = 0.08840186\n",
      "Iteration 84, loss = 0.08940323\n",
      "Iteration 85, loss = 0.08814523\n",
      "Iteration 86, loss = 0.08699284\n",
      "Iteration 87, loss = 0.09583415\n",
      "Iteration 88, loss = 0.10169877\n",
      "Iteration 89, loss = 0.08789446\n",
      "Iteration 90, loss = 0.08969805\n",
      "Iteration 91, loss = 0.08826290\n",
      "Iteration 92, loss = 0.08044478\n",
      "Iteration 93, loss = 0.07622110\n",
      "Iteration 94, loss = 0.07656330\n",
      "Iteration 95, loss = 0.07161672\n",
      "Iteration 96, loss = 0.07023465\n",
      "Iteration 97, loss = 0.07978129\n",
      "Iteration 98, loss = 0.08336074\n",
      "Iteration 99, loss = 0.07250035\n",
      "Iteration 100, loss = 0.06950835\n",
      "Iteration 101, loss = 0.06462739\n",
      "Iteration 102, loss = 0.06033821\n",
      "Iteration 103, loss = 0.06021645\n",
      "Iteration 104, loss = 0.06411600\n",
      "Iteration 105, loss = 0.06467995\n",
      "Iteration 106, loss = 0.06437679\n",
      "Iteration 107, loss = 0.06946455\n",
      "Iteration 108, loss = 0.09896222\n",
      "Iteration 109, loss = 0.10586600\n",
      "Iteration 110, loss = 0.07213452\n",
      "Iteration 111, loss = 0.06480291\n",
      "Iteration 112, loss = 0.05655857\n",
      "Iteration 113, loss = 0.06086789\n",
      "Iteration 114, loss = 0.05223377\n",
      "Iteration 115, loss = 0.05052464\n",
      "Iteration 116, loss = 0.05298187\n",
      "Iteration 117, loss = 0.04811579\n",
      "Iteration 118, loss = 0.04793729\n",
      "Iteration 119, loss = 0.04670522\n",
      "Iteration 120, loss = 0.05035380\n",
      "Iteration 121, loss = 0.04580758\n",
      "Iteration 122, loss = 0.04289877\n",
      "Iteration 123, loss = 0.04928553\n",
      "Iteration 124, loss = 0.05663689\n",
      "Iteration 125, loss = 0.04484964\n",
      "Iteration 126, loss = 0.04216841\n",
      "Iteration 127, loss = 0.04180535\n",
      "Iteration 128, loss = 0.04043462\n",
      "Iteration 129, loss = 0.03861113\n",
      "Iteration 130, loss = 0.04458317\n",
      "Iteration 131, loss = 0.03936832\n",
      "Iteration 132, loss = 0.04149354\n",
      "Iteration 133, loss = 0.03622659\n",
      "Iteration 134, loss = 0.03724373\n",
      "Iteration 135, loss = 0.03495516\n",
      "Iteration 136, loss = 0.04009981\n",
      "Iteration 137, loss = 0.04002313\n",
      "Iteration 138, loss = 0.03585945\n",
      "Iteration 139, loss = 0.03249585\n",
      "Iteration 140, loss = 0.03631918\n",
      "Iteration 141, loss = 0.02895354\n",
      "Iteration 142, loss = 0.03503510\n",
      "Iteration 143, loss = 0.03162735\n",
      "Iteration 144, loss = 0.03590804\n",
      "Iteration 145, loss = 0.05313473\n",
      "Iteration 146, loss = 0.07396269\n",
      "Iteration 147, loss = 0.06553523\n",
      "Iteration 148, loss = 0.08083643\n",
      "Iteration 149, loss = 0.09603044\n",
      "Iteration 150, loss = 0.05910235\n",
      "Iteration 151, loss = 0.05684915\n",
      "Iteration 152, loss = 0.05507418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 41 of 48...\n",
      "Iteration 1, loss = 0.62588199\n",
      "Iteration 2, loss = 0.54794806\n",
      "Iteration 3, loss = 0.49324982\n",
      "Iteration 4, loss = 0.45880451\n",
      "Iteration 5, loss = 0.42843273\n",
      "Iteration 6, loss = 0.40974527\n",
      "Iteration 7, loss = 0.39859585\n",
      "Iteration 8, loss = 0.38396083\n",
      "Iteration 9, loss = 0.36924120\n",
      "Iteration 10, loss = 0.37064952\n",
      "Iteration 11, loss = 0.34502760\n",
      "Iteration 12, loss = 0.33814070\n",
      "Iteration 13, loss = 0.32910265\n",
      "Iteration 14, loss = 0.32762826\n",
      "Iteration 15, loss = 0.31796729\n",
      "Iteration 16, loss = 0.31793686\n",
      "Iteration 17, loss = 0.30287642\n",
      "Iteration 18, loss = 0.29539716\n",
      "Iteration 19, loss = 0.29076550\n",
      "Iteration 20, loss = 0.29010521\n",
      "Iteration 21, loss = 0.28583383\n",
      "Iteration 22, loss = 0.28078355\n",
      "Iteration 23, loss = 0.27354641\n",
      "Iteration 24, loss = 0.26112052\n",
      "Iteration 25, loss = 0.25789009\n",
      "Iteration 26, loss = 0.25295105\n",
      "Iteration 27, loss = 0.24661202\n",
      "Iteration 28, loss = 0.24929095\n",
      "Iteration 29, loss = 0.25334740\n",
      "Iteration 30, loss = 0.24060842\n",
      "Iteration 31, loss = 0.24092114\n",
      "Iteration 32, loss = 0.24808882\n",
      "Iteration 33, loss = 0.26324016\n",
      "Iteration 34, loss = 0.24769535\n",
      "Iteration 35, loss = 0.23436336\n",
      "Iteration 36, loss = 0.21251704\n",
      "Iteration 37, loss = 0.20751674\n",
      "Iteration 38, loss = 0.20994117\n",
      "Iteration 39, loss = 0.20037979\n",
      "Iteration 40, loss = 0.19801583\n",
      "Iteration 41, loss = 0.19184357\n",
      "Iteration 42, loss = 0.19279579\n",
      "Iteration 43, loss = 0.18766244\n",
      "Iteration 44, loss = 0.19369449\n",
      "Iteration 45, loss = 0.18610787\n",
      "Iteration 46, loss = 0.18243209\n",
      "Iteration 47, loss = 0.17887600\n",
      "Iteration 48, loss = 0.17415012\n",
      "Iteration 49, loss = 0.17345442\n",
      "Iteration 50, loss = 0.17014128\n",
      "Iteration 51, loss = 0.16836124\n",
      "Iteration 52, loss = 0.16348230\n",
      "Iteration 53, loss = 0.15930223\n",
      "Iteration 54, loss = 0.15643575\n",
      "Iteration 55, loss = 0.15883646\n",
      "Iteration 56, loss = 0.15383679\n",
      "Iteration 57, loss = 0.15676393\n",
      "Iteration 58, loss = 0.15269600\n",
      "Iteration 59, loss = 0.14511807\n",
      "Iteration 60, loss = 0.15009690\n",
      "Iteration 61, loss = 0.13993567\n",
      "Iteration 62, loss = 0.13403798\n",
      "Iteration 63, loss = 0.14643374\n",
      "Iteration 64, loss = 0.13173314\n",
      "Iteration 65, loss = 0.13024934\n",
      "Iteration 66, loss = 0.13965852\n",
      "Iteration 67, loss = 0.13649469\n",
      "Iteration 68, loss = 0.12519957\n",
      "Iteration 69, loss = 0.11986188\n",
      "Iteration 70, loss = 0.12736803\n",
      "Iteration 71, loss = 0.11678940\n",
      "Iteration 72, loss = 0.10925935\n",
      "Iteration 73, loss = 0.11998562\n",
      "Iteration 74, loss = 0.11027095\n",
      "Iteration 75, loss = 0.12322468\n",
      "Iteration 76, loss = 0.10873536\n",
      "Iteration 77, loss = 0.10312136\n",
      "Iteration 78, loss = 0.10577227\n",
      "Iteration 79, loss = 0.09825465\n",
      "Iteration 80, loss = 0.10001095\n",
      "Iteration 81, loss = 0.09146375\n",
      "Iteration 82, loss = 0.09568351\n",
      "Iteration 83, loss = 0.09376970\n",
      "Iteration 84, loss = 0.08860437\n",
      "Iteration 85, loss = 0.09194634\n",
      "Iteration 86, loss = 0.08828737\n",
      "Iteration 87, loss = 0.08492083\n",
      "Iteration 88, loss = 0.10244344\n",
      "Iteration 89, loss = 0.09361710\n",
      "Iteration 90, loss = 0.08525346\n",
      "Iteration 91, loss = 0.09466039\n",
      "Iteration 92, loss = 0.10591068\n",
      "Iteration 93, loss = 0.09140124\n",
      "Iteration 94, loss = 0.10154763\n",
      "Iteration 95, loss = 0.08023553\n",
      "Iteration 96, loss = 0.07815007\n",
      "Iteration 97, loss = 0.07424550\n",
      "Iteration 98, loss = 0.07686483\n",
      "Iteration 99, loss = 0.08946608\n",
      "Iteration 100, loss = 0.09725476\n",
      "Iteration 101, loss = 0.07716522\n",
      "Iteration 102, loss = 0.07225475\n",
      "Iteration 103, loss = 0.06915890\n",
      "Iteration 104, loss = 0.07137535\n",
      "Iteration 105, loss = 0.07214883\n",
      "Iteration 106, loss = 0.06642376\n",
      "Iteration 107, loss = 0.06025248\n",
      "Iteration 108, loss = 0.06015397\n",
      "Iteration 109, loss = 0.06585487\n",
      "Iteration 110, loss = 0.05993378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 111, loss = 0.05840641\n",
      "Iteration 112, loss = 0.05162904\n",
      "Iteration 113, loss = 0.06355891\n",
      "Iteration 114, loss = 0.05383115\n",
      "Iteration 115, loss = 0.05982291\n",
      "Iteration 116, loss = 0.07015296\n",
      "Iteration 117, loss = 0.07097158\n",
      "Iteration 118, loss = 0.05717728\n",
      "Iteration 119, loss = 0.04841171\n",
      "Iteration 120, loss = 0.05690193\n",
      "Iteration 121, loss = 0.05352719\n",
      "Iteration 122, loss = 0.04841471\n",
      "Iteration 123, loss = 0.04691514\n",
      "Iteration 124, loss = 0.05534523\n",
      "Iteration 125, loss = 0.06051142\n",
      "Iteration 126, loss = 0.04463703\n",
      "Iteration 127, loss = 0.04731376\n",
      "Iteration 128, loss = 0.05153426\n",
      "Iteration 129, loss = 0.04188320\n",
      "Iteration 130, loss = 0.04018871\n",
      "Iteration 131, loss = 0.04155922\n",
      "Iteration 132, loss = 0.04041451\n",
      "Iteration 133, loss = 0.04406530\n",
      "Iteration 134, loss = 0.04052738\n",
      "Iteration 135, loss = 0.05260043\n",
      "Iteration 136, loss = 0.04524797\n",
      "Iteration 137, loss = 0.04195479\n",
      "Iteration 138, loss = 0.03781185\n",
      "Iteration 139, loss = 0.04425951\n",
      "Iteration 140, loss = 0.04712304\n",
      "Iteration 141, loss = 0.04697081\n",
      "Iteration 142, loss = 0.04187548\n",
      "Iteration 143, loss = 0.05365887\n",
      "Iteration 144, loss = 0.05143339\n",
      "Iteration 145, loss = 0.03651304\n",
      "Iteration 146, loss = 0.03118523\n",
      "Iteration 147, loss = 0.02921746\n",
      "Iteration 148, loss = 0.03008096\n",
      "Iteration 149, loss = 0.02867980\n",
      "Iteration 150, loss = 0.02827319\n",
      "Iteration 151, loss = 0.02835898\n",
      "Iteration 152, loss = 0.02687104\n",
      "Iteration 153, loss = 0.02740479\n",
      "Iteration 154, loss = 0.02606191\n",
      "Iteration 155, loss = 0.02849610\n",
      "Iteration 156, loss = 0.02696851\n",
      "Iteration 157, loss = 0.02570906\n",
      "Iteration 158, loss = 0.02520480\n",
      "Iteration 159, loss = 0.02414402\n",
      "Iteration 160, loss = 0.02607643\n",
      "Iteration 161, loss = 0.03189093\n",
      "Iteration 162, loss = 0.02657166\n",
      "Iteration 163, loss = 0.02517231\n",
      "Iteration 164, loss = 0.02514160\n",
      "Iteration 165, loss = 0.02312469\n",
      "Iteration 166, loss = 0.02407520\n",
      "Iteration 167, loss = 0.04162916\n",
      "Iteration 168, loss = 0.04726464\n",
      "Iteration 169, loss = 0.04096564\n",
      "Iteration 170, loss = 0.02960504\n",
      "Iteration 171, loss = 0.03133007\n",
      "Iteration 172, loss = 0.02624893\n",
      "Iteration 173, loss = 0.02139728\n",
      "Iteration 174, loss = 0.01888255\n",
      "Iteration 175, loss = 0.02140752\n",
      "Iteration 176, loss = 0.02282165\n",
      "Iteration 177, loss = 0.04000106\n",
      "Iteration 178, loss = 0.04875969\n",
      "Iteration 179, loss = 0.03534277\n",
      "Iteration 180, loss = 0.04040980\n",
      "Iteration 181, loss = 0.03666001\n",
      "Iteration 182, loss = 0.04481393\n",
      "Iteration 183, loss = 0.05540793\n",
      "Iteration 184, loss = 0.05447099\n",
      "Iteration 185, loss = 0.04542560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 42 of 48...\n",
      "Iteration 1, loss = 0.62091063\n",
      "Iteration 2, loss = 0.52972051\n",
      "Iteration 3, loss = 0.48827103\n",
      "Iteration 4, loss = 0.45613111\n",
      "Iteration 5, loss = 0.42212630\n",
      "Iteration 6, loss = 0.40557254\n",
      "Iteration 7, loss = 0.38522047\n",
      "Iteration 8, loss = 0.37463556\n",
      "Iteration 9, loss = 0.35935268\n",
      "Iteration 10, loss = 0.35139673\n",
      "Iteration 11, loss = 0.35085627\n",
      "Iteration 12, loss = 0.34264437\n",
      "Iteration 13, loss = 0.33894514\n",
      "Iteration 14, loss = 0.32148493\n",
      "Iteration 15, loss = 0.31279100\n",
      "Iteration 16, loss = 0.30530503\n",
      "Iteration 17, loss = 0.29918528\n",
      "Iteration 18, loss = 0.29390838\n",
      "Iteration 19, loss = 0.28933088\n",
      "Iteration 20, loss = 0.28243601\n",
      "Iteration 21, loss = 0.29434233\n",
      "Iteration 22, loss = 0.28870001\n",
      "Iteration 23, loss = 0.27060405\n",
      "Iteration 24, loss = 0.25989653\n",
      "Iteration 25, loss = 0.25539993\n",
      "Iteration 26, loss = 0.25036238\n",
      "Iteration 27, loss = 0.26084413\n",
      "Iteration 28, loss = 0.28212193\n",
      "Iteration 29, loss = 0.26078987\n",
      "Iteration 30, loss = 0.23647588\n",
      "Iteration 31, loss = 0.24243561\n",
      "Iteration 32, loss = 0.22776333\n",
      "Iteration 33, loss = 0.22086980\n",
      "Iteration 34, loss = 0.21702446\n",
      "Iteration 35, loss = 0.21886191\n",
      "Iteration 36, loss = 0.21573932\n",
      "Iteration 37, loss = 0.21091801\n",
      "Iteration 38, loss = 0.20808046\n",
      "Iteration 39, loss = 0.21972391\n",
      "Iteration 40, loss = 0.21153649\n",
      "Iteration 41, loss = 0.20095252\n",
      "Iteration 42, loss = 0.19548979\n",
      "Iteration 43, loss = 0.18978642\n",
      "Iteration 44, loss = 0.18840333\n",
      "Iteration 45, loss = 0.18259044\n",
      "Iteration 46, loss = 0.18011223\n",
      "Iteration 47, loss = 0.19124934\n",
      "Iteration 48, loss = 0.20074191\n",
      "Iteration 49, loss = 0.18992770\n",
      "Iteration 50, loss = 0.16627840\n",
      "Iteration 51, loss = 0.16536855\n",
      "Iteration 52, loss = 0.16673353\n",
      "Iteration 53, loss = 0.15706835\n",
      "Iteration 54, loss = 0.15419194\n",
      "Iteration 55, loss = 0.14744934\n",
      "Iteration 56, loss = 0.14705544\n",
      "Iteration 57, loss = 0.14363720\n",
      "Iteration 58, loss = 0.15840921\n",
      "Iteration 59, loss = 0.14291377\n",
      "Iteration 60, loss = 0.13317549\n",
      "Iteration 61, loss = 0.13187247\n",
      "Iteration 62, loss = 0.12919671\n",
      "Iteration 63, loss = 0.12893586\n",
      "Iteration 64, loss = 0.12330627\n",
      "Iteration 65, loss = 0.12559192\n",
      "Iteration 66, loss = 0.12843368\n",
      "Iteration 67, loss = 0.13365973\n",
      "Iteration 68, loss = 0.11986013\n",
      "Iteration 69, loss = 0.11487112\n",
      "Iteration 70, loss = 0.11184833\n",
      "Iteration 71, loss = 0.10880356\n",
      "Iteration 72, loss = 0.10728794\n",
      "Iteration 73, loss = 0.10591325\n",
      "Iteration 74, loss = 0.10412505\n",
      "Iteration 75, loss = 0.12116548\n",
      "Iteration 76, loss = 0.11915836\n",
      "Iteration 77, loss = 0.10034333\n",
      "Iteration 78, loss = 0.09634152\n",
      "Iteration 79, loss = 0.09943155\n",
      "Iteration 80, loss = 0.10464797\n",
      "Iteration 81, loss = 0.10774763\n",
      "Iteration 82, loss = 0.10134759\n",
      "Iteration 83, loss = 0.09435950\n",
      "Iteration 84, loss = 0.09675865\n",
      "Iteration 85, loss = 0.09213379\n",
      "Iteration 86, loss = 0.08637806\n",
      "Iteration 87, loss = 0.08325302\n",
      "Iteration 88, loss = 0.08320661\n",
      "Iteration 89, loss = 0.08178565\n",
      "Iteration 90, loss = 0.10711959\n",
      "Iteration 91, loss = 0.12071514\n",
      "Iteration 92, loss = 0.10518874\n",
      "Iteration 93, loss = 0.07999552\n",
      "Iteration 94, loss = 0.08388219\n",
      "Iteration 95, loss = 0.07526848\n",
      "Iteration 96, loss = 0.07373473\n",
      "Iteration 97, loss = 0.07114712\n",
      "Iteration 98, loss = 0.07296031\n",
      "Iteration 99, loss = 0.07252380\n",
      "Iteration 100, loss = 0.06870304\n",
      "Iteration 101, loss = 0.06599390\n",
      "Iteration 102, loss = 0.06494480\n",
      "Iteration 103, loss = 0.06709455\n",
      "Iteration 104, loss = 0.06832869\n",
      "Iteration 105, loss = 0.06556842\n",
      "Iteration 106, loss = 0.07418345\n",
      "Iteration 107, loss = 0.07301274\n",
      "Iteration 108, loss = 0.06923220\n",
      "Iteration 109, loss = 0.06437673\n",
      "Iteration 110, loss = 0.06538215\n",
      "Iteration 111, loss = 0.05975999\n",
      "Iteration 112, loss = 0.06048672\n",
      "Iteration 113, loss = 0.05938197\n",
      "Iteration 114, loss = 0.07088222\n",
      "Iteration 115, loss = 0.06324740\n",
      "Iteration 116, loss = 0.05528274\n",
      "Iteration 117, loss = 0.05688995\n",
      "Iteration 118, loss = 0.05252065\n",
      "Iteration 119, loss = 0.05357493\n",
      "Iteration 120, loss = 0.05464597\n",
      "Iteration 121, loss = 0.04967661\n",
      "Iteration 122, loss = 0.05335394\n",
      "Iteration 123, loss = 0.06378282\n",
      "Iteration 124, loss = 0.05987911\n",
      "Iteration 125, loss = 0.05426029\n",
      "Iteration 126, loss = 0.05455523\n",
      "Iteration 127, loss = 0.06406471\n",
      "Iteration 128, loss = 0.09075105\n",
      "Iteration 129, loss = 0.08523748\n",
      "Iteration 130, loss = 0.07733960\n",
      "Iteration 131, loss = 0.06791592\n",
      "Iteration 132, loss = 0.06410644\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 43 of 48...\n",
      "Iteration 1, loss = 0.68495009\n",
      "Iteration 2, loss = 0.66751405\n",
      "Iteration 3, loss = 0.63527871\n",
      "Iteration 4, loss = 0.59434839\n",
      "Iteration 5, loss = 0.55484726\n",
      "Iteration 6, loss = 0.52050547\n",
      "Iteration 7, loss = 0.49191176\n",
      "Iteration 8, loss = 0.46946669\n",
      "Iteration 9, loss = 0.44913507\n",
      "Iteration 10, loss = 0.43698553\n",
      "Iteration 11, loss = 0.42176478\n",
      "Iteration 12, loss = 0.41133572\n",
      "Iteration 13, loss = 0.40159529\n",
      "Iteration 14, loss = 0.39815171\n",
      "Iteration 15, loss = 0.38606125\n",
      "Iteration 16, loss = 0.37898582\n",
      "Iteration 17, loss = 0.37416310\n",
      "Iteration 18, loss = 0.36905184\n",
      "Iteration 19, loss = 0.36299602\n",
      "Iteration 20, loss = 0.36212472\n",
      "Iteration 21, loss = 0.36533605\n",
      "Iteration 22, loss = 0.35199239\n",
      "Iteration 23, loss = 0.34602956\n",
      "Iteration 24, loss = 0.34337881\n",
      "Iteration 25, loss = 0.34060026\n",
      "Iteration 26, loss = 0.33947614\n",
      "Iteration 27, loss = 0.33538651\n",
      "Iteration 28, loss = 0.33724257\n",
      "Iteration 29, loss = 0.32717649\n",
      "Iteration 30, loss = 0.32605524\n",
      "Iteration 31, loss = 0.32517567\n",
      "Iteration 32, loss = 0.32881069\n",
      "Iteration 33, loss = 0.32995425\n",
      "Iteration 34, loss = 0.32418829\n",
      "Iteration 35, loss = 0.31456990\n",
      "Iteration 36, loss = 0.31022920\n",
      "Iteration 37, loss = 0.31023553\n",
      "Iteration 38, loss = 0.30763868\n",
      "Iteration 39, loss = 0.30204735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 0.29970263\n",
      "Iteration 41, loss = 0.29922915\n",
      "Iteration 42, loss = 0.29535232\n",
      "Iteration 43, loss = 0.29294074\n",
      "Iteration 44, loss = 0.29106452\n",
      "Iteration 45, loss = 0.29149594\n",
      "Iteration 46, loss = 0.28861068\n",
      "Iteration 47, loss = 0.28621187\n",
      "Iteration 48, loss = 0.28234855\n",
      "Iteration 49, loss = 0.28303245\n",
      "Iteration 50, loss = 0.27960064\n",
      "Iteration 51, loss = 0.27793731\n",
      "Iteration 52, loss = 0.28540035\n",
      "Iteration 53, loss = 0.27973709\n",
      "Iteration 54, loss = 0.27220149\n",
      "Iteration 55, loss = 0.27908012\n",
      "Iteration 56, loss = 0.27216530\n",
      "Iteration 57, loss = 0.27068992\n",
      "Iteration 58, loss = 0.26811151\n",
      "Iteration 59, loss = 0.26335626\n",
      "Iteration 60, loss = 0.26584525\n",
      "Iteration 61, loss = 0.26553061\n",
      "Iteration 62, loss = 0.26465609\n",
      "Iteration 63, loss = 0.26362956\n",
      "Iteration 64, loss = 0.26624303\n",
      "Iteration 65, loss = 0.26085348\n",
      "Iteration 66, loss = 0.25643762\n",
      "Iteration 67, loss = 0.25749380\n",
      "Iteration 68, loss = 0.25986907\n",
      "Iteration 69, loss = 0.25455259\n",
      "Iteration 70, loss = 0.25884345\n",
      "Iteration 71, loss = 0.25090090\n",
      "Iteration 72, loss = 0.24940636\n",
      "Iteration 73, loss = 0.24641676\n",
      "Iteration 74, loss = 0.24623319\n",
      "Iteration 75, loss = 0.24328734\n",
      "Iteration 76, loss = 0.24534615\n",
      "Iteration 77, loss = 0.24520085\n",
      "Iteration 78, loss = 0.24432236\n",
      "Iteration 79, loss = 0.25101708\n",
      "Iteration 80, loss = 0.24258086\n",
      "Iteration 81, loss = 0.24334714\n",
      "Iteration 82, loss = 0.24494659\n",
      "Iteration 83, loss = 0.24064912\n",
      "Iteration 84, loss = 0.23785564\n",
      "Iteration 85, loss = 0.23801794\n",
      "Iteration 86, loss = 0.23491252\n",
      "Iteration 87, loss = 0.23646383\n",
      "Iteration 88, loss = 0.23430608\n",
      "Iteration 89, loss = 0.23080654\n",
      "Iteration 90, loss = 0.23803609\n",
      "Iteration 91, loss = 0.23291731\n",
      "Iteration 92, loss = 0.22877608\n",
      "Iteration 93, loss = 0.23324369\n",
      "Iteration 94, loss = 0.23649700\n",
      "Iteration 95, loss = 0.23376104\n",
      "Iteration 96, loss = 0.22682190\n",
      "Iteration 97, loss = 0.22877034\n",
      "Iteration 98, loss = 0.22876831\n",
      "Iteration 99, loss = 0.23536473\n",
      "Iteration 100, loss = 0.22633737\n",
      "Iteration 101, loss = 0.22229106\n",
      "Iteration 102, loss = 0.22024671\n",
      "Iteration 103, loss = 0.22110116\n",
      "Iteration 104, loss = 0.22676289\n",
      "Iteration 105, loss = 0.21767609\n",
      "Iteration 106, loss = 0.21630388\n",
      "Iteration 107, loss = 0.21501081\n",
      "Iteration 108, loss = 0.21725952\n",
      "Iteration 109, loss = 0.21665316\n",
      "Iteration 110, loss = 0.21521359\n",
      "Iteration 111, loss = 0.21612479\n",
      "Iteration 112, loss = 0.21921836\n",
      "Iteration 113, loss = 0.22245965\n",
      "Iteration 114, loss = 0.21587781\n",
      "Iteration 115, loss = 0.21419393\n",
      "Iteration 116, loss = 0.21771270\n",
      "Iteration 117, loss = 0.21780891\n",
      "Iteration 118, loss = 0.22202519\n",
      "Iteration 119, loss = 0.22054213\n",
      "Iteration 120, loss = 0.20931449\n",
      "Iteration 121, loss = 0.22070762\n",
      "Iteration 122, loss = 0.22080243\n",
      "Iteration 123, loss = 0.21633492\n",
      "Iteration 124, loss = 0.20623313\n",
      "Iteration 125, loss = 0.20343477\n",
      "Iteration 126, loss = 0.20359491\n",
      "Iteration 127, loss = 0.21128130\n",
      "Iteration 128, loss = 0.20342215\n",
      "Iteration 129, loss = 0.20249575\n",
      "Iteration 130, loss = 0.20488522\n",
      "Iteration 131, loss = 0.20775797\n",
      "Iteration 132, loss = 0.20707520\n",
      "Iteration 133, loss = 0.20024804\n",
      "Iteration 134, loss = 0.20697861\n",
      "Iteration 135, loss = 0.20296048\n",
      "Iteration 136, loss = 0.19925316\n",
      "Iteration 137, loss = 0.19833527\n",
      "Iteration 138, loss = 0.19394733\n",
      "Iteration 139, loss = 0.19528837\n",
      "Iteration 140, loss = 0.19544502\n",
      "Iteration 141, loss = 0.19828778\n",
      "Iteration 142, loss = 0.19801104\n",
      "Iteration 143, loss = 0.19798837\n",
      "Iteration 144, loss = 0.19423853\n",
      "Iteration 145, loss = 0.19156141\n",
      "Iteration 146, loss = 0.19702426\n",
      "Iteration 147, loss = 0.19026446\n",
      "Iteration 148, loss = 0.19733410\n",
      "Iteration 149, loss = 0.19291519\n",
      "Iteration 150, loss = 0.18998861\n",
      "Iteration 151, loss = 0.18894483\n",
      "Iteration 152, loss = 0.18693024\n",
      "Iteration 153, loss = 0.18777702\n",
      "Iteration 154, loss = 0.18641320\n",
      "Iteration 155, loss = 0.18981061\n",
      "Iteration 156, loss = 0.20105550\n",
      "Iteration 157, loss = 0.19395018\n",
      "Iteration 158, loss = 0.18686896\n",
      "Iteration 159, loss = 0.18319865\n",
      "Iteration 160, loss = 0.18080085\n",
      "Iteration 161, loss = 0.18079007\n",
      "Iteration 162, loss = 0.18433931\n",
      "Iteration 163, loss = 0.18552546\n",
      "Iteration 164, loss = 0.18818382\n",
      "Iteration 165, loss = 0.18743541\n",
      "Iteration 166, loss = 0.17922613\n",
      "Iteration 167, loss = 0.17725422\n",
      "Iteration 168, loss = 0.17900527\n",
      "Iteration 169, loss = 0.17556750\n",
      "Iteration 170, loss = 0.17662917\n",
      "Iteration 171, loss = 0.18762801\n",
      "Iteration 172, loss = 0.17973875\n",
      "Iteration 173, loss = 0.17748997\n",
      "Iteration 174, loss = 0.17734384\n",
      "Iteration 175, loss = 0.17587601\n",
      "Iteration 176, loss = 0.17637680\n",
      "Iteration 177, loss = 0.17351811\n",
      "Iteration 178, loss = 0.17140715\n",
      "Iteration 179, loss = 0.17424659\n",
      "Iteration 180, loss = 0.16894540\n",
      "Iteration 181, loss = 0.16930712\n",
      "Iteration 182, loss = 0.16810722\n",
      "Iteration 183, loss = 0.17048955\n",
      "Iteration 184, loss = 0.16855596\n",
      "Iteration 185, loss = 0.16559352\n",
      "Iteration 186, loss = 0.16746079\n",
      "Iteration 187, loss = 0.16426201\n",
      "Iteration 188, loss = 0.16502472\n",
      "Iteration 189, loss = 0.16367381\n",
      "Iteration 190, loss = 0.16359350\n",
      "Iteration 191, loss = 0.16311339\n",
      "Iteration 192, loss = 0.16842624\n",
      "Iteration 193, loss = 0.17738520\n",
      "Iteration 194, loss = 0.16423299\n",
      "Iteration 195, loss = 0.17274899\n",
      "Iteration 196, loss = 0.16203286\n",
      "Iteration 197, loss = 0.15784826\n",
      "Iteration 198, loss = 0.15734676\n",
      "Iteration 199, loss = 0.15610642\n",
      "Iteration 200, loss = 0.16222956\n",
      "Iteration 201, loss = 0.16230242\n",
      "Iteration 202, loss = 0.16020366\n",
      "Iteration 203, loss = 0.16284413\n",
      "Iteration 204, loss = 0.16432802\n",
      "Iteration 205, loss = 0.15442304\n",
      "Iteration 206, loss = 0.15588592\n",
      "Iteration 207, loss = 0.14958533\n",
      "Iteration 208, loss = 0.15617787\n",
      "Iteration 209, loss = 0.15262838\n",
      "Iteration 210, loss = 0.15162778\n",
      "Iteration 211, loss = 0.15352928\n",
      "Iteration 212, loss = 0.17043999\n",
      "Iteration 213, loss = 0.16701543\n",
      "Iteration 214, loss = 0.15455412\n",
      "Iteration 215, loss = 0.14797656\n",
      "Iteration 216, loss = 0.14516104\n",
      "Iteration 217, loss = 0.14569878\n",
      "Iteration 218, loss = 0.14579100\n",
      "Iteration 219, loss = 0.14582044\n",
      "Iteration 220, loss = 0.14739992\n",
      "Iteration 221, loss = 0.14014377\n",
      "Iteration 222, loss = 0.14011466\n",
      "Iteration 223, loss = 0.13772558\n",
      "Iteration 224, loss = 0.13896558\n",
      "Iteration 225, loss = 0.15102727\n",
      "Iteration 226, loss = 0.14014681\n",
      "Iteration 227, loss = 0.13465008\n",
      "Iteration 228, loss = 0.14528092\n",
      "Iteration 229, loss = 0.13897953\n",
      "Iteration 230, loss = 0.13254876\n",
      "Iteration 231, loss = 0.13095618\n",
      "Iteration 232, loss = 0.13430992\n",
      "Iteration 233, loss = 0.13299511\n",
      "Iteration 234, loss = 0.13345954\n",
      "Iteration 235, loss = 0.13136880\n",
      "Iteration 236, loss = 0.12847364\n",
      "Iteration 237, loss = 0.13119101\n",
      "Iteration 238, loss = 0.12722333\n",
      "Iteration 239, loss = 0.12834537\n",
      "Iteration 240, loss = 0.12661949\n",
      "Iteration 241, loss = 0.12627115\n",
      "Iteration 242, loss = 0.12960869\n",
      "Iteration 243, loss = 0.12944102\n",
      "Iteration 244, loss = 0.12693290\n",
      "Iteration 245, loss = 0.12750217\n",
      "Iteration 246, loss = 0.12396289\n",
      "Iteration 247, loss = 0.12107242\n",
      "Iteration 248, loss = 0.12071812\n",
      "Iteration 249, loss = 0.11933789\n",
      "Iteration 250, loss = 0.11774919\n",
      "Iteration 251, loss = 0.11826066\n",
      "Iteration 252, loss = 0.11830831\n",
      "Iteration 253, loss = 0.11644159\n",
      "Iteration 254, loss = 0.11502123\n",
      "Iteration 255, loss = 0.11565966\n",
      "Iteration 256, loss = 0.11746487\n",
      "Iteration 257, loss = 0.12333568\n",
      "Iteration 258, loss = 0.11322192\n",
      "Iteration 259, loss = 0.11102459\n",
      "Iteration 260, loss = 0.11224316\n",
      "Iteration 261, loss = 0.11130672\n",
      "Iteration 262, loss = 0.11047200\n",
      "Iteration 263, loss = 0.10917044\n",
      "Iteration 264, loss = 0.10980534\n",
      "Iteration 265, loss = 0.10986570\n",
      "Iteration 266, loss = 0.11210664\n",
      "Iteration 267, loss = 0.10538528\n",
      "Iteration 268, loss = 0.10799608\n",
      "Iteration 269, loss = 0.10640363\n",
      "Iteration 270, loss = 0.10424588\n",
      "Iteration 271, loss = 0.10270513\n",
      "Iteration 272, loss = 0.10397471\n",
      "Iteration 273, loss = 0.10530929\n",
      "Iteration 274, loss = 0.11010867\n",
      "Iteration 275, loss = 0.11315563\n",
      "Iteration 276, loss = 0.10840433\n",
      "Iteration 277, loss = 0.10062703\n",
      "Iteration 278, loss = 0.10146158\n",
      "Iteration 279, loss = 0.11073600\n",
      "Iteration 280, loss = 0.09629549\n",
      "Iteration 281, loss = 0.09737063\n",
      "Iteration 282, loss = 0.09584771\n",
      "Iteration 283, loss = 0.09515964\n",
      "Iteration 284, loss = 0.09342529\n",
      "Iteration 285, loss = 0.09364816\n",
      "Iteration 286, loss = 0.09070942\n",
      "Iteration 287, loss = 0.09219852\n",
      "Iteration 288, loss = 0.09304181\n",
      "Iteration 289, loss = 0.09250847\n",
      "Iteration 290, loss = 0.09087120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 291, loss = 0.08826655\n",
      "Iteration 292, loss = 0.08738713\n",
      "Iteration 293, loss = 0.08661377\n",
      "Iteration 294, loss = 0.08681542\n",
      "Iteration 295, loss = 0.09015212\n",
      "Iteration 296, loss = 0.09012874\n",
      "Iteration 297, loss = 0.09607776\n",
      "Iteration 298, loss = 0.08408992\n",
      "Iteration 299, loss = 0.08752744\n",
      "Iteration 300, loss = 0.09213851\n",
      "Iteration 301, loss = 0.08496062\n",
      "Iteration 302, loss = 0.08737334\n",
      "Iteration 303, loss = 0.08944569\n",
      "Iteration 304, loss = 0.08101398\n",
      "Iteration 305, loss = 0.08091385\n",
      "Iteration 306, loss = 0.07908984\n",
      "Iteration 307, loss = 0.08007432\n",
      "Iteration 308, loss = 0.07757222\n",
      "Iteration 309, loss = 0.08138466\n",
      "Iteration 310, loss = 0.07938464\n",
      "Iteration 311, loss = 0.07849641\n",
      "Iteration 312, loss = 0.07772322\n",
      "Iteration 313, loss = 0.07692457\n",
      "Iteration 314, loss = 0.07541368\n",
      "Iteration 315, loss = 0.07333593\n",
      "Iteration 316, loss = 0.07595944\n",
      "Iteration 317, loss = 0.08093396\n",
      "Iteration 318, loss = 0.07257776\n",
      "Iteration 319, loss = 0.07084379\n",
      "Iteration 320, loss = 0.07200526\n",
      "Iteration 321, loss = 0.06951989\n",
      "Iteration 322, loss = 0.06936995\n",
      "Iteration 323, loss = 0.06935534\n",
      "Iteration 324, loss = 0.06954279\n",
      "Iteration 325, loss = 0.06755694\n",
      "Iteration 326, loss = 0.06626584\n",
      "Iteration 327, loss = 0.06938609\n",
      "Iteration 328, loss = 0.07061242\n",
      "Iteration 329, loss = 0.07299175\n",
      "Iteration 330, loss = 0.06487179\n",
      "Iteration 331, loss = 0.06327299\n",
      "Iteration 332, loss = 0.06656695\n",
      "Iteration 333, loss = 0.06769456\n",
      "Iteration 334, loss = 0.06432888\n",
      "Iteration 335, loss = 0.06236276\n",
      "Iteration 336, loss = 0.06103707\n",
      "Iteration 337, loss = 0.06379585\n",
      "Iteration 338, loss = 0.06908957\n",
      "Iteration 339, loss = 0.06381198\n",
      "Iteration 340, loss = 0.06105511\n",
      "Iteration 341, loss = 0.06436398\n",
      "Iteration 342, loss = 0.06534780\n",
      "Iteration 343, loss = 0.06333554\n",
      "Iteration 344, loss = 0.05933167\n",
      "Iteration 345, loss = 0.05815190\n",
      "Iteration 346, loss = 0.05682724\n",
      "Iteration 347, loss = 0.05884771\n",
      "Iteration 348, loss = 0.05667385\n",
      "Iteration 349, loss = 0.05715427\n",
      "Iteration 350, loss = 0.05554728\n",
      "Iteration 351, loss = 0.06499300\n",
      "Iteration 352, loss = 0.07029572\n",
      "Iteration 353, loss = 0.07167010\n",
      "Iteration 354, loss = 0.06192213\n",
      "Iteration 355, loss = 0.05326768\n",
      "Iteration 356, loss = 0.05252624\n",
      "Iteration 357, loss = 0.05379969\n",
      "Iteration 358, loss = 0.05153819\n",
      "Iteration 359, loss = 0.05034995\n",
      "Iteration 360, loss = 0.05020856\n",
      "Iteration 361, loss = 0.05311513\n",
      "Iteration 362, loss = 0.04968505\n",
      "Iteration 363, loss = 0.05008095\n",
      "Iteration 364, loss = 0.05840913\n",
      "Iteration 365, loss = 0.05218224\n",
      "Iteration 366, loss = 0.05246003\n",
      "Iteration 367, loss = 0.05109779\n",
      "Iteration 368, loss = 0.05329482\n",
      "Iteration 369, loss = 0.04951117\n",
      "Iteration 370, loss = 0.04906358\n",
      "Iteration 371, loss = 0.04639269\n",
      "Iteration 372, loss = 0.04541480\n",
      "Iteration 373, loss = 0.04612071\n",
      "Iteration 374, loss = 0.04938099\n",
      "Iteration 375, loss = 0.04588814\n",
      "Iteration 376, loss = 0.04548997\n",
      "Iteration 377, loss = 0.04585067\n",
      "Iteration 378, loss = 0.05273654\n",
      "Iteration 379, loss = 0.04494328\n",
      "Iteration 380, loss = 0.04663933\n",
      "Iteration 381, loss = 0.04659222\n",
      "Iteration 382, loss = 0.04496966\n",
      "Iteration 383, loss = 0.04637777\n",
      "Iteration 384, loss = 0.05025749\n",
      "Iteration 385, loss = 0.04700952\n",
      "Iteration 386, loss = 0.04495399\n",
      "Iteration 387, loss = 0.04768385\n",
      "Iteration 388, loss = 0.14124251\n",
      "Iteration 389, loss = 0.11057096\n",
      "Iteration 390, loss = 0.08351191\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 44 of 48...\n",
      "Iteration 1, loss = 0.68847652\n",
      "Iteration 2, loss = 0.67587115\n",
      "Iteration 3, loss = 0.65424891\n",
      "Iteration 4, loss = 0.61913693\n",
      "Iteration 5, loss = 0.57910140\n",
      "Iteration 6, loss = 0.54127240\n",
      "Iteration 7, loss = 0.50960033\n",
      "Iteration 8, loss = 0.48335384\n",
      "Iteration 9, loss = 0.46350362\n",
      "Iteration 10, loss = 0.44504176\n",
      "Iteration 11, loss = 0.43296254\n",
      "Iteration 12, loss = 0.42006172\n",
      "Iteration 13, loss = 0.40900545\n",
      "Iteration 14, loss = 0.39963515\n",
      "Iteration 15, loss = 0.39300688\n",
      "Iteration 16, loss = 0.38553740\n",
      "Iteration 17, loss = 0.37880019\n",
      "Iteration 18, loss = 0.37685014\n",
      "Iteration 19, loss = 0.37123793\n",
      "Iteration 20, loss = 0.36184238\n",
      "Iteration 21, loss = 0.36015472\n",
      "Iteration 22, loss = 0.36109920\n",
      "Iteration 23, loss = 0.35358367\n",
      "Iteration 24, loss = 0.35096704\n",
      "Iteration 25, loss = 0.35176881\n",
      "Iteration 26, loss = 0.33979157\n",
      "Iteration 27, loss = 0.33814458\n",
      "Iteration 28, loss = 0.33425248\n",
      "Iteration 29, loss = 0.33367676\n",
      "Iteration 30, loss = 0.32978144\n",
      "Iteration 31, loss = 0.32530721\n",
      "Iteration 32, loss = 0.32261222\n",
      "Iteration 33, loss = 0.31971562\n",
      "Iteration 34, loss = 0.31778805\n",
      "Iteration 35, loss = 0.31977161\n",
      "Iteration 36, loss = 0.31730893\n",
      "Iteration 37, loss = 0.31658375\n",
      "Iteration 38, loss = 0.31329682\n",
      "Iteration 39, loss = 0.30763464\n",
      "Iteration 40, loss = 0.30744217\n",
      "Iteration 41, loss = 0.31390371\n",
      "Iteration 42, loss = 0.30484987\n",
      "Iteration 43, loss = 0.29830457\n",
      "Iteration 44, loss = 0.29688092\n",
      "Iteration 45, loss = 0.29424577\n",
      "Iteration 46, loss = 0.29256744\n",
      "Iteration 47, loss = 0.29101150\n",
      "Iteration 48, loss = 0.29090326\n",
      "Iteration 49, loss = 0.29311782\n",
      "Iteration 50, loss = 0.28327612\n",
      "Iteration 51, loss = 0.28536151\n",
      "Iteration 52, loss = 0.28072358\n",
      "Iteration 53, loss = 0.27904936\n",
      "Iteration 54, loss = 0.27707105\n",
      "Iteration 55, loss = 0.27587623\n",
      "Iteration 56, loss = 0.27382998\n",
      "Iteration 57, loss = 0.27329789\n",
      "Iteration 58, loss = 0.27520519\n",
      "Iteration 59, loss = 0.27218569\n",
      "Iteration 60, loss = 0.27080345\n",
      "Iteration 61, loss = 0.26488856\n",
      "Iteration 62, loss = 0.27602704\n",
      "Iteration 63, loss = 0.26976590\n",
      "Iteration 64, loss = 0.26311188\n",
      "Iteration 65, loss = 0.26112465\n",
      "Iteration 66, loss = 0.27195390\n",
      "Iteration 67, loss = 0.25877632\n",
      "Iteration 68, loss = 0.25787716\n",
      "Iteration 69, loss = 0.25863605\n",
      "Iteration 70, loss = 0.25866423\n",
      "Iteration 71, loss = 0.26276463\n",
      "Iteration 72, loss = 0.25327740\n",
      "Iteration 73, loss = 0.25220596\n",
      "Iteration 74, loss = 0.25222313\n",
      "Iteration 75, loss = 0.24986057\n",
      "Iteration 76, loss = 0.24670800\n",
      "Iteration 77, loss = 0.25090129\n",
      "Iteration 78, loss = 0.25449186\n",
      "Iteration 79, loss = 0.25188866\n",
      "Iteration 80, loss = 0.25649506\n",
      "Iteration 81, loss = 0.24839870\n",
      "Iteration 82, loss = 0.24382376\n",
      "Iteration 83, loss = 0.24556409\n",
      "Iteration 84, loss = 0.24367445\n",
      "Iteration 85, loss = 0.24468036\n",
      "Iteration 86, loss = 0.24393227\n",
      "Iteration 87, loss = 0.23824400\n",
      "Iteration 88, loss = 0.24402521\n",
      "Iteration 89, loss = 0.23973318\n",
      "Iteration 90, loss = 0.23689131\n",
      "Iteration 91, loss = 0.23406090\n",
      "Iteration 92, loss = 0.23355749\n",
      "Iteration 93, loss = 0.23397084\n",
      "Iteration 94, loss = 0.23099563\n",
      "Iteration 95, loss = 0.23697455\n",
      "Iteration 96, loss = 0.24227964\n",
      "Iteration 97, loss = 0.23298468\n",
      "Iteration 98, loss = 0.23751565\n",
      "Iteration 99, loss = 0.24485172\n",
      "Iteration 100, loss = 0.23295505\n",
      "Iteration 101, loss = 0.23163197\n",
      "Iteration 102, loss = 0.22521061\n",
      "Iteration 103, loss = 0.22692310\n",
      "Iteration 104, loss = 0.22410384\n",
      "Iteration 105, loss = 0.22476973\n",
      "Iteration 106, loss = 0.22736562\n",
      "Iteration 107, loss = 0.22292248\n",
      "Iteration 108, loss = 0.22258425\n",
      "Iteration 109, loss = 0.22452590\n",
      "Iteration 110, loss = 0.22209411\n",
      "Iteration 111, loss = 0.22058020\n",
      "Iteration 112, loss = 0.22139881\n",
      "Iteration 113, loss = 0.22353591\n",
      "Iteration 114, loss = 0.22350257\n",
      "Iteration 115, loss = 0.22188729\n",
      "Iteration 116, loss = 0.21527693\n",
      "Iteration 117, loss = 0.22382168\n",
      "Iteration 118, loss = 0.21867754\n",
      "Iteration 119, loss = 0.21838117\n",
      "Iteration 120, loss = 0.21823611\n",
      "Iteration 121, loss = 0.22167771\n",
      "Iteration 122, loss = 0.24043949\n",
      "Iteration 123, loss = 0.21469336\n",
      "Iteration 124, loss = 0.21270661\n",
      "Iteration 125, loss = 0.21822287\n",
      "Iteration 126, loss = 0.21170386\n",
      "Iteration 127, loss = 0.21166331\n",
      "Iteration 128, loss = 0.21022881\n",
      "Iteration 129, loss = 0.21351916\n",
      "Iteration 130, loss = 0.21346773\n",
      "Iteration 131, loss = 0.21477035\n",
      "Iteration 132, loss = 0.21421105\n",
      "Iteration 133, loss = 0.21756112\n",
      "Iteration 134, loss = 0.21384606\n",
      "Iteration 135, loss = 0.20680719\n",
      "Iteration 136, loss = 0.20375841\n",
      "Iteration 137, loss = 0.20125208\n",
      "Iteration 138, loss = 0.20266619\n",
      "Iteration 139, loss = 0.20005597\n",
      "Iteration 140, loss = 0.19917258\n",
      "Iteration 141, loss = 0.20633680\n",
      "Iteration 142, loss = 0.20614927\n",
      "Iteration 143, loss = 0.20307718\n",
      "Iteration 144, loss = 0.19861889\n",
      "Iteration 145, loss = 0.19781208\n",
      "Iteration 146, loss = 0.19689455\n",
      "Iteration 147, loss = 0.19784490\n",
      "Iteration 148, loss = 0.19684297\n",
      "Iteration 149, loss = 0.20107286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 150, loss = 0.19657327\n",
      "Iteration 151, loss = 0.19910070\n",
      "Iteration 152, loss = 0.19767121\n",
      "Iteration 153, loss = 0.19519796\n",
      "Iteration 154, loss = 0.19275025\n",
      "Iteration 155, loss = 0.19118994\n",
      "Iteration 156, loss = 0.18898438\n",
      "Iteration 157, loss = 0.19323952\n",
      "Iteration 158, loss = 0.19523410\n",
      "Iteration 159, loss = 0.19254362\n",
      "Iteration 160, loss = 0.18831439\n",
      "Iteration 161, loss = 0.19260988\n",
      "Iteration 162, loss = 0.19000058\n",
      "Iteration 163, loss = 0.19000659\n",
      "Iteration 164, loss = 0.18644016\n",
      "Iteration 165, loss = 0.18511410\n",
      "Iteration 166, loss = 0.18545184\n",
      "Iteration 167, loss = 0.18736601\n",
      "Iteration 168, loss = 0.18240445\n",
      "Iteration 169, loss = 0.18328246\n",
      "Iteration 170, loss = 0.19432280\n",
      "Iteration 171, loss = 0.20281958\n",
      "Iteration 172, loss = 0.18732490\n",
      "Iteration 173, loss = 0.18171745\n",
      "Iteration 174, loss = 0.18470164\n",
      "Iteration 175, loss = 0.18236595\n",
      "Iteration 176, loss = 0.18184645\n",
      "Iteration 177, loss = 0.17646508\n",
      "Iteration 178, loss = 0.17920433\n",
      "Iteration 179, loss = 0.17423288\n",
      "Iteration 180, loss = 0.17426836\n",
      "Iteration 181, loss = 0.17100423\n",
      "Iteration 182, loss = 0.17141216\n",
      "Iteration 183, loss = 0.17420644\n",
      "Iteration 184, loss = 0.18083061\n",
      "Iteration 185, loss = 0.17586267\n",
      "Iteration 186, loss = 0.17820278\n",
      "Iteration 187, loss = 0.17064971\n",
      "Iteration 188, loss = 0.16948188\n",
      "Iteration 189, loss = 0.16415987\n",
      "Iteration 190, loss = 0.16641278\n",
      "Iteration 191, loss = 0.16358536\n",
      "Iteration 192, loss = 0.16662907\n",
      "Iteration 193, loss = 0.16616033\n",
      "Iteration 194, loss = 0.16836932\n",
      "Iteration 195, loss = 0.17002113\n",
      "Iteration 196, loss = 0.16562181\n",
      "Iteration 197, loss = 0.16938227\n",
      "Iteration 198, loss = 0.16326773\n",
      "Iteration 199, loss = 0.16401214\n",
      "Iteration 200, loss = 0.17553875\n",
      "Iteration 201, loss = 0.16393783\n",
      "Iteration 202, loss = 0.15801193\n",
      "Iteration 203, loss = 0.16358911\n",
      "Iteration 204, loss = 0.15909382\n",
      "Iteration 205, loss = 0.15691216\n",
      "Iteration 206, loss = 0.16315229\n",
      "Iteration 207, loss = 0.16370080\n",
      "Iteration 208, loss = 0.16095335\n",
      "Iteration 209, loss = 0.17248921\n",
      "Iteration 210, loss = 0.16015852\n",
      "Iteration 211, loss = 0.15307055\n",
      "Iteration 212, loss = 0.14838562\n",
      "Iteration 213, loss = 0.14504986\n",
      "Iteration 214, loss = 0.14446090\n",
      "Iteration 215, loss = 0.14212908\n",
      "Iteration 216, loss = 0.14500999\n",
      "Iteration 217, loss = 0.15265611\n",
      "Iteration 218, loss = 0.14515311\n",
      "Iteration 219, loss = 0.14189339\n",
      "Iteration 220, loss = 0.14246713\n",
      "Iteration 221, loss = 0.13996136\n",
      "Iteration 222, loss = 0.13927444\n",
      "Iteration 223, loss = 0.14078513\n",
      "Iteration 224, loss = 0.13866471\n",
      "Iteration 225, loss = 0.13664551\n",
      "Iteration 226, loss = 0.13784755\n",
      "Iteration 227, loss = 0.14299676\n",
      "Iteration 228, loss = 0.13326435\n",
      "Iteration 229, loss = 0.13282369\n",
      "Iteration 230, loss = 0.13829381\n",
      "Iteration 231, loss = 0.13582356\n",
      "Iteration 232, loss = 0.13274348\n",
      "Iteration 233, loss = 0.13442690\n",
      "Iteration 234, loss = 0.13571441\n",
      "Iteration 235, loss = 0.12750958\n",
      "Iteration 236, loss = 0.12905128\n",
      "Iteration 237, loss = 0.12694886\n",
      "Iteration 238, loss = 0.12582063\n",
      "Iteration 239, loss = 0.12790684\n",
      "Iteration 240, loss = 0.12955228\n",
      "Iteration 241, loss = 0.12565798\n",
      "Iteration 242, loss = 0.12638220\n",
      "Iteration 243, loss = 0.12403788\n",
      "Iteration 244, loss = 0.12385600\n",
      "Iteration 245, loss = 0.12778752\n",
      "Iteration 246, loss = 0.13038721\n",
      "Iteration 247, loss = 0.12365493\n",
      "Iteration 248, loss = 0.12194320\n",
      "Iteration 249, loss = 0.12079725\n",
      "Iteration 250, loss = 0.11865373\n",
      "Iteration 251, loss = 0.11616206\n",
      "Iteration 252, loss = 0.11571668\n",
      "Iteration 253, loss = 0.11699214\n",
      "Iteration 254, loss = 0.11603097\n",
      "Iteration 255, loss = 0.11393791\n",
      "Iteration 256, loss = 0.11494839\n",
      "Iteration 257, loss = 0.11170210\n",
      "Iteration 258, loss = 0.12284283\n",
      "Iteration 259, loss = 0.11327090\n",
      "Iteration 260, loss = 0.11001387\n",
      "Iteration 261, loss = 0.11185242\n",
      "Iteration 262, loss = 0.10761308\n",
      "Iteration 263, loss = 0.11180857\n",
      "Iteration 264, loss = 0.10903640\n",
      "Iteration 265, loss = 0.10881620\n",
      "Iteration 266, loss = 0.10904599\n",
      "Iteration 267, loss = 0.11533125\n",
      "Iteration 268, loss = 0.11078273\n",
      "Iteration 269, loss = 0.10410614\n",
      "Iteration 270, loss = 0.10697467\n",
      "Iteration 271, loss = 0.10968616\n",
      "Iteration 272, loss = 0.10460712\n",
      "Iteration 273, loss = 0.10134009\n",
      "Iteration 274, loss = 0.10098149\n",
      "Iteration 275, loss = 0.10234949\n",
      "Iteration 276, loss = 0.10172805\n",
      "Iteration 277, loss = 0.09755062\n",
      "Iteration 278, loss = 0.09554892\n",
      "Iteration 279, loss = 0.11051663\n",
      "Iteration 280, loss = 0.10186665\n",
      "Iteration 281, loss = 0.10891287\n",
      "Iteration 282, loss = 0.10724012\n",
      "Iteration 283, loss = 0.10316266\n",
      "Iteration 284, loss = 0.10154335\n",
      "Iteration 285, loss = 0.10422990\n",
      "Iteration 286, loss = 0.10350936\n",
      "Iteration 287, loss = 0.10824062\n",
      "Iteration 288, loss = 0.09655180\n",
      "Iteration 289, loss = 0.09675273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 45 of 48...\n",
      "Iteration 1, loss = 0.69255613\n",
      "Iteration 2, loss = 0.68056087\n",
      "Iteration 3, loss = 0.66607975\n",
      "Iteration 4, loss = 0.63724854\n",
      "Iteration 5, loss = 0.60121731\n",
      "Iteration 6, loss = 0.56843594\n",
      "Iteration 7, loss = 0.53529844\n",
      "Iteration 8, loss = 0.50826072\n",
      "Iteration 9, loss = 0.48469926\n",
      "Iteration 10, loss = 0.46947876\n",
      "Iteration 11, loss = 0.45337250\n",
      "Iteration 12, loss = 0.43979656\n",
      "Iteration 13, loss = 0.42939969\n",
      "Iteration 14, loss = 0.42180800\n",
      "Iteration 15, loss = 0.41018046\n",
      "Iteration 16, loss = 0.40362875\n",
      "Iteration 17, loss = 0.39815630\n",
      "Iteration 18, loss = 0.39265415\n",
      "Iteration 19, loss = 0.38730826\n",
      "Iteration 20, loss = 0.38105432\n",
      "Iteration 21, loss = 0.37787752\n",
      "Iteration 22, loss = 0.37485629\n",
      "Iteration 23, loss = 0.36859223\n",
      "Iteration 24, loss = 0.36758775\n",
      "Iteration 25, loss = 0.36404835\n",
      "Iteration 26, loss = 0.36363195\n",
      "Iteration 27, loss = 0.35803477\n",
      "Iteration 28, loss = 0.35206543\n",
      "Iteration 29, loss = 0.35078431\n",
      "Iteration 30, loss = 0.34497248\n",
      "Iteration 31, loss = 0.34589942\n",
      "Iteration 32, loss = 0.34162666\n",
      "Iteration 33, loss = 0.34007353\n",
      "Iteration 34, loss = 0.33911951\n",
      "Iteration 35, loss = 0.33757843\n",
      "Iteration 36, loss = 0.33429379\n",
      "Iteration 37, loss = 0.32930074\n",
      "Iteration 38, loss = 0.33006706\n",
      "Iteration 39, loss = 0.33014069\n",
      "Iteration 40, loss = 0.32515362\n",
      "Iteration 41, loss = 0.32292066\n",
      "Iteration 42, loss = 0.31936377\n",
      "Iteration 43, loss = 0.31843048\n",
      "Iteration 44, loss = 0.31860187\n",
      "Iteration 45, loss = 0.31690998\n",
      "Iteration 46, loss = 0.31979447\n",
      "Iteration 47, loss = 0.32210649\n",
      "Iteration 48, loss = 0.30818146\n",
      "Iteration 49, loss = 0.30736754\n",
      "Iteration 50, loss = 0.30448152\n",
      "Iteration 51, loss = 0.30526469\n",
      "Iteration 52, loss = 0.30159013\n",
      "Iteration 53, loss = 0.30160771\n",
      "Iteration 54, loss = 0.29934839\n",
      "Iteration 55, loss = 0.29982108\n",
      "Iteration 56, loss = 0.29922464\n",
      "Iteration 57, loss = 0.30137116\n",
      "Iteration 58, loss = 0.30653580\n",
      "Iteration 59, loss = 0.30231658\n",
      "Iteration 60, loss = 0.29748963\n",
      "Iteration 61, loss = 0.29367161\n",
      "Iteration 62, loss = 0.29164886\n",
      "Iteration 63, loss = 0.29035933\n",
      "Iteration 64, loss = 0.29407203\n",
      "Iteration 65, loss = 0.29324628\n",
      "Iteration 66, loss = 0.28659252\n",
      "Iteration 67, loss = 0.28711172\n",
      "Iteration 68, loss = 0.29000169\n",
      "Iteration 69, loss = 0.28325091\n",
      "Iteration 70, loss = 0.28536918\n",
      "Iteration 71, loss = 0.29499180\n",
      "Iteration 72, loss = 0.28864852\n",
      "Iteration 73, loss = 0.27966628\n",
      "Iteration 74, loss = 0.28075635\n",
      "Iteration 75, loss = 0.27843857\n",
      "Iteration 76, loss = 0.27659903\n",
      "Iteration 77, loss = 0.27971445\n",
      "Iteration 78, loss = 0.27681787\n",
      "Iteration 79, loss = 0.27283427\n",
      "Iteration 80, loss = 0.28028636\n",
      "Iteration 81, loss = 0.27910850\n",
      "Iteration 82, loss = 0.27859647\n",
      "Iteration 83, loss = 0.27012839\n",
      "Iteration 84, loss = 0.27281233\n",
      "Iteration 85, loss = 0.27041975\n",
      "Iteration 86, loss = 0.27191460\n",
      "Iteration 87, loss = 0.26848659\n",
      "Iteration 88, loss = 0.26765400\n",
      "Iteration 89, loss = 0.26993765\n",
      "Iteration 90, loss = 0.27314671\n",
      "Iteration 91, loss = 0.26506886\n",
      "Iteration 92, loss = 0.26948805\n",
      "Iteration 93, loss = 0.27084165\n",
      "Iteration 94, loss = 0.26957464\n",
      "Iteration 95, loss = 0.27704340\n",
      "Iteration 96, loss = 0.26690231\n",
      "Iteration 97, loss = 0.26816788\n",
      "Iteration 98, loss = 0.27065450\n",
      "Iteration 99, loss = 0.25903490\n",
      "Iteration 100, loss = 0.26449949\n",
      "Iteration 101, loss = 0.26247725\n",
      "Iteration 102, loss = 0.25663039\n",
      "Iteration 103, loss = 0.25781637\n",
      "Iteration 104, loss = 0.25574767\n",
      "Iteration 105, loss = 0.25947235\n",
      "Iteration 106, loss = 0.26594931\n",
      "Iteration 107, loss = 0.26758611\n",
      "Iteration 108, loss = 0.26294271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 109, loss = 0.25438206\n",
      "Iteration 110, loss = 0.26047044\n",
      "Iteration 111, loss = 0.26633815\n",
      "Iteration 112, loss = 0.25816436\n",
      "Iteration 113, loss = 0.25393712\n",
      "Iteration 114, loss = 0.25152465\n",
      "Iteration 115, loss = 0.25193204\n",
      "Iteration 116, loss = 0.25137664\n",
      "Iteration 117, loss = 0.25658235\n",
      "Iteration 118, loss = 0.24800206\n",
      "Iteration 119, loss = 0.25249853\n",
      "Iteration 120, loss = 0.26074550\n",
      "Iteration 121, loss = 0.25878966\n",
      "Iteration 122, loss = 0.25215408\n",
      "Iteration 123, loss = 0.24942633\n",
      "Iteration 124, loss = 0.24953704\n",
      "Iteration 125, loss = 0.24309580\n",
      "Iteration 126, loss = 0.24443262\n",
      "Iteration 127, loss = 0.24292287\n",
      "Iteration 128, loss = 0.24044410\n",
      "Iteration 129, loss = 0.24175739\n",
      "Iteration 130, loss = 0.24024265\n",
      "Iteration 131, loss = 0.24148385\n",
      "Iteration 132, loss = 0.24263649\n",
      "Iteration 133, loss = 0.24231521\n",
      "Iteration 134, loss = 0.24048090\n",
      "Iteration 135, loss = 0.24031454\n",
      "Iteration 136, loss = 0.23907729\n",
      "Iteration 137, loss = 0.23697326\n",
      "Iteration 138, loss = 0.24357236\n",
      "Iteration 139, loss = 0.23837132\n",
      "Iteration 140, loss = 0.23515429\n",
      "Iteration 141, loss = 0.23381113\n",
      "Iteration 142, loss = 0.23420584\n",
      "Iteration 143, loss = 0.23473332\n",
      "Iteration 144, loss = 0.23374954\n",
      "Iteration 145, loss = 0.23671956\n",
      "Iteration 146, loss = 0.23011879\n",
      "Iteration 147, loss = 0.23105535\n",
      "Iteration 148, loss = 0.23595482\n",
      "Iteration 149, loss = 0.24128160\n",
      "Iteration 150, loss = 0.23991860\n",
      "Iteration 151, loss = 0.23149059\n",
      "Iteration 152, loss = 0.23338424\n",
      "Iteration 153, loss = 0.22857519\n",
      "Iteration 154, loss = 0.22274264\n",
      "Iteration 155, loss = 0.22858798\n",
      "Iteration 156, loss = 0.22771894\n",
      "Iteration 157, loss = 0.22962052\n",
      "Iteration 158, loss = 0.23698905\n",
      "Iteration 159, loss = 0.23089300\n",
      "Iteration 160, loss = 0.22300698\n",
      "Iteration 161, loss = 0.22321078\n",
      "Iteration 162, loss = 0.22672395\n",
      "Iteration 163, loss = 0.22285670\n",
      "Iteration 164, loss = 0.21879668\n",
      "Iteration 165, loss = 0.21646565\n",
      "Iteration 166, loss = 0.21865005\n",
      "Iteration 167, loss = 0.21647555\n",
      "Iteration 168, loss = 0.21587308\n",
      "Iteration 169, loss = 0.21487508\n",
      "Iteration 170, loss = 0.22196043\n",
      "Iteration 171, loss = 0.22766897\n",
      "Iteration 172, loss = 0.22330269\n",
      "Iteration 173, loss = 0.21608559\n",
      "Iteration 174, loss = 0.22665278\n",
      "Iteration 175, loss = 0.22330971\n",
      "Iteration 176, loss = 0.22238480\n",
      "Iteration 177, loss = 0.22188800\n",
      "Iteration 178, loss = 0.22422680\n",
      "Iteration 179, loss = 0.22579573\n",
      "Iteration 180, loss = 0.21278281\n",
      "Iteration 181, loss = 0.21262324\n",
      "Iteration 182, loss = 0.21962183\n",
      "Iteration 183, loss = 0.21742054\n",
      "Iteration 184, loss = 0.22230382\n",
      "Iteration 185, loss = 0.21221276\n",
      "Iteration 186, loss = 0.20842638\n",
      "Iteration 187, loss = 0.20304110\n",
      "Iteration 188, loss = 0.20331650\n",
      "Iteration 189, loss = 0.20211148\n",
      "Iteration 190, loss = 0.20217487\n",
      "Iteration 191, loss = 0.19824112\n",
      "Iteration 192, loss = 0.19882405\n",
      "Iteration 193, loss = 0.20054182\n",
      "Iteration 194, loss = 0.19840950\n",
      "Iteration 195, loss = 0.20226078\n",
      "Iteration 196, loss = 0.20849579\n",
      "Iteration 197, loss = 0.20407484\n",
      "Iteration 198, loss = 0.19870984\n",
      "Iteration 199, loss = 0.19088447\n",
      "Iteration 200, loss = 0.20104454\n",
      "Iteration 201, loss = 0.19808519\n",
      "Iteration 202, loss = 0.19338067\n",
      "Iteration 203, loss = 0.19858122\n",
      "Iteration 204, loss = 0.19324625\n",
      "Iteration 205, loss = 0.19015752\n",
      "Iteration 206, loss = 0.18786076\n",
      "Iteration 207, loss = 0.18926085\n",
      "Iteration 208, loss = 0.18613675\n",
      "Iteration 209, loss = 0.18289316\n",
      "Iteration 210, loss = 0.18342305\n",
      "Iteration 211, loss = 0.18610929\n",
      "Iteration 212, loss = 0.18488913\n",
      "Iteration 213, loss = 0.18347333\n",
      "Iteration 214, loss = 0.18326532\n",
      "Iteration 215, loss = 0.18462611\n",
      "Iteration 216, loss = 0.17983567\n",
      "Iteration 217, loss = 0.17740717\n",
      "Iteration 218, loss = 0.18103760\n",
      "Iteration 219, loss = 0.19441066\n",
      "Iteration 220, loss = 0.18254653\n",
      "Iteration 221, loss = 0.17846979\n",
      "Iteration 222, loss = 0.18003740\n",
      "Iteration 223, loss = 0.17635710\n",
      "Iteration 224, loss = 0.17386966\n",
      "Iteration 225, loss = 0.17281365\n",
      "Iteration 226, loss = 0.17339917\n",
      "Iteration 227, loss = 0.17724508\n",
      "Iteration 228, loss = 0.18231341\n",
      "Iteration 229, loss = 0.18097044\n",
      "Iteration 230, loss = 0.17792518\n",
      "Iteration 231, loss = 0.19250076\n",
      "Iteration 232, loss = 0.17532351\n",
      "Iteration 233, loss = 0.17768554\n",
      "Iteration 234, loss = 0.17359333\n",
      "Iteration 235, loss = 0.17316858\n",
      "Iteration 236, loss = 0.16849391\n",
      "Iteration 237, loss = 0.16440892\n",
      "Iteration 238, loss = 0.16418684\n",
      "Iteration 239, loss = 0.16628936\n",
      "Iteration 240, loss = 0.16001601\n",
      "Iteration 241, loss = 0.15981731\n",
      "Iteration 242, loss = 0.16774508\n",
      "Iteration 243, loss = 0.16507730\n",
      "Iteration 244, loss = 0.16320971\n",
      "Iteration 245, loss = 0.16750303\n",
      "Iteration 246, loss = 0.16702919\n",
      "Iteration 247, loss = 0.16106676\n",
      "Iteration 248, loss = 0.15494359\n",
      "Iteration 249, loss = 0.15674867\n",
      "Iteration 250, loss = 0.15805720\n",
      "Iteration 251, loss = 0.15475174\n",
      "Iteration 252, loss = 0.15496517\n",
      "Iteration 253, loss = 0.15950122\n",
      "Iteration 254, loss = 0.15341733\n",
      "Iteration 255, loss = 0.15433701\n",
      "Iteration 256, loss = 0.14812680\n",
      "Iteration 257, loss = 0.15052783\n",
      "Iteration 258, loss = 0.14810158\n",
      "Iteration 259, loss = 0.15380512\n",
      "Iteration 260, loss = 0.14971556\n",
      "Iteration 261, loss = 0.14812798\n",
      "Iteration 262, loss = 0.14652504\n",
      "Iteration 263, loss = 0.14876127\n",
      "Iteration 264, loss = 0.14583799\n",
      "Iteration 265, loss = 0.15184867\n",
      "Iteration 266, loss = 0.14859087\n",
      "Iteration 267, loss = 0.14247582\n",
      "Iteration 268, loss = 0.14384825\n",
      "Iteration 269, loss = 0.14511827\n",
      "Iteration 270, loss = 0.15133693\n",
      "Iteration 271, loss = 0.13879472\n",
      "Iteration 272, loss = 0.13917636\n",
      "Iteration 273, loss = 0.13824030\n",
      "Iteration 274, loss = 0.14050273\n",
      "Iteration 275, loss = 0.13331757\n",
      "Iteration 276, loss = 0.13459667\n",
      "Iteration 277, loss = 0.13487249\n",
      "Iteration 278, loss = 0.13599448\n",
      "Iteration 279, loss = 0.13153889\n",
      "Iteration 280, loss = 0.13948142\n",
      "Iteration 281, loss = 0.13517969\n",
      "Iteration 282, loss = 0.13717495\n",
      "Iteration 283, loss = 0.13509783\n",
      "Iteration 284, loss = 0.14089787\n",
      "Iteration 285, loss = 0.13540108\n",
      "Iteration 286, loss = 0.13026603\n",
      "Iteration 287, loss = 0.13232832\n",
      "Iteration 288, loss = 0.13739093\n",
      "Iteration 289, loss = 0.14427017\n",
      "Iteration 290, loss = 0.13080225\n",
      "Iteration 291, loss = 0.13172977\n",
      "Iteration 292, loss = 0.12851608\n",
      "Iteration 293, loss = 0.13338619\n",
      "Iteration 294, loss = 0.13949875\n",
      "Iteration 295, loss = 0.13044440\n",
      "Iteration 296, loss = 0.12990273\n",
      "Iteration 297, loss = 0.12159300\n",
      "Iteration 298, loss = 0.12734243\n",
      "Iteration 299, loss = 0.12705911\n",
      "Iteration 300, loss = 0.12458992\n",
      "Iteration 301, loss = 0.12398823\n",
      "Iteration 302, loss = 0.11963710\n",
      "Iteration 303, loss = 0.11783500\n",
      "Iteration 304, loss = 0.12290169\n",
      "Iteration 305, loss = 0.13473989\n",
      "Iteration 306, loss = 0.11849105\n",
      "Iteration 307, loss = 0.11794756\n",
      "Iteration 308, loss = 0.11823431\n",
      "Iteration 309, loss = 0.11564437\n",
      "Iteration 310, loss = 0.12146175\n",
      "Iteration 311, loss = 0.11501604\n",
      "Iteration 312, loss = 0.11527059\n",
      "Iteration 313, loss = 0.11211593\n",
      "Iteration 314, loss = 0.11781797\n",
      "Iteration 315, loss = 0.12229353\n",
      "Iteration 316, loss = 0.11488213\n",
      "Iteration 317, loss = 0.11367508\n",
      "Iteration 318, loss = 0.11688797\n",
      "Iteration 319, loss = 0.11230130\n",
      "Iteration 320, loss = 0.11601954\n",
      "Iteration 321, loss = 0.11384194\n",
      "Iteration 322, loss = 0.11218867\n",
      "Iteration 323, loss = 0.11343562\n",
      "Iteration 324, loss = 0.10958036\n",
      "Iteration 325, loss = 0.11837644\n",
      "Iteration 326, loss = 0.11307690\n",
      "Iteration 327, loss = 0.10844274\n",
      "Iteration 328, loss = 0.10959906\n",
      "Iteration 329, loss = 0.10988255\n",
      "Iteration 330, loss = 0.10967372\n",
      "Iteration 331, loss = 0.11309414\n",
      "Iteration 332, loss = 0.11052342\n",
      "Iteration 333, loss = 0.10591001\n",
      "Iteration 334, loss = 0.10582573\n",
      "Iteration 335, loss = 0.10652820\n",
      "Iteration 336, loss = 0.10578958\n",
      "Iteration 337, loss = 0.10999160\n",
      "Iteration 338, loss = 0.10165957\n",
      "Iteration 339, loss = 0.10293490\n",
      "Iteration 340, loss = 0.10379406\n",
      "Iteration 341, loss = 0.10101931\n",
      "Iteration 342, loss = 0.10156226\n",
      "Iteration 343, loss = 0.10135870\n",
      "Iteration 344, loss = 0.10150559\n",
      "Iteration 345, loss = 0.09780917\n",
      "Iteration 346, loss = 0.09975861\n",
      "Iteration 347, loss = 0.09806601\n",
      "Iteration 348, loss = 0.10080926\n",
      "Iteration 349, loss = 0.10487304\n",
      "Iteration 350, loss = 0.09554185\n",
      "Iteration 351, loss = 0.10119628\n",
      "Iteration 352, loss = 0.09900228\n",
      "Iteration 353, loss = 0.09550788\n",
      "Iteration 354, loss = 0.09494920\n",
      "Iteration 355, loss = 0.09404611\n",
      "Iteration 356, loss = 0.09539702\n",
      "Iteration 357, loss = 0.09534726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 358, loss = 0.09721363\n",
      "Iteration 359, loss = 0.09983341\n",
      "Iteration 360, loss = 0.09749840\n",
      "Iteration 361, loss = 0.09399469\n",
      "Iteration 362, loss = 0.10002069\n",
      "Iteration 363, loss = 0.09876131\n",
      "Iteration 364, loss = 0.10169361\n",
      "Iteration 365, loss = 0.09791745\n",
      "Iteration 366, loss = 0.09227204\n",
      "Iteration 367, loss = 0.09478756\n",
      "Iteration 368, loss = 0.09945187\n",
      "Iteration 369, loss = 0.10346641\n",
      "Iteration 370, loss = 0.09974316\n",
      "Iteration 371, loss = 0.10912281\n",
      "Iteration 372, loss = 0.10239760\n",
      "Iteration 373, loss = 0.10107332\n",
      "Iteration 374, loss = 0.09621889\n",
      "Iteration 375, loss = 0.09522878\n",
      "Iteration 376, loss = 0.08788298\n",
      "Iteration 377, loss = 0.09114577\n",
      "Iteration 378, loss = 0.08789450\n",
      "Iteration 379, loss = 0.08749399\n",
      "Iteration 380, loss = 0.08790317\n",
      "Iteration 381, loss = 0.08575638\n",
      "Iteration 382, loss = 0.08487849\n",
      "Iteration 383, loss = 0.08404803\n",
      "Iteration 384, loss = 0.08317659\n",
      "Iteration 385, loss = 0.08758357\n",
      "Iteration 386, loss = 0.08542172\n",
      "Iteration 387, loss = 0.08442330\n",
      "Iteration 388, loss = 0.08684551\n",
      "Iteration 389, loss = 0.08346349\n",
      "Iteration 390, loss = 0.08265613\n",
      "Iteration 391, loss = 0.08643011\n",
      "Iteration 392, loss = 0.08533356\n",
      "Iteration 393, loss = 0.08338251\n",
      "Iteration 394, loss = 0.08314741\n",
      "Iteration 395, loss = 0.08186616\n",
      "Iteration 396, loss = 0.08289672\n",
      "Iteration 397, loss = 0.08745372\n",
      "Iteration 398, loss = 0.08040801\n",
      "Iteration 399, loss = 0.08055685\n",
      "Iteration 400, loss = 0.08028881\n",
      "Iteration 401, loss = 0.08110967\n",
      "Iteration 402, loss = 0.07988314\n",
      "Iteration 403, loss = 0.07907919\n",
      "Iteration 404, loss = 0.08463705\n",
      "Iteration 405, loss = 0.08665027\n",
      "Iteration 406, loss = 0.07959099\n",
      "Iteration 407, loss = 0.07891937\n",
      "Iteration 408, loss = 0.07828063\n",
      "Iteration 409, loss = 0.08495764\n",
      "Iteration 410, loss = 0.08116488\n",
      "Iteration 411, loss = 0.07708408\n",
      "Iteration 412, loss = 0.07610346\n",
      "Iteration 413, loss = 0.07655471\n",
      "Iteration 414, loss = 0.07833215\n",
      "Iteration 415, loss = 0.08708668\n",
      "Iteration 416, loss = 0.08484683\n",
      "Iteration 417, loss = 0.07950789\n",
      "Iteration 418, loss = 0.08004399\n",
      "Iteration 419, loss = 0.07715392\n",
      "Iteration 420, loss = 0.07536717\n",
      "Iteration 421, loss = 0.08142886\n",
      "Iteration 422, loss = 0.07597804\n",
      "Iteration 423, loss = 0.07945090\n",
      "Iteration 424, loss = 0.07455193\n",
      "Iteration 425, loss = 0.07665545\n",
      "Iteration 426, loss = 0.07608402\n",
      "Iteration 427, loss = 0.07458472\n",
      "Iteration 428, loss = 0.07575735\n",
      "Iteration 429, loss = 0.08576446\n",
      "Iteration 430, loss = 0.08148030\n",
      "Iteration 431, loss = 0.08900215\n",
      "Iteration 432, loss = 0.08301834\n",
      "Iteration 433, loss = 0.08365951\n",
      "Iteration 434, loss = 0.07795711\n",
      "Iteration 435, loss = 0.08050860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 46 of 48...\n",
      "Iteration 1, loss = 0.64455371\n",
      "Iteration 2, loss = 0.53701962\n",
      "Iteration 3, loss = 0.47762863\n",
      "Iteration 4, loss = 0.44013950\n",
      "Iteration 5, loss = 0.41868007\n",
      "Iteration 6, loss = 0.39509140\n",
      "Iteration 7, loss = 0.38019889\n",
      "Iteration 8, loss = 0.36517239\n",
      "Iteration 9, loss = 0.36277473\n",
      "Iteration 10, loss = 0.37136816\n",
      "Iteration 11, loss = 0.34846363\n",
      "Iteration 12, loss = 0.32948083\n",
      "Iteration 13, loss = 0.31713807\n",
      "Iteration 14, loss = 0.30564160\n",
      "Iteration 15, loss = 0.30433659\n",
      "Iteration 16, loss = 0.29536323\n",
      "Iteration 17, loss = 0.28877734\n",
      "Iteration 18, loss = 0.28520318\n",
      "Iteration 19, loss = 0.27569569\n",
      "Iteration 20, loss = 0.27256580\n",
      "Iteration 21, loss = 0.26525159\n",
      "Iteration 22, loss = 0.25713267\n",
      "Iteration 23, loss = 0.25164270\n",
      "Iteration 24, loss = 0.25060721\n",
      "Iteration 25, loss = 0.24920190\n",
      "Iteration 26, loss = 0.25399360\n",
      "Iteration 27, loss = 0.23559590\n",
      "Iteration 28, loss = 0.23039206\n",
      "Iteration 29, loss = 0.22764202\n",
      "Iteration 30, loss = 0.21836698\n",
      "Iteration 31, loss = 0.21245156\n",
      "Iteration 32, loss = 0.20679484\n",
      "Iteration 33, loss = 0.22839932\n",
      "Iteration 34, loss = 0.21818067\n",
      "Iteration 35, loss = 0.20943383\n",
      "Iteration 36, loss = 0.20505393\n",
      "Iteration 37, loss = 0.19290276\n",
      "Iteration 38, loss = 0.19195766\n",
      "Iteration 39, loss = 0.18059713\n",
      "Iteration 40, loss = 0.17869676\n",
      "Iteration 41, loss = 0.17017065\n",
      "Iteration 42, loss = 0.17490101\n",
      "Iteration 43, loss = 0.16996049\n",
      "Iteration 44, loss = 0.17359999\n",
      "Iteration 45, loss = 0.17127541\n",
      "Iteration 46, loss = 0.18270627\n",
      "Iteration 47, loss = 0.16730591\n",
      "Iteration 48, loss = 0.15516795\n",
      "Iteration 49, loss = 0.16135030\n",
      "Iteration 50, loss = 0.14829763\n",
      "Iteration 51, loss = 0.14509386\n",
      "Iteration 52, loss = 0.13506398\n",
      "Iteration 53, loss = 0.13961737\n",
      "Iteration 54, loss = 0.14777237\n",
      "Iteration 55, loss = 0.14047567\n",
      "Iteration 56, loss = 0.12810114\n",
      "Iteration 57, loss = 0.12499474\n",
      "Iteration 58, loss = 0.12550765\n",
      "Iteration 59, loss = 0.11753515\n",
      "Iteration 60, loss = 0.13054302\n",
      "Iteration 61, loss = 0.12950942\n",
      "Iteration 62, loss = 0.12040388\n",
      "Iteration 63, loss = 0.11324860\n",
      "Iteration 64, loss = 0.10709956\n",
      "Iteration 65, loss = 0.10463553\n",
      "Iteration 66, loss = 0.10512286\n",
      "Iteration 67, loss = 0.10129312\n",
      "Iteration 68, loss = 0.10056664\n",
      "Iteration 69, loss = 0.10074540\n",
      "Iteration 70, loss = 0.09815579\n",
      "Iteration 71, loss = 0.09863219\n",
      "Iteration 72, loss = 0.11089815\n",
      "Iteration 73, loss = 0.11190494\n",
      "Iteration 74, loss = 0.10779200\n",
      "Iteration 75, loss = 0.09740058\n",
      "Iteration 76, loss = 0.08817249\n",
      "Iteration 77, loss = 0.08499629\n",
      "Iteration 78, loss = 0.08724909\n",
      "Iteration 79, loss = 0.07849712\n",
      "Iteration 80, loss = 0.07714959\n",
      "Iteration 81, loss = 0.07180722\n",
      "Iteration 82, loss = 0.07225997\n",
      "Iteration 83, loss = 0.07084510\n",
      "Iteration 84, loss = 0.07017938\n",
      "Iteration 85, loss = 0.06823410\n",
      "Iteration 86, loss = 0.07892374\n",
      "Iteration 87, loss = 0.07925768\n",
      "Iteration 88, loss = 0.07606671\n",
      "Iteration 89, loss = 0.07254364\n",
      "Iteration 90, loss = 0.06834236\n",
      "Iteration 91, loss = 0.07100837\n",
      "Iteration 92, loss = 0.05833939\n",
      "Iteration 93, loss = 0.05681254\n",
      "Iteration 94, loss = 0.06165573\n",
      "Iteration 95, loss = 0.05874034\n",
      "Iteration 96, loss = 0.05674848\n",
      "Iteration 97, loss = 0.05388262\n",
      "Iteration 98, loss = 0.05924711\n",
      "Iteration 99, loss = 0.05944470\n",
      "Iteration 100, loss = 0.06408741\n",
      "Iteration 101, loss = 0.06258820\n",
      "Iteration 102, loss = 0.06518872\n",
      "Iteration 103, loss = 0.05494943\n",
      "Iteration 104, loss = 0.05371002\n",
      "Iteration 105, loss = 0.05059041\n",
      "Iteration 106, loss = 0.05374702\n",
      "Iteration 107, loss = 0.05013865\n",
      "Iteration 108, loss = 0.05581872\n",
      "Iteration 109, loss = 0.06409873\n",
      "Iteration 110, loss = 0.06219350\n",
      "Iteration 111, loss = 0.04864943\n",
      "Iteration 112, loss = 0.04536086\n",
      "Iteration 113, loss = 0.04402638\n",
      "Iteration 114, loss = 0.04063946\n",
      "Iteration 115, loss = 0.04321568\n",
      "Iteration 116, loss = 0.04838412\n",
      "Iteration 117, loss = 0.04134749\n",
      "Iteration 118, loss = 0.04231132\n",
      "Iteration 119, loss = 0.04329527\n",
      "Iteration 120, loss = 0.04299849\n",
      "Iteration 121, loss = 0.04611748\n",
      "Iteration 122, loss = 0.03707277\n",
      "Iteration 123, loss = 0.03549896\n",
      "Iteration 124, loss = 0.03761819\n",
      "Iteration 125, loss = 0.04209285\n",
      "Iteration 126, loss = 0.04322299\n",
      "Iteration 127, loss = 0.03208446\n",
      "Iteration 128, loss = 0.03236330\n",
      "Iteration 129, loss = 0.03137098\n",
      "Iteration 130, loss = 0.03080392\n",
      "Iteration 131, loss = 0.04060922\n",
      "Iteration 132, loss = 0.03709963\n",
      "Iteration 133, loss = 0.03821455\n",
      "Iteration 134, loss = 0.04161340\n",
      "Iteration 135, loss = 0.03714363\n",
      "Iteration 136, loss = 0.03698459\n",
      "Iteration 137, loss = 0.02761213\n",
      "Iteration 138, loss = 0.02780893\n",
      "Iteration 139, loss = 0.02846667\n",
      "Iteration 140, loss = 0.02550116\n",
      "Iteration 141, loss = 0.02633411\n",
      "Iteration 142, loss = 0.02740610\n",
      "Iteration 143, loss = 0.02446292\n",
      "Iteration 144, loss = 0.03057765\n",
      "Iteration 145, loss = 0.04261836\n",
      "Iteration 146, loss = 0.06973701\n",
      "Iteration 147, loss = 0.05037891\n",
      "Iteration 148, loss = 0.04511414\n",
      "Iteration 149, loss = 0.03440810\n",
      "Iteration 150, loss = 0.04812471\n",
      "Iteration 151, loss = 0.05247957\n",
      "Iteration 152, loss = 0.06433516\n",
      "Iteration 153, loss = 0.05375630\n",
      "Iteration 154, loss = 0.05716231\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 47 of 48...\n",
      "Iteration 1, loss = 0.64427326\n",
      "Iteration 2, loss = 0.53770107\n",
      "Iteration 3, loss = 0.47130499\n",
      "Iteration 4, loss = 0.42929603\n",
      "Iteration 5, loss = 0.40598919\n",
      "Iteration 6, loss = 0.38886406\n",
      "Iteration 7, loss = 0.38181152\n",
      "Iteration 8, loss = 0.36363798\n",
      "Iteration 9, loss = 0.35157618\n",
      "Iteration 10, loss = 0.35811898\n",
      "Iteration 11, loss = 0.34599298\n",
      "Iteration 12, loss = 0.32677211\n",
      "Iteration 13, loss = 0.31785583\n",
      "Iteration 14, loss = 0.31095698\n",
      "Iteration 15, loss = 0.29668733\n",
      "Iteration 16, loss = 0.28967322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.28631963\n",
      "Iteration 18, loss = 0.28618258\n",
      "Iteration 19, loss = 0.27776540\n",
      "Iteration 20, loss = 0.27702663\n",
      "Iteration 21, loss = 0.26365601\n",
      "Iteration 22, loss = 0.25363766\n",
      "Iteration 23, loss = 0.25111341\n",
      "Iteration 24, loss = 0.24939037\n",
      "Iteration 25, loss = 0.24086034\n",
      "Iteration 26, loss = 0.24568638\n",
      "Iteration 27, loss = 0.24807405\n",
      "Iteration 28, loss = 0.21980309\n",
      "Iteration 29, loss = 0.22102436\n",
      "Iteration 30, loss = 0.22131945\n",
      "Iteration 31, loss = 0.20789474\n",
      "Iteration 32, loss = 0.20677190\n",
      "Iteration 33, loss = 0.22461138\n",
      "Iteration 34, loss = 0.20217023\n",
      "Iteration 35, loss = 0.19430670\n",
      "Iteration 36, loss = 0.21416119\n",
      "Iteration 37, loss = 0.21634611\n",
      "Iteration 38, loss = 0.22104676\n",
      "Iteration 39, loss = 0.20093977\n",
      "Iteration 40, loss = 0.18286523\n",
      "Iteration 41, loss = 0.17675174\n",
      "Iteration 42, loss = 0.18344484\n",
      "Iteration 43, loss = 0.18013508\n",
      "Iteration 44, loss = 0.16895818\n",
      "Iteration 45, loss = 0.15774370\n",
      "Iteration 46, loss = 0.15442168\n",
      "Iteration 47, loss = 0.14975945\n",
      "Iteration 48, loss = 0.15048372\n",
      "Iteration 49, loss = 0.14875356\n",
      "Iteration 50, loss = 0.13983484\n",
      "Iteration 51, loss = 0.14276632\n",
      "Iteration 52, loss = 0.13765180\n",
      "Iteration 53, loss = 0.14604651\n",
      "Iteration 54, loss = 0.13479508\n",
      "Iteration 55, loss = 0.14547526\n",
      "Iteration 56, loss = 0.13858132\n",
      "Iteration 57, loss = 0.12462795\n",
      "Iteration 58, loss = 0.13257632\n",
      "Iteration 59, loss = 0.13688828\n",
      "Iteration 60, loss = 0.11743217\n",
      "Iteration 61, loss = 0.11493853\n",
      "Iteration 62, loss = 0.11725096\n",
      "Iteration 63, loss = 0.11666487\n",
      "Iteration 64, loss = 0.12041804\n",
      "Iteration 65, loss = 0.11403972\n",
      "Iteration 66, loss = 0.11536266\n",
      "Iteration 67, loss = 0.12016217\n",
      "Iteration 68, loss = 0.10528644\n",
      "Iteration 69, loss = 0.09461920\n",
      "Iteration 70, loss = 0.10430823\n",
      "Iteration 71, loss = 0.10026367\n",
      "Iteration 72, loss = 0.11080403\n",
      "Iteration 73, loss = 0.10151643\n",
      "Iteration 74, loss = 0.08983334\n",
      "Iteration 75, loss = 0.09099274\n",
      "Iteration 76, loss = 0.08331140\n",
      "Iteration 77, loss = 0.08087961\n",
      "Iteration 78, loss = 0.07588966\n",
      "Iteration 79, loss = 0.07828100\n",
      "Iteration 80, loss = 0.08690082\n",
      "Iteration 81, loss = 0.08219460\n",
      "Iteration 82, loss = 0.08236344\n",
      "Iteration 83, loss = 0.09780420\n",
      "Iteration 84, loss = 0.09604130\n",
      "Iteration 85, loss = 0.09286716\n",
      "Iteration 86, loss = 0.11234415\n",
      "Iteration 87, loss = 0.09920209\n",
      "Iteration 88, loss = 0.08911205\n",
      "Iteration 89, loss = 0.08839372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training model 48 of 48...\n",
      "Iteration 1, loss = 0.66919336\n",
      "Iteration 2, loss = 0.55721339\n",
      "Iteration 3, loss = 0.49882470\n",
      "Iteration 4, loss = 0.45924975\n",
      "Iteration 5, loss = 0.42479822\n",
      "Iteration 6, loss = 0.40271782\n",
      "Iteration 7, loss = 0.39343343\n",
      "Iteration 8, loss = 0.39131036\n",
      "Iteration 9, loss = 0.36765968\n",
      "Iteration 10, loss = 0.35583775\n",
      "Iteration 11, loss = 0.34546699\n",
      "Iteration 12, loss = 0.33220088\n",
      "Iteration 13, loss = 0.32115467\n",
      "Iteration 14, loss = 0.32034582\n",
      "Iteration 15, loss = 0.31880817\n",
      "Iteration 16, loss = 0.30947477\n",
      "Iteration 17, loss = 0.29886563\n",
      "Iteration 18, loss = 0.29704413\n",
      "Iteration 19, loss = 0.28569296\n",
      "Iteration 20, loss = 0.28692162\n",
      "Iteration 21, loss = 0.27948638\n",
      "Iteration 22, loss = 0.26143418\n",
      "Iteration 23, loss = 0.26756634\n",
      "Iteration 24, loss = 0.27289712\n",
      "Iteration 25, loss = 0.25299018\n",
      "Iteration 26, loss = 0.24967025\n",
      "Iteration 27, loss = 0.24470334\n",
      "Iteration 28, loss = 0.23766233\n",
      "Iteration 29, loss = 0.23047717\n",
      "Iteration 30, loss = 0.22563058\n",
      "Iteration 31, loss = 0.23217479\n",
      "Iteration 32, loss = 0.25536434\n",
      "Iteration 33, loss = 0.22088257\n",
      "Iteration 34, loss = 0.23957023\n",
      "Iteration 35, loss = 0.21256127\n",
      "Iteration 36, loss = 0.21630380\n",
      "Iteration 37, loss = 0.20231132\n",
      "Iteration 38, loss = 0.20234653\n",
      "Iteration 39, loss = 0.20656963\n",
      "Iteration 40, loss = 0.20770179\n",
      "Iteration 41, loss = 0.19795663\n",
      "Iteration 42, loss = 0.18461981\n",
      "Iteration 43, loss = 0.17777013\n",
      "Iteration 44, loss = 0.17932612\n",
      "Iteration 45, loss = 0.20195824\n",
      "Iteration 46, loss = 0.17938371\n",
      "Iteration 47, loss = 0.18501681\n",
      "Iteration 48, loss = 0.16423698\n",
      "Iteration 49, loss = 0.16500324\n",
      "Iteration 50, loss = 0.15611110\n",
      "Iteration 51, loss = 0.15477808\n",
      "Iteration 52, loss = 0.18540136\n",
      "Iteration 53, loss = 0.16721627\n",
      "Iteration 54, loss = 0.16788677\n",
      "Iteration 55, loss = 0.15536489\n",
      "Iteration 56, loss = 0.15731536\n",
      "Iteration 57, loss = 0.15300644\n",
      "Iteration 58, loss = 0.14466759\n",
      "Iteration 59, loss = 0.13775313\n",
      "Iteration 60, loss = 0.14214468\n",
      "Iteration 61, loss = 0.13280007\n",
      "Iteration 62, loss = 0.13131633\n",
      "Iteration 63, loss = 0.14805580\n",
      "Iteration 64, loss = 0.13145587\n",
      "Iteration 65, loss = 0.13037784\n",
      "Iteration 66, loss = 0.12481648\n",
      "Iteration 67, loss = 0.13551827\n",
      "Iteration 68, loss = 0.13074786\n",
      "Iteration 69, loss = 0.11379975\n",
      "Iteration 70, loss = 0.12638535\n",
      "Iteration 71, loss = 0.12293711\n",
      "Iteration 72, loss = 0.11620759\n",
      "Iteration 73, loss = 0.10689038\n",
      "Iteration 74, loss = 0.10622598\n",
      "Iteration 75, loss = 0.10356355\n",
      "Iteration 76, loss = 0.10427295\n",
      "Iteration 77, loss = 0.10120888\n",
      "Iteration 78, loss = 0.10373316\n",
      "Iteration 79, loss = 0.10302317\n",
      "Iteration 80, loss = 0.10073827\n",
      "Iteration 81, loss = 0.10269137\n",
      "Iteration 82, loss = 0.10805066\n",
      "Iteration 83, loss = 0.10566291\n",
      "Iteration 84, loss = 0.09701483\n",
      "Iteration 85, loss = 0.09098380\n",
      "Iteration 86, loss = 0.09078941\n",
      "Iteration 87, loss = 0.10081966\n",
      "Iteration 88, loss = 0.08889949\n",
      "Iteration 89, loss = 0.08737026\n",
      "Iteration 90, loss = 0.08779018\n",
      "Iteration 91, loss = 0.08393131\n",
      "Iteration 92, loss = 0.08359266\n",
      "Iteration 93, loss = 0.11052404\n",
      "Iteration 94, loss = 0.09673988\n",
      "Iteration 95, loss = 0.08398724\n",
      "Iteration 96, loss = 0.08053518\n",
      "Iteration 97, loss = 0.09791464\n",
      "Iteration 98, loss = 0.11305378\n",
      "Iteration 99, loss = 0.12249521\n",
      "Iteration 100, loss = 0.09375409\n",
      "Iteration 101, loss = 0.08310284\n",
      "Iteration 102, loss = 0.07687298\n",
      "Iteration 103, loss = 0.09602919\n",
      "Iteration 104, loss = 0.08930592\n",
      "Iteration 105, loss = 0.08095445\n",
      "Iteration 106, loss = 0.07624315\n",
      "Iteration 107, loss = 0.07573595\n",
      "Iteration 108, loss = 0.07269594\n",
      "Iteration 109, loss = 0.06838969\n",
      "Iteration 110, loss = 0.06629914\n",
      "Iteration 111, loss = 0.06725399\n",
      "Iteration 112, loss = 0.06519704\n",
      "Iteration 113, loss = 0.07026224\n",
      "Iteration 114, loss = 0.05867607\n",
      "Iteration 115, loss = 0.05896531\n",
      "Iteration 116, loss = 0.05861408\n",
      "Iteration 117, loss = 0.06692859\n",
      "Iteration 118, loss = 0.06327879\n",
      "Iteration 119, loss = 0.05555586\n",
      "Iteration 120, loss = 0.05681833\n",
      "Iteration 121, loss = 0.06760985\n",
      "Iteration 122, loss = 0.05571347\n",
      "Iteration 123, loss = 0.05395428\n",
      "Iteration 124, loss = 0.06586305\n",
      "Iteration 125, loss = 0.05792207\n",
      "Iteration 126, loss = 0.05533251\n",
      "Iteration 127, loss = 0.06620225\n",
      "Iteration 128, loss = 0.09574413\n",
      "Iteration 129, loss = 0.08520082\n",
      "Iteration 130, loss = 0.07016886\n",
      "Iteration 131, loss = 0.06094018\n",
      "Iteration 132, loss = 0.06238811\n",
      "Iteration 133, loss = 0.05627766\n",
      "Iteration 134, loss = 0.06210201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "layers = [1, 2]\n",
    "sizes = [8, 16, 32, 64]\n",
    "activations = ['logistic', 'tanh']\n",
    "alphas = [0.0001, 0.001, 0.01]\n",
    "\n",
    "hyperparameters_dic = {\n",
    "    'layers': layers,\n",
    "    'sizes': sizes,\n",
    "    'activations': activations,\n",
    "    'alphas': alphas\n",
    "}\n",
    "\n",
    "results_nn = {}\n",
    "\n",
    "results_nn = evaluate_nn(train_x, train_y, test_x, test_y, hyperparameters_dic, results_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7749667110519307\n",
      "Train accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "train_acc_list = [results_nn[i][1] for i in list(results_nn.keys())]\n",
    "test_acc_list = [results_nn[i][2] for i in list(results_nn.keys())]\n",
    "model_list = [results_nn[i][3] for i in list(results_nn.keys())]\n",
    "\n",
    "max_index = np.argmax(test_acc_list)\n",
    "print('Test accuracy: ' + str(test_acc_list[max_index]))\n",
    "print('Train accuracy: ' + str(train_acc_list[max_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current model progress\n",
    "save_results(results_nn, 'results_nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nn = load_results('results_nn.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [results_nn[i][0] for i in list(results_nn.keys())]\n",
    "train_acc_list = [results_nn[i][1]*100 for i in list(results_nn.keys())]\n",
    "test_acc_list = [results_nn[i][2]*100 for i in list(results_nn.keys())]\n",
    "number_layers_list = [results_nn[i][3] for i in list(results_nn.keys())]\n",
    "layer_size_list = [results_nn[i][4] for i in list(results_nn.keys())]\n",
    "activation_list = [results_nn[i][5] for i in list(results_nn.keys())]\n",
    "\n",
    "alpha_list, train_acc_list, test_acc_list = zip(*sorted(zip(alpha_list, train_acc_list, test_acc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(alpha_list, train_acc_list, label='Training set')\n",
    "plt.scatter(alpha_list, test_acc_list, label='Test set')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('L2 Regularization Penalty')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy (%)')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(number_layers_list, train_acc_list, label='Training set')\n",
    "plt.scatter(number_layers_list, test_acc_list, label='Test set')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Number of Layers')\n",
    "plt.ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy (%)')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(layer_size_list, train_acc_list, label='Training set')\n",
    "plt.scatter(layer_size_list, test_acc_list, label='Test set')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Layer size')\n",
    "plt.ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy (%)')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(activation_list, train_acc_list, label='Training set')\n",
    "plt.scatter(activation_list, test_acc_list, label='Test set')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Activation Type')\n",
    "plt.ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001\n",
      "Number of Layers: 1\n",
      "Layer Size: 8\n",
      "Activation type: logistic\n",
      "Training accuracy: 98.0%\n",
      "Test accuracy: 77.5%\n"
     ]
    }
   ],
   "source": [
    "max_index = np.argmax(test_acc_list)\n",
    "\n",
    "print('Alpha: ' + str(alpha_list[max_index]))\n",
    "print('Number of Layers: ' + str(number_layers_list[max_index]))\n",
    "print('Layer Size: ' + str(layer_size_list[max_index]))\n",
    "print('Activation type: ' + str(activation_list[max_index]))\n",
    "print('Training accuracy: ' + str(round(train_acc_list[max_index], 2)) + '%')\n",
    "print('Test accuracy: ' + str(round(test_acc_list[max_index], 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_knn(train_x, train_y, test_x, test_y, hyperparameters_dic, results_knn):\n",
    "    \n",
    "    neighbors_list = hyperparameters_dic['neighbors_list']\n",
    "    weights_list = hyperparameters_dic['weights_list']\n",
    "    \n",
    "    i = len(list(results_knn.keys()))\n",
    "    \n",
    "    n_models = len(neighbors_list)*len(weights_list)\n",
    "    \n",
    "    for neighbors in neighbors_list:\n",
    "        for weights in weights_list:\n",
    "            \n",
    "            print('Training model {} of {}...'.format(i+1, n_models))\n",
    "            \n",
    "            for r_i in list(results_knn.keys()):\n",
    "                if neighbors == results_knn[r_i][0]:\n",
    "                    if weights == results_knn[r_i][1]:\n",
    "                        continue\n",
    "            \n",
    "            results_knn[i] = []\n",
    "            \n",
    "            model = KNeighborsClassifier(n_neighbors=neighbors, weights=weights)\n",
    "            \n",
    "            model.fit(train_x, train_y)\n",
    "            \n",
    "            train_accuracy = model.score(train_x, train_y)\n",
    "            test_accuracy = model.score(test_x, test_y)\n",
    "            \n",
    "            results_knn[i].append(neighbors)\n",
    "            results_knn[i].append(weights)\n",
    "            results_knn[i].append(train_accuracy)\n",
    "            results_knn[i].append(test_accuracy)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "    return results_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 6...\n",
      "Training model 2 of 6...\n",
      "Training model 3 of 6...\n",
      "Training model 4 of 6...\n",
      "Training model 5 of 6...\n",
      "Training model 6 of 6...\n"
     ]
    }
   ],
   "source": [
    "neighbors_list = [3, 5, 7]\n",
    "weights_list = ['uniform', 'distance']\n",
    "\n",
    "hyperparameter_dic = {'neighbors_list': neighbors_list, 'weights_list': weights_list}\n",
    "\n",
    "results_knn = evaluate_knn(train_x, train_y, test_x, test_y, hyperparameter_dic, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number nerighbors: 3\n",
      "Weighting: uniform\n",
      "Training accuracy: 0.8666666666666667\n",
      "Test accuracy: 0.758988015978695\n",
      "\n",
      "Number nerighbors: 3\n",
      "Weighting: distance\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.7656458055925432\n",
      "\n",
      "Number nerighbors: 5\n",
      "Weighting: uniform\n",
      "Training accuracy: 0.828\n",
      "Test accuracy: 0.7416777629826897\n",
      "\n",
      "Number nerighbors: 5\n",
      "Weighting: distance\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.7576564580559254\n",
      "\n",
      "Number nerighbors: 7\n",
      "Weighting: uniform\n",
      "Training accuracy: 0.803\n",
      "Test accuracy: 0.7443408788282291\n",
      "\n",
      "Number nerighbors: 7\n",
      "Weighting: distance\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.7603195739014648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in results_knn.keys():\n",
    "    print('Number nerighbors: ' + str(results_knn[key][0]))\n",
    "    print('Weighting: ' + str(results_knn[key][1]))\n",
    "    print('Training accuracy: ' + str(results_knn[key][2]))\n",
    "    print('Test accuracy: ' + str(results_knn[key][3]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results_knn, 'results_knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_knn = load_results('results_knn.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_list = [results_knn[i][0] for i in list(results_knn.keys())]\n",
    "weights_list = [results_knn[i][1] for i in list(results_knn.keys())]\n",
    "train_acc_list = [results_knn[i][2]*100 for i in list(results_knn.keys())]\n",
    "test_acc_list = [results_knn[i][3]*100 for i in list(results_knn.keys())]\n",
    "\n",
    "neighboars_list, train_acc_list, test_acc_list, weights_list = zip(*sorted(zip(alpha_list, train_acc_list, test_acc_list, weights_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy (%)')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhEElEQVR4nO3de5xVdb3/8ddbGGEUE0M0BA3xwcEUcFDEC2YaKZU38miJmJgdL2lieCShi3oqk0QfdszUo2bZ0TyQF7Q0ITkqHk09IMTlp0QqKsiRCQWlUMbh8/tjrVmO456ZvWdm7z2X9/PxmMfe67u+a30/+8swn73Wd63vUkRgZmYGsE25AzAzs/bDScHMzDJOCmZmlnFSMDOzjJOCmZllupc7gNbYeeedY+DAgeUOw8ysQ1m4cOHfIqJvrnUdOikMHDiQBQsWlDsMM7MORdIrja3z6SMzM8s4KZiZWcZJwczMMk4KZmaWcVIwM7NM0a4+knQbcCywLiKGpmUfB2YCA4FVwJcj4q103TTg60AtMCki5hQrNrNimr1oDTPmrOD1DZvZrXclU8YOYdyI/uUOyzqJ781eyl3PvEZtBN0kxh+0Oz8aN6zN9l/MI4VfAZ9vUDYVmBcRg4F56TKS9gFOAfZNt7lBUrcixmZWFLMXrWHavUtZs2EzAazZsJlp9y5l9qI15Q7NOoHvzV7KHU+/Sm06u3VtBHc8/Srfm720zdooWlKIiPnAmw2KTwBuT9/fDoyrV/5fEfFeRLwM/BUYVazYzIplxpwVbK6p/VDZ5ppaZsxZUaaIrDO565nXCipviVKPKewaEWsB0tdd0vL+QP1PtTot+whJZ0taIGlBdXV1UYM1K9TrGzYXVG5WiNpGnn/TWHlLtJeBZuUoy/kpI+LmiBgZESP79s15l7ZZ2ezWu7KgcrNCdFOuP5WNl7dEqZPCG5L6AaSv69Ly1cDu9eoNAF4vcWxmrTZl7BAqKz48HFZZ0Y0pY4eUKSLrTMYftHtB5S1R6qTwADAxfT8RuL9e+SmSekjaExgMPFvi2MxabdyI/lx54jD6965EQP/elVx54jBffWRt4kfjhnHawXtkRwbdJE47eI82vfpIxXpGs6S7gCOAnYE3gMuA2cAsYA/gVeDkiHgzrf9d4EzgfeBbEfGH5toYOXJkeEI8M7PCSFoYESNzrSvafQoRMb6RVWMaqX8FcEWx4jEzs+a1l4FmMzNrB5wUzMws46RgZmYZJwUzM8s4KZiZWcZJwczMMk4KZmaWcVIwM7OMk4KZmWWcFMzMLOOkYGZmGScFMzPLOCmYmVnGScHMzDJOCmZmlnFSMDOzjJOCmZllnBTMzCzjpGBmZhknBTMzyzgpmJlZxknBzMwyTgpmZpZxUjAzs4yTgpmZZZwUzMws46RgZmYZJwUzM8s4KZiZWaYsSUHShZKWSVou6Vtp2eWS1khanP58sRyxmZl1Zd1L3aCkocBZwChgC/CwpAfT1ddGxNWljsnMzBIlTwrAp4CnI+IfAJIeB75UhjjMzKyBcpw+WgYcLqmPpO2ALwK7p+u+KWmJpNsk7VSG2MzMurSSJ4WIeB74CfBH4GHgz8D7wI3AXkAVsBa4Jtf2ks6WtEDSgurq6pLEbGbWVZRloDkifhER+0fE4cCbwMqIeCMiaiNiK3ALyZhDrm1vjoiRETGyb9++pQzbzKzTK9fVR7ukr3sAJwJ3SepXr8qXSE4zmZlZCZVjoBngHkl9gBrg/Ih4S9J/SqoCAlgFnFOm2MzMuqyyJIWI+HSOsq+WIxYzM/uA72g2M7OMk4KZmWWcFMzMLOOkYGZmGScFMzPLOCmYmVnGScHMzDJOCmZmlnFSMDOzjJOCmZllnBTMzCzjpGBmZhknBTMzyzgpmJlZxknBzMwyTgpmZpZxUjAzs4yTgpmZZZwUzMws46RgZmYZJwUzM8s4KZiZWcZJwczMMk4KZmaW6Z5PJUk7AbsBm4FVEbG1qFGZmVlZNJoUJO0InA+MB7YFqoGewK6SngZuiIhHSxKlmZmVRFNHCncDvwY+HREb6q+QdADwVUmDIuIXRYzPzMxKqNGkEBFHNbFuIbCwKBGZmVnZ5DWmACCpL3AhUAncGBF/LVpUZmZWFoVcfXQNMB94GLirOOGYmVk5NZoUJD0s6dP1irYFVqU/PVrTqKQLJS2TtFzSt9Kyj0v6o6SV6etOrWnDzMwK19SRwleAEyT9RtJewPeBS4HpwHktbVDSUOAsYBSwH3CspMHAVGBeRAwG5qXLZmZWQk0NNG8ELpY0CLgCWAOcn5a3xqeApyPiHwCSHge+BJwAHJHWuR14DLiklW2ZmVkBmjp9NEjSDOBfgH8F7gdmSbpAUrdWtLkMOFxSH0nbAV8Edgd2jYi1AOnrLo3EdbakBZIWVFdXtyIMMzNrqKnTR3eRDCo/DfxnRDwREWOBt4G5LW0wIp4HfgL8Md3/n4H3C9j+5ogYGREj+/bt29IwzMwsh6aSQk/g5fRnu7rCiLgdOLY1jUbELyJi/4g4HHgTWAm8IakfQPq6rjVtmJlZ4Zq6T+E8YAawBTi3/oqI2NyaRiXtEhHrJO0BnAgcAuwJTCQZyJ5IcrrKzMxKqKmB5ieBJ4vU7j2S+gA1JIPXb0maTjJm8XXgVeDkIrVtZmaNaGpCvN8B/wHMiYiaBusGAWeQzJh6W6GNRsSnc5StB8YUui8zM2s7TZ0+Ogu4CPh3SW/ywSypA4EXgesjwqd4zCwvNTU1rF69mnfffbfcoXQZPXv2ZMCAAVRUVOS9TVOnj/4P+DbwbUkDgX4kz1P4S909BmZm+Vq9ejU77LADAwcORFK5w+n0IoL169ezevVq9txzz7y3y2tCvIhYRTK9hZlZi7z77rtOCCUkiT59+lDo/Vx+HKeZlYwTQmm1pL+dFMysS1i/fj1VVVVUVVXxiU98gv79+2fLW7ZsaXLbBQsWMGnSpGbbOPTQQ9sq3IL8+Mc/brN9KSKariAdCzzUHp/LPHLkyFiwYEG5wzCzPDz//PN86lOfKncYAFx++eX06tWLiy++OCt7//336d4970fMtCu9evVi06ZNOdfl6ndJCyNiZK76+RwpnAKslHSVpPbxL2pmnd7sRWsYPf2/2XPqg4ye/t/MXrSmzds444wzuOiiizjyyCO55JJLePbZZzn00EMZMWIEhx56KCtWrADgscce49hjk4kcLr/8cs4880yOOOIIBg0axHXXXZftr1evXln9I444gpNOOom9996bCRMmUPcF/KGHHmLvvffmsMMOY9KkSdl+61u+fDmjRo2iqqqK4cOHs3LlSgDuuOOOrPycc86htraWqVOnsnnzZqqqqpgwYUKr+6TZtBgRp0n6GDAe+KWkAH4J3BUR77Q6AjOzBmYvWsO0e5eyuaYWgDUbNjPt3qUAjBvRv03b+stf/sIjjzxCt27dePvtt5k/fz7du3fnkUce4Tvf+Q733HPPR7Z54YUXePTRR3nnnXcYMmQI3/jGNz5y2eeiRYtYvnw5u+22G6NHj+bJJ59k5MiRnHPOOcyfP58999yT8ePH54zppptu4sILL2TChAls2bKF2tpann/+eWbOnMmTTz5JRUUF5513HnfeeSfTp0/n+uuvZ/HixW3SH/leffS2pHtIHsX5LZKprqdIui4iftYmkZiZpWbMWZElhDqba2qZMWdFmyeFk08+mW7dkomfN27cyMSJE1m5ciWSqKmpybnNMcccQ48ePejRowe77LILb7zxBgMGDPhQnVGjRmVlVVVVrFq1il69ejFo0KDsEtHx48dz8803f2T/hxxyCFdccQWrV6/mxBNPZPDgwcybN4+FCxdy4IEHArB582Z22SXnZNKt0mxSkHQccCawF/CfwKh03qLtgOcBJwUza1Ovb8g9vVpj5a2x/fbbZ++///3vc+SRR3LfffexatUqjjjiiJzb9OjxwcMnu3Xrxvvvf3Si51x1mhvDrXPqqady0EEH8eCDDzJ27FhuvfVWIoKJEydy5ZVX5vnJWiafMYWTgWsjYnhEzIiIdQDpDWxnFjU6M+uSdutdWVB5W9m4cSP9+ydHIr/61a/afP977703L730EqtWrQJg5syZOeu99NJLDBo0iEmTJnH88cezZMkSxowZw9133826dckE0m+++SavvPIKABUVFY0e1RQqn6RwGfBs3YKkyvQOZyJiXptEYWZWz5SxQ6is+PCzvCorujFl7JCitvvtb3+badOmMXr0aGpra5vfoECVlZXccMMNfP7zn+ewww5j1113Zccdd/xIvZkzZzJ06FCqqqp44YUXOP3009lnn3340Y9+xNFHH83w4cM56qijWLt2LQBnn302w4cPb5OB5nwuSV0AHBoRW9LlbYEnI+LAVrfeSr4k1azjKPSS1NmL1jBjzgpe37CZ3XpXMmXskDYfTyiHTZs20atXLyKC888/n8GDBzN58uSitVfoJan5DDR3r0sIABGxJU0MZmZFM25E/06RBBq65ZZbuP3229myZQsjRozgnHPOKXdIH5JPUqiWdHxEPAAg6QTgb8UNy8ysc5o8eXJRjwxaK5+kcC5wp6TrAQGvAacXNSozMyuLfG5eexE4WFIvkjEI37Bm1oTOei7cuoa8bl6TdAywL9Czbta9iPhBEeMy65BKeSeuWTE0e0mqpJuArwAXkJw+Ohn4ZJHjMuuQmroT16wjyOc+hUMj4nTgrYj4N+AQYPfihmXWMZXyTlwrTGumzoZkkrunnnqq1XFs2LCBG264odX7KZZ8kkLdA1X/IWk3oAbI/9luZl1Iue7Eteb16dOHxYsXs3jxYs4991wmT56cLW+7bfNX2TspfOB3knoDM4DnSB7LeVcRYzLrsMp1J26ntGQWXDsULu+dvC6Z1eZNLFy4kM985jMccMABjB07NrtD+LrrrmOfffZh+PDhnHLKKaxatYqbbrqJa6+9lqqqKp544okP7efxxx/PjjpGjBjBO+8k1+PMmDGDAw88kOHDh3PZZZcBMHXqVF588UWqqqqYMmVKm3+m1mpyoFnSNsC8iNgA3CPp90DPiNhYiuDMOpq6wWRffdRKS2bB7yZBTXrabeNryTLA8C+3SRMRwQUXXMD9999P3759mTlzJt/97ne57bbbmD59Oi+//DI9evRgw4YN9O7dm3PPPfcjD+apc/XVV/Pzn/+c0aNHs2nTJnr27MncuXNZuXIlzz77LBHB8ccfz/z585k+fTrLli1rs6mu21qTSSEitkq6hmQcgYh4D3ivFIGZdVSd9U7ckpr3gw8SQp2azUl5GyWF9957j2XLlnHUUUcBUFtbS79+/QCyeYTGjRvHuHHjmt3X6NGjueiii5gwYQInnngiAwYMYO7cucydO5cRI0YAyfQWK1euZI899miT+Isln0tS50r6Z+DeyHfeVzOz1ti4urDyFogI9t13X/70pz99ZN2DDz7I/PnzeeCBB/jhD3/I8uXLm9zX1KlTOeaYY3jooYc4+OCDeeSRR4gIpk2b9pFpLOpmSG2v8hlTuAj4LfCepLclvSPp7SLHZWZd2Y4DCitvgR49elBdXZ0lhZqaGpYvX87WrVt57bXXOPLII7nqqqvYsGEDmzZtYocddsjGChp68cUXGTZsGJdccgkjR47khRdeYOzYsdx2223Zs5PXrFnDunXrmtxPe9BsUoiIHSJim4jYNiI+li5/rBTBmVkXNeZSqGhwxVZFZVLeRrbZZhvuvvtuLrnkEvbbbz+qqqp46qmnqK2t5bTTTmPYsGGMGDGCyZMn07t3b4477jjuu+++nAPNP/3pTxk6dCj77bcflZWVfOELX+Doo4/m1FNP5ZBDDmHYsGGcdNJJvPPOO/Tp04fRo0czdOjQdjnQnM/U2YfnKo+I+UWJqACeOtus4yh06myWzErGEDauTo4QxlzaZuMJXUkxps6un8p6AqOAhcBnWxqkmVmzhn/ZSaAM8pkQ77j6y5J2B65qTaOSJgP/AgSwFPgaMBU4C6hOq30nIh5qTTtmZlaYvCbEa2A1MLSlDUrqD0wC9omIzZJmAaekq6+NiKtbum8zM2udZpOCpJ+RfKOHZGC6CvhzG7RbKakG2A54HRjYyn2aWTsXEdTNtGzF15K7CPK5JHUByRjCQuBPwCURcVrBLaUiYg1wNfAqsBbYGBFz09XflLRE0m2Sdsq1vaSzJS2QtKC6ujpXFTNrh3r27Mn69etb9IfKChcRrF+/np49exa0XT5XH20PvBsRtelyN6BHRPyjJYGmf+zvIZmOewPJPRB3A38kecxnAD8E+kXEmU3ty1cfmXUcNTU1rF69mnfffbf5ytYmevbsyYABA6ioqPhQeWuvPpoHfA7YlC5XAnOBQ1sY5+eAlyOiOg3uXpLpue+oF/AtwO9buH8za4cqKirYc09PsNze5XP6qGdE1CUE0vfbtaLNV0ke77mdkpOLY4DnJfWrV+dLwLJWtGFmZi2Qz5HC3yXtHxHPAUg6AGjxE0Mi4hlJd5NMw/0+sAi4GbhVUhXJ6aNVwDmN7cPMzIojn6TwLeC3kl5Pl/uRjAe0WERcBlzWoPirrdmnmZm1Xj43r/2vpL2BISTPaH4hImqKHpmZmZVcs2MKks4Hto+IZRGxFOgl6bzih2ZmZqWWz0DzWemT1wCIiLdIpqMwM7NOJp+ksI3q3YKY3qfQ/FOuzcysw8lnoHkOMEvSTSRXBp0LPFzUqMzMrCzySQqXAGcD3yAZaJ4L3FLMoMzMrDzyefLa1oi4KSJOioh/BpYDPyt+aGZmVmp5TZ2d3lQ2nuT+hJeBe4sYk5mZlUmjSUHSP5E852A8sB6YSTKB3pEliq1oZi9aw4w5K3h9w2Z2613JlLFDGDeif7nDMjMru6aOFF4AngCOi4i/QvbEtA5t9qI1TLt3KZtragFYs2Ez0+5dCuDEYGZdXlNjCv8M/B/wqKRbJI0hGWju0GbMWZElhDqba2qZMWdFmSIyM2s/Gk0KEXFfRHwF2Bt4DJgM7CrpRklHlyi+Nvf6htxz+TVWbmbWleRz9dHfI+LOiDgWGAAsBqYWO7Bi2a13ZUHlZmZdST53NGci4s2I+I+I+GyxAiq2KWOHUFnR7UNllRXdmDJ2SJkiMjNrP/K6JLUzqRtM9tVHZmYf1eWSAiSJwUnAzOyjCjp9ZGZmnZuTgpmZZZwUzMws46RgZmYZJwUzM8s4KZiZWcZJwczMMk4KZmaWcVIwM7OMk4KZmWWcFMzMLNMl5z7y4zjNzHLrcknBj+M0M2tcWU4fSZosabmkZZLuktRT0scl/VHSyvR1p2K07cdxmpk1ruRJQVJ/YBIwMiKGAt2AU0ie5jYvIgYD8yjS0938OE4zs8aVa6C5O1ApqTuwHfA6cAJwe7r+dmBcMRr24zjNzBpX8qQQEWuAq4FXgbXAxoiYC+waEWvTOmuBXXJtL+lsSQskLaiuri64fT+O08ysceU4fbQTyVHBnsBuwPaSTst3+4i4OSJGRsTIvn37Ftz+uBH9ufLEYfTvXYmA/r0rufLEYR5kNjOjPFcffQ54OSKqASTdCxwKvCGpX0SsldQPWFesAPw4TjOz3MoxpvAqcLCk7SQJGAM8DzwATEzrTATuL0NsZmZdWsmPFCLiGUl3A88B7wOLgJuBXsAsSV8nSRwnlzo2M7Ouriw3r0XEZcBlDYrfIzlqMDOzMvHcR2ZmlnFSMDOzjJOCmZllnBTMzCzjpGBmZhknBTMzyzgpmJlZxknBzMwyTgpmZpZxUjAzs4yTgpmZZZwUzMws46RgZmYZJwUzM8s4KZiZWcZJwczMMk4KZmaWcVIwM7OMk4KZWUeyZBZcOxQu7528LpnVprsvyzOazcysBZbMgt9NgprNyfLG15JlgOFfbpMmfKRgZtZRzPvBBwmhTs3mpLyNOCmYmXUUG1cXVt4CTgpmZh3FjgMKK28BJwWztlbkgUDrwgYfXVh5C3ig2awtLZkFs8+DrTXJ8sbXkmVos4FA68JWzi2svAV8pGDWlv5wyQcJoc7WmqTcrLU8pmDWwWx+s7Bys0J4TMHMzDJjLoWKyg+XVVQm5W3EScGsLVV+vLBys0IM/zIcdx3suDug5PW469p0vKrkA82ShgAz6xUNAi4FegNnAdVp+Xci4qHSRmfWSl/4Cdx/PtRu+aCs27ZJuVlbGP7lol60UPKkEBErgCoASd2ANcB9wNeAayPi6lLHZNZm6v6zzvtBMvi344Dk0N5XHlkHUe5LUscAL0bEK5LKHIpZGynyNzmzYir3mMIpwF31lr8paYmk2yTtVK6gzMy6qrIlBUnbAscDv02LbgT2Ijm1tBa4ppHtzpa0QNKC6urqXFXMzKyFynmk8AXguYh4AyAi3oiI2ojYCtwCjMq1UUTcHBEjI2Jk3759W9aypyEwM8upnGMK46l36khSv4hYmy5+CVhWlFZLMB+5mVlHVZYjBUnbAUcB99YrvkrSUklLgCOByUVpvATzkZuZdVRlOVKIiH8AfRqUfbUkjW98rbByM7MupNxXH5WeuhVWbmbWhXS9pBC1hZWbmXUhXS8p7Lh7YeVmZl1I10sKJZhl0Myso+p6SaEEswyamXVU5Z77qDw8N42ZWU5d70jBzMwa5aRgZmYZJwUzM8s4KZiZWcZJwczMMoqIcsfQYpKqgVdasYudgb+1UThtyXEVxnEVxnEVpjPG9cmIyPnsgQ6dFFpL0oKIGFnuOBpyXIVxXIVxXIXpanH59JGZmWWcFMzMLNPVk8LN5Q6gEY6rMI6rMI6rMF0qri49pmBmZh/W1Y8UzMysHicFMzPLdOqkIKmnpGcl/VnSckn/lqOOJF0n6a+Slkjav53EdYSkjZIWpz8le+CDpG6SFkn6fY51Je+vPOMqZ3+tkrQ0bXdBjvVl6bM84ipLn0nqLeluSS9Iel7SIQ3Wl6u/mour5P0laUi99hZLelvStxrUadP+6uxTZ78HfDYiNkmqAP5H0h8i4ul6db4ADE5/DgJuTF/LHRfAExFxbJFjyeVC4HngYznWlaO/8okLytdfAEdGRGM3EpWzz5qKC8rTZ/8OPBwRJ0naFtiuwfpy9VdzcUGJ+ysiVgBVkHwpAtYA9zWo1qb91amPFCKxKV2sSH8ajqyfAPw6rfs00FtSv3YQV1lIGgAcA9zaSJWS91eecbVnZemz9kjSx4DDgV8ARMSWiNjQoFrJ+yvPuMptDPBiRDScxaFN+6tTJwXITjksBtYBf4yIZxpU6Q+8Vm95dVpW7rgADklPMf1B0r7Fjin1U+DbwNZG1pelv2g+LihPf0GS0OdKWijp7Bzry9VnzcUFpe+zQUA18Mv0VOCtkrZvUKcc/ZVPXFC+3zGAU4C7cpS3aX91+qQQEbURUQUMAEZJGtqginJt1g7ieo5kfpL9gJ8Bs4sdk6RjgXURsbCpajnKitpfecZV8v6qZ3RE7E9yGH++pMMbrC/L7xjNx1WOPusO7A/cGBEjgL8DUxvUKUd/5RNX2X7H0tNZxwO/zbU6R1mL+6vTJ4U66aHgY8DnG6xaDexeb3kA8Hppomo8roh4u+4UU0Q8BFRI2rnI4YwGjpe0Cvgv4LOS7mhQpxz91WxcZeqvurZfT1/XkZzvHdWgSll+x5qLq0x9thpYXe/I+G6SP8YN65S6v5qNq5y/YySJ/bmIeCPHujbtr06dFCT1ldQ7fV8JfA54oUG1B4DT0xH8g4GNEbG23HFJ+oQkpe9HkfxbrS9mXBExLSIGRMRAkkPV/46I0xpUK3l/5RNXOforbWt7STvUvQeOBpY1qFaO37Fm4yrT79j/Aa9JGpIWjQH+X4Nq5fgdazaucv2OpcaT+9QRtHF/dfarj/oBt6ej9tsAsyLi95LOBYiIm4CHgC8CfwX+AXytncR1EvANSe8Dm4FToky3n7eD/sonrnL1167Afenfiu7AbyLi4XbQZ/nEVa4+uwC4Mz0l8hLwtXbQX/nEVZb+krQdcBRwTr2yovWXp7kwM7NMpz59ZGZmhXFSMDOzjJOCmZllnBTMzCzjpGBmZhknBWsXJIWka+otXyzp8jba968kndQW+2qmnZOVzK75aIPygennu6Be2fWSzmhmf+dKOr2ZOmdIur6RdZtylZs1xUnB2ov3gBNLeIdoXtJ7SfL1deC8iDgyx7p1wIXpNfB5iYibIuLXBbTfZiR19nuYrBFOCtZevE/yzNnJDVc0/KZf9w1Yyfz2j0uaJekvkqZLmqDkWRVLJe1Vbzefk/REWu/YdPtukmZI+l8l89CfU2+/j0r6DbA0Rzzj0/0vk/STtOxS4DDgJkkzcny+amAeMDHH/vaS9LCSieuekLR3Wn65pIvT9wemMf4pjbn+3cm7pduvlHRVg31fI+k5SfMk9U3LqiQ9ne7vPkk7peWPSfqxpMdJEtjJ6Wf8s6T5OT6TdUJOCtae/ByYIGnHArbZj+Q5C8OArwL/FBGjSKbYvqBevYHAZ0im375JUk+Sb/YbI+JA4EDgLEl7pvVHAd+NiH3qNyZpN+AnwGdJ5rk/UNK4iPgBsACYEBFTGol1OvCvOY4+bgYuiIgDgIuBG3Js+0vg3Ig4BKhtsK4K+EraB1+RVDcPzvYk8+XsDzwOXJaW/xq4JCKGkyS9y+rtq3dEfCYirgEuBcamE8Ad38hnsk7GScHajYh4m+QP1qQCNvvfiFgbEe8BLwJz0/KlJImgzqyI2BoRK0mmMNibZD6g05VMYf4M0IfkQSUAz0bEyznaOxB4LCKqI+J94E6Sefjz+XwvA88Cp9aVSeoFHAr8No3jP0imQaFend7ADhHxVFr0mwa7nhcRGyPiXZL5ej6Zlm8FZqbv7wAOSxNu74h4PC2/vUH8M+u9fxL4laSzgEJOo1kH5vOG1t78lGSK4l/WK3uf9AuMksl86p+Xf6/e+631lrfy4d/vhvO5BMmUwxdExJz6KyQdQTJ1ci65pikuxI9JZuCsOx2zDbAhnUa9Mc21Wb8Pamn8/3U+c9pknzsizpV0EMnR1WJJVRFRqgngrEx8pGDtSkS8CcwiObVTZxVwQPr+BJIn1RXqZEnbpOMMg4AVwBySCc4qACT9k3I/WKW+Z4DPSNo5PQ00nuTUTF4i4gWSb/PHpstvAy9LOjmNQZL2a7DNW8A7SmbAhGSm2HxsQzKJGyRHJ/8TERuBtyR9Oi3/amPxS9orIp6JiEuBv/Hh6Zmtk/KRgrVH1wDfrLd8C3C/pGdJBmsb+xbflBUkf/x2JTk3/66kW0lOMT2XHoFUA+Oa2klErJU0DXiU5Bv8QxFxf4GxXAEsqrc8AbhR0vdIEt5/AX9usM3XgVsk/Z3k+Rsb82jn78C+kham9b+Slk8kGVfZjnQ20Ea2nyFpMMnnnJcjJuuEPEuqWQcgqVfdA14kTQX6RcSFZQ7LOiEfKZh1DMekRyjdgVeAM8objnVWPlIwM7OMB5rNzCzjpGBmZhknBTMzyzgpmJlZxknBzMwy/x8klRB/ZPGHhAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(neighbors_list, train_acc_list, label='Training set')\n",
    "plt.scatter(neighbors_list, test_acc_list, label='Test set')\n",
    "plt.legend(loc='center right')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy (%)')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(weights_list, train_acc_list, label='Training set')\n",
    "plt.scatter(weights_list, test_acc_list, label='Test set')\n",
    "plt.legend(loc='center right')\n",
    "plt.xlabel('Weighting Method')\n",
    "plt.ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Neighbors: 7\n",
      "Weighting Methond: distance\n",
      "Training accuracy: 100.0%\n",
      "Test accuracy: 76.56%\n"
     ]
    }
   ],
   "source": [
    "max_index = np.argmax(test_acc_list)\n",
    "\n",
    "print('Number of Neighbors: ' + str(neighbors_list[max_index]))\n",
    "print('Weighting Methond: ' + str(weights_list[max_index]))\n",
    "print('Training accuracy: ' + str(round(train_acc_list[max_index], 2)) + '%')\n",
    "print('Test accuracy: ' + str(round(test_acc_list[max_index], 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gbc(train_x, train_y, test_x, test_y, hyperparameters_dic, results_gbc):\n",
    "    \n",
    "    alphas = hyperparameters_dic['alphas']\n",
    "    \n",
    "    i = len(list(results_gbc.keys()))\n",
    "    \n",
    "    n_models = len(alphas)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "            \n",
    "        print('Training model {} of {}...'.format(i+1, n_models))\n",
    "\n",
    "        if alpha in [results_gbc[i][0] for i in list(results_gbc.keys())]:\n",
    "            continue\n",
    "            \n",
    "        results_gbc[i] = []\n",
    "\n",
    "        model = GradientBoostingClassifier(ccp_alpha = alpha)\n",
    "\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        train_accuracy = model.score(train_x, train_y)\n",
    "        test_accuracy = model.score(test_x, test_y)\n",
    "\n",
    "        results_gbc[i].append(alpha)\n",
    "        results_gbc[i].append(train_accuracy)\n",
    "        results_gbc[i].append(test_accuracy)\n",
    "\n",
    "        i += 1\n",
    "            \n",
    "    return results_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 4...\n",
      "Training model 2 of 4...\n",
      "Training model 3 of 4...\n",
      "Training model 4 of 4...\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01, 0.001, 0.0001, 0]\n",
    "\n",
    "hyperparameters_dic = {'alphas': alphas}\n",
    "\n",
    "results_gbc = evaluate_gbc(train_x, train_y, test_x, test_y, hyperparameters_dic, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphas: 0.01\n",
      "Training accuracy: 0.7386666666666667\n",
      "Test accuracy: 0.729693741677763\n",
      "\n",
      "Alphas: 0.001\n",
      "Training accuracy: 0.793\n",
      "Test accuracy: 0.7576564580559254\n",
      "\n",
      "Alphas: 0.0001\n",
      "Training accuracy: 0.8593333333333333\n",
      "Test accuracy: 0.7842876165113183\n",
      "\n",
      "Alphas: 0\n",
      "Training accuracy: 0.861\n",
      "Test accuracy: 0.7869507323568575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in results_gbc.keys():\n",
    "    print('Alphas: ' + str(results_gbc[key][0]))\n",
    "    print('Training accuracy: ' + str(results_gbc[key][1]))\n",
    "    print('Test accuracy: ' + str(results_gbc[key][2]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results_gbc, 'results_gbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gbc = load_results('results_gbc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [results_gbc[i][0] for i in list(results_gbc.keys())]\n",
    "train_acc_list = [results_gbc[i][1]*100 for i in list(results_gbc.keys())]\n",
    "test_acc_list = [results_gbc[i][2]*100 for i in list(results_gbc.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy (%)')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_list = [results_gbc[i][0] for i in list(results_gbc.keys())]\n",
    "train_acc_list = [results_gbc[i][1]*100 for i in list(results_gbc.keys())]\n",
    "test_acc_list = [results_gbc[i][2]*100 for i in list(results_gbc.keys())]\n",
    "\n",
    "alpha_list, train_acc_list, test_acc_list = zip(*sorted(zip(alpha_list, train_acc_list, test_acc_list)))\n",
    "\n",
    "plt.scatter(alpha_list, train_acc_list, label='Training set')\n",
    "plt.scatter(alpha_list, test_acc_list, label='Test set')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Cost Complexity Pruning Threshold')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "#plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0\n",
      "Training accuracy: 86.1%\n",
      "Test accuracy: 78.7%\n"
     ]
    }
   ],
   "source": [
    "max_index = np.argmax(test_acc_list)\n",
    "\n",
    "print('Alpha: ' + str(alpha_list[max_index]))\n",
    "print('Training accuracy: ' + str(round(train_acc_list[max_index], 2)) + '%')\n",
    "print('Test accuracy: ' + str(round(test_acc_list[max_index], 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_svm(train_x, train_y, test_x, test_y, hyperparameters_dic, results_svm):\n",
    "    \n",
    "    alphas = hyperparameters_dic['alphas']\n",
    "    \n",
    "    i = len(list(results_svm.keys()))\n",
    "    \n",
    "    n_models = len(alphas)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "            \n",
    "        print('Training model {} of {}...'.format(i+1, n_models))\n",
    "\n",
    "        if alpha in [results_svm[i][0] for i in list(results_svm.keys())]:\n",
    "            continue\n",
    "            \n",
    "        results_svm[i] = []\n",
    "\n",
    "        model = SVC(C = alpha)\n",
    "\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        train_accuracy = model.score(train_x, train_y)\n",
    "        test_accuracy = model.score(test_x, test_y)\n",
    "\n",
    "        results_svm[i].append(alpha)\n",
    "        results_svm[i].append(train_accuracy)\n",
    "        results_svm[i].append(test_accuracy)\n",
    "\n",
    "        i += 1\n",
    "            \n",
    "    return results_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 5...\n",
      "Training model 2 of 5...\n",
      "Training model 3 of 5...\n",
      "Training model 4 of 5...\n",
      "Training model 5 of 5...\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "hyperparameters_dic = {'alphas': alphas}\n",
    "\n",
    "results_svm = evaluate_svm(train_x, train_y, test_x, test_y, hyperparameters_dic, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphas: 0.01\n",
      "Training accuracy: 0.5423333333333333\n",
      "Test accuracy: 0.5419440745672437\n",
      "\n",
      "Alphas: 0.1\n",
      "Training accuracy: 0.716\n",
      "Test accuracy: 0.6844207723035952\n",
      "\n",
      "Alphas: 1\n",
      "Training accuracy: 0.8486666666666667\n",
      "Test accuracy: 0.7869507323568575\n",
      "\n",
      "Alphas: 10\n",
      "Training accuracy: 0.9453333333333334\n",
      "Test accuracy: 0.7882822902796272\n",
      "\n",
      "Alphas: 100\n",
      "Training accuracy: 0.9896666666666667\n",
      "Test accuracy: 0.7683089214380826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in results_svm.keys():\n",
    "    print('Alphas: ' + str(results_svm[key][0]))\n",
    "    print('Training accuracy: ' + str(results_svm[key][1]))\n",
    "    print('Test accuracy: ' + str(results_svm[key][2]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results_svm, 'results_svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm = load_results('results_svm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_list = [results_svm[i][0] for i in list(results_svm.keys())]\n",
    "train_acc_list = [results_svm[i][1]*100 for i in list(results_svm.keys())]\n",
    "test_acc_list = [results_svm[i][2]*100 for i in list(results_svm.keys())]\n",
    "\n",
    "alpha_list, train_acc_list, test_acc_list = zip(*sorted(zip(alpha_list, train_acc_list, test_acc_list)))\n",
    "\n",
    "plt.scatter(alpha_list, train_acc_list, label='Training set')\n",
    "plt.scatter(alpha_list, test_acc_list, label='Test set')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Inverse Squared L2 Regularization')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 10\n",
      "Training accuracy: 94.53%\n",
      "Test accuracy: 78.83%\n"
     ]
    }
   ],
   "source": [
    "max_index = np.argmax(test_acc_list)\n",
    "\n",
    "print('Alpha: ' + str(alpha_list[max_index]))\n",
    "print('Training accuracy: ' + str(round(train_acc_list[max_index], 2)) + '%')\n",
    "print('Test accuracy: ' + str(round(test_acc_list[max_index], 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
